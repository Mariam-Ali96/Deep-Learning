{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the true model and generate some noisy samples\n",
    "# Set random seed\n",
    "np.random.seed(3)\n",
    "\n",
    "### Define a simple quadratic model\n",
    "# y = a + b * x + c * x^2\n",
    "# a = -1.45, b = 1.12, c = 2.3\n",
    "\n",
    "beta_true = [-1.45, 1.12, 2.3]\n",
    "def poly_model(x, beta):\n",
    "    \"\"\"\n",
    "    INPUT\n",
    "        x: x vector\n",
    "        beta: polynomial parameters\n",
    "        noise: enable noisy sampling\n",
    "    \"\"\"\n",
    "    pol_order = len(beta)\n",
    "    x_matrix = np.array([x**i for i in range(pol_order)]).transpose()\n",
    "    y_true = np.matmul(x_matrix, beta)\n",
    "    return y_true\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import genfromtxt\n",
    "my_data = genfromtxt('training_set.txt', delimiter=',')\n",
    "test_data = genfromtxt('test_set.txt', delimiter=',')\n",
    "x_train = my_data.T[0]\n",
    "y_train = my_data.T[1]\n",
    "x_test = test_data.T[0]\n",
    "y_test= test_data.T[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.close('all')\n",
    "plt.figure(figsize=(12,8))\n",
    "x_highres = np.linspace(-4,2,1000)\n",
    "plt.plot(x_highres, poly_model(x_highres, beta_true), color='b', ls='--', label='True data model')\n",
    "plt.plot(x_train, y_train, color='r', ls='', marker='.', label='Train data points')\n",
    "plt.plot(x_test, y_test, color='g', ls='', marker='.', label='Test data points')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3XlclVX+wPHPYd9RZHEBxX03FRSzUiwzM7NsMq2stMxpr3HGcrJfOTbaMjU1jU1malqZSzYulY2liVtq7isuqKgoKiA7XODC+f3xIOHKFYHn3sv3/XrdF3c597nfI/jlcJ7zfI/SWiOEEMK5uJgdgBBCiKonyV0IIZyQJHchhHBCktyFEMIJSXIXQggnJMldCCGckCR3IYRwQpLchRDCCUlyF0IIJ+Rm1gcHBwfryMhIsz6+0nJzc/H19TU7jBpV2/pc2/oL0mdHsnXr1lStdUhF7UxL7pGRkWzZssWsj6+0uLg4YmNjzQ6jRtW2Pte2/oL02ZEopY7Z0k6mZYQQwglJchdCCCckyV0IIZxQhXPuSqmZwEDgrNa6w2VeV8C/gAFAHjBCa72tMsEUFRWRlJSExWKpzNtrRGBgIPHx8WaHUaMcsc9eXl6Eh4fj7u5udihCmMKWE6qzgCnAF1d4/U6gZektBvik9Os1S0pKwt/fn8jISIzfGfYnOzsbf39/s8OoUY7WZ601aWlpJCUl0bRpU7PDEcIUFU7LaK3XAOeu0uQe4Att2AjUUUo1qEwwFouFevXq2W1iF45BKUW9evXs+i9AIapbVcy5NwJOlHucVPpcpUhiF1VBfo5EbVcV69wv97/osnv3KaVGA6MBwsLCiIuLu+D1wMBAsrOzqyCk6lNcXGz3MVY1R+2zxWK55GfMFjk5OZV6nyOTPlcNa4kmtwjyijT5Vo2lGCzlvhaUfr0h1JVmga5V+tkXq4rkngRElHscDpy6XEOt9TRgGkB0dLS++AKC+Ph40+d2J02axNdff42rqysuLi58+umnxMTEMGrUKMaMGUNERES1xThgwAC+/vpr6tSpc8HzEyZMwM/Pj7/85S8XPJ+SksLAgQMpLCzko48+4pZbbqmSOGbNmkW/fv1o2LAhAI899hivvPIK7dq1q5Lj1xQvLy+6dOlyze9z1Itbrof0+UIlJZosSxGpOYWk5RSQlmt8Tc0pJD2vkMz8IrLyi8iyWMvdL8JSVGLTZ3fr2JrYHk2qsDeXqorkvhR4Tik1D+NEaqbWOrkKjlvjNmzYwPfff8+2bdvw9PQkNTWVwsJCAKZPnw5QrSPYZcuWXVP7lStX0qZNG2bPnl2lccyaNYsOHTqUJfcpU6aY/ktXiKpiKSrmVEY++9KKSd2aRHJGPslZFpIz8jmdVUBaTgHncguxllw6AaEUBHi5E+ht3AK83Qj19zOe83EnwMuNAG93Arzc8fdyw9fTDT9PN3w8XI2vnm74uLvi4lL904a2LIWcC8QCwUqpJOANwB1Aaz0VWIaxDDIBYynkyOoKtrolJycTHByMp6cnAMHBwWWvxcbG8t5779G6dWtmzJjBO++8Q8OGDWnZsiWenp5MmTKFESNG4O3tzf79+zl27Biff/45s2fPZsOGDcTExDBr1iwA5s6dy+TJk9Fac9ddd/HOO+8Av5dkCA4OZtKkSXzxxRdEREQQEhJCVFTUBbHu2LGDl19+mfz8fDp37syGDRsICQkhJycHgIULF/L9998za9YsRowYQUBAAFu2bOH06dO8++673H///QC8++67fPnll7i4uHDnnXcSHR3Nli1bePjhh/H29mbDhg0MGDCADz74gOjo6CvG7ufnx4svvsj333+Pt7c3S5YsISwsrFq/X0JcSXpuIYlpuRxLy+Noai7H0nI5mpbH8bRc0vOKfm+4eScAQb4eNAj0omGgFzeEB1LPz4N6vp7U8/Mg2M+z7HFdH3fcXB3j8qAKk7vW+sEKXtfAs1UWUam/fbeXfaeyqvSY7RoG8Mbd7a/4er9+/Zg4cSKtWrWib9++DB06lN69e1/QJjk5mTfffJNt27bh7+/Prbfeyg033FD2enp6Or/88gtLly7l7rvvZv369UyfPp1u3bqxY8cOQkNDeeWVV9i6dSt169alX79+LF68mHvvvbfsGFu3bmXevHls374dq9VK165dL0nunTt3ZuLEiWzZsoUpU6ZU2Pfk5GTWrVvH/v37GTRoEPfffz8//vgjixcvZtOmTfj4+HDu3DmCgoKYMmUK7733HtHR0Rcc49SpU1eMPTc3lx49ejBp0iRefvllPvvsM1577bUK4xLiemTmF3HwTDb7T2dz4HQWB05nc/BMDpn5vydwpaBhoDeRwT7c2bEBjep4Uz/Ai7OJB7izdw/qB3rh5V69899mMK1wmD3y8/Nj69atrF27llWrVjF06FDefvttRowYUdZm69at9O7dm6CgIACGDBnCwYMHy16/++67UUrRsWNHwsLC6NixIwDt27cnMTGRY8eOERsbS0iIUdTt4YcfZs2aNRck97Vr1zJ48GB8fHwAGDRo0HX37d5778XFxYV27dpx5swZAFasWMHIkSPLPud8n65k8+bNV4zdw8ODgQMHAhAVFcXPP/983TELUV5mXhE7kzLYeSKDnUkZ7D2VRXLm78td/b3caFPfn7s6NaBZsC+R9XyJDPYhIsgHT7dLk3dcdgKRwY5XFdJWdpvcrzbCrk6urq7ExsYSGxtLx44dmT179gXJ3fhD5crOT+m4uLiU3T//2Gq14uZm2z95ZZbylX/PxWu8y8dyvg9a62v6nKv13d3dvexYrq6uWK1Wm48rxMW01hxOyWXjkTS2JJ5jZ1ImR1Nzy15vEepHTNMgWtcPoE19f1rX96dBoJcsgS3HMSaPasiBAwc4dOhQ2eMdO3bQpMmFZ7SjoqJYvXo16enpWK1Wvv3222v6jJiYGFavXk1qairFxcXMnTv3kqmfXr16sWjRIvLz88nOzua7776z6dhhYWHEx8dTUlLCokWLKmzfr18/Zs6cSV5eHgDnzhnXqvn7+1/2xLEtsQtRWUdTc5mz6RjPz91O98kr6fvP1by2eA+/Hk6jVZgfL/dvzdejYtg1oR8rxvTmw2FdeDq2OX3ahNKwjrck9ovY7cjdDDk5OTz//PNkZGTg5uZGixYtmDZt2gVtGjZsyKuvvkpMTAwNGzakXbt2BAYG2vwZDRo04K233qJPnz5orRkwYAD33HPPBW26du3K0KFD6dy5M02aNLF5iePbb7/NwIEDiYiIoEOHDmUnV6+kf//+7Nixg+joaDw8PBgwYACTJ09mxIgRPPXUU2UnVK8ldiFsVWAt5rej5/hl/1lW7T9LYpoxyAgL8OSm5vXo0cy4NannI4m7ElRF0wzVJTo6Wl+8WUd8fDxt27Y1JR5bZWdno5TCz88Pq9XK4MGDefzxxxk8eLDZoVUbR6stc15lf55kzXf1yS2wsiL+DD/uPs3aQynkFhbj6eZCz+b1uLVNKLe0DKmxZO6o32el1FatdXRF7WTkXgkTJkxgxYoVWCwW+vXrd8HJUCHEhSxFxazaf5bvdyWzcv8ZLEUlhAV4cm+XRtzaJpSezYPx9nC+1Spmk+ReCe+9957ZIQhh17TW7D6ZybzNJ1i64xQ5BVaC/TwYEhXBwE4N6BYZVCMX8tRmktyFEFUmy1LE4u0nmfvbCeKTs/Byd2FAhwbc1zWcHs2CHOYCIGcgyV0Icd2Op+Uxc/1RvtlygtzCYto3DODNezsw6IaGBHrLhilmkOQuhKi0rcfS+WzNEZbvO42rUtx9Q0NG3hRJp/A6Fb9ZVCtJ7kKIa7Y58Rz/WnGIdQmp1PFx55nY5jx6YyRhAV5mhyZKyQTYRSZNmkT79u3p1KkTnTt3ZtOmTQCMGjWKffv2VetnDxgwgIyMjEuenzBhwmVP4q5Zs4auXbvi5ubGwoULKzz+5MmTbYpj1qxZPPfccza1PW/Lli288MIL1/Se8+Li4vj111/LHk+dOpUvvrjSro7CTFsSzzF8+iaGTN3A/tNZvHZXW34ddytj72gjid3OyMi9HEcr+du4cWNmzZpl8+qdyZMn8+qrr1YmtKuyWq1ER0dfUmjMVnFxcfj5+dGzZ08AnnrqqaoMT1SBIyk5vPXjfn7ed4ZgP09eu6stD8c0kSWMdkxG7uVcruTv+ZrmsbGxnL/oasaMGbRq1YrY2FiefPLJslHuiBEjePrpp+nTpw/NmjVj9erVPP7447Rt2/aC+jRz586lY8eOdOjQgVdeeaXs+cjISFJTUwHjL4jWrVvTt29fDhw4cNl4IyMj6dSpEy4uF34bk5OT6dWrF507d6ZDhw6sXbuWcePGlZUHfvjhhy851ueff06rVq3o3bs369evL3s+JSWF4cOH061bN7p161b22oQJExg9ejT9+vXj0UcfJS4ujoEDB1JSUkJkZOQFf4G0aNGCM2fO8N133xETE0OXLl3o27cvZ86cITExkalTp/LBBx/QuXNn1q5dW/aXSnx8PN27dy87TmJiIp06dQJ+L+AWFRXFHXfcQXKyQ24hYPfScwuZsHQv/T5Yw4bDaYy9ozVrX+7DqFuaSWK3c/Y7cv9xHJzeXbXHrN8R7nz7ii87Usnfq/n666+54447GD9+PMXFxeTl5XHLLbcwZcoUduzYcUn75ORk3njjDbZu3UpgYCB9+vQp28HoxRdf5Nlnn6Vfv34cP36cO+64g/j4+LI4161bh7e3d9l2ZS4uLtxzzz0sWrSIkSNHsmnTJiIjIwkLC+Pmm29m48aNKKWYPn067777Lu+//z5PPfXUBTtNrVy5EoC2bdtSWFjIkSNHaNasGfPnz+eBBx6gqKiI559/niVLlhASEsL8+fMZP348M2fOtPnfSFxdcYlmzqZjvLf8ADkFVh7s3pg/3d6KYD/Pit8s7IL9JncTOEvJ327duvH4449TVFTEvffeS+fOna/aftOmTRfENHTo0LI+rVixgj179pT9dZCVlVU2NTVo0CC8vb0vOd7QoUOZOHEiI0eOZN68eQwdOhSApKQkhg4dSnJyMoWFhTRt2rTCvjzwwAMsWLCAcePGMX/+fObPn8+BAwfYs2cPt99+O2Ds8dqgQQMb/3VERfaeyuTV/+5mZ1Imt7QM5v8GtqNVmOOVn6jt7De5X2WEXZ0cueTveb169WLNmjX88MMPPPLII4wdO5ZHH320Up9XUlLCihUrCA0NveQ1X9/L18K+8cYbSUhIICUlhcWLF5dt2vH8888zZswYBg0aRFxcHBMmTKiwL0OHDmXIkCHcd999KKVo2bIlu3fvpn379hcUNRPXL6/Qygc/H2Tm+kTq+rjzr2GdGXRDQyna5aBkzr0cRy/5e96xY8cIDQ3lySef5IknnmDbtm2AUXO9qKjokvYxMTHExcWRlpZGUVER33zzTdlr/fr1u6Ay5uWmdS6mlGLw4MGMGTOGtm3bUq9ePQAyMzNp1KgRwAX7vl6pxDBA8+bNcXV15c033yz7C6B169akpKSUJfeioiL27t1bYVziyhIyirnro3V8tvYoD0RHsHJMLPd0biSJ3YHZ78jdBI5W8nfz5s0MHjyY9PR0vvvuO9544w327t1LXFwc//jHP3B3d8fPz69sWeHo0aPp1KkTXbt2Zc6cORfENGHCBG688UYaNGhA165dKS4uBuCjjz7ij3/8I506dcJqtdKrVy+mTp1aYT+HDh1Kt27dyvaNBeMk7JAhQ2jUqBE9evTg6NGjgDGVdf/997NkyRL+/e9/X/ZYY8eOLWvv4eHBwoULeeGFF8jMzMRqtfLSSy/Rvr05G7w4skJrCf/+5RBTNlpoWMebuU/24Mbm9cwOS1QBKfl7jaTkr+OQkr9XdyQlhxfmbWfPySxubuTGf568lQCv2lMqwFG/z1LytxpJyV/h6L7fdYpXFu7Cw82FqcOj8ErdX6sSe20gyb0SpOSvcFQF1mIm/xDP7A3HiGpSlykPdaFBoDdxcfvNDk1UMbtL7te6abMQl2PWdKM9O5WRz9NfbWVnUiZP3tKUl/u3wV1K8Dotu0ruXl5epKWlUa9ePUnwotK01qSlpeHlJbVOztt+PJ0nv9iKpaiYqcOj6N+hvtkhiWpmV8k9PDycpKQkUlJSzA7liiwWS61LGo7YZy8vL8LDw80Owy4s2XGSsQt3UT/Ai7lPxtBSLkiqFewqubu7u9t01aKZ4uLiyi7Nry1qY5+dQUmJ5oMVB/n3Lwl0bxrE1OFRBPl6mB2WqCF2ldyFEFWjqLiEVxbu4r/bT/JAdDh/v7cjHm4yv16bSHIXwsnkFxbzzJytrDqQwpjbW/H8rS3kHFYtJMldCCeSkVfIE7O3sP14OpMGd+DhmCYVv0k4JUnuQjiJs1kWhs/YRGJqHh8/1JU7O0qlzNpMkrsQTuBMloUHp23kdJaFWSO70bNFsNkhCZPZdIZFKdVfKXVAKZWglBp3mdcbK6VWKaW2K6V2KaUGVH2oQojLOZ1pYdi0jZzJsjD78e6S2AVgQ3JXSrkCHwN3Au2AB5VS7S5q9hqwQGvdBRgG/KeqAxVCXCo5M59h0zaQkl3AF090p1tkkNkhCTthy8i9O5CgtT6itS4E5gH3XNRGAwGl9wOBU1UXohDics5PxaTmFDL78e5ENZHELn5ny5x7I+BEucdJQMxFbSYAPymlngd8gb5VEp0Q4rIy8gp5dMZvpGQX8OWoGLo2rmt2SMLOVFjPXSk1BLhDaz2q9PEjQHet9fPl2owpPdb7SqkbgRlAB611yUXHGg2MBggLC4uaN29elXamJuTk5ODn52d2GDWqtvXZ3vtrsWr+sdnCsawSxkR70a6e63Uf0977XB0ctc99+vSpsnruSUBEucfhXDrt8gTQH0BrvUEp5QUEA2fLN9JaTwOmgbFZhyMWynfUAv/Xo7b12Z77W2AtZtTsLRzNyuOT4VHc0b5qCoDZc5+ri7P32ZY5981AS6VUU6WUB8YJ06UXtTkO3AaglGoLeAH2W/1LCAdUXKJ5ad4O1h5K5Z0/dKqyxC6cU4XJXWttBZ4DlgPxGKti9iqlJiqlBpU2+zPwpFJqJzAXGKGloLYQVerN7/fx457TvHZXW4ZER1T8BlGr2XQRk9Z6GbDsoudeL3d/H3BT1YYmhDjv8/VHmfVrIk/c3JRRtzQzOxzhAKRMnBB27ud9Z5j4/T7uaB/GqwPsewN5YT8kuQthx3YnZfLC3O10ahTIh0O74Ooi1R2FbSS5C2GnkjPzeXz2ZoJ8PZj+WDe8Pa5/yaOoPSS5C2GHLEXFPPXlVvIKrHw+shsh/p5mhyQcjFSFFMLOaK0Zv2gPO5My+fSRKFrJnqeiEmTkLoSdmf1rIt9uS+LF21rKWnZRaZLchbAjGw6n8eYP8fRtG8aLt7U0OxzhwCS5C2EnTmbk8+zX24is58MHQ2/ARVbGiOsgyV0IO1BoLeHZOdsotJbw2aPR+Hu5mx2ScHByQlUIO/DO//az40QG/3m4K81CHK9SobA/MnIXwmTL955mxrqjPHZjEwbIptaiikhyF8JEJ87lMfabnXQKD+TVu6S0gKg6ktyFMEmhtYTnvt6GBqY82BVPN7kCVVQdmXMXwiRv/RjPzqRMpg6PonE9H7PDEU5GRu5CmGDV/rN8vj6RET0j6d9BLlQSVU+SuxA1LDWngLELd9Kmvj/j7mxjdjjCScm0jBA1SGvNKwt3kWWxMmdUD7zcZZ5dVA8ZuQtRg+ZsOs7K/WcZ178NretLQTBRfSS5C1FDEs7m8Pcf9nFLy2BG9Iw0Oxzh5CS5C1EDCq0lvDR/O97urrw3ROrGiOonc+5C1IB/rTzInpNZTB0eRViAl9nhiFpARu5CVLPtx9P5JO4wD0SHy7JHUWMkuQtRjSxFxfzlm52EBXjx2sB2ZocjahGZlhGiGn2w4iCHU3KZ/Xh3AqSMr6hBMnIXoppsO57OZ2uOMKxbBL1bhZgdjqhlJLkLUQ0sRcWM/WYn9QO8GC/VHoUJZFpGiGpwfjrmi8e7y65KwhQycheiip2fjnmwewS9ZDpGmESSuxBVqPx0zKsDZDpGmEemZYSoQh+vSpDpGGEXZOQuRBVJOJvD1NWHGdylkUzHCNPZlNyVUv2VUgeUUglKqXFXaPOAUmqfUmqvUurrqg1TCPumtea1xbvxdneV6RhhFyqcllFKuQIfA7cDScBmpdRSrfW+cm1aAn8FbtJapyulQqsrYCHs0aLtJ9l45ByTB3ckxN/T7HCEsGnk3h1I0Fof0VoXAvOAey5q8yTwsdY6HUBrfbZqwxTCfmXkFTLph3i6NK7DsG4RZocjBGBbcm8EnCj3OKn0ufJaAa2UUuuVUhuVUv2rKkAh7N07/9tPRn4Rk+7tKKV8hd2wZbXM5X5a9WWO0xKIBcKBtUqpDlrrjAsOpNRoYDRAWFgYcXFx1xqv6XJychwy7utR2/p8Lf09lF7M3N8s9I904+zBbZw9WL2xVZfa9j0G5++zLck9CSj/t2Y4cOoybTZqrYuAo0qpAxjJfnP5RlrracA0gOjoaB0bG1vJsM0TFxeHI8Z9PWpbn23tb1FxCW//ex0NA+H9kb3x9XTclcW17XsMzt9nW6ZlNgMtlVJNlVIewDBg6UVtFgN9AJRSwRjTNEeqMlAh7M3MdUfZfzqbCYPaO3RiF86pwuSutbYCzwHLgXhggdZ6r1JqolJqUGmz5UCaUmofsAoYq7VOq66ghTBbUnoeH644RN+2YfRrLxtwCPtj03BDa70MWHbRc6+Xu6+BMaU3IZzehKXGSuAJg2QDDmGf5ApVIa7RT3tPsyL+DH+6vSXhdX3MDkeIy5LkLsQ1yC2wMmHpXtrU92fkTU3NDkeIK5LkLsQ1+HDFQU5lWpg0uAPurvLfR9gv+ekUwkb7TmUxc30iD3aPIKpJkNnhCHFVktyFsEFJiWb84t3U8Xbnlf5tzA5HiApJchfCBvM2n2D78QzG39WWOj4eZocjRIUkuQtRgZTsAt7+MZ4ezYIY3OXiskpC2CdJ7kJUYPKyePKLivn7vR1RSgqDCccgyV2Iq/g1IZVF20/ydO/mtAj1MzscIWwmyV2IKyiwFvPa4j00qefDM31amB2OENdEqh0JcQVT445wJDWX2Y93x8vd1exwhLgmMnIX4jKOpubycVwCAzs1oLdsdi0ckCR3IS6iteb1JXvwdHXh9YFSGEw4JknuQlxk0+li1h5KZWz/1oQGeJkdjhCVIsldiHIy84v4Or6QTuGBPBzTxOxwhKg0Se5ClPPe8gNkF2omD+6Iq2x2LRyYJHchSu04kcFXm47Rt4kbHRoFmh2OENdFlkIKAViLSxi/aDeh/p7c11KWPQrHJyN3IYAvNhxj76ks3ri7Pd5uMh0jHJ8kd1HrJWfm8/5PB4htHcKdHWSza+EcJLmLWm/id/uwlmjevKeDFAYTTkOSu6jVftl/hh/3nOaF21oSESSbXQvnIcld1Fr5hcW8vmQvLUL9ePKWZmaHI0SVktUyotb66JdDJKXnM390DzzcZJwjnIv8RIta6eCZbD5bc4QhUeHENKtndjhCVDlJ7qLW0Vrz2qI9+Hm58dcBbc0OR4hqIcld1DrfbEnit8RzvHpnW4J8ZbNr4ZwkuYtaJTWngEnL4uneNIgh0eFmhyNEtZHkLmqVST/Ek1doZfJgWdMunJskd1FrrDtUfrNrf7PDEaJaSXIXtYKlqJjXFu8mUja7FrWETcldKdVfKXVAKZWglBp3lXb3K6W0Uiq66kIU4vp9vCqBxLQ8Jg3uKJtdi1qhwuSulHIFPgbuBNoBDyqlLtlYUinlD7wAbKrqIIW4HofOZDN19WHu69KIm1oEmx2OEDXClpF7dyBBa31Ea10IzAPuuUy7N4F3AUsVxifEdSkp0by6aDe+nm6Mv0vWtIvaw5bk3gg4Ue5xUulzZZRSXYAIrfX3VRibENdtwZYTbE5M59UBbann52l2OELUGFtqy1xuvZgue1EpF+ADYESFB1JqNDAaICwsjLi4OJuCtCc5OTkOGff1cNQ+ZxZoJq7No3VdF0KyE4iLO2zT+xy1v9dD+ux8bEnuSUBEucfhwKlyj/2BDkBc6brh+sBSpdQgrfWW8gfSWk8DpgFER0fr2NjYykdukri4OBwx7uvhqH1+cd52inQ+H4+8hRahfja/z1H7ez2kz87HlmmZzUBLpVRTpZQHMAxYev5FrXWm1jpYax2ptY4ENgKXJHYhatIv+8+wZMcpno5tcU2JXQhnUWFy11pbgeeA5UA8sEBrvVcpNVEpNai6AxTiWmVZinj1v3toFebHc7KmXdRSNtVz11ovA5Zd9NzrV2gbe/1hCVF5by3bz9lsC1MfuUnqtItaS37yhVP59XAqc387zqhbmtE5oo7Z4QhhGknuwmnkFVoZ961RYuBPfVuZHY4QppJt9oTTeP+ngxw/l8f80T3w9pASA6J2k5G7cArbjqczc/1RhvdoLNvmCYEkd+EECqzFvLxwFw0CvHilfxuzwxHCLsi0jHB4H644RMLZHGaN7Ia/l7vZ4QhhF2TkLhzalsRzfLr6MA92jyC2dajZ4QhhNyS5C4eVW2Dlz9/spFFdb8bfdUkVaiFqNZmWEQ5r8rJ4jp/LY96TPfDzlB9lIcqTkbtwSHEHzjJn03GevKWZrI4R4jIkuQuHk5FXyCvf7qJVmB9jbpeLlYS4HPlbVjic15fsJS2nkBmPdZP9UIW4Ahm5C4eyZMdJlu48xQu3taRDo0CzwxHCbklyFw7jWFou4xftoVtkXZ6JbW52OELYNUnuwiEUWkt4Ye52XBR8OKwLbq7yoyvE1cicu3AI7/98gJ1JmXzycFca1fE2Oxwh7J4Mf4TdW3MwhU9XH+GhmMbc2bGB2eEI4RAkuQu7lppTwJgFO2kZ6sf/yVWoQthMpmWE3Sou0YxZsJNsSxFfjeouNdqFuAYychd266OVh1hzMIXX725Hm/oBZocjhEOR5C7s0qoDZ/nol0P8oWs4D3VvbHY4QjgcSe7C7pw4l8dL83bQOsyfv9/bAaWU2SEJ4XAkuQu7Yikq5pk52yjRmqnDo2SeXYhKkhOqwq5MWLqX3SczmfZIFJHBvmaHI4TDkuSKE1kgAAAVbklEQVQu7MaXGxKZt/kET8c2p1/7+tX7YVpDzlk4dxjOHYW8NLBkQEEOLU6eBMtycPMEn2DwDYbACAhpbdwXwgFIchd24deEVCZ8t49b24Tyl36tq/4DCvPg2Ho4sQmSNsPJbVCQdWEb5QqefoRZrZDqCkV5UGK9sI13ENTvCE16QuMeEN4dPHyqPl4hrpMkd2G6xNRcnp6zjWbBvvxrWGdcXaroBGpOCuxbDAeXQ+JasFpAuUBYe+h4P4S0gaDmENQUfEPA0x+UYn1cHLGxscbo3pIJuamQngipByFlv/GLIe5tQIObFzTrA20GQOsBMrIXdkOSuzBVlqWIJ2ZvxkXBjMe64e/lfn0HLLLAwR9h5zw49DPoYghqBlEjoeXtEBEDnn62HUsp8K5j3IJbQMu+v79myYQTvxmfcWCZ8Zkuf4JW/aHzw8ZnuV5nX4S4DpLchWmsxSU8//V2jqXl8eUTMTSudx3TG9mnYfMM2DIT8lLBvyH0fA46DYOwaihb4BVoJPCWt8Od78DpXbBrAeyaD/u/B99Q6DYKuj0ho3lhCknuwhRaa15bvIfVB1N4676O3Ni8kvugnt0P6z6APd8a8+Ot7oDuTxpTJS41tIxSKWhwg3HrO8EYzW+ZCXGTYd0/4YZhcOPzxuhfiBoiyV2Y4qOVCczbfILn+rTgwcpcgZpyAFa/ayR1dx+Ifhxi/gj1TN7Ew9XdmH9vM8D4xbPxY9gxF7Z9AZ2GQq+x5scoagVJ7qLGLdh8gg9WHOQPXcP5c79r3OA67TDEvQW7FxpJ/aYXoecL4FvJkX91Cm0Dg/4Nt/4frP8XbJ5uTN10fhB6vQx1m5gdoXBiNl2hqpTqr5Q6oJRKUEqNu8zrY5RS+5RSu5RSK5VS8lMrLmvV/rP8ddFuerUK4e0/dLS9tEB+OvzvVfg4Bvb/AD2fh5d2we1/s8/EXp5fKNwxCV7caUwZ7foGpnSDn98AS1bF7xeiEipM7kopV+Bj4E6gHfCgUuriM1TbgWitdSdgIfBuVQcqHN/mxHM8M2cbbRv485+Hu+Juy1Z5xUWwcSp81AU2/scY9b6wHfq96XgnKv3rGydfX9gGHe6D9R8a/doyE4qtFb9fiGtgy8i9O5CgtT6itS4E5gH3lG+gtV6ltc4rfbgRCK/aMIWj23kig5Gfb6ZBHS8+H9EdP08bZgQTVsB/esD/XoH6neCptcY0h381X71a3QLDYfBUeHIVBLeC7/8En94CR9eYHZlwIkprffUGSt0P9Ndajyp9/AgQo7V+7grtpwCntdZ/v8xro4HRAGFhYVHz5s27zvBrXk5ODn5+Nq6TdhLX2+cT2SW8/Vs+Pm6Kv8Z4EeR19TGFR8E5WiTMIDRlHXneDUlo8QTngqKMVSk1oEa/x1oTnLqR5oc/x9tyhjOhvTncfCSFnnVr5vNLyc+14+jTp89WrXV0hQ211le9AUOA6eUePwL8+wpth2OM3D0rOm5UVJR2RKtWrTI7hBp3PX0+dCZbd534k+4xeYU+npZ79cbFVq03TdN6crjWE0O0XvW21kWWSn92ZZnyPS7M03rl37WeGGz0f+Onxr9HDZGfa8cBbNEV5FettU3TMklARLnH4cCpixsppfoC44FBWusCG44rnFzC2Wwe+mwjSsFXo2KICLrKRUrJu2DG7bDsL9CwCzyzAWJfMYp31Qbu3nDreHhmI4RHw49jYVosJG0xOzLhoGxJ7puBlkqppkopD2AYsLR8A6VUF+BTjMR+turDFI5m36kshn66kRINc0b1oHnIFf78LciB5eONRJZxHO77DB5dUnvXgtdrDsP/C0NmGTVtpveF714yVgsJcQ0qPKultbYqpZ4DlgOuwEyt9V6l1ESMPw+WAv8A/IBvSpe2HddaD6rGuIUd25WUwSMzfsPb3ZU5T8ZcObHv/wGWvQxZSRA1wri607tm55rtklLQfjC06GsUKNv4iVHSoN8k6PRAjZ17EI7NpouYtNbLgGUXPfd6uft9L3mTqJW2JJ5j5OebCfRx5+tRPS5fLyYzyUjqB36A0HZw/3KjfK64kKe/sT6+01BjRc2i0bDjK7jrnxDc0uzohJ2TbfZElfl53xmGz9hEsL8nC/5446WJvdgKv06BKd3h8C/GSP2PaySxV6RBJ3jiJyOpn9oJn/SEVZONCphCXIGUHxBV4quNx3h9yR46NApkxmPdCPG/6ERo0lb4/kU4vRta9oMB/4C6kabE6pBcXI0Kk20Gwk/jYfU7sPsbuOt9aH6r2dEJOyQjd3FdtNa8+7/9vLZ4D7GtQ5k3useFiT0/A374M0y/zThBOGQ2PLRAEntl+YfBH6bDI4sBBV8OhoVPQPYZsyMTdkZG7qLSLEXFjPt2F4t3nOLB7o158572uJ0vKaC1UbFx+auQmwLdR8Otr4FXgLlBO4vmfeDpX41yx+v+aZQZ7vu6sSlJTZU6FnZNkruolFMZ+Tz11VZ2JWUy9o7WPBPb/PciYGmHjdH6kVXGmvWH5htfRdVy94I+f4WOQ+CHPxn/5ju+hoEfGLXlRa0myV1cs9+OnuOZOVuxFJUw/dFo+rYLM16wFsC6D2Ht+8bFRwPeM+qsy0iyegW3gEeXGnPwy181rhmIedpI/J7+ZkcnTCLJXdhMa82XG48x8bt9NA7yYd7oaFqElq5hP7IafhgDaQnQ/j64YzIENDA34NpEKWMNfMvbYcXfjE1C9i4yqlC2vVvWxtdCckJV2CQzr4inv9rG60v20qtVCIuevclI7OnHYP4j8MUgY5u74d/CkM8lsZvFuy7c/SE88TP4BMGCR2DuMOP7JGoVGbmLCh1KL2b8R2s5k2Xh1QFtGHVzM1ysefDLu/DrR4CCPuONDTTcvc0OVwBEdIfRq2HTJ7DqLWOTk9hXoMez4OZhdnSiBkhyF1dUVFzCJ3GH+fA3C+F1fVj4dE86hwcaq2B+fh2yThon8/pOMGqUC/vi6mb8wm13L/z4CqyYAFtnG7tXtR0kUzVOTpK7uKz45CzGLtzJnpNZ9GjgyrQ/3kxAynaY+X9wYqOxGuP+mXJ1qSOoEwEPfm0sl/zpNVjwKDS+0ahVEx5ldnSimkhyFxc4P1r/9y+HCPR255OHu1Lv+AoCFo8wilf5hsLdH0GX4bIKxtG0vB2a9YHtX8KqSTD9VujwB7jtDbMjE9VAkrso89vRc7y+ZA/7T2cz6IaG/K1PXepumoTePgc8fI2LkHo8Y9wXjsnVDaJHQsf7Yf2/jFo/8d/Rov7tENXG8bcwFGUkuQvOZlt4e9l+/rv9JI3qeDNrSGNiU+fB9OmgS0gKH0jEgx+Cbz2zQxVVxdPf+GUdNRJWv0PD7V/Bv1ZCt1Fw00vgF2J2hOI6SXKvxSxFxXy54RgfrTxEgbWEcT0DGOWyFLcfv4DiIqPUbOw4Du88SoQkducU2AgGfcRv7j3pYVkNG/8DWz6HmNFw4/PyC92BSXKvhYpLNIu2n+SfPx3gVKaF+5qV8EbQTwTunAe6BG4YBjePKbcb0lFT4xXVz+JdH+78BG7+E6x+27jSeNOn0OUR6Pkc1GlsdojiGklyr0W01qyMP8s/lh/gwJls/hB2mr/WX0nw8f/BaRfjJOnNf4K6TcwOVZglpJWxCqrXWFj/EWyZAZunGydeb3oR6ncwO0JhI0nutUBxiebHPcl8vOowB5IzeDhwN3Mb/UxQ2jawBMKNz0DMU7JWXfwutC0M/sTYtHvDf2DrLNi9wKgd3320UZNfVkvZNUnuTqzAWsySHaeYGneY7NQk/hiwgQfrrsI3/yR4N4H+70CXh6W4lLiywHDoPxl6jzVG8JtnGuUMAhsbq266Pgq+wWZHKS5DkrsTOpWRz5xNx1iwKZF2lm383XcNPbx/w6WwGJr2gm5vGTv6yMhL2Mq7rjFVc9NLcGAZ/PYZrPybsYF3+3uh80MQ2QtcpFyVvZDk7iRKSjQbjqTx1cZjHI3fwiC1jv95baSex1m0ezCq+3PQ9bFyJ0mFqARXd2h3j3E7u98Yze9aALvmQ2CEscKq80Pyc2YHJLk7uMMpOfx3WxLrt+4kJncVL7n/Smv3Y2jlimoSC12Go9oMlGJRouqFtoG73oN+b8L+H2DnXGNXqLXvQXh3aD8Y2g2SczkmkeTugJLS81i+5zQ7tm8i/Ewc/Vy3MNYlAdyhpGEU3PAsqv1g8As1O1RRG7h7G1e8drwfspKNUfzub2D5X41bo+jS0f4g2Tu3BklydxBHU3NZvus4iTtXE5m2jttdtvCESzK4Q1FYZ2j/GnT4Ay5BzcwOVdRmAQ3g5peMW9ph2LfEuP38f8YtrAO06GvUuYmIMaZ5RLWQ5G6ncgusbEhIZd/uzXBkFe3ztzHcJR4/ZaHY3Y2C8Jug01+g9QDcAxqaHa4Ql6rXHG4ZY9zSE2HfUjj0E2yYAus/BA9/aNbbSPZNe0FQMylDXIUkuduJvEIrOxJTObpvMwVHfyU4fSfd1T76qnQAsvwbQ4uh0PZ2XJv2wscr0OSIhbgGdSPhpheMmyULjq42ShAnrDCqjQL41YfIm6BJT2hyM4S0lmR/HSS5m0BrzeksC/sPHeLsgU3ok9sIz9lFZ5VAT2UBINszmIKGN1LUoR/uLW4lQK4aFc7CK8DY17Xt3aA1pB6CY+sgcT0cW29sBgPgHQSNukLDrtAoyrgv55FsJsm9mpWUaJLScjiasJeMo1tRp3dRL2s/rfRR+qhMow0upPi1IKPRH3Bpcws+zXriX6cx/jJqEc5OKaPkQUgriH7cSPbnjhhJ/sRvcGq7sfpGlxjtAyOMjWLC2kNoO+MW1MwoZSwuIP8iVaSouITjp9M4fXQPuSf3UnL2IL7ZhwktOE4kyTRWRQBYceWsVySZQb2xhHcmrE0MHo1uIEyuEhXCSPb1mhu3ro8azxXmQvIuOLkVTm2D5J3GhVTnE76rp/HLIbQ9BLc0kv35m1eAeX0xmSR3G5WUaFIzMzl7+gS//vQNBSlH0OnH8cpNIsByirCSMzRXmZy/dKMYF1Lc6pNVpymHg27DP7wtoS2j8WzQnobuXqb2RQiH4uELTW40bucV5UPKATgbD2f3Gbeja2DXvAvf6xvye6KvGwkBjYwyxwHhuBRbarQbNa3WJ3dLQSEZ6WfJOZdC9rlk8tNOYs1KRmWfxi3vLN4FKQRY0wgqOUeoyuWBcu+14kqqSwiZ3g1J9mvDmbqR+DVqQ2izTviEtaS+uxeyr40Q1cDdGxp2Nm7lFebCuaPG1E7529E1xkVW5fQC2BwIAeFGwvdvYMzp+4YY9XJ8Q4xtJX1DjPILDlZawabkrpTqD/wLcAWma63fvuh1T+ALIApIA4ZqrROrNtTLKyoqIjcnk/zsdPJzMinIzaQwN5Oi/CyK87MotmSj8zNR+edwK8jAoygD76JMfIuz8NfZBKrcyybgIlw5p4LIcg8mxy+SdO/uqIAGpOZp2nWLJTi8NV5Bjajv6iYJXAh74eFrlCW+XGliayFkn4LMk5B1kiPb19As2BOyTkFmkjG/n5f2+3RPecoVfOoZN69A8K5jfPWqU+5xufueAeDpZyz39PA1fhnV8Dm0CpO7UsoV+Bi4HUgCNiullmqt95Vr9gSQrrVuoZQaBrwDDK2OgH/79kPq7/kUH52Ht87HVxVQB6hTwfty8SJbBZDrGoDFPZAU3wiSPeuATxDKJwg3v2B86oQQENKYOmEReAWEEKYUYRcdJy4ujvAusdXRNSFEdXLzMKZmSq+SPX4ulGaxsRe2KSmB/HOQm/L7Lef8/bOQnw75GcYvhLP7wJJpLO1EX/2zlQt4+BmJ3sMPYscZV/RWI1tG7t2BBK31EQCl1DzgHqB8cr8HmFB6fyEwRSmltNYV9PjaeQaGkurXGqu7H9rDD+3hj/L0x9XLH1fvANy8A/DwCcDTrw5efoH4+NbBN7Auvu6eyLbOQoircnEpnZIJBtra9p6SEijIAkuGkezzS78W5kJhTuktFwpyfn/sE1St3QBQFeVfpdT9QH+t9ajSx48AMVrr58q12VPaJqn08eHSNqkXHWs0MBogLCwsat68i05+OICcnBz8/PzMDqNG1bY+17b+gvTZkfTp02er1jq6ona2jNwvN1F08W8EW9qgtZ4GTAOIjo7WsRf/SeQA4uLicMS4r0dt63Nt6y9In52RLad/k4CIco/DgVNXaqOUcgMCgXNVEaAQQohrZ0ty3wy0VEo1VUp5AMOApRe1WQo8Vnr/fuCX6phvF0IIYZsKp2W01lal1HPAcoylkDO11nuVUhOBLVrrpcAM4EulVALGiH1YdQYthBDi6mxa5661XgYsu+i518vdtwBDqjY0IYQQleVYl1wJIYSwiSR3IYRwQpLchRDCCVV4EVO1fbBSKcAxUz78+gQDqRW2ci61rc+1rb8gfXYkTbTWIRU1Mi25Oyql1BZbrg5zJrWtz7WtvyB9dkYyLSOEEE5IkrsQQjghSe7XbprZAZigtvW5tvUXpM9OR+bchRDCCcnIXQghnJAk9+uglPqLUkorpYLNjqU6KaX+oZTar5TapZRapJSqaOMrh6WU6q+UOqCUSlBKjTM7nuqmlIpQSq1SSsUrpfYqpV40O6aaopRyVUptV0p9b3Ys1UGSeyUppSIwth48bnYsNeBnoIPWuhNwEPiryfFUi3JbSt4JtAMeVEq1MzeqamcF/qy1bgv0AJ6tBX0+70Ug3uwgqosk98r7AHiZCjdPdHxa65+01tbShxsxavo7o7ItJbXWhcD5LSWdltY6WWu9rfR+Nkaya2RuVNVPKRUO3AVMNzuW6iLJvRKUUoOAk1rrnWbHYoLHgR/NDqKaNAJOlHucRC1IdOcppSKBLsAmcyOpER9iDM5KzA6kuthU8rc2UkqtAOpf5qXxwKtAv5qNqHpdrb9a6yWlbcZj/Bk/pyZjq0E2bRfpjJRSfsC3wEta6yyz46lOSqmBwFmt9ValVKzZ8VQXSe5XoLXue7nnlVIdgabATqUUGFMU25RS3bXWp2swxCp1pf6ep5R6DBgI3ObEu2zZsqWk01FKuWMk9jla6/+aHU8NuAkYpJQaAHgBAUqpr7TWw02Oq0rJOvfrpJRKBKK11o5YgMgmSqn+wD+B3lrrFLPjqS6l+/8eBG4DTmJsMfmQ1nqvqYFVI2WMUGYD57TWL5kdT00rHbn/RWs90OxYqprMuQtbTAH8gZ+VUjuUUlPNDqg6lJ40Pr+lZDywwJkTe6mbgEeAW0u/tztKR7TCwcnIXQghnJCM3IUQwglJchdCCCckyV0IIZyQJHchhHBCktyFEMIJSXIXQggnJMldCCGckCR3IYRwQv8PTvCfEse0sOMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Activation function\n",
    "\n",
    "# Define activation function\n",
    "from scipy.special import expit\n",
    "act = expit\n",
    "# 1st derivative\n",
    "act_der = lambda x: act(x) * (1 - act(x))\n",
    "\n",
    "# Plot activation function\n",
    "x_plot = np.linspace(-5,5,1000)\n",
    "y_act = act(x_plot)\n",
    "y_act_der = act_der(x_plot)\n",
    "\n",
    "plt.close('all')\n",
    "plt.plot(x_plot, y_act, label='Sigmoid function')\n",
    "plt.plot(x_plot, y_act_der, label='Sigmoid 1st derivative')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network():\n",
    "    \n",
    "    def __init__(self, Ni, Nh1, Nh2, No):\n",
    "            \n",
    "        ### WEIGHT INITIALIZATION (Xavier)\n",
    "        # Initialize hidden weights and biases (layer 1)\n",
    "        Wh1 = (np.random.rand(Nh1, Ni) - 0.5) * np.sqrt(12 / (Nh1 + Ni))\n",
    "        Bh1 = np.zeros([Nh1, 1])\n",
    "        self.WBh1 = np.concatenate([Wh1, Bh1], 1) # Weight matrix including biases\n",
    "        # Initialize hidden weights and biases (layer 2)\n",
    "        Wh2 = (np.random.rand(Nh2, Nh1) - 0.5) * np.sqrt(12 / (Nh2 + Nh1))\n",
    "        Bh2 = np.zeros([Nh2, 1])\n",
    "        self.WBh2 = np.concatenate([Wh2, Bh2], 1) # Weight matrix including biases\n",
    "        # Initialize output weights and biases\n",
    "        Wo = (np.random.rand(No, Nh2) - 0.5) * np.sqrt(12 / (No + Nh2))\n",
    "        Bo = np.zeros([No, 1])\n",
    "        self.WBo = np.concatenate([Wo, Bo], 1) # Weight matrix including biases\n",
    "        \n",
    "        ### ACTIVATION FUNCTION\n",
    "        self.act = expit\n",
    "        self.act_der = lambda x: act(x) * (1 - act(x))\n",
    "        \n",
    "    def forward(self, x, additional_out=False):\n",
    "        \n",
    "        # Convert to numpy array\n",
    "        x = np.array(x)\n",
    "        \n",
    "        ### Hidden layer 1\n",
    "        # Add bias term\n",
    "        X = np.append(x, 1)\n",
    "        # Forward pass (linear)\n",
    "        H1 = np.matmul(self.WBh1, X)\n",
    "        # Activation function\n",
    "        Z1 = self.act(H1)\n",
    "        \n",
    "        ### Hidden layer 2\n",
    "        # Add bias term\n",
    "        Z1 = np.append(Z1, 1)\n",
    "        # Forward pass (linear)\n",
    "        H2 = np.matmul(self.WBh2, Z1)\n",
    "        # Activation function\n",
    "        Z2 = self.act(H2)\n",
    "        \n",
    "        ### Output layer\n",
    "        # Add bias term\n",
    "        Z2 = np.append(Z2, 1)\n",
    "        # Forward pass (linear)\n",
    "        Y = np.matmul(self.WBo, Z2)\n",
    "        # NO activation function\n",
    "        \n",
    "        if additional_out:\n",
    "            return Y.squeeze(), Z2\n",
    "        \n",
    "        return Y.squeeze()\n",
    "        \n",
    "    def update(self, x, label, lr):\n",
    "        #training part >> here it's not an sgd but convuluted derivatives\n",
    "        # Convert to numpy array\n",
    "        X = np.array(x)\n",
    "        \n",
    "        ### Hidden layer 1\n",
    "        # Add bias term\n",
    "        X = np.append(X, 1)\n",
    "        # Forward pass (linear)\n",
    "        H1 = np.matmul(self.WBh1, X)\n",
    "        # Activation function\n",
    "        Z1 = self.act(H1)\n",
    "        \n",
    "        ### Hidden layer 2\n",
    "        # Add bias term\n",
    "        Z1 = np.append(Z1, 1)\n",
    "        # Forward pass (linear)\n",
    "        H2 = np.matmul(self.WBh2, Z1)\n",
    "        # Activation function\n",
    "        Z2 = self.act(H2)\n",
    "        \n",
    "        ### Output layer\n",
    "        # Add bias term\n",
    "        Z2 = np.append(Z2, 1)\n",
    "        # Forward pass (linear)\n",
    "        Y = np.matmul(self.WBo, Z2)\n",
    "        # NO activation function\n",
    "        \n",
    "        # Evaluate the derivative terms\n",
    "        D1 = Y - label\n",
    "        D2 = Z2\n",
    "        D3 = self.WBo[:,:-1]\n",
    "        D4 = self.act_der(H2)\n",
    "        D5 = Z1\n",
    "        D6 = self.WBh2[:,:-1]\n",
    "        D7 = self.act_der(H1)\n",
    "        D8 = X\n",
    "        \n",
    "        # Layer Error\n",
    "        Eo = D1\n",
    "        Eh2 = np.matmul(Eo, D3) * D4\n",
    "        Eh1 = np.matmul(Eh2, D6) * D7\n",
    "        \n",
    "        \n",
    "        # Derivative for weight matrices\n",
    "        dWBo = np.matmul(Eo.reshape(-1,1), D2.reshape(1,-1))\n",
    "        dWBh2 = np.matmul(Eh2.reshape(-1,1), D5.reshape(1,-1))\n",
    "        dWBh1 = np.matmul(Eh1.reshape(-1,1), D8.reshape(1,-1))\n",
    "        \n",
    "        # Update the weights\n",
    "        self.WBh1 -= lr * dWBh1\n",
    "        self.WBh2 -= lr * dWBh2\n",
    "        self.WBo -= lr * dWBo\n",
    "        \n",
    "        # Evaluate loss function\n",
    "        loss = (Y - label)**2/2\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def plot_weights(self):\n",
    "    \n",
    "        fig, axs = plt.subplots(3, 1, figsize=(12,6))\n",
    "        axs[0].hist(self.WBh1.flatten(), 20)\n",
    "        axs[1].hist(self.WBh2.flatten(), 50)\n",
    "        axs[2].hist(self.WBo.flatten(), 20)\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parametters and initial network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " I believe that the most important task in this code is to decied the number of hisdden neurons in each layer, based on some research I will try duplicates of number of points in the input layer until convergence to a close solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ni= 1\n",
    "Nh1_vec = [len(x_train),2*len(x_train), 3*len(x_train),4*len(x_train) ]\n",
    "\n",
    "Nh2_vec= [4*len(x_train), 3*len(x_train), 2*len(x_train), len(x_train)]\n",
    "No = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of hidden neurons in first layer is: 120 number of hidden neurons in second layer is: 480\n",
      "Epoch 1 - lr: 0.05000 - Train loss: 30.54247 - Test loss: 72.82657\n",
      "Epoch 2 - lr: 0.05000 - Train loss: 18.62437 - Test loss: 54.66924\n",
      "Epoch 3 - lr: 0.05000 - Train loss: 18.63015 - Test loss: 54.16140\n",
      "Epoch 4 - lr: 0.05000 - Train loss: 18.57228 - Test loss: 53.91261\n",
      "Epoch 5 - lr: 0.05000 - Train loss: 18.53516 - Test loss: 53.76548\n",
      "Epoch 6 - lr: 0.05000 - Train loss: 18.51004 - Test loss: 53.66929\n",
      "Epoch 7 - lr: 0.05000 - Train loss: 18.49222 - Test loss: 53.60297\n",
      "Epoch 8 - lr: 0.05000 - Train loss: 18.47928 - Test loss: 53.55631\n",
      "Epoch 9 - lr: 0.05000 - Train loss: 18.46996 - Test loss: 53.52392\n",
      "Epoch 10 - lr: 0.05000 - Train loss: 18.46360 - Test loss: 53.50296\n",
      "Epoch 11 - lr: 0.05000 - Train loss: 18.46002 - Test loss: 53.49212\n",
      "Epoch 12 - lr: 0.05000 - Train loss: 18.45946 - Test loss: 53.49115\n",
      "Epoch 13 - lr: 0.05000 - Train loss: 18.46254 - Test loss: 53.50043\n",
      "Epoch 14 - lr: 0.05000 - Train loss: 18.47021 - Test loss: 53.52017\n",
      "Epoch 15 - lr: 0.05000 - Train loss: 18.48313 - Test loss: 53.54833\n",
      "Epoch 16 - lr: 0.05000 - Train loss: 18.50012 - Test loss: 53.57768\n",
      "Epoch 17 - lr: 0.05000 - Train loss: 18.51668 - Test loss: 53.59650\n",
      "Epoch 18 - lr: 0.05000 - Train loss: 18.52722 - Test loss: 53.59726\n",
      "Epoch 19 - lr: 0.05000 - Train loss: 18.52981 - Test loss: 53.58287\n",
      "Epoch 20 - lr: 0.05000 - Train loss: 18.52664 - Test loss: 53.56091\n",
      "Epoch 21 - lr: 0.05000 - Train loss: 18.52055 - Test loss: 53.53695\n",
      "Epoch 22 - lr: 0.05000 - Train loss: 18.51323 - Test loss: 53.51359\n",
      "Epoch 23 - lr: 0.05000 - Train loss: 18.50545 - Test loss: 53.49179\n",
      "Epoch 24 - lr: 0.05000 - Train loss: 18.49750 - Test loss: 53.47179\n",
      "Epoch 25 - lr: 0.05000 - Train loss: 18.48951 - Test loss: 53.45359\n",
      "Epoch 26 - lr: 0.05000 - Train loss: 18.48155 - Test loss: 53.43710\n",
      "Epoch 27 - lr: 0.05000 - Train loss: 18.47365 - Test loss: 53.42221\n",
      "Epoch 28 - lr: 0.05000 - Train loss: 18.46585 - Test loss: 53.40879\n",
      "Epoch 29 - lr: 0.05000 - Train loss: 18.45815 - Test loss: 53.39674\n",
      "Epoch 30 - lr: 0.05000 - Train loss: 18.45058 - Test loss: 53.38594\n",
      "Epoch 31 - lr: 0.05000 - Train loss: 18.44313 - Test loss: 53.37629\n",
      "Epoch 32 - lr: 0.05000 - Train loss: 18.43582 - Test loss: 53.36770\n",
      "Epoch 33 - lr: 0.05000 - Train loss: 18.42865 - Test loss: 53.36007\n",
      "Epoch 34 - lr: 0.05000 - Train loss: 18.42161 - Test loss: 53.35333\n",
      "Epoch 35 - lr: 0.05000 - Train loss: 18.41473 - Test loss: 53.34741\n",
      "Epoch 36 - lr: 0.05000 - Train loss: 18.40798 - Test loss: 53.34223\n",
      "Epoch 37 - lr: 0.05000 - Train loss: 18.40137 - Test loss: 53.33773\n",
      "Epoch 38 - lr: 0.05000 - Train loss: 18.39490 - Test loss: 53.33386\n",
      "Epoch 39 - lr: 0.05000 - Train loss: 18.38857 - Test loss: 53.33057\n",
      "Epoch 40 - lr: 0.05000 - Train loss: 18.38237 - Test loss: 53.32781\n",
      "Epoch 41 - lr: 0.05000 - Train loss: 18.37630 - Test loss: 53.32554\n",
      "Epoch 42 - lr: 0.05000 - Train loss: 18.37036 - Test loss: 53.32371\n",
      "Epoch 43 - lr: 0.05000 - Train loss: 18.36455 - Test loss: 53.32230\n",
      "Epoch 44 - lr: 0.05000 - Train loss: 18.35885 - Test loss: 53.32126\n",
      "Epoch 45 - lr: 0.05000 - Train loss: 18.35328 - Test loss: 53.32058\n",
      "Epoch 46 - lr: 0.05000 - Train loss: 18.34783 - Test loss: 53.32022\n",
      "Epoch 47 - lr: 0.05000 - Train loss: 18.34249 - Test loss: 53.32016\n",
      "Epoch 48 - lr: 0.05000 - Train loss: 18.33726 - Test loss: 53.32038\n",
      "Epoch 49 - lr: 0.05000 - Train loss: 18.33213 - Test loss: 53.32084\n",
      "Epoch 50 - lr: 0.05000 - Train loss: 18.32711 - Test loss: 53.32155\n",
      "Epoch 51 - lr: 0.05000 - Train loss: 18.32220 - Test loss: 53.32247\n",
      "Epoch 52 - lr: 0.05000 - Train loss: 18.31738 - Test loss: 53.32358\n",
      "Epoch 53 - lr: 0.05000 - Train loss: 18.31266 - Test loss: 53.32489\n",
      "Epoch 54 - lr: 0.05000 - Train loss: 18.30803 - Test loss: 53.32637\n",
      "Epoch 55 - lr: 0.05000 - Train loss: 18.30349 - Test loss: 53.32800\n",
      "Epoch 56 - lr: 0.05000 - Train loss: 18.29904 - Test loss: 53.32978\n",
      "Epoch 57 - lr: 0.05000 - Train loss: 18.29468 - Test loss: 53.33170\n",
      "Epoch 58 - lr: 0.05000 - Train loss: 18.29040 - Test loss: 53.33374\n",
      "Epoch 59 - lr: 0.05000 - Train loss: 18.28620 - Test loss: 53.33589\n",
      "Epoch 60 - lr: 0.05000 - Train loss: 18.28208 - Test loss: 53.33816\n",
      "Epoch 61 - lr: 0.05000 - Train loss: 18.27804 - Test loss: 53.34052\n",
      "Epoch 62 - lr: 0.05000 - Train loss: 18.27408 - Test loss: 53.34297\n",
      "Epoch 63 - lr: 0.05000 - Train loss: 18.27018 - Test loss: 53.34551\n",
      "Epoch 64 - lr: 0.05000 - Train loss: 18.26636 - Test loss: 53.34812\n",
      "Epoch 65 - lr: 0.05000 - Train loss: 18.26260 - Test loss: 53.35081\n",
      "Epoch 66 - lr: 0.05000 - Train loss: 18.25892 - Test loss: 53.35356\n",
      "Epoch 67 - lr: 0.05000 - Train loss: 18.25529 - Test loss: 53.35638\n",
      "Epoch 68 - lr: 0.05000 - Train loss: 18.25174 - Test loss: 53.35924\n",
      "Epoch 69 - lr: 0.05000 - Train loss: 18.24824 - Test loss: 53.36216\n",
      "Epoch 70 - lr: 0.05000 - Train loss: 18.24480 - Test loss: 53.36513\n",
      "Epoch 71 - lr: 0.05000 - Train loss: 18.24143 - Test loss: 53.36814\n",
      "Epoch 72 - lr: 0.05000 - Train loss: 18.23811 - Test loss: 53.37119\n",
      "Epoch 73 - lr: 0.05000 - Train loss: 18.23484 - Test loss: 53.37427\n",
      "Epoch 74 - lr: 0.05000 - Train loss: 18.23163 - Test loss: 53.37739\n",
      "Epoch 75 - lr: 0.05000 - Train loss: 18.22847 - Test loss: 53.38053\n",
      "Epoch 76 - lr: 0.05000 - Train loss: 18.22537 - Test loss: 53.38371\n",
      "Epoch 77 - lr: 0.05000 - Train loss: 18.22231 - Test loss: 53.38690\n",
      "Epoch 78 - lr: 0.05000 - Train loss: 18.21931 - Test loss: 53.39012\n",
      "Epoch 79 - lr: 0.05000 - Train loss: 18.21635 - Test loss: 53.39336\n",
      "Epoch 80 - lr: 0.05000 - Train loss: 18.21344 - Test loss: 53.39661\n",
      "Epoch 81 - lr: 0.05000 - Train loss: 18.21057 - Test loss: 53.39988\n",
      "Epoch 82 - lr: 0.05000 - Train loss: 18.20775 - Test loss: 53.40316\n",
      "Epoch 83 - lr: 0.05000 - Train loss: 18.20498 - Test loss: 53.40645\n",
      "Epoch 84 - lr: 0.05000 - Train loss: 18.20224 - Test loss: 53.40976\n",
      "Epoch 85 - lr: 0.05000 - Train loss: 18.19955 - Test loss: 53.41307\n",
      "Epoch 86 - lr: 0.05000 - Train loss: 18.19690 - Test loss: 53.41639\n",
      "Epoch 87 - lr: 0.05000 - Train loss: 18.19429 - Test loss: 53.41971\n",
      "Epoch 88 - lr: 0.05000 - Train loss: 18.19171 - Test loss: 53.42303\n",
      "Epoch 89 - lr: 0.05000 - Train loss: 18.18918 - Test loss: 53.42636\n",
      "Epoch 90 - lr: 0.05000 - Train loss: 18.18668 - Test loss: 53.42969\n",
      "Epoch 91 - lr: 0.05000 - Train loss: 18.18421 - Test loss: 53.43302\n",
      "Epoch 92 - lr: 0.05000 - Train loss: 18.18179 - Test loss: 53.43635\n",
      "Epoch 93 - lr: 0.05000 - Train loss: 18.17939 - Test loss: 53.43968\n",
      "Epoch 94 - lr: 0.05000 - Train loss: 18.17704 - Test loss: 53.44301\n",
      "Epoch 95 - lr: 0.05000 - Train loss: 18.17471 - Test loss: 53.44633\n",
      "Epoch 96 - lr: 0.05000 - Train loss: 18.17242 - Test loss: 53.44965\n",
      "Epoch 97 - lr: 0.05000 - Train loss: 18.17016 - Test loss: 53.45297\n",
      "Epoch 98 - lr: 0.05000 - Train loss: 18.16793 - Test loss: 53.45628\n",
      "Epoch 99 - lr: 0.05000 - Train loss: 18.16573 - Test loss: 53.45958\n",
      "Epoch 100 - lr: 0.05000 - Train loss: 18.16356 - Test loss: 53.46288\n",
      "Epoch 101 - lr: 0.05000 - Train loss: 18.16142 - Test loss: 53.46617\n",
      "Epoch 102 - lr: 0.05000 - Train loss: 18.15931 - Test loss: 53.46945\n",
      "Epoch 103 - lr: 0.05000 - Train loss: 18.15722 - Test loss: 53.47272\n",
      "Epoch 104 - lr: 0.05000 - Train loss: 18.15517 - Test loss: 53.47599\n",
      "Epoch 105 - lr: 0.05000 - Train loss: 18.15314 - Test loss: 53.47925\n",
      "Epoch 106 - lr: 0.05000 - Train loss: 18.15113 - Test loss: 53.48249\n",
      "Epoch 107 - lr: 0.05000 - Train loss: 18.14916 - Test loss: 53.48573\n",
      "Epoch 108 - lr: 0.05000 - Train loss: 18.14721 - Test loss: 53.48896\n",
      "Epoch 109 - lr: 0.05000 - Train loss: 18.14528 - Test loss: 53.49218\n",
      "Epoch 110 - lr: 0.05000 - Train loss: 18.14338 - Test loss: 53.49538\n",
      "Epoch 111 - lr: 0.05000 - Train loss: 18.14150 - Test loss: 53.49858\n",
      "Epoch 112 - lr: 0.05000 - Train loss: 18.13964 - Test loss: 53.50176\n",
      "Epoch 113 - lr: 0.05000 - Train loss: 18.13781 - Test loss: 53.50494\n",
      "Epoch 114 - lr: 0.05000 - Train loss: 18.13600 - Test loss: 53.50810\n",
      "Epoch 115 - lr: 0.05000 - Train loss: 18.13422 - Test loss: 53.51125\n",
      "Epoch 116 - lr: 0.05000 - Train loss: 18.13245 - Test loss: 53.51439\n",
      "Epoch 117 - lr: 0.05000 - Train loss: 18.13071 - Test loss: 53.51751\n",
      "Epoch 118 - lr: 0.05000 - Train loss: 18.12898 - Test loss: 53.52062\n",
      "Epoch 119 - lr: 0.05000 - Train loss: 18.12728 - Test loss: 53.52372\n",
      "Epoch 120 - lr: 0.05000 - Train loss: 18.12560 - Test loss: 53.52681\n",
      "Epoch 121 - lr: 0.05000 - Train loss: 18.12394 - Test loss: 53.52989\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 122 - lr: 0.05000 - Train loss: 18.12230 - Test loss: 53.53295\n",
      "Epoch 123 - lr: 0.05000 - Train loss: 18.12067 - Test loss: 53.53600\n",
      "Epoch 124 - lr: 0.05000 - Train loss: 18.11907 - Test loss: 53.53904\n",
      "Epoch 125 - lr: 0.05000 - Train loss: 18.11748 - Test loss: 53.54206\n",
      "Epoch 126 - lr: 0.05000 - Train loss: 18.11591 - Test loss: 53.54507\n",
      "Epoch 127 - lr: 0.05000 - Train loss: 18.11436 - Test loss: 53.54806\n",
      "Epoch 128 - lr: 0.05000 - Train loss: 18.11283 - Test loss: 53.55105\n",
      "Epoch 129 - lr: 0.05000 - Train loss: 18.11132 - Test loss: 53.55402\n",
      "Epoch 130 - lr: 0.05000 - Train loss: 18.10982 - Test loss: 53.55697\n",
      "Epoch 131 - lr: 0.05000 - Train loss: 18.10834 - Test loss: 53.55992\n",
      "Epoch 132 - lr: 0.05000 - Train loss: 18.10687 - Test loss: 53.56285\n",
      "Epoch 133 - lr: 0.05000 - Train loss: 18.10543 - Test loss: 53.56576\n",
      "Epoch 134 - lr: 0.05000 - Train loss: 18.10399 - Test loss: 53.56866\n",
      "Epoch 135 - lr: 0.05000 - Train loss: 18.10258 - Test loss: 53.57155\n",
      "Epoch 136 - lr: 0.05000 - Train loss: 18.10117 - Test loss: 53.57443\n",
      "Epoch 137 - lr: 0.05000 - Train loss: 18.09979 - Test loss: 53.57729\n",
      "Epoch 138 - lr: 0.05000 - Train loss: 18.09842 - Test loss: 53.58014\n",
      "Epoch 139 - lr: 0.05000 - Train loss: 18.09706 - Test loss: 53.58297\n",
      "Epoch 140 - lr: 0.05000 - Train loss: 18.09572 - Test loss: 53.58579\n",
      "Epoch 141 - lr: 0.05000 - Train loss: 18.09439 - Test loss: 53.58860\n",
      "Epoch 142 - lr: 0.05000 - Train loss: 18.09307 - Test loss: 53.59139\n",
      "Epoch 143 - lr: 0.05000 - Train loss: 18.09177 - Test loss: 53.59417\n",
      "Epoch 144 - lr: 0.05000 - Train loss: 18.09049 - Test loss: 53.59694\n",
      "Epoch 145 - lr: 0.05000 - Train loss: 18.08921 - Test loss: 53.59969\n",
      "Epoch 146 - lr: 0.05000 - Train loss: 18.08795 - Test loss: 53.60243\n",
      "Epoch 147 - lr: 0.05000 - Train loss: 18.08670 - Test loss: 53.60516\n",
      "Epoch 148 - lr: 0.05000 - Train loss: 18.08547 - Test loss: 53.60787\n",
      "Epoch 149 - lr: 0.05000 - Train loss: 18.08425 - Test loss: 53.61057\n",
      "Epoch 150 - lr: 0.05000 - Train loss: 18.08304 - Test loss: 53.61326\n",
      "Epoch 151 - lr: 0.05000 - Train loss: 18.08184 - Test loss: 53.61593\n",
      "Epoch 152 - lr: 0.05000 - Train loss: 18.08065 - Test loss: 53.61859\n",
      "Epoch 153 - lr: 0.05000 - Train loss: 18.07948 - Test loss: 53.62124\n",
      "Epoch 154 - lr: 0.05000 - Train loss: 18.07832 - Test loss: 53.62387\n",
      "Epoch 155 - lr: 0.05000 - Train loss: 18.07717 - Test loss: 53.62649\n",
      "Epoch 156 - lr: 0.05000 - Train loss: 18.07603 - Test loss: 53.62910\n",
      "Epoch 157 - lr: 0.05000 - Train loss: 18.07490 - Test loss: 53.63169\n",
      "Epoch 158 - lr: 0.05000 - Train loss: 18.07378 - Test loss: 53.63428\n",
      "Epoch 159 - lr: 0.05000 - Train loss: 18.07268 - Test loss: 53.63684\n",
      "Epoch 160 - lr: 0.05000 - Train loss: 18.07158 - Test loss: 53.63940\n",
      "Epoch 161 - lr: 0.05000 - Train loss: 18.07050 - Test loss: 53.64194\n",
      "Epoch 162 - lr: 0.05000 - Train loss: 18.06942 - Test loss: 53.64447\n",
      "Epoch 163 - lr: 0.05000 - Train loss: 18.06836 - Test loss: 53.64699\n",
      "Epoch 164 - lr: 0.05000 - Train loss: 18.06730 - Test loss: 53.64950\n",
      "Epoch 165 - lr: 0.05000 - Train loss: 18.06626 - Test loss: 53.65199\n",
      "Epoch 166 - lr: 0.05000 - Train loss: 18.06522 - Test loss: 53.65447\n",
      "Epoch 167 - lr: 0.05000 - Train loss: 18.06420 - Test loss: 53.65693\n",
      "Epoch 168 - lr: 0.05000 - Train loss: 18.06318 - Test loss: 53.65939\n",
      "Epoch 169 - lr: 0.05000 - Train loss: 18.06218 - Test loss: 53.66183\n",
      "Epoch 170 - lr: 0.05000 - Train loss: 18.06118 - Test loss: 53.66426\n",
      "Epoch 171 - lr: 0.05000 - Train loss: 18.06019 - Test loss: 53.66668\n",
      "Epoch 172 - lr: 0.05000 - Train loss: 18.05922 - Test loss: 53.66908\n",
      "Epoch 173 - lr: 0.05000 - Train loss: 18.05825 - Test loss: 53.67148\n",
      "Epoch 174 - lr: 0.05000 - Train loss: 18.05729 - Test loss: 53.67386\n",
      "Epoch 175 - lr: 0.05000 - Train loss: 18.05633 - Test loss: 53.67623\n",
      "Epoch 176 - lr: 0.05000 - Train loss: 18.05539 - Test loss: 53.67859\n",
      "Epoch 177 - lr: 0.05000 - Train loss: 18.05446 - Test loss: 53.68093\n",
      "Epoch 178 - lr: 0.05000 - Train loss: 18.05353 - Test loss: 53.68326\n",
      "Epoch 179 - lr: 0.05000 - Train loss: 18.05261 - Test loss: 53.68559\n",
      "Epoch 180 - lr: 0.05000 - Train loss: 18.05170 - Test loss: 53.68790\n",
      "Epoch 181 - lr: 0.05000 - Train loss: 18.05080 - Test loss: 53.69019\n",
      "Epoch 182 - lr: 0.05000 - Train loss: 18.04991 - Test loss: 53.69248\n",
      "Epoch 183 - lr: 0.05000 - Train loss: 18.04902 - Test loss: 53.69476\n",
      "Epoch 184 - lr: 0.05000 - Train loss: 18.04814 - Test loss: 53.69702\n",
      "Epoch 185 - lr: 0.05000 - Train loss: 18.04727 - Test loss: 53.69927\n",
      "Epoch 186 - lr: 0.05000 - Train loss: 18.04641 - Test loss: 53.70151\n",
      "Epoch 187 - lr: 0.05000 - Train loss: 18.04555 - Test loss: 53.70374\n",
      "Epoch 188 - lr: 0.05000 - Train loss: 18.04470 - Test loss: 53.70596\n",
      "Epoch 189 - lr: 0.05000 - Train loss: 18.04386 - Test loss: 53.70817\n",
      "Epoch 190 - lr: 0.05000 - Train loss: 18.04303 - Test loss: 53.71037\n",
      "Epoch 191 - lr: 0.05000 - Train loss: 18.04220 - Test loss: 53.71255\n",
      "Epoch 192 - lr: 0.05000 - Train loss: 18.04138 - Test loss: 53.71473\n",
      "Epoch 193 - lr: 0.05000 - Train loss: 18.04057 - Test loss: 53.71689\n",
      "Epoch 194 - lr: 0.05000 - Train loss: 18.03977 - Test loss: 53.71904\n",
      "Epoch 195 - lr: 0.05000 - Train loss: 18.03897 - Test loss: 53.72118\n",
      "Epoch 196 - lr: 0.05000 - Train loss: 18.03818 - Test loss: 53.72331\n",
      "Epoch 197 - lr: 0.05000 - Train loss: 18.03739 - Test loss: 53.72543\n",
      "Epoch 198 - lr: 0.05000 - Train loss: 18.03661 - Test loss: 53.72754\n",
      "Epoch 199 - lr: 0.05000 - Train loss: 18.03584 - Test loss: 53.72964\n",
      "Epoch 200 - lr: 0.05000 - Train loss: 18.03507 - Test loss: 53.73173\n",
      "Epoch 201 - lr: 0.05000 - Train loss: 18.03431 - Test loss: 53.73381\n",
      "Epoch 202 - lr: 0.05000 - Train loss: 18.03356 - Test loss: 53.73587\n",
      "Epoch 203 - lr: 0.05000 - Train loss: 18.03281 - Test loss: 53.73793\n",
      "Epoch 204 - lr: 0.05000 - Train loss: 18.03207 - Test loss: 53.73998\n",
      "Epoch 205 - lr: 0.05000 - Train loss: 18.03133 - Test loss: 53.74201\n",
      "Epoch 206 - lr: 0.05000 - Train loss: 18.03061 - Test loss: 53.74404\n",
      "Epoch 207 - lr: 0.05000 - Train loss: 18.02988 - Test loss: 53.74606\n",
      "Epoch 208 - lr: 0.05000 - Train loss: 18.02916 - Test loss: 53.74806\n",
      "Epoch 209 - lr: 0.05000 - Train loss: 18.02845 - Test loss: 53.75006\n",
      "Epoch 210 - lr: 0.05000 - Train loss: 18.02775 - Test loss: 53.75204\n",
      "Epoch 211 - lr: 0.05000 - Train loss: 18.02705 - Test loss: 53.75402\n",
      "Epoch 212 - lr: 0.05000 - Train loss: 18.02635 - Test loss: 53.75599\n",
      "Epoch 213 - lr: 0.05000 - Train loss: 18.02566 - Test loss: 53.75794\n",
      "Epoch 214 - lr: 0.05000 - Train loss: 18.02498 - Test loss: 53.75989\n",
      "Epoch 215 - lr: 0.05000 - Train loss: 18.02430 - Test loss: 53.76183\n",
      "Epoch 216 - lr: 0.05000 - Train loss: 18.02363 - Test loss: 53.76375\n",
      "Epoch 217 - lr: 0.05000 - Train loss: 18.02296 - Test loss: 53.76567\n",
      "Epoch 218 - lr: 0.05000 - Train loss: 18.02230 - Test loss: 53.76758\n",
      "Epoch 219 - lr: 0.05000 - Train loss: 18.02164 - Test loss: 53.76948\n",
      "Epoch 220 - lr: 0.05000 - Train loss: 18.02099 - Test loss: 53.77137\n",
      "Epoch 221 - lr: 0.05000 - Train loss: 18.02034 - Test loss: 53.77325\n",
      "Epoch 222 - lr: 0.05000 - Train loss: 18.01970 - Test loss: 53.77512\n",
      "Epoch 223 - lr: 0.05000 - Train loss: 18.01906 - Test loss: 53.77698\n",
      "Epoch 224 - lr: 0.05000 - Train loss: 18.01843 - Test loss: 53.77883\n",
      "Epoch 225 - lr: 0.05000 - Train loss: 18.01780 - Test loss: 53.78067\n",
      "Epoch 226 - lr: 0.05000 - Train loss: 18.01718 - Test loss: 53.78251\n",
      "Epoch 227 - lr: 0.05000 - Train loss: 18.01656 - Test loss: 53.78433\n",
      "Epoch 228 - lr: 0.05000 - Train loss: 18.01594 - Test loss: 53.78615\n",
      "Epoch 229 - lr: 0.05000 - Train loss: 18.01533 - Test loss: 53.78796\n",
      "Epoch 230 - lr: 0.05000 - Train loss: 18.01473 - Test loss: 53.78975\n",
      "Epoch 231 - lr: 0.05000 - Train loss: 18.01413 - Test loss: 53.79154\n",
      "Epoch 232 - lr: 0.05000 - Train loss: 18.01354 - Test loss: 53.79332\n",
      "Epoch 233 - lr: 0.05000 - Train loss: 18.01294 - Test loss: 53.79509\n",
      "Epoch 234 - lr: 0.05000 - Train loss: 18.01236 - Test loss: 53.79686\n",
      "Epoch 235 - lr: 0.05000 - Train loss: 18.01178 - Test loss: 53.79861\n",
      "Epoch 236 - lr: 0.05000 - Train loss: 18.01120 - Test loss: 53.80036\n",
      "Epoch 237 - lr: 0.05000 - Train loss: 18.01063 - Test loss: 53.80209\n",
      "Epoch 238 - lr: 0.05000 - Train loss: 18.01006 - Test loss: 53.80382\n",
      "Epoch 239 - lr: 0.05000 - Train loss: 18.00949 - Test loss: 53.80554\n",
      "Epoch 240 - lr: 0.05000 - Train loss: 18.00893 - Test loss: 53.80725\n",
      "Epoch 241 - lr: 0.05000 - Train loss: 18.00837 - Test loss: 53.80895\n",
      "Epoch 242 - lr: 0.05000 - Train loss: 18.00782 - Test loss: 53.81065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 243 - lr: 0.05000 - Train loss: 18.00727 - Test loss: 53.81233\n",
      "Epoch 244 - lr: 0.05000 - Train loss: 18.00673 - Test loss: 53.81401\n",
      "Epoch 245 - lr: 0.05000 - Train loss: 18.00619 - Test loss: 53.81568\n",
      "Epoch 246 - lr: 0.05000 - Train loss: 18.00565 - Test loss: 53.81734\n",
      "Epoch 247 - lr: 0.05000 - Train loss: 18.00512 - Test loss: 53.81900\n",
      "Epoch 248 - lr: 0.05000 - Train loss: 18.00459 - Test loss: 53.82064\n",
      "Epoch 249 - lr: 0.05000 - Train loss: 18.00406 - Test loss: 53.82228\n",
      "Epoch 250 - lr: 0.05000 - Train loss: 18.00354 - Test loss: 53.82391\n",
      "Epoch 251 - lr: 0.05000 - Train loss: 18.00302 - Test loss: 53.82553\n",
      "Epoch 252 - lr: 0.05000 - Train loss: 18.00251 - Test loss: 53.82715\n",
      "Epoch 253 - lr: 0.05000 - Train loss: 18.00200 - Test loss: 53.82875\n",
      "Epoch 254 - lr: 0.05000 - Train loss: 18.00149 - Test loss: 53.83035\n",
      "Epoch 255 - lr: 0.05000 - Train loss: 18.00099 - Test loss: 53.83194\n",
      "Epoch 256 - lr: 0.05000 - Train loss: 18.00049 - Test loss: 53.83352\n",
      "Epoch 257 - lr: 0.05000 - Train loss: 17.99999 - Test loss: 53.83510\n",
      "Epoch 258 - lr: 0.05000 - Train loss: 17.99950 - Test loss: 53.83666\n",
      "Epoch 259 - lr: 0.05000 - Train loss: 17.99901 - Test loss: 53.83822\n",
      "Epoch 260 - lr: 0.05000 - Train loss: 17.99852 - Test loss: 53.83978\n",
      "Epoch 261 - lr: 0.05000 - Train loss: 17.99804 - Test loss: 53.84132\n",
      "Epoch 262 - lr: 0.05000 - Train loss: 17.99756 - Test loss: 53.84286\n",
      "Epoch 263 - lr: 0.05000 - Train loss: 17.99709 - Test loss: 53.84439\n",
      "Epoch 264 - lr: 0.05000 - Train loss: 17.99661 - Test loss: 53.84591\n",
      "Epoch 265 - lr: 0.05000 - Train loss: 17.99614 - Test loss: 53.84742\n",
      "Epoch 266 - lr: 0.05000 - Train loss: 17.99568 - Test loss: 53.84893\n",
      "Epoch 267 - lr: 0.05000 - Train loss: 17.99521 - Test loss: 53.85043\n",
      "Epoch 268 - lr: 0.05000 - Train loss: 17.99475 - Test loss: 53.85192\n",
      "Epoch 269 - lr: 0.05000 - Train loss: 17.99430 - Test loss: 53.85341\n",
      "Epoch 270 - lr: 0.05000 - Train loss: 17.99384 - Test loss: 53.85489\n",
      "Epoch 271 - lr: 0.05000 - Train loss: 17.99339 - Test loss: 53.85636\n",
      "Epoch 272 - lr: 0.05000 - Train loss: 17.99295 - Test loss: 53.85782\n",
      "Epoch 273 - lr: 0.05000 - Train loss: 17.99250 - Test loss: 53.85928\n",
      "Epoch 274 - lr: 0.05000 - Train loss: 17.99206 - Test loss: 53.86073\n",
      "Epoch 275 - lr: 0.05000 - Train loss: 17.99162 - Test loss: 53.86218\n",
      "Epoch 276 - lr: 0.05000 - Train loss: 17.99119 - Test loss: 53.86361\n",
      "Epoch 277 - lr: 0.05000 - Train loss: 17.99075 - Test loss: 53.86504\n",
      "Epoch 278 - lr: 0.05000 - Train loss: 17.99032 - Test loss: 53.86647\n",
      "Epoch 279 - lr: 0.05000 - Train loss: 17.98990 - Test loss: 53.86788\n",
      "Epoch 280 - lr: 0.05000 - Train loss: 17.98947 - Test loss: 53.86929\n",
      "Epoch 281 - lr: 0.05000 - Train loss: 17.98905 - Test loss: 53.87069\n",
      "Epoch 282 - lr: 0.05000 - Train loss: 17.98863 - Test loss: 53.87209\n",
      "Epoch 283 - lr: 0.05000 - Train loss: 17.98822 - Test loss: 53.87348\n",
      "Epoch 284 - lr: 0.05000 - Train loss: 17.98780 - Test loss: 53.87486\n",
      "Epoch 285 - lr: 0.05000 - Train loss: 17.98739 - Test loss: 53.87624\n",
      "Epoch 286 - lr: 0.05000 - Train loss: 17.98699 - Test loss: 53.87761\n",
      "Epoch 287 - lr: 0.05000 - Train loss: 17.98658 - Test loss: 53.87897\n",
      "Epoch 288 - lr: 0.05000 - Train loss: 17.98618 - Test loss: 53.88033\n",
      "Epoch 289 - lr: 0.05000 - Train loss: 17.98578 - Test loss: 53.88167\n",
      "Epoch 290 - lr: 0.05000 - Train loss: 17.98538 - Test loss: 53.88302\n",
      "Epoch 291 - lr: 0.05000 - Train loss: 17.98499 - Test loss: 53.88436\n",
      "Epoch 292 - lr: 0.05000 - Train loss: 17.98460 - Test loss: 53.88569\n",
      "Epoch 293 - lr: 0.05000 - Train loss: 17.98421 - Test loss: 53.88701\n",
      "Epoch 294 - lr: 0.05000 - Train loss: 17.98382 - Test loss: 53.88833\n",
      "Epoch 295 - lr: 0.05000 - Train loss: 17.98344 - Test loss: 53.88964\n",
      "Epoch 296 - lr: 0.05000 - Train loss: 17.98306 - Test loss: 53.89095\n",
      "Epoch 297 - lr: 0.05000 - Train loss: 17.98268 - Test loss: 53.89225\n",
      "Epoch 298 - lr: 0.05000 - Train loss: 17.98230 - Test loss: 53.89354\n",
      "Epoch 299 - lr: 0.05000 - Train loss: 17.98192 - Test loss: 53.89483\n",
      "Epoch 300 - lr: 0.05000 - Train loss: 17.98155 - Test loss: 53.89611\n",
      "Epoch 301 - lr: 0.05000 - Train loss: 17.98118 - Test loss: 53.89738\n",
      "Epoch 302 - lr: 0.05000 - Train loss: 17.98081 - Test loss: 53.89865\n",
      "Epoch 303 - lr: 0.05000 - Train loss: 17.98045 - Test loss: 53.89991\n",
      "Epoch 304 - lr: 0.05000 - Train loss: 17.98009 - Test loss: 53.90117\n",
      "Epoch 305 - lr: 0.05000 - Train loss: 17.97973 - Test loss: 53.90242\n",
      "Epoch 306 - lr: 0.05000 - Train loss: 17.97937 - Test loss: 53.90367\n",
      "Epoch 307 - lr: 0.05000 - Train loss: 17.97901 - Test loss: 53.90491\n",
      "Epoch 308 - lr: 0.05000 - Train loss: 17.97866 - Test loss: 53.90614\n",
      "Epoch 309 - lr: 0.05000 - Train loss: 17.97831 - Test loss: 53.90737\n",
      "Epoch 310 - lr: 0.05000 - Train loss: 17.97796 - Test loss: 53.90859\n",
      "Epoch 311 - lr: 0.05000 - Train loss: 17.97761 - Test loss: 53.90981\n",
      "Epoch 312 - lr: 0.05000 - Train loss: 17.97726 - Test loss: 53.91102\n",
      "Epoch 313 - lr: 0.05000 - Train loss: 17.97692 - Test loss: 53.91222\n",
      "Epoch 314 - lr: 0.05000 - Train loss: 17.97658 - Test loss: 53.91342\n",
      "Epoch 315 - lr: 0.05000 - Train loss: 17.97624 - Test loss: 53.91462\n",
      "Epoch 316 - lr: 0.05000 - Train loss: 17.97591 - Test loss: 53.91580\n",
      "Epoch 317 - lr: 0.05000 - Train loss: 17.97557 - Test loss: 53.91699\n",
      "Epoch 318 - lr: 0.05000 - Train loss: 17.97524 - Test loss: 53.91816\n",
      "Epoch 319 - lr: 0.05000 - Train loss: 17.97491 - Test loss: 53.91933\n",
      "Epoch 320 - lr: 0.05000 - Train loss: 17.97458 - Test loss: 53.92050\n",
      "Epoch 321 - lr: 0.05000 - Train loss: 17.97425 - Test loss: 53.92166\n",
      "Epoch 322 - lr: 0.05000 - Train loss: 17.97393 - Test loss: 53.92282\n",
      "Epoch 323 - lr: 0.05000 - Train loss: 17.97361 - Test loss: 53.92397\n",
      "Epoch 324 - lr: 0.05000 - Train loss: 17.97329 - Test loss: 53.92511\n",
      "Epoch 325 - lr: 0.05000 - Train loss: 17.97297 - Test loss: 53.92625\n",
      "Epoch 326 - lr: 0.05000 - Train loss: 17.97265 - Test loss: 53.92738\n",
      "Epoch 327 - lr: 0.05000 - Train loss: 17.97234 - Test loss: 53.92851\n",
      "Epoch 328 - lr: 0.05000 - Train loss: 17.97202 - Test loss: 53.92963\n",
      "Epoch 329 - lr: 0.05000 - Train loss: 17.97171 - Test loss: 53.93075\n",
      "Epoch 330 - lr: 0.05000 - Train loss: 17.97140 - Test loss: 53.93186\n",
      "Epoch 331 - lr: 0.05000 - Train loss: 17.97110 - Test loss: 53.93297\n",
      "Epoch 332 - lr: 0.05000 - Train loss: 17.97079 - Test loss: 53.93407\n",
      "Epoch 333 - lr: 0.05000 - Train loss: 17.97049 - Test loss: 53.93517\n",
      "Epoch 334 - lr: 0.05000 - Train loss: 17.97019 - Test loss: 53.93626\n",
      "Epoch 335 - lr: 0.05000 - Train loss: 17.96989 - Test loss: 53.93735\n",
      "Epoch 336 - lr: 0.05000 - Train loss: 17.96959 - Test loss: 53.93843\n",
      "Epoch 337 - lr: 0.05000 - Train loss: 17.96929 - Test loss: 53.93951\n",
      "Epoch 338 - lr: 0.05000 - Train loss: 17.96900 - Test loss: 53.94058\n",
      "Epoch 339 - lr: 0.05000 - Train loss: 17.96870 - Test loss: 53.94165\n",
      "Epoch 340 - lr: 0.05000 - Train loss: 17.96841 - Test loss: 53.94271\n",
      "Epoch 341 - lr: 0.05000 - Train loss: 17.96812 - Test loss: 53.94377\n",
      "Epoch 342 - lr: 0.05000 - Train loss: 17.96784 - Test loss: 53.94482\n",
      "Epoch 343 - lr: 0.05000 - Train loss: 17.96755 - Test loss: 53.94587\n",
      "Epoch 344 - lr: 0.05000 - Train loss: 17.96727 - Test loss: 53.94691\n",
      "Epoch 345 - lr: 0.05000 - Train loss: 17.96698 - Test loss: 53.94795\n",
      "Epoch 346 - lr: 0.05000 - Train loss: 17.96670 - Test loss: 53.94898\n",
      "Epoch 347 - lr: 0.05000 - Train loss: 17.96642 - Test loss: 53.95001\n",
      "Epoch 348 - lr: 0.05000 - Train loss: 17.96614 - Test loss: 53.95104\n",
      "Epoch 349 - lr: 0.05000 - Train loss: 17.96587 - Test loss: 53.95205\n",
      "Epoch 350 - lr: 0.05000 - Train loss: 17.96559 - Test loss: 53.95307\n",
      "Epoch 351 - lr: 0.05000 - Train loss: 17.96532 - Test loss: 53.95408\n",
      "Epoch 352 - lr: 0.05000 - Train loss: 17.96505 - Test loss: 53.95508\n",
      "Epoch 353 - lr: 0.05000 - Train loss: 17.96478 - Test loss: 53.95608\n",
      "Epoch 354 - lr: 0.05000 - Train loss: 17.96451 - Test loss: 53.95708\n",
      "Epoch 355 - lr: 0.05000 - Train loss: 17.96424 - Test loss: 53.95807\n",
      "Epoch 356 - lr: 0.05000 - Train loss: 17.96398 - Test loss: 53.95906\n",
      "Epoch 357 - lr: 0.05000 - Train loss: 17.96372 - Test loss: 53.96004\n",
      "Epoch 358 - lr: 0.05000 - Train loss: 17.96345 - Test loss: 53.96102\n",
      "Epoch 359 - lr: 0.05000 - Train loss: 17.96319 - Test loss: 53.96199\n",
      "Epoch 360 - lr: 0.05000 - Train loss: 17.96293 - Test loss: 53.96296\n",
      "Epoch 361 - lr: 0.05000 - Train loss: 17.96268 - Test loss: 53.96392\n",
      "Epoch 362 - lr: 0.05000 - Train loss: 17.96242 - Test loss: 53.96488\n",
      "Epoch 363 - lr: 0.05000 - Train loss: 17.96216 - Test loss: 53.96584\n",
      "Epoch 364 - lr: 0.05000 - Train loss: 17.96191 - Test loss: 53.96679\n",
      "Epoch 365 - lr: 0.05000 - Train loss: 17.96166 - Test loss: 53.96774\n",
      "Epoch 366 - lr: 0.05000 - Train loss: 17.96141 - Test loss: 53.96868\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 367 - lr: 0.05000 - Train loss: 17.96116 - Test loss: 53.96962\n",
      "Epoch 368 - lr: 0.05000 - Train loss: 17.96091 - Test loss: 53.97055\n",
      "Epoch 369 - lr: 0.05000 - Train loss: 17.96067 - Test loss: 53.97148\n",
      "Epoch 370 - lr: 0.05000 - Train loss: 17.96042 - Test loss: 53.97241\n",
      "Epoch 371 - lr: 0.05000 - Train loss: 17.96018 - Test loss: 53.97333\n",
      "Epoch 372 - lr: 0.05000 - Train loss: 17.95993 - Test loss: 53.97425\n",
      "Epoch 373 - lr: 0.05000 - Train loss: 17.95969 - Test loss: 53.97516\n",
      "Epoch 374 - lr: 0.05000 - Train loss: 17.95945 - Test loss: 53.97607\n",
      "Epoch 375 - lr: 0.05000 - Train loss: 17.95922 - Test loss: 53.97697\n",
      "Epoch 376 - lr: 0.05000 - Train loss: 17.95898 - Test loss: 53.97787\n",
      "Epoch 377 - lr: 0.05000 - Train loss: 17.95874 - Test loss: 53.97877\n",
      "Epoch 378 - lr: 0.05000 - Train loss: 17.95851 - Test loss: 53.97966\n",
      "Epoch 379 - lr: 0.05000 - Train loss: 17.95828 - Test loss: 53.98055\n",
      "Epoch 380 - lr: 0.05000 - Train loss: 17.95804 - Test loss: 53.98143\n",
      "Epoch 381 - lr: 0.05000 - Train loss: 17.95781 - Test loss: 53.98231\n",
      "Epoch 382 - lr: 0.05000 - Train loss: 17.95758 - Test loss: 53.98319\n",
      "Epoch 383 - lr: 0.05000 - Train loss: 17.95736 - Test loss: 53.98406\n",
      "Epoch 384 - lr: 0.05000 - Train loss: 17.95713 - Test loss: 53.98493\n",
      "Epoch 385 - lr: 0.05000 - Train loss: 17.95690 - Test loss: 53.98580\n",
      "Epoch 386 - lr: 0.05000 - Train loss: 17.95668 - Test loss: 53.98666\n",
      "Epoch 387 - lr: 0.05000 - Train loss: 17.95646 - Test loss: 53.98751\n",
      "Epoch 388 - lr: 0.05000 - Train loss: 17.95624 - Test loss: 53.98837\n",
      "Epoch 389 - lr: 0.05000 - Train loss: 17.95602 - Test loss: 53.98922\n",
      "Epoch 390 - lr: 0.05000 - Train loss: 17.95580 - Test loss: 53.99006\n",
      "Epoch 391 - lr: 0.05000 - Train loss: 17.95558 - Test loss: 53.99090\n",
      "Epoch 392 - lr: 0.05000 - Train loss: 17.95536 - Test loss: 53.99174\n",
      "Epoch 393 - lr: 0.05000 - Train loss: 17.95514 - Test loss: 53.99257\n",
      "Epoch 394 - lr: 0.05000 - Train loss: 17.95493 - Test loss: 53.99341\n",
      "Epoch 395 - lr: 0.05000 - Train loss: 17.95472 - Test loss: 53.99423\n",
      "Epoch 396 - lr: 0.05000 - Train loss: 17.95450 - Test loss: 53.99505\n",
      "Epoch 397 - lr: 0.05000 - Train loss: 17.95429 - Test loss: 53.99587\n",
      "Epoch 398 - lr: 0.05000 - Train loss: 17.95408 - Test loss: 53.99669\n",
      "Epoch 399 - lr: 0.05000 - Train loss: 17.95387 - Test loss: 53.99750\n",
      "Epoch 400 - lr: 0.05000 - Train loss: 17.95367 - Test loss: 53.99831\n",
      "Epoch 401 - lr: 0.05000 - Train loss: 17.95346 - Test loss: 53.99911\n",
      "Epoch 402 - lr: 0.05000 - Train loss: 17.95325 - Test loss: 53.99991\n",
      "Epoch 403 - lr: 0.05000 - Train loss: 17.95305 - Test loss: 54.00071\n",
      "Epoch 404 - lr: 0.05000 - Train loss: 17.95285 - Test loss: 54.00150\n",
      "Epoch 405 - lr: 0.05000 - Train loss: 17.95264 - Test loss: 54.00230\n",
      "Epoch 406 - lr: 0.05000 - Train loss: 17.95244 - Test loss: 54.00308\n",
      "Epoch 407 - lr: 0.05000 - Train loss: 17.95224 - Test loss: 54.00386\n",
      "Epoch 408 - lr: 0.05000 - Train loss: 17.95204 - Test loss: 54.00464\n",
      "Epoch 409 - lr: 0.05000 - Train loss: 17.95184 - Test loss: 54.00542\n",
      "Epoch 410 - lr: 0.05000 - Train loss: 17.95165 - Test loss: 54.00619\n",
      "Epoch 411 - lr: 0.05000 - Train loss: 17.95145 - Test loss: 54.00696\n",
      "Epoch 412 - lr: 0.05000 - Train loss: 17.95126 - Test loss: 54.00773\n",
      "Epoch 413 - lr: 0.05000 - Train loss: 17.95106 - Test loss: 54.00849\n",
      "Epoch 414 - lr: 0.05000 - Train loss: 17.95087 - Test loss: 54.00925\n",
      "Epoch 415 - lr: 0.05000 - Train loss: 17.95068 - Test loss: 54.01001\n",
      "Epoch 416 - lr: 0.05000 - Train loss: 17.95049 - Test loss: 54.01076\n",
      "Epoch 417 - lr: 0.05000 - Train loss: 17.95030 - Test loss: 54.01151\n",
      "Epoch 418 - lr: 0.05000 - Train loss: 17.95011 - Test loss: 54.01225\n",
      "Epoch 419 - lr: 0.05000 - Train loss: 17.94992 - Test loss: 54.01300\n",
      "Epoch 420 - lr: 0.05000 - Train loss: 17.94973 - Test loss: 54.01373\n",
      "Epoch 421 - lr: 0.05000 - Train loss: 17.94955 - Test loss: 54.01447\n",
      "Epoch 422 - lr: 0.05000 - Train loss: 17.94936 - Test loss: 54.01520\n",
      "Epoch 423 - lr: 0.05000 - Train loss: 17.94918 - Test loss: 54.01593\n",
      "Epoch 424 - lr: 0.05000 - Train loss: 17.94899 - Test loss: 54.01666\n",
      "Epoch 425 - lr: 0.05000 - Train loss: 17.94881 - Test loss: 54.01738\n",
      "Epoch 426 - lr: 0.05000 - Train loss: 17.94863 - Test loss: 54.01810\n",
      "Epoch 427 - lr: 0.05000 - Train loss: 17.94845 - Test loss: 54.01882\n",
      "Epoch 428 - lr: 0.05000 - Train loss: 17.94827 - Test loss: 54.01953\n",
      "Epoch 429 - lr: 0.05000 - Train loss: 17.94809 - Test loss: 54.02024\n",
      "Epoch 430 - lr: 0.05000 - Train loss: 17.94791 - Test loss: 54.02095\n",
      "Epoch 431 - lr: 0.05000 - Train loss: 17.94774 - Test loss: 54.02165\n",
      "Epoch 432 - lr: 0.05000 - Train loss: 17.94756 - Test loss: 54.02235\n",
      "Epoch 433 - lr: 0.05000 - Train loss: 17.94739 - Test loss: 54.02305\n",
      "Epoch 434 - lr: 0.05000 - Train loss: 17.94721 - Test loss: 54.02374\n",
      "Epoch 435 - lr: 0.05000 - Train loss: 17.94704 - Test loss: 54.02444\n",
      "Epoch 436 - lr: 0.05000 - Train loss: 17.94687 - Test loss: 54.02512\n",
      "Epoch 437 - lr: 0.05000 - Train loss: 17.94670 - Test loss: 54.02581\n",
      "Epoch 438 - lr: 0.05000 - Train loss: 17.94652 - Test loss: 54.02649\n",
      "Epoch 439 - lr: 0.05000 - Train loss: 17.94635 - Test loss: 54.02717\n",
      "Epoch 440 - lr: 0.05000 - Train loss: 17.94619 - Test loss: 54.02785\n",
      "Epoch 441 - lr: 0.05000 - Train loss: 17.94602 - Test loss: 54.02852\n",
      "Epoch 442 - lr: 0.05000 - Train loss: 17.94585 - Test loss: 54.02919\n",
      "Epoch 443 - lr: 0.05000 - Train loss: 17.94568 - Test loss: 54.02986\n",
      "Epoch 444 - lr: 0.05000 - Train loss: 17.94552 - Test loss: 54.03052\n",
      "Epoch 445 - lr: 0.05000 - Train loss: 17.94535 - Test loss: 54.03119\n",
      "Epoch 446 - lr: 0.05000 - Train loss: 17.94519 - Test loss: 54.03185\n",
      "Epoch 447 - lr: 0.05000 - Train loss: 17.94503 - Test loss: 54.03250\n",
      "Epoch 448 - lr: 0.05000 - Train loss: 17.94487 - Test loss: 54.03315\n",
      "Epoch 449 - lr: 0.05000 - Train loss: 17.94470 - Test loss: 54.03380\n",
      "Epoch 450 - lr: 0.05000 - Train loss: 17.94454 - Test loss: 54.03445\n",
      "Epoch 451 - lr: 0.05000 - Train loss: 17.94438 - Test loss: 54.03510\n",
      "Epoch 452 - lr: 0.05000 - Train loss: 17.94422 - Test loss: 54.03574\n",
      "Epoch 453 - lr: 0.05000 - Train loss: 17.94407 - Test loss: 54.03638\n",
      "Epoch 454 - lr: 0.05000 - Train loss: 17.94391 - Test loss: 54.03701\n",
      "Epoch 455 - lr: 0.05000 - Train loss: 17.94375 - Test loss: 54.03765\n",
      "Epoch 456 - lr: 0.05000 - Train loss: 17.94360 - Test loss: 54.03828\n",
      "Epoch 457 - lr: 0.05000 - Train loss: 17.94344 - Test loss: 54.03891\n",
      "Epoch 458 - lr: 0.05000 - Train loss: 17.94329 - Test loss: 54.03953\n",
      "Epoch 459 - lr: 0.05000 - Train loss: 17.94313 - Test loss: 54.04015\n",
      "Epoch 460 - lr: 0.05000 - Train loss: 17.94298 - Test loss: 54.04077\n",
      "Epoch 461 - lr: 0.05000 - Train loss: 17.94283 - Test loss: 54.04139\n",
      "Epoch 462 - lr: 0.05000 - Train loss: 17.94268 - Test loss: 54.04200\n",
      "Epoch 463 - lr: 0.05000 - Train loss: 17.94253 - Test loss: 54.04262\n",
      "Epoch 464 - lr: 0.05000 - Train loss: 17.94238 - Test loss: 54.04322\n",
      "Epoch 465 - lr: 0.05000 - Train loss: 17.94223 - Test loss: 54.04383\n",
      "Epoch 466 - lr: 0.05000 - Train loss: 17.94208 - Test loss: 54.04443\n",
      "Epoch 467 - lr: 0.05000 - Train loss: 17.94193 - Test loss: 54.04504\n",
      "Epoch 468 - lr: 0.05000 - Train loss: 17.94178 - Test loss: 54.04563\n",
      "Epoch 469 - lr: 0.05000 - Train loss: 17.94164 - Test loss: 54.04623\n",
      "Epoch 470 - lr: 0.05000 - Train loss: 17.94149 - Test loss: 54.04682\n",
      "Epoch 471 - lr: 0.05000 - Train loss: 17.94135 - Test loss: 54.04741\n",
      "Epoch 472 - lr: 0.05000 - Train loss: 17.94120 - Test loss: 54.04800\n",
      "Epoch 473 - lr: 0.05000 - Train loss: 17.94106 - Test loss: 54.04859\n",
      "Epoch 474 - lr: 0.05000 - Train loss: 17.94092 - Test loss: 54.04917\n",
      "Epoch 475 - lr: 0.05000 - Train loss: 17.94077 - Test loss: 54.04975\n",
      "Epoch 476 - lr: 0.05000 - Train loss: 17.94063 - Test loss: 54.05033\n",
      "Epoch 477 - lr: 0.05000 - Train loss: 17.94049 - Test loss: 54.05090\n",
      "Epoch 478 - lr: 0.05000 - Train loss: 17.94035 - Test loss: 54.05148\n",
      "Epoch 479 - lr: 0.05000 - Train loss: 17.94021 - Test loss: 54.05205\n",
      "Epoch 480 - lr: 0.05000 - Train loss: 17.94007 - Test loss: 54.05262\n",
      "Epoch 481 - lr: 0.05000 - Train loss: 17.93994 - Test loss: 54.05318\n",
      "Epoch 482 - lr: 0.05000 - Train loss: 17.93980 - Test loss: 54.05375\n",
      "Epoch 483 - lr: 0.05000 - Train loss: 17.93966 - Test loss: 54.05431\n",
      "Epoch 484 - lr: 0.05000 - Train loss: 17.93953 - Test loss: 54.05486\n",
      "Epoch 485 - lr: 0.05000 - Train loss: 17.93939 - Test loss: 54.05542\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 486 - lr: 0.05000 - Train loss: 17.93925 - Test loss: 54.05597\n",
      "Epoch 487 - lr: 0.05000 - Train loss: 17.93912 - Test loss: 54.05652\n",
      "Epoch 488 - lr: 0.05000 - Train loss: 17.93899 - Test loss: 54.05707\n",
      "Epoch 489 - lr: 0.05000 - Train loss: 17.93885 - Test loss: 54.05762\n",
      "Epoch 490 - lr: 0.05000 - Train loss: 17.93872 - Test loss: 54.05816\n",
      "Epoch 491 - lr: 0.05000 - Train loss: 17.93859 - Test loss: 54.05871\n",
      "Epoch 492 - lr: 0.05000 - Train loss: 17.93846 - Test loss: 54.05925\n",
      "Epoch 493 - lr: 0.05000 - Train loss: 17.93833 - Test loss: 54.05978\n",
      "Epoch 494 - lr: 0.05000 - Train loss: 17.93820 - Test loss: 54.06032\n",
      "Epoch 495 - lr: 0.05000 - Train loss: 17.93807 - Test loss: 54.06085\n",
      "Epoch 496 - lr: 0.05000 - Train loss: 17.93794 - Test loss: 54.06138\n",
      "Epoch 497 - lr: 0.05000 - Train loss: 17.93781 - Test loss: 54.06191\n",
      "Epoch 498 - lr: 0.05000 - Train loss: 17.93769 - Test loss: 54.06243\n",
      "Epoch 499 - lr: 0.05000 - Train loss: 17.93756 - Test loss: 54.06296\n",
      "Epoch 500 - lr: 0.05000 - Train loss: 17.93743 - Test loss: 54.06348\n",
      "Epoch 501 - lr: 0.05000 - Train loss: 17.93731 - Test loss: 54.06400\n",
      "Epoch 502 - lr: 0.05000 - Train loss: 17.93718 - Test loss: 54.06451\n",
      "Epoch 503 - lr: 0.05000 - Train loss: 17.93706 - Test loss: 54.06503\n",
      "Epoch 504 - lr: 0.05000 - Train loss: 17.93693 - Test loss: 54.06554\n",
      "Epoch 505 - lr: 0.05000 - Train loss: 17.93681 - Test loss: 54.06605\n",
      "Epoch 506 - lr: 0.05000 - Train loss: 17.93669 - Test loss: 54.06656\n",
      "Epoch 507 - lr: 0.05000 - Train loss: 17.93656 - Test loss: 54.06706\n",
      "Epoch 508 - lr: 0.05000 - Train loss: 17.93644 - Test loss: 54.06757\n",
      "Epoch 509 - lr: 0.05000 - Train loss: 17.93632 - Test loss: 54.06807\n",
      "Epoch 510 - lr: 0.05000 - Train loss: 17.93620 - Test loss: 54.06857\n",
      "Epoch 511 - lr: 0.05000 - Train loss: 17.93608 - Test loss: 54.06906\n",
      "Epoch 512 - lr: 0.05000 - Train loss: 17.93596 - Test loss: 54.06956\n",
      "Epoch 513 - lr: 0.05000 - Train loss: 17.93584 - Test loss: 54.07005\n",
      "Epoch 514 - lr: 0.05000 - Train loss: 17.93572 - Test loss: 54.07054\n",
      "Epoch 515 - lr: 0.05000 - Train loss: 17.93561 - Test loss: 54.07103\n",
      "Epoch 516 - lr: 0.05000 - Train loss: 17.93549 - Test loss: 54.07152\n",
      "Epoch 517 - lr: 0.05000 - Train loss: 17.93537 - Test loss: 54.07200\n",
      "Epoch 518 - lr: 0.05000 - Train loss: 17.93526 - Test loss: 54.07249\n",
      "Epoch 519 - lr: 0.05000 - Train loss: 17.93514 - Test loss: 54.07297\n",
      "Epoch 520 - lr: 0.05000 - Train loss: 17.93503 - Test loss: 54.07344\n",
      "Epoch 521 - lr: 0.05000 - Train loss: 17.93491 - Test loss: 54.07392\n",
      "Epoch 522 - lr: 0.05000 - Train loss: 17.93480 - Test loss: 54.07439\n",
      "Epoch 523 - lr: 0.05000 - Train loss: 17.93468 - Test loss: 54.07487\n",
      "Epoch 524 - lr: 0.05000 - Train loss: 17.93457 - Test loss: 54.07534\n",
      "Epoch 525 - lr: 0.05000 - Train loss: 17.93446 - Test loss: 54.07581\n",
      "Epoch 526 - lr: 0.05000 - Train loss: 17.93435 - Test loss: 54.07627\n",
      "Epoch 527 - lr: 0.05000 - Train loss: 17.93423 - Test loss: 54.07674\n",
      "Epoch 528 - lr: 0.05000 - Train loss: 17.93412 - Test loss: 54.07720\n",
      "Epoch 529 - lr: 0.05000 - Train loss: 17.93401 - Test loss: 54.07766\n",
      "Epoch 530 - lr: 0.05000 - Train loss: 17.93390 - Test loss: 54.07812\n",
      "Epoch 531 - lr: 0.05000 - Train loss: 17.93379 - Test loss: 54.07857\n",
      "Epoch 532 - lr: 0.05000 - Train loss: 17.93368 - Test loss: 54.07903\n",
      "Epoch 533 - lr: 0.05000 - Train loss: 17.93357 - Test loss: 54.07948\n",
      "Epoch 534 - lr: 0.05000 - Train loss: 17.93347 - Test loss: 54.07993\n",
      "Epoch 535 - lr: 0.05000 - Train loss: 17.93336 - Test loss: 54.08038\n",
      "Epoch 536 - lr: 0.05000 - Train loss: 17.93325 - Test loss: 54.08083\n",
      "Epoch 537 - lr: 0.05000 - Train loss: 17.93315 - Test loss: 54.08127\n",
      "Epoch 538 - lr: 0.05000 - Train loss: 17.93304 - Test loss: 54.08171\n",
      "Epoch 539 - lr: 0.05000 - Train loss: 17.93293 - Test loss: 54.08216\n",
      "Epoch 540 - lr: 0.05000 - Train loss: 17.93283 - Test loss: 54.08259\n",
      "Epoch 541 - lr: 0.05000 - Train loss: 17.93272 - Test loss: 54.08303\n",
      "Epoch 542 - lr: 0.05000 - Train loss: 17.93262 - Test loss: 54.08347\n",
      "Epoch 543 - lr: 0.05000 - Train loss: 17.93251 - Test loss: 54.08390\n",
      "Epoch 544 - lr: 0.05000 - Train loss: 17.93241 - Test loss: 54.08433\n",
      "Epoch 545 - lr: 0.05000 - Train loss: 17.93231 - Test loss: 54.08476\n",
      "Epoch 546 - lr: 0.05000 - Train loss: 17.93221 - Test loss: 54.08519\n",
      "Epoch 547 - lr: 0.05000 - Train loss: 17.93210 - Test loss: 54.08562\n",
      "Epoch 548 - lr: 0.05000 - Train loss: 17.93200 - Test loss: 54.08604\n",
      "Epoch 549 - lr: 0.05000 - Train loss: 17.93190 - Test loss: 54.08647\n",
      "Epoch 550 - lr: 0.05000 - Train loss: 17.93180 - Test loss: 54.08689\n",
      "Epoch 551 - lr: 0.05000 - Train loss: 17.93170 - Test loss: 54.08731\n",
      "Epoch 552 - lr: 0.05000 - Train loss: 17.93160 - Test loss: 54.08772\n",
      "Epoch 553 - lr: 0.05000 - Train loss: 17.93150 - Test loss: 54.08814\n",
      "Epoch 554 - lr: 0.05000 - Train loss: 17.93140 - Test loss: 54.08855\n",
      "Epoch 555 - lr: 0.05000 - Train loss: 17.93130 - Test loss: 54.08896\n",
      "Epoch 556 - lr: 0.05000 - Train loss: 17.93121 - Test loss: 54.08938\n",
      "Epoch 557 - lr: 0.05000 - Train loss: 17.93111 - Test loss: 54.08978\n",
      "Epoch 558 - lr: 0.05000 - Train loss: 17.93101 - Test loss: 54.09019\n",
      "Epoch 559 - lr: 0.05000 - Train loss: 17.93091 - Test loss: 54.09060\n",
      "Epoch 560 - lr: 0.05000 - Train loss: 17.93082 - Test loss: 54.09100\n",
      "Epoch 561 - lr: 0.05000 - Train loss: 17.93072 - Test loss: 54.09140\n",
      "Epoch 562 - lr: 0.05000 - Train loss: 17.93063 - Test loss: 54.09180\n",
      "Epoch 563 - lr: 0.05000 - Train loss: 17.93053 - Test loss: 54.09220\n",
      "Epoch 564 - lr: 0.05000 - Train loss: 17.93044 - Test loss: 54.09260\n",
      "Epoch 565 - lr: 0.05000 - Train loss: 17.93034 - Test loss: 54.09299\n",
      "Epoch 566 - lr: 0.05000 - Train loss: 17.93025 - Test loss: 54.09339\n",
      "Epoch 567 - lr: 0.05000 - Train loss: 17.93015 - Test loss: 54.09378\n",
      "Epoch 568 - lr: 0.05000 - Train loss: 17.93006 - Test loss: 54.09417\n",
      "Epoch 569 - lr: 0.05000 - Train loss: 17.92997 - Test loss: 54.09456\n",
      "Epoch 570 - lr: 0.05000 - Train loss: 17.92988 - Test loss: 54.09494\n",
      "Epoch 571 - lr: 0.05000 - Train loss: 17.92978 - Test loss: 54.09533\n",
      "Epoch 572 - lr: 0.05000 - Train loss: 17.92969 - Test loss: 54.09571\n",
      "Epoch 573 - lr: 0.05000 - Train loss: 17.92960 - Test loss: 54.09609\n",
      "Epoch 574 - lr: 0.05000 - Train loss: 17.92951 - Test loss: 54.09647\n",
      "Epoch 575 - lr: 0.05000 - Train loss: 17.92942 - Test loss: 54.09685\n",
      "Epoch 576 - lr: 0.05000 - Train loss: 17.92933 - Test loss: 54.09723\n",
      "Epoch 577 - lr: 0.05000 - Train loss: 17.92924 - Test loss: 54.09761\n",
      "Epoch 578 - lr: 0.05000 - Train loss: 17.92915 - Test loss: 54.09798\n",
      "Epoch 579 - lr: 0.05000 - Train loss: 17.92906 - Test loss: 54.09835\n",
      "Epoch 580 - lr: 0.05000 - Train loss: 17.92897 - Test loss: 54.09872\n",
      "Epoch 581 - lr: 0.05000 - Train loss: 17.92888 - Test loss: 54.09909\n",
      "Epoch 582 - lr: 0.05000 - Train loss: 17.92880 - Test loss: 54.09946\n",
      "Epoch 583 - lr: 0.05000 - Train loss: 17.92871 - Test loss: 54.09983\n",
      "Epoch 584 - lr: 0.05000 - Train loss: 17.92862 - Test loss: 54.10019\n",
      "Epoch 585 - lr: 0.05000 - Train loss: 17.92854 - Test loss: 54.10056\n",
      "Epoch 586 - lr: 0.05000 - Train loss: 17.92845 - Test loss: 54.10092\n",
      "Epoch 587 - lr: 0.05000 - Train loss: 17.92836 - Test loss: 54.10128\n",
      "Epoch 588 - lr: 0.05000 - Train loss: 17.92828 - Test loss: 54.10164\n",
      "Epoch 589 - lr: 0.05000 - Train loss: 17.92819 - Test loss: 54.10199\n",
      "Epoch 590 - lr: 0.05000 - Train loss: 17.92811 - Test loss: 54.10235\n",
      "Epoch 591 - lr: 0.05000 - Train loss: 17.92802 - Test loss: 54.10270\n",
      "Epoch 592 - lr: 0.05000 - Train loss: 17.92794 - Test loss: 54.10306\n",
      "Epoch 593 - lr: 0.05000 - Train loss: 17.92786 - Test loss: 54.10341\n",
      "Epoch 594 - lr: 0.05000 - Train loss: 17.92777 - Test loss: 54.10376\n",
      "Epoch 595 - lr: 0.05000 - Train loss: 17.92769 - Test loss: 54.10410\n",
      "Epoch 596 - lr: 0.05000 - Train loss: 17.92761 - Test loss: 54.10445\n",
      "Epoch 597 - lr: 0.05000 - Train loss: 17.92752 - Test loss: 54.10480\n",
      "Epoch 598 - lr: 0.05000 - Train loss: 17.92744 - Test loss: 54.10514\n",
      "Epoch 599 - lr: 0.05000 - Train loss: 17.92736 - Test loss: 54.10548\n",
      "Epoch 600 - lr: 0.05000 - Train loss: 17.92728 - Test loss: 54.10582\n",
      "Epoch 601 - lr: 0.05000 - Train loss: 17.92720 - Test loss: 54.10616\n",
      "Epoch 602 - lr: 0.05000 - Train loss: 17.92712 - Test loss: 54.10650\n",
      "Epoch 603 - lr: 0.05000 - Train loss: 17.92704 - Test loss: 54.10684\n",
      "Epoch 604 - lr: 0.05000 - Train loss: 17.92696 - Test loss: 54.10717\n",
      "Epoch 605 - lr: 0.05000 - Train loss: 17.92688 - Test loss: 54.10751\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 606 - lr: 0.05000 - Train loss: 17.92680 - Test loss: 54.10784\n",
      "Epoch 607 - lr: 0.05000 - Train loss: 17.92672 - Test loss: 54.10817\n",
      "Epoch 608 - lr: 0.05000 - Train loss: 17.92664 - Test loss: 54.10850\n",
      "Epoch 609 - lr: 0.05000 - Train loss: 17.92656 - Test loss: 54.10883\n",
      "Epoch 610 - lr: 0.05000 - Train loss: 17.92648 - Test loss: 54.10916\n",
      "Epoch 611 - lr: 0.05000 - Train loss: 17.92641 - Test loss: 54.10948\n",
      "Epoch 612 - lr: 0.05000 - Train loss: 17.92633 - Test loss: 54.10981\n",
      "Epoch 613 - lr: 0.05000 - Train loss: 17.92625 - Test loss: 54.11013\n",
      "Epoch 614 - lr: 0.05000 - Train loss: 17.92617 - Test loss: 54.11045\n",
      "Epoch 615 - lr: 0.05000 - Train loss: 17.92610 - Test loss: 54.11077\n",
      "Epoch 616 - lr: 0.05000 - Train loss: 17.92602 - Test loss: 54.11109\n",
      "Epoch 617 - lr: 0.05000 - Train loss: 17.92594 - Test loss: 54.11141\n",
      "Epoch 618 - lr: 0.05000 - Train loss: 17.92587 - Test loss: 54.11172\n",
      "Epoch 619 - lr: 0.05000 - Train loss: 17.92579 - Test loss: 54.11204\n",
      "Epoch 620 - lr: 0.05000 - Train loss: 17.92572 - Test loss: 54.11235\n",
      "Epoch 621 - lr: 0.05000 - Train loss: 17.92564 - Test loss: 54.11266\n",
      "Epoch 622 - lr: 0.05000 - Train loss: 17.92557 - Test loss: 54.11297\n",
      "Epoch 623 - lr: 0.05000 - Train loss: 17.92550 - Test loss: 54.11328\n",
      "Epoch 624 - lr: 0.05000 - Train loss: 17.92542 - Test loss: 54.11359\n",
      "Epoch 625 - lr: 0.05000 - Train loss: 17.92535 - Test loss: 54.11390\n",
      "Epoch 626 - lr: 0.05000 - Train loss: 17.92528 - Test loss: 54.11420\n",
      "Epoch 627 - lr: 0.05000 - Train loss: 17.92520 - Test loss: 54.11451\n",
      "Epoch 628 - lr: 0.05000 - Train loss: 17.92513 - Test loss: 54.11481\n",
      "Epoch 629 - lr: 0.05000 - Train loss: 17.92506 - Test loss: 54.11511\n",
      "Epoch 630 - lr: 0.05000 - Train loss: 17.92499 - Test loss: 54.11542\n",
      "Epoch 631 - lr: 0.05000 - Train loss: 17.92491 - Test loss: 54.11571\n",
      "Epoch 632 - lr: 0.05000 - Train loss: 17.92484 - Test loss: 54.11601\n",
      "Epoch 633 - lr: 0.05000 - Train loss: 17.92477 - Test loss: 54.11631\n",
      "Epoch 634 - lr: 0.05000 - Train loss: 17.92470 - Test loss: 54.11661\n",
      "Epoch 635 - lr: 0.05000 - Train loss: 17.92463 - Test loss: 54.11690\n",
      "Epoch 636 - lr: 0.05000 - Train loss: 17.92456 - Test loss: 54.11719\n",
      "Epoch 637 - lr: 0.05000 - Train loss: 17.92449 - Test loss: 54.11748\n",
      "Epoch 638 - lr: 0.05000 - Train loss: 17.92442 - Test loss: 54.11778\n",
      "Epoch 639 - lr: 0.05000 - Train loss: 17.92435 - Test loss: 54.11806\n",
      "Epoch 640 - lr: 0.05000 - Train loss: 17.92428 - Test loss: 54.11835\n",
      "Epoch 641 - lr: 0.05000 - Train loss: 17.92421 - Test loss: 54.11864\n",
      "Epoch 642 - lr: 0.05000 - Train loss: 17.92414 - Test loss: 54.11893\n",
      "Epoch 643 - lr: 0.05000 - Train loss: 17.92408 - Test loss: 54.11921\n",
      "Epoch 644 - lr: 0.05000 - Train loss: 17.92401 - Test loss: 54.11949\n",
      "Epoch 645 - lr: 0.05000 - Train loss: 17.92394 - Test loss: 54.11978\n",
      "Epoch 646 - lr: 0.05000 - Train loss: 17.92387 - Test loss: 54.12006\n",
      "Epoch 647 - lr: 0.05000 - Train loss: 17.92380 - Test loss: 54.12034\n",
      "Epoch 648 - lr: 0.05000 - Train loss: 17.92374 - Test loss: 54.12062\n",
      "Epoch 649 - lr: 0.05000 - Train loss: 17.92367 - Test loss: 54.12089\n",
      "Epoch 650 - lr: 0.05000 - Train loss: 17.92360 - Test loss: 54.12117\n",
      "Epoch 651 - lr: 0.05000 - Train loss: 17.92354 - Test loss: 54.12145\n",
      "Epoch 652 - lr: 0.05000 - Train loss: 17.92347 - Test loss: 54.12172\n",
      "Epoch 653 - lr: 0.05000 - Train loss: 17.92341 - Test loss: 54.12199\n",
      "Epoch 654 - lr: 0.05000 - Train loss: 17.92334 - Test loss: 54.12226\n",
      "Epoch 655 - lr: 0.05000 - Train loss: 17.92328 - Test loss: 54.12254\n",
      "Epoch 656 - lr: 0.05000 - Train loss: 17.92321 - Test loss: 54.12281\n",
      "Epoch 657 - lr: 0.05000 - Train loss: 17.92315 - Test loss: 54.12307\n",
      "Epoch 658 - lr: 0.05000 - Train loss: 17.92308 - Test loss: 54.12334\n",
      "Epoch 659 - lr: 0.05000 - Train loss: 17.92302 - Test loss: 54.12361\n",
      "Epoch 660 - lr: 0.05000 - Train loss: 17.92295 - Test loss: 54.12387\n",
      "Epoch 661 - lr: 0.05000 - Train loss: 17.92289 - Test loss: 54.12414\n",
      "Epoch 662 - lr: 0.05000 - Train loss: 17.92283 - Test loss: 54.12440\n",
      "Epoch 663 - lr: 0.05000 - Train loss: 17.92276 - Test loss: 54.12466\n",
      "Epoch 664 - lr: 0.05000 - Train loss: 17.92270 - Test loss: 54.12492\n",
      "Epoch 665 - lr: 0.05000 - Train loss: 17.92264 - Test loss: 54.12518\n",
      "Epoch 666 - lr: 0.05000 - Train loss: 17.92258 - Test loss: 54.12544\n",
      "Epoch 667 - lr: 0.05000 - Train loss: 17.92251 - Test loss: 54.12570\n",
      "Epoch 668 - lr: 0.05000 - Train loss: 17.92245 - Test loss: 54.12595\n",
      "Epoch 669 - lr: 0.05000 - Train loss: 17.92239 - Test loss: 54.12621\n",
      "Epoch 670 - lr: 0.05000 - Train loss: 17.92233 - Test loss: 54.12646\n",
      "Epoch 671 - lr: 0.05000 - Train loss: 17.92227 - Test loss: 54.12672\n",
      "Epoch 672 - lr: 0.05000 - Train loss: 17.92221 - Test loss: 54.12697\n",
      "Epoch 673 - lr: 0.05000 - Train loss: 17.92214 - Test loss: 54.12722\n",
      "Epoch 674 - lr: 0.05000 - Train loss: 17.92208 - Test loss: 54.12747\n",
      "Epoch 675 - lr: 0.05000 - Train loss: 17.92202 - Test loss: 54.12772\n",
      "Epoch 676 - lr: 0.05000 - Train loss: 17.92196 - Test loss: 54.12797\n",
      "Epoch 677 - lr: 0.05000 - Train loss: 17.92190 - Test loss: 54.12821\n",
      "Epoch 678 - lr: 0.05000 - Train loss: 17.92184 - Test loss: 54.12846\n",
      "Epoch 679 - lr: 0.05000 - Train loss: 17.92178 - Test loss: 54.12870\n",
      "Epoch 680 - lr: 0.05000 - Train loss: 17.92173 - Test loss: 54.12895\n",
      "Epoch 681 - lr: 0.05000 - Train loss: 17.92167 - Test loss: 54.12919\n",
      "Epoch 682 - lr: 0.05000 - Train loss: 17.92161 - Test loss: 54.12943\n",
      "Epoch 683 - lr: 0.05000 - Train loss: 17.92155 - Test loss: 54.12967\n",
      "Epoch 684 - lr: 0.05000 - Train loss: 17.92149 - Test loss: 54.12991\n",
      "Epoch 685 - lr: 0.05000 - Train loss: 17.92143 - Test loss: 54.13015\n",
      "Epoch 686 - lr: 0.05000 - Train loss: 17.92138 - Test loss: 54.13039\n",
      "Epoch 687 - lr: 0.05000 - Train loss: 17.92132 - Test loss: 54.13062\n",
      "Epoch 688 - lr: 0.05000 - Train loss: 17.92126 - Test loss: 54.13086\n",
      "Epoch 689 - lr: 0.05000 - Train loss: 17.92120 - Test loss: 54.13109\n",
      "Epoch 690 - lr: 0.05000 - Train loss: 17.92115 - Test loss: 54.13133\n",
      "Epoch 691 - lr: 0.05000 - Train loss: 17.92109 - Test loss: 54.13156\n",
      "Epoch 692 - lr: 0.05000 - Train loss: 17.92103 - Test loss: 54.13179\n",
      "Epoch 693 - lr: 0.05000 - Train loss: 17.92098 - Test loss: 54.13202\n",
      "Epoch 694 - lr: 0.05000 - Train loss: 17.92092 - Test loss: 54.13225\n",
      "Epoch 695 - lr: 0.05000 - Train loss: 17.92086 - Test loss: 54.13248\n",
      "Epoch 696 - lr: 0.05000 - Train loss: 17.92081 - Test loss: 54.13271\n",
      "Epoch 697 - lr: 0.05000 - Train loss: 17.92075 - Test loss: 54.13294\n",
      "Epoch 698 - lr: 0.05000 - Train loss: 17.92070 - Test loss: 54.13316\n",
      "Epoch 699 - lr: 0.05000 - Train loss: 17.92064 - Test loss: 54.13339\n",
      "Epoch 700 - lr: 0.05000 - Train loss: 17.92059 - Test loss: 54.13361\n",
      "Epoch 701 - lr: 0.05000 - Train loss: 17.92053 - Test loss: 54.13383\n",
      "Epoch 702 - lr: 0.05000 - Train loss: 17.92048 - Test loss: 54.13406\n",
      "Epoch 703 - lr: 0.05000 - Train loss: 17.92042 - Test loss: 54.13428\n",
      "Epoch 704 - lr: 0.05000 - Train loss: 17.92037 - Test loss: 54.13450\n",
      "Epoch 705 - lr: 0.05000 - Train loss: 17.92032 - Test loss: 54.13472\n",
      "Epoch 706 - lr: 0.05000 - Train loss: 17.92026 - Test loss: 54.13493\n",
      "Epoch 707 - lr: 0.05000 - Train loss: 17.92021 - Test loss: 54.13515\n",
      "Epoch 708 - lr: 0.05000 - Train loss: 17.92016 - Test loss: 54.13537\n",
      "Epoch 709 - lr: 0.05000 - Train loss: 17.92010 - Test loss: 54.13558\n",
      "Epoch 710 - lr: 0.05000 - Train loss: 17.92005 - Test loss: 54.13580\n",
      "Epoch 711 - lr: 0.05000 - Train loss: 17.92000 - Test loss: 54.13601\n",
      "Epoch 712 - lr: 0.05000 - Train loss: 17.91995 - Test loss: 54.13622\n",
      "Epoch 713 - lr: 0.05000 - Train loss: 17.91989 - Test loss: 54.13644\n",
      "Epoch 714 - lr: 0.05000 - Train loss: 17.91984 - Test loss: 54.13665\n",
      "Epoch 715 - lr: 0.05000 - Train loss: 17.91979 - Test loss: 54.13686\n",
      "Epoch 716 - lr: 0.05000 - Train loss: 17.91974 - Test loss: 54.13707\n",
      "Epoch 717 - lr: 0.05000 - Train loss: 17.91969 - Test loss: 54.13728\n",
      "Epoch 718 - lr: 0.05000 - Train loss: 17.91963 - Test loss: 54.13748\n",
      "Epoch 719 - lr: 0.05000 - Train loss: 17.91958 - Test loss: 54.13769\n",
      "Epoch 720 - lr: 0.05000 - Train loss: 17.91953 - Test loss: 54.13790\n",
      "Epoch 721 - lr: 0.05000 - Train loss: 17.91948 - Test loss: 54.13810\n",
      "Epoch 722 - lr: 0.05000 - Train loss: 17.91943 - Test loss: 54.13830\n",
      "Epoch 723 - lr: 0.05000 - Train loss: 17.91938 - Test loss: 54.13851\n",
      "Epoch 724 - lr: 0.05000 - Train loss: 17.91933 - Test loss: 54.13871\n",
      "Epoch 725 - lr: 0.05000 - Train loss: 17.91928 - Test loss: 54.13891\n",
      "Epoch 726 - lr: 0.05000 - Train loss: 17.91923 - Test loss: 54.13911\n",
      "Epoch 727 - lr: 0.05000 - Train loss: 17.91918 - Test loss: 54.13931\n",
      "Epoch 728 - lr: 0.05000 - Train loss: 17.91913 - Test loss: 54.13951\n",
      "Epoch 729 - lr: 0.05000 - Train loss: 17.91908 - Test loss: 54.13971\n",
      "Epoch 730 - lr: 0.05000 - Train loss: 17.91903 - Test loss: 54.13991\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 731 - lr: 0.05000 - Train loss: 17.91898 - Test loss: 54.14010\n",
      "Epoch 732 - lr: 0.05000 - Train loss: 17.91894 - Test loss: 54.14030\n",
      "Epoch 733 - lr: 0.05000 - Train loss: 17.91889 - Test loss: 54.14049\n",
      "Epoch 734 - lr: 0.05000 - Train loss: 17.91884 - Test loss: 54.14069\n",
      "Epoch 735 - lr: 0.05000 - Train loss: 17.91879 - Test loss: 54.14088\n",
      "Epoch 736 - lr: 0.05000 - Train loss: 17.91874 - Test loss: 54.14107\n",
      "Epoch 737 - lr: 0.05000 - Train loss: 17.91869 - Test loss: 54.14126\n",
      "Epoch 738 - lr: 0.05000 - Train loss: 17.91865 - Test loss: 54.14145\n",
      "Epoch 739 - lr: 0.05000 - Train loss: 17.91860 - Test loss: 54.14164\n",
      "Epoch 740 - lr: 0.05000 - Train loss: 17.91855 - Test loss: 54.14183\n",
      "Epoch 741 - lr: 0.05000 - Train loss: 17.91851 - Test loss: 54.14202\n",
      "Epoch 742 - lr: 0.05000 - Train loss: 17.91846 - Test loss: 54.14221\n",
      "Epoch 743 - lr: 0.05000 - Train loss: 17.91841 - Test loss: 54.14239\n",
      "Epoch 744 - lr: 0.05000 - Train loss: 17.91836 - Test loss: 54.14258\n",
      "Epoch 745 - lr: 0.05000 - Train loss: 17.91832 - Test loss: 54.14276\n",
      "Epoch 746 - lr: 0.05000 - Train loss: 17.91827 - Test loss: 54.14295\n",
      "Epoch 747 - lr: 0.05000 - Train loss: 17.91823 - Test loss: 54.14313\n",
      "Epoch 748 - lr: 0.05000 - Train loss: 17.91818 - Test loss: 54.14331\n",
      "Epoch 749 - lr: 0.05000 - Train loss: 17.91813 - Test loss: 54.14349\n",
      "Epoch 750 - lr: 0.05000 - Train loss: 17.91809 - Test loss: 54.14368\n",
      "Epoch 751 - lr: 0.05000 - Train loss: 17.91804 - Test loss: 54.14386\n",
      "Epoch 752 - lr: 0.05000 - Train loss: 17.91800 - Test loss: 54.14403\n",
      "Epoch 753 - lr: 0.05000 - Train loss: 17.91795 - Test loss: 54.14421\n",
      "Epoch 754 - lr: 0.05000 - Train loss: 17.91791 - Test loss: 54.14439\n",
      "Epoch 755 - lr: 0.05000 - Train loss: 17.91786 - Test loss: 54.14457\n",
      "Epoch 756 - lr: 0.05000 - Train loss: 17.91782 - Test loss: 54.14474\n",
      "Epoch 757 - lr: 0.05000 - Train loss: 17.91777 - Test loss: 54.14492\n",
      "Epoch 758 - lr: 0.05000 - Train loss: 17.91773 - Test loss: 54.14509\n",
      "Epoch 759 - lr: 0.05000 - Train loss: 17.91768 - Test loss: 54.14527\n",
      "Epoch 760 - lr: 0.05000 - Train loss: 17.91764 - Test loss: 54.14544\n",
      "Epoch 761 - lr: 0.05000 - Train loss: 17.91760 - Test loss: 54.14561\n",
      "Epoch 762 - lr: 0.05000 - Train loss: 17.91755 - Test loss: 54.14579\n",
      "Epoch 763 - lr: 0.05000 - Train loss: 17.91751 - Test loss: 54.14596\n",
      "Epoch 764 - lr: 0.05000 - Train loss: 17.91747 - Test loss: 54.14613\n",
      "Epoch 765 - lr: 0.05000 - Train loss: 17.91742 - Test loss: 54.14630\n",
      "Epoch 766 - lr: 0.05000 - Train loss: 17.91738 - Test loss: 54.14647\n",
      "Epoch 767 - lr: 0.05000 - Train loss: 17.91734 - Test loss: 54.14663\n",
      "Epoch 768 - lr: 0.05000 - Train loss: 17.91729 - Test loss: 54.14680\n",
      "Epoch 769 - lr: 0.05000 - Train loss: 17.91725 - Test loss: 54.14697\n",
      "Epoch 770 - lr: 0.05000 - Train loss: 17.91721 - Test loss: 54.14713\n",
      "Epoch 771 - lr: 0.05000 - Train loss: 17.91717 - Test loss: 54.14730\n",
      "Epoch 772 - lr: 0.05000 - Train loss: 17.91712 - Test loss: 54.14746\n",
      "Epoch 773 - lr: 0.05000 - Train loss: 17.91708 - Test loss: 54.14763\n",
      "Epoch 774 - lr: 0.05000 - Train loss: 17.91704 - Test loss: 54.14779\n",
      "Epoch 775 - lr: 0.05000 - Train loss: 17.91700 - Test loss: 54.14795\n",
      "Epoch 776 - lr: 0.05000 - Train loss: 17.91696 - Test loss: 54.14811\n",
      "Epoch 777 - lr: 0.05000 - Train loss: 17.91691 - Test loss: 54.14828\n",
      "Epoch 778 - lr: 0.05000 - Train loss: 17.91687 - Test loss: 54.14844\n",
      "Epoch 779 - lr: 0.05000 - Train loss: 17.91683 - Test loss: 54.14859\n",
      "Epoch 780 - lr: 0.05000 - Train loss: 17.91679 - Test loss: 54.14875\n",
      "Epoch 781 - lr: 0.05000 - Train loss: 17.91675 - Test loss: 54.14891\n",
      "Epoch 782 - lr: 0.05000 - Train loss: 17.91671 - Test loss: 54.14907\n",
      "Epoch 783 - lr: 0.05000 - Train loss: 17.91667 - Test loss: 54.14923\n",
      "Epoch 784 - lr: 0.05000 - Train loss: 17.91663 - Test loss: 54.14938\n",
      "Epoch 785 - lr: 0.05000 - Train loss: 17.91659 - Test loss: 54.14954\n",
      "Epoch 786 - lr: 0.05000 - Train loss: 17.91655 - Test loss: 54.14969\n",
      "Epoch 787 - lr: 0.05000 - Train loss: 17.91651 - Test loss: 54.14985\n",
      "Epoch 788 - lr: 0.05000 - Train loss: 17.91647 - Test loss: 54.15000\n",
      "Epoch 789 - lr: 0.05000 - Train loss: 17.91643 - Test loss: 54.15015\n",
      "Epoch 790 - lr: 0.05000 - Train loss: 17.91639 - Test loss: 54.15030\n",
      "Epoch 791 - lr: 0.05000 - Train loss: 17.91635 - Test loss: 54.15045\n",
      "Epoch 792 - lr: 0.05000 - Train loss: 17.91631 - Test loss: 54.15061\n",
      "Epoch 793 - lr: 0.05000 - Train loss: 17.91627 - Test loss: 54.15076\n",
      "Epoch 794 - lr: 0.05000 - Train loss: 17.91623 - Test loss: 54.15090\n",
      "Epoch 795 - lr: 0.05000 - Train loss: 17.91619 - Test loss: 54.15105\n",
      "Epoch 796 - lr: 0.05000 - Train loss: 17.91615 - Test loss: 54.15120\n",
      "Epoch 797 - lr: 0.05000 - Train loss: 17.91611 - Test loss: 54.15135\n",
      "Epoch 798 - lr: 0.05000 - Train loss: 17.91608 - Test loss: 54.15150\n",
      "Epoch 799 - lr: 0.05000 - Train loss: 17.91604 - Test loss: 54.15164\n",
      "Epoch 800 - lr: 0.05000 - Train loss: 17.91600 - Test loss: 54.15179\n",
      "Epoch 801 - lr: 0.05000 - Train loss: 17.91596 - Test loss: 54.15193\n",
      "Epoch 802 - lr: 0.05000 - Train loss: 17.91592 - Test loss: 54.15208\n",
      "Epoch 803 - lr: 0.05000 - Train loss: 17.91589 - Test loss: 54.15222\n",
      "Epoch 804 - lr: 0.05000 - Train loss: 17.91585 - Test loss: 54.15236\n",
      "Epoch 805 - lr: 0.05000 - Train loss: 17.91581 - Test loss: 54.15250\n",
      "Epoch 806 - lr: 0.05000 - Train loss: 17.91577 - Test loss: 54.15264\n",
      "Epoch 807 - lr: 0.05000 - Train loss: 17.91574 - Test loss: 54.15279\n",
      "Epoch 808 - lr: 0.05000 - Train loss: 17.91570 - Test loss: 54.15293\n",
      "Epoch 809 - lr: 0.05000 - Train loss: 17.91566 - Test loss: 54.15307\n",
      "Epoch 810 - lr: 0.05000 - Train loss: 17.91562 - Test loss: 54.15320\n",
      "Epoch 811 - lr: 0.05000 - Train loss: 17.91559 - Test loss: 54.15334\n",
      "Epoch 812 - lr: 0.05000 - Train loss: 17.91555 - Test loss: 54.15348\n",
      "Epoch 813 - lr: 0.05000 - Train loss: 17.91551 - Test loss: 54.15362\n",
      "Epoch 814 - lr: 0.05000 - Train loss: 17.91548 - Test loss: 54.15375\n",
      "Epoch 815 - lr: 0.05000 - Train loss: 17.91544 - Test loss: 54.15389\n",
      "Epoch 816 - lr: 0.05000 - Train loss: 17.91540 - Test loss: 54.15403\n",
      "Epoch 817 - lr: 0.05000 - Train loss: 17.91537 - Test loss: 54.15416\n",
      "Epoch 818 - lr: 0.05000 - Train loss: 17.91533 - Test loss: 54.15429\n",
      "Epoch 819 - lr: 0.05000 - Train loss: 17.91530 - Test loss: 54.15443\n",
      "Epoch 820 - lr: 0.05000 - Train loss: 17.91526 - Test loss: 54.15456\n",
      "Epoch 821 - lr: 0.05000 - Train loss: 17.91522 - Test loss: 54.15469\n",
      "Epoch 822 - lr: 0.05000 - Train loss: 17.91519 - Test loss: 54.15482\n",
      "Epoch 823 - lr: 0.05000 - Train loss: 17.91515 - Test loss: 54.15496\n",
      "Epoch 824 - lr: 0.05000 - Train loss: 17.91512 - Test loss: 54.15509\n",
      "Epoch 825 - lr: 0.05000 - Train loss: 17.91508 - Test loss: 54.15522\n",
      "Epoch 826 - lr: 0.05000 - Train loss: 17.91505 - Test loss: 54.15535\n",
      "Epoch 827 - lr: 0.05000 - Train loss: 17.91501 - Test loss: 54.15547\n",
      "Epoch 828 - lr: 0.05000 - Train loss: 17.91498 - Test loss: 54.15560\n",
      "Epoch 829 - lr: 0.05000 - Train loss: 17.91494 - Test loss: 54.15573\n",
      "Epoch 830 - lr: 0.05000 - Train loss: 17.91491 - Test loss: 54.15586\n",
      "Epoch 831 - lr: 0.05000 - Train loss: 17.91488 - Test loss: 54.15598\n",
      "Epoch 832 - lr: 0.05000 - Train loss: 17.91484 - Test loss: 54.15611\n",
      "Epoch 833 - lr: 0.05000 - Train loss: 17.91481 - Test loss: 54.15623\n",
      "Epoch 834 - lr: 0.05000 - Train loss: 17.91477 - Test loss: 54.15636\n",
      "Epoch 835 - lr: 0.05000 - Train loss: 17.91474 - Test loss: 54.15648\n",
      "Epoch 836 - lr: 0.05000 - Train loss: 17.91470 - Test loss: 54.15661\n",
      "Epoch 837 - lr: 0.05000 - Train loss: 17.91467 - Test loss: 54.15673\n",
      "Epoch 838 - lr: 0.05000 - Train loss: 17.91464 - Test loss: 54.15685\n",
      "Epoch 839 - lr: 0.05000 - Train loss: 17.91460 - Test loss: 54.15697\n",
      "Epoch 840 - lr: 0.05000 - Train loss: 17.91457 - Test loss: 54.15709\n",
      "Epoch 841 - lr: 0.05000 - Train loss: 17.91454 - Test loss: 54.15722\n",
      "Epoch 842 - lr: 0.05000 - Train loss: 17.91450 - Test loss: 54.15734\n",
      "Epoch 843 - lr: 0.05000 - Train loss: 17.91447 - Test loss: 54.15745\n",
      "Epoch 844 - lr: 0.05000 - Train loss: 17.91444 - Test loss: 54.15757\n",
      "Epoch 845 - lr: 0.05000 - Train loss: 17.91440 - Test loss: 54.15769\n",
      "Epoch 846 - lr: 0.05000 - Train loss: 17.91437 - Test loss: 54.15781\n",
      "Epoch 847 - lr: 0.05000 - Train loss: 17.91434 - Test loss: 54.15793\n",
      "Epoch 848 - lr: 0.05000 - Train loss: 17.91431 - Test loss: 54.15804\n",
      "Epoch 849 - lr: 0.05000 - Train loss: 17.91427 - Test loss: 54.15816\n",
      "Epoch 850 - lr: 0.05000 - Train loss: 17.91424 - Test loss: 54.15828\n",
      "Epoch 851 - lr: 0.05000 - Train loss: 17.91421 - Test loss: 54.15839\n",
      "Epoch 852 - lr: 0.05000 - Train loss: 17.91418 - Test loss: 54.15851\n",
      "Epoch 853 - lr: 0.05000 - Train loss: 17.91415 - Test loss: 54.15862\n",
      "Epoch 854 - lr: 0.05000 - Train loss: 17.91411 - Test loss: 54.15873\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 855 - lr: 0.05000 - Train loss: 17.91408 - Test loss: 54.15885\n",
      "Epoch 856 - lr: 0.05000 - Train loss: 17.91405 - Test loss: 54.15896\n",
      "Epoch 857 - lr: 0.05000 - Train loss: 17.91402 - Test loss: 54.15907\n",
      "Epoch 858 - lr: 0.05000 - Train loss: 17.91399 - Test loss: 54.15918\n",
      "Epoch 859 - lr: 0.05000 - Train loss: 17.91396 - Test loss: 54.15929\n",
      "Epoch 860 - lr: 0.05000 - Train loss: 17.91392 - Test loss: 54.15940\n",
      "Epoch 861 - lr: 0.05000 - Train loss: 17.91389 - Test loss: 54.15951\n",
      "Epoch 862 - lr: 0.05000 - Train loss: 17.91386 - Test loss: 54.15962\n",
      "Epoch 863 - lr: 0.05000 - Train loss: 17.91383 - Test loss: 54.15973\n",
      "Epoch 864 - lr: 0.05000 - Train loss: 17.91380 - Test loss: 54.15984\n",
      "Epoch 865 - lr: 0.05000 - Train loss: 17.91377 - Test loss: 54.15995\n",
      "Epoch 866 - lr: 0.05000 - Train loss: 17.91374 - Test loss: 54.16006\n",
      "Epoch 867 - lr: 0.05000 - Train loss: 17.91371 - Test loss: 54.16016\n",
      "Epoch 868 - lr: 0.05000 - Train loss: 17.91368 - Test loss: 54.16027\n",
      "Epoch 869 - lr: 0.05000 - Train loss: 17.91365 - Test loss: 54.16038\n",
      "Epoch 870 - lr: 0.05000 - Train loss: 17.91362 - Test loss: 54.16048\n",
      "Epoch 871 - lr: 0.05000 - Train loss: 17.91359 - Test loss: 54.16059\n",
      "Epoch 872 - lr: 0.05000 - Train loss: 17.91356 - Test loss: 54.16069\n",
      "Epoch 873 - lr: 0.05000 - Train loss: 17.91353 - Test loss: 54.16079\n",
      "Epoch 874 - lr: 0.05000 - Train loss: 17.91350 - Test loss: 54.16090\n",
      "Epoch 875 - lr: 0.05000 - Train loss: 17.91347 - Test loss: 54.16100\n",
      "Epoch 876 - lr: 0.05000 - Train loss: 17.91344 - Test loss: 54.16110\n",
      "Epoch 877 - lr: 0.05000 - Train loss: 17.91341 - Test loss: 54.16120\n",
      "Epoch 878 - lr: 0.05000 - Train loss: 17.91338 - Test loss: 54.16130\n",
      "Epoch 879 - lr: 0.05000 - Train loss: 17.91335 - Test loss: 54.16141\n",
      "Epoch 880 - lr: 0.05000 - Train loss: 17.91332 - Test loss: 54.16151\n",
      "Epoch 881 - lr: 0.05000 - Train loss: 17.91329 - Test loss: 54.16161\n",
      "Epoch 882 - lr: 0.05000 - Train loss: 17.91326 - Test loss: 54.16171\n",
      "Epoch 883 - lr: 0.05000 - Train loss: 17.91323 - Test loss: 54.16180\n",
      "Epoch 884 - lr: 0.05000 - Train loss: 17.91320 - Test loss: 54.16190\n",
      "Epoch 885 - lr: 0.05000 - Train loss: 17.91317 - Test loss: 54.16200\n",
      "Epoch 886 - lr: 0.05000 - Train loss: 17.91315 - Test loss: 54.16210\n",
      "Epoch 887 - lr: 0.05000 - Train loss: 17.91312 - Test loss: 54.16219\n",
      "Epoch 888 - lr: 0.05000 - Train loss: 17.91309 - Test loss: 54.16229\n",
      "Epoch 889 - lr: 0.05000 - Train loss: 17.91306 - Test loss: 54.16239\n",
      "Epoch 890 - lr: 0.05000 - Train loss: 17.91303 - Test loss: 54.16248\n",
      "Epoch 891 - lr: 0.05000 - Train loss: 17.91300 - Test loss: 54.16258\n",
      "Epoch 892 - lr: 0.05000 - Train loss: 17.91297 - Test loss: 54.16267\n",
      "Epoch 893 - lr: 0.05000 - Train loss: 17.91295 - Test loss: 54.16277\n",
      "Epoch 894 - lr: 0.05000 - Train loss: 17.91292 - Test loss: 54.16286\n",
      "Epoch 895 - lr: 0.05000 - Train loss: 17.91289 - Test loss: 54.16295\n",
      "Epoch 896 - lr: 0.05000 - Train loss: 17.91286 - Test loss: 54.16305\n",
      "Epoch 897 - lr: 0.05000 - Train loss: 17.91284 - Test loss: 54.16314\n",
      "Epoch 898 - lr: 0.05000 - Train loss: 17.91281 - Test loss: 54.16323\n",
      "Epoch 899 - lr: 0.05000 - Train loss: 17.91278 - Test loss: 54.16332\n",
      "Epoch 900 - lr: 0.05000 - Train loss: 17.91275 - Test loss: 54.16341\n",
      "Epoch 901 - lr: 0.05000 - Train loss: 17.91273 - Test loss: 54.16350\n",
      "Epoch 902 - lr: 0.05000 - Train loss: 17.91270 - Test loss: 54.16359\n",
      "Epoch 903 - lr: 0.05000 - Train loss: 17.91267 - Test loss: 54.16368\n",
      "Epoch 904 - lr: 0.05000 - Train loss: 17.91264 - Test loss: 54.16377\n",
      "Epoch 905 - lr: 0.05000 - Train loss: 17.91262 - Test loss: 54.16386\n",
      "Epoch 906 - lr: 0.05000 - Train loss: 17.91259 - Test loss: 54.16395\n",
      "Epoch 907 - lr: 0.05000 - Train loss: 17.91256 - Test loss: 54.16404\n",
      "Epoch 908 - lr: 0.05000 - Train loss: 17.91254 - Test loss: 54.16412\n",
      "Epoch 909 - lr: 0.05000 - Train loss: 17.91251 - Test loss: 54.16421\n",
      "Epoch 910 - lr: 0.05000 - Train loss: 17.91248 - Test loss: 54.16430\n",
      "Epoch 911 - lr: 0.05000 - Train loss: 17.91246 - Test loss: 54.16438\n",
      "Epoch 912 - lr: 0.05000 - Train loss: 17.91243 - Test loss: 54.16447\n",
      "Epoch 913 - lr: 0.05000 - Train loss: 17.91240 - Test loss: 54.16455\n",
      "Epoch 914 - lr: 0.05000 - Train loss: 17.91238 - Test loss: 54.16464\n",
      "Epoch 915 - lr: 0.05000 - Train loss: 17.91235 - Test loss: 54.16472\n",
      "Epoch 916 - lr: 0.05000 - Train loss: 17.91232 - Test loss: 54.16481\n",
      "Epoch 917 - lr: 0.05000 - Train loss: 17.91230 - Test loss: 54.16489\n",
      "Epoch 918 - lr: 0.05000 - Train loss: 17.91227 - Test loss: 54.16497\n",
      "Epoch 919 - lr: 0.05000 - Train loss: 17.91225 - Test loss: 54.16506\n",
      "Epoch 920 - lr: 0.05000 - Train loss: 17.91222 - Test loss: 54.16514\n",
      "Epoch 921 - lr: 0.05000 - Train loss: 17.91220 - Test loss: 54.16522\n",
      "Epoch 922 - lr: 0.05000 - Train loss: 17.91217 - Test loss: 54.16530\n",
      "Epoch 923 - lr: 0.05000 - Train loss: 17.91214 - Test loss: 54.16538\n",
      "Epoch 924 - lr: 0.05000 - Train loss: 17.91212 - Test loss: 54.16546\n",
      "Epoch 925 - lr: 0.05000 - Train loss: 17.91209 - Test loss: 54.16554\n",
      "Epoch 926 - lr: 0.05000 - Train loss: 17.91207 - Test loss: 54.16562\n",
      "Epoch 927 - lr: 0.05000 - Train loss: 17.91204 - Test loss: 54.16570\n",
      "Epoch 928 - lr: 0.05000 - Train loss: 17.91202 - Test loss: 54.16578\n",
      "Epoch 929 - lr: 0.05000 - Train loss: 17.91199 - Test loss: 54.16586\n",
      "Epoch 930 - lr: 0.05000 - Train loss: 17.91197 - Test loss: 54.16594\n",
      "Epoch 931 - lr: 0.05000 - Train loss: 17.91194 - Test loss: 54.16601\n",
      "Epoch 932 - lr: 0.05000 - Train loss: 17.91192 - Test loss: 54.16609\n",
      "Epoch 933 - lr: 0.05000 - Train loss: 17.91189 - Test loss: 54.16617\n",
      "Epoch 934 - lr: 0.05000 - Train loss: 17.91187 - Test loss: 54.16624\n",
      "Epoch 935 - lr: 0.05000 - Train loss: 17.91184 - Test loss: 54.16632\n",
      "Epoch 936 - lr: 0.05000 - Train loss: 17.91182 - Test loss: 54.16640\n",
      "Epoch 937 - lr: 0.05000 - Train loss: 17.91179 - Test loss: 54.16647\n",
      "Epoch 938 - lr: 0.05000 - Train loss: 17.91177 - Test loss: 54.16655\n",
      "Epoch 939 - lr: 0.05000 - Train loss: 17.91175 - Test loss: 54.16662\n",
      "Epoch 940 - lr: 0.05000 - Train loss: 17.91172 - Test loss: 54.16669\n",
      "Epoch 941 - lr: 0.05000 - Train loss: 17.91170 - Test loss: 54.16677\n",
      "Epoch 942 - lr: 0.05000 - Train loss: 17.91167 - Test loss: 54.16684\n",
      "Epoch 943 - lr: 0.05000 - Train loss: 17.91165 - Test loss: 54.16691\n",
      "Epoch 944 - lr: 0.05000 - Train loss: 17.91163 - Test loss: 54.16699\n",
      "Epoch 945 - lr: 0.05000 - Train loss: 17.91160 - Test loss: 54.16706\n",
      "Epoch 946 - lr: 0.05000 - Train loss: 17.91158 - Test loss: 54.16713\n",
      "Epoch 947 - lr: 0.05000 - Train loss: 17.91155 - Test loss: 54.16720\n",
      "Epoch 948 - lr: 0.05000 - Train loss: 17.91153 - Test loss: 54.16727\n",
      "Epoch 949 - lr: 0.05000 - Train loss: 17.91151 - Test loss: 54.16734\n",
      "Epoch 950 - lr: 0.05000 - Train loss: 17.91148 - Test loss: 54.16741\n",
      "Epoch 951 - lr: 0.05000 - Train loss: 17.91146 - Test loss: 54.16748\n",
      "Epoch 952 - lr: 0.05000 - Train loss: 17.91144 - Test loss: 54.16755\n",
      "Epoch 953 - lr: 0.05000 - Train loss: 17.91141 - Test loss: 54.16762\n",
      "Epoch 954 - lr: 0.05000 - Train loss: 17.91139 - Test loss: 54.16769\n",
      "Epoch 955 - lr: 0.05000 - Train loss: 17.91137 - Test loss: 54.16776\n",
      "Epoch 956 - lr: 0.05000 - Train loss: 17.91134 - Test loss: 54.16782\n",
      "Epoch 957 - lr: 0.05000 - Train loss: 17.91132 - Test loss: 54.16789\n",
      "Epoch 958 - lr: 0.05000 - Train loss: 17.91130 - Test loss: 54.16796\n",
      "Epoch 959 - lr: 0.05000 - Train loss: 17.91128 - Test loss: 54.16802\n",
      "Epoch 960 - lr: 0.05000 - Train loss: 17.91125 - Test loss: 54.16809\n",
      "Epoch 961 - lr: 0.05000 - Train loss: 17.91123 - Test loss: 54.16816\n",
      "Epoch 962 - lr: 0.05000 - Train loss: 17.91121 - Test loss: 54.16822\n",
      "Epoch 963 - lr: 0.05000 - Train loss: 17.91118 - Test loss: 54.16829\n",
      "Epoch 964 - lr: 0.05000 - Train loss: 17.91116 - Test loss: 54.16835\n",
      "Epoch 965 - lr: 0.05000 - Train loss: 17.91114 - Test loss: 54.16842\n",
      "Epoch 966 - lr: 0.05000 - Train loss: 17.91112 - Test loss: 54.16848\n",
      "Epoch 967 - lr: 0.05000 - Train loss: 17.91110 - Test loss: 54.16854\n",
      "Epoch 968 - lr: 0.05000 - Train loss: 17.91107 - Test loss: 54.16861\n",
      "Epoch 969 - lr: 0.05000 - Train loss: 17.91105 - Test loss: 54.16867\n",
      "Epoch 970 - lr: 0.05000 - Train loss: 17.91103 - Test loss: 54.16873\n",
      "Epoch 971 - lr: 0.05000 - Train loss: 17.91101 - Test loss: 54.16879\n",
      "Epoch 972 - lr: 0.05000 - Train loss: 17.91098 - Test loss: 54.16886\n",
      "Epoch 973 - lr: 0.05000 - Train loss: 17.91096 - Test loss: 54.16892\n",
      "Epoch 974 - lr: 0.05000 - Train loss: 17.91094 - Test loss: 54.16898\n",
      "Epoch 975 - lr: 0.05000 - Train loss: 17.91092 - Test loss: 54.16904\n",
      "Epoch 976 - lr: 0.05000 - Train loss: 17.91090 - Test loss: 54.16910\n",
      "Epoch 977 - lr: 0.05000 - Train loss: 17.91088 - Test loss: 54.16916\n",
      "Epoch 978 - lr: 0.05000 - Train loss: 17.91085 - Test loss: 54.16922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 979 - lr: 0.05000 - Train loss: 17.91083 - Test loss: 54.16928\n",
      "Epoch 980 - lr: 0.05000 - Train loss: 17.91081 - Test loss: 54.16934\n",
      "Epoch 981 - lr: 0.05000 - Train loss: 17.91079 - Test loss: 54.16940\n",
      "Epoch 982 - lr: 0.05000 - Train loss: 17.91077 - Test loss: 54.16945\n",
      "Epoch 983 - lr: 0.05000 - Train loss: 17.91075 - Test loss: 54.16951\n",
      "Epoch 984 - lr: 0.05000 - Train loss: 17.91073 - Test loss: 54.16957\n",
      "Epoch 985 - lr: 0.05000 - Train loss: 17.91071 - Test loss: 54.16963\n",
      "Epoch 986 - lr: 0.05000 - Train loss: 17.91068 - Test loss: 54.16968\n",
      "Epoch 987 - lr: 0.05000 - Train loss: 17.91066 - Test loss: 54.16974\n",
      "Epoch 988 - lr: 0.05000 - Train loss: 17.91064 - Test loss: 54.16980\n",
      "Epoch 989 - lr: 0.05000 - Train loss: 17.91062 - Test loss: 54.16985\n",
      "Epoch 990 - lr: 0.05000 - Train loss: 17.91060 - Test loss: 54.16991\n",
      "Epoch 991 - lr: 0.05000 - Train loss: 17.91058 - Test loss: 54.16996\n",
      "Epoch 992 - lr: 0.05000 - Train loss: 17.91056 - Test loss: 54.17002\n",
      "Epoch 993 - lr: 0.05000 - Train loss: 17.91054 - Test loss: 54.17007\n",
      "Epoch 994 - lr: 0.05000 - Train loss: 17.91052 - Test loss: 54.17013\n",
      "Epoch 995 - lr: 0.05000 - Train loss: 17.91050 - Test loss: 54.17018\n",
      "Epoch 996 - lr: 0.05000 - Train loss: 17.91048 - Test loss: 54.17023\n",
      "Epoch 997 - lr: 0.05000 - Train loss: 17.91046 - Test loss: 54.17029\n",
      "Epoch 998 - lr: 0.05000 - Train loss: 17.91044 - Test loss: 54.17034\n",
      "Epoch 999 - lr: 0.05000 - Train loss: 17.91042 - Test loss: 54.17039\n",
      "Epoch 1000 - lr: 0.05000 - Train loss: 17.91040 - Test loss: 54.17045\n",
      "Epoch 1001 - lr: 0.05000 - Train loss: 17.91038 - Test loss: 54.17050\n",
      "Epoch 1002 - lr: 0.05000 - Train loss: 17.91036 - Test loss: 54.17055\n",
      "Epoch 1003 - lr: 0.05000 - Train loss: 17.91034 - Test loss: 54.17060\n",
      "Epoch 1004 - lr: 0.05000 - Train loss: 17.91032 - Test loss: 54.17065\n",
      "Epoch 1005 - lr: 0.05000 - Train loss: 17.91030 - Test loss: 54.17070\n",
      "Epoch 1006 - lr: 0.05000 - Train loss: 17.91028 - Test loss: 54.17075\n",
      "Epoch 1007 - lr: 0.05000 - Train loss: 17.91026 - Test loss: 54.17080\n",
      "Epoch 1008 - lr: 0.05000 - Train loss: 17.91024 - Test loss: 54.17085\n",
      "Epoch 1009 - lr: 0.05000 - Train loss: 17.91022 - Test loss: 54.17090\n",
      "Epoch 1010 - lr: 0.05000 - Train loss: 17.91020 - Test loss: 54.17095\n",
      "Epoch 1011 - lr: 0.05000 - Train loss: 17.91018 - Test loss: 54.17100\n",
      "Epoch 1012 - lr: 0.05000 - Train loss: 17.91016 - Test loss: 54.17105\n",
      "Epoch 1013 - lr: 0.05000 - Train loss: 17.91014 - Test loss: 54.17110\n",
      "Epoch 1014 - lr: 0.05000 - Train loss: 17.91012 - Test loss: 54.17114\n",
      "Epoch 1015 - lr: 0.05000 - Train loss: 17.91010 - Test loss: 54.17119\n",
      "Epoch 1016 - lr: 0.05000 - Train loss: 17.91008 - Test loss: 54.17124\n",
      "Epoch 1017 - lr: 0.05000 - Train loss: 17.91006 - Test loss: 54.17128\n",
      "Epoch 1018 - lr: 0.05000 - Train loss: 17.91004 - Test loss: 54.17133\n",
      "Epoch 1019 - lr: 0.05000 - Train loss: 17.91002 - Test loss: 54.17138\n",
      "Epoch 1020 - lr: 0.05000 - Train loss: 17.91001 - Test loss: 54.17142\n",
      "Epoch 1021 - lr: 0.05000 - Train loss: 17.90999 - Test loss: 54.17147\n",
      "Epoch 1022 - lr: 0.05000 - Train loss: 17.90997 - Test loss: 54.17151\n",
      "Epoch 1023 - lr: 0.05000 - Train loss: 17.90995 - Test loss: 54.17156\n",
      "Epoch 1024 - lr: 0.05000 - Train loss: 17.90993 - Test loss: 54.17160\n",
      "Epoch 1025 - lr: 0.05000 - Train loss: 17.90991 - Test loss: 54.17165\n",
      "Epoch 1026 - lr: 0.05000 - Train loss: 17.90989 - Test loss: 54.17169\n",
      "Epoch 1027 - lr: 0.05000 - Train loss: 17.90987 - Test loss: 54.17174\n",
      "Epoch 1028 - lr: 0.05000 - Train loss: 17.90986 - Test loss: 54.17178\n",
      "Epoch 1029 - lr: 0.05000 - Train loss: 17.90984 - Test loss: 54.17182\n",
      "Epoch 1030 - lr: 0.05000 - Train loss: 17.90982 - Test loss: 54.17187\n",
      "Epoch 1031 - lr: 0.05000 - Train loss: 17.90980 - Test loss: 54.17191\n",
      "Epoch 1032 - lr: 0.05000 - Train loss: 17.90978 - Test loss: 54.17195\n",
      "Epoch 1033 - lr: 0.05000 - Train loss: 17.90976 - Test loss: 54.17199\n",
      "Epoch 1034 - lr: 0.05000 - Train loss: 17.90975 - Test loss: 54.17203\n",
      "Epoch 1035 - lr: 0.05000 - Train loss: 17.90973 - Test loss: 54.17208\n",
      "Epoch 1036 - lr: 0.05000 - Train loss: 17.90971 - Test loss: 54.17212\n",
      "Epoch 1037 - lr: 0.05000 - Train loss: 17.90969 - Test loss: 54.17216\n",
      "Epoch 1038 - lr: 0.05000 - Train loss: 17.90967 - Test loss: 54.17220\n",
      "Epoch 1039 - lr: 0.05000 - Train loss: 17.90965 - Test loss: 54.17224\n",
      "Epoch 1040 - lr: 0.05000 - Train loss: 17.90964 - Test loss: 54.17228\n",
      "Epoch 1041 - lr: 0.05000 - Train loss: 17.90962 - Test loss: 54.17232\n",
      "Epoch 1042 - lr: 0.05000 - Train loss: 17.90960 - Test loss: 54.17236\n",
      "Epoch 1043 - lr: 0.05000 - Train loss: 17.90958 - Test loss: 54.17240\n",
      "Epoch 1044 - lr: 0.05000 - Train loss: 17.90957 - Test loss: 54.17243\n",
      "Epoch 1045 - lr: 0.05000 - Train loss: 17.90955 - Test loss: 54.17247\n",
      "Epoch 1046 - lr: 0.05000 - Train loss: 17.90953 - Test loss: 54.17251\n",
      "Epoch 1047 - lr: 0.05000 - Train loss: 17.90951 - Test loss: 54.17255\n",
      "Epoch 1048 - lr: 0.05000 - Train loss: 17.90950 - Test loss: 54.17259\n",
      "Epoch 1049 - lr: 0.05000 - Train loss: 17.90948 - Test loss: 54.17262\n",
      "Epoch 1050 - lr: 0.05000 - Train loss: 17.90946 - Test loss: 54.17266\n",
      "Epoch 1051 - lr: 0.05000 - Train loss: 17.90944 - Test loss: 54.17270\n",
      "Epoch 1052 - lr: 0.05000 - Train loss: 17.90943 - Test loss: 54.17273\n",
      "Epoch 1053 - lr: 0.05000 - Train loss: 17.90941 - Test loss: 54.17277\n",
      "Epoch 1054 - lr: 0.05000 - Train loss: 17.90939 - Test loss: 54.17281\n",
      "Epoch 1055 - lr: 0.05000 - Train loss: 17.90937 - Test loss: 54.17284\n",
      "Epoch 1056 - lr: 0.05000 - Train loss: 17.90936 - Test loss: 54.17288\n",
      "Epoch 1057 - lr: 0.05000 - Train loss: 17.90934 - Test loss: 54.17291\n",
      "Epoch 1058 - lr: 0.05000 - Train loss: 17.90932 - Test loss: 54.17295\n",
      "Epoch 1059 - lr: 0.05000 - Train loss: 17.90931 - Test loss: 54.17298\n",
      "Epoch 1060 - lr: 0.05000 - Train loss: 17.90929 - Test loss: 54.17302\n",
      "Epoch 1061 - lr: 0.05000 - Train loss: 17.90927 - Test loss: 54.17305\n",
      "Epoch 1062 - lr: 0.05000 - Train loss: 17.90926 - Test loss: 54.17308\n",
      "Epoch 1063 - lr: 0.05000 - Train loss: 17.90924 - Test loss: 54.17312\n",
      "Epoch 1064 - lr: 0.05000 - Train loss: 17.90922 - Test loss: 54.17315\n",
      "Epoch 1065 - lr: 0.05000 - Train loss: 17.90921 - Test loss: 54.17318\n",
      "Epoch 1066 - lr: 0.05000 - Train loss: 17.90919 - Test loss: 54.17322\n",
      "Epoch 1067 - lr: 0.05000 - Train loss: 17.90917 - Test loss: 54.17325\n",
      "Epoch 1068 - lr: 0.05000 - Train loss: 17.90916 - Test loss: 54.17328\n",
      "Epoch 1069 - lr: 0.05000 - Train loss: 17.90914 - Test loss: 54.17331\n",
      "Epoch 1070 - lr: 0.05000 - Train loss: 17.90912 - Test loss: 54.17334\n",
      "Epoch 1071 - lr: 0.05000 - Train loss: 17.90911 - Test loss: 54.17338\n",
      "Epoch 1072 - lr: 0.05000 - Train loss: 17.90909 - Test loss: 54.17341\n",
      "Epoch 1073 - lr: 0.05000 - Train loss: 17.90907 - Test loss: 54.17344\n",
      "Epoch 1074 - lr: 0.05000 - Train loss: 17.90906 - Test loss: 54.17347\n",
      "Epoch 1075 - lr: 0.05000 - Train loss: 17.90904 - Test loss: 54.17350\n",
      "Epoch 1076 - lr: 0.05000 - Train loss: 17.90903 - Test loss: 54.17353\n",
      "Epoch 1077 - lr: 0.05000 - Train loss: 17.90901 - Test loss: 54.17356\n",
      "Epoch 1078 - lr: 0.05000 - Train loss: 17.90899 - Test loss: 54.17359\n",
      "Epoch 1079 - lr: 0.05000 - Train loss: 17.90898 - Test loss: 54.17362\n",
      "Epoch 1080 - lr: 0.05000 - Train loss: 17.90896 - Test loss: 54.17365\n",
      "Epoch 1081 - lr: 0.05000 - Train loss: 17.90895 - Test loss: 54.17368\n",
      "Epoch 1082 - lr: 0.05000 - Train loss: 17.90893 - Test loss: 54.17370\n",
      "Epoch 1083 - lr: 0.05000 - Train loss: 17.90891 - Test loss: 54.17373\n",
      "Epoch 1084 - lr: 0.05000 - Train loss: 17.90890 - Test loss: 54.17376\n",
      "Epoch 1085 - lr: 0.05000 - Train loss: 17.90888 - Test loss: 54.17379\n",
      "Epoch 1086 - lr: 0.05000 - Train loss: 17.90887 - Test loss: 54.17382\n",
      "Epoch 1087 - lr: 0.05000 - Train loss: 17.90885 - Test loss: 54.17384\n",
      "Epoch 1088 - lr: 0.05000 - Train loss: 17.90884 - Test loss: 54.17387\n",
      "Epoch 1089 - lr: 0.05000 - Train loss: 17.90882 - Test loss: 54.17390\n",
      "Epoch 1090 - lr: 0.05000 - Train loss: 17.90880 - Test loss: 54.17392\n",
      "Epoch 1091 - lr: 0.05000 - Train loss: 17.90879 - Test loss: 54.17395\n",
      "Epoch 1092 - lr: 0.05000 - Train loss: 17.90877 - Test loss: 54.17398\n",
      "Epoch 1093 - lr: 0.05000 - Train loss: 17.90876 - Test loss: 54.17400\n",
      "Epoch 1094 - lr: 0.05000 - Train loss: 17.90874 - Test loss: 54.17403\n",
      "Epoch 1095 - lr: 0.05000 - Train loss: 17.90873 - Test loss: 54.17405\n",
      "Epoch 1096 - lr: 0.05000 - Train loss: 17.90871 - Test loss: 54.17408\n",
      "Epoch 1097 - lr: 0.05000 - Train loss: 17.90870 - Test loss: 54.17410\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1098 - lr: 0.05000 - Train loss: 17.90868 - Test loss: 54.17413\n",
      "Epoch 1099 - lr: 0.05000 - Train loss: 17.90867 - Test loss: 54.17415\n",
      "Epoch 1100 - lr: 0.05000 - Train loss: 17.90865 - Test loss: 54.17417\n",
      "Epoch 1101 - lr: 0.05000 - Train loss: 17.90864 - Test loss: 54.17420\n",
      "Epoch 1102 - lr: 0.05000 - Train loss: 17.90862 - Test loss: 54.17422\n",
      "Epoch 1103 - lr: 0.05000 - Train loss: 17.90861 - Test loss: 54.17425\n",
      "Epoch 1104 - lr: 0.05000 - Train loss: 17.90859 - Test loss: 54.17427\n",
      "Epoch 1105 - lr: 0.05000 - Train loss: 17.90858 - Test loss: 54.17429\n",
      "Epoch 1106 - lr: 0.05000 - Train loss: 17.90856 - Test loss: 54.17431\n",
      "Epoch 1107 - lr: 0.05000 - Train loss: 17.90855 - Test loss: 54.17434\n",
      "Epoch 1108 - lr: 0.05000 - Train loss: 17.90853 - Test loss: 54.17436\n",
      "Epoch 1109 - lr: 0.05000 - Train loss: 17.90852 - Test loss: 54.17438\n",
      "Epoch 1110 - lr: 0.05000 - Train loss: 17.90850 - Test loss: 54.17440\n",
      "Epoch 1111 - lr: 0.05000 - Train loss: 17.90849 - Test loss: 54.17442\n",
      "Epoch 1112 - lr: 0.05000 - Train loss: 17.90847 - Test loss: 54.17444\n",
      "Epoch 1113 - lr: 0.05000 - Train loss: 17.90846 - Test loss: 54.17447\n",
      "Epoch 1114 - lr: 0.05000 - Train loss: 17.90845 - Test loss: 54.17449\n",
      "Epoch 1115 - lr: 0.05000 - Train loss: 17.90843 - Test loss: 54.17451\n",
      "Epoch 1116 - lr: 0.05000 - Train loss: 17.90842 - Test loss: 54.17453\n",
      "Epoch 1117 - lr: 0.05000 - Train loss: 17.90840 - Test loss: 54.17455\n",
      "Epoch 1118 - lr: 0.05000 - Train loss: 17.90839 - Test loss: 54.17457\n",
      "Epoch 1119 - lr: 0.05000 - Train loss: 17.90837 - Test loss: 54.17459\n",
      "Epoch 1120 - lr: 0.05000 - Train loss: 17.90836 - Test loss: 54.17461\n",
      "Epoch 1121 - lr: 0.05000 - Train loss: 17.90835 - Test loss: 54.17463\n",
      "Epoch 1122 - lr: 0.05000 - Train loss: 17.90833 - Test loss: 54.17464\n",
      "Epoch 1123 - lr: 0.05000 - Train loss: 17.90832 - Test loss: 54.17466\n",
      "Epoch 1124 - lr: 0.05000 - Train loss: 17.90830 - Test loss: 54.17468\n",
      "Epoch 1125 - lr: 0.05000 - Train loss: 17.90829 - Test loss: 54.17470\n",
      "Epoch 1126 - lr: 0.05000 - Train loss: 17.90828 - Test loss: 54.17472\n",
      "Epoch 1127 - lr: 0.05000 - Train loss: 17.90826 - Test loss: 54.17474\n",
      "Epoch 1128 - lr: 0.05000 - Train loss: 17.90825 - Test loss: 54.17475\n",
      "Epoch 1129 - lr: 0.05000 - Train loss: 17.90823 - Test loss: 54.17477\n",
      "Epoch 1130 - lr: 0.05000 - Train loss: 17.90822 - Test loss: 54.17479\n",
      "Epoch 1131 - lr: 0.05000 - Train loss: 17.90821 - Test loss: 54.17480\n",
      "Epoch 1132 - lr: 0.05000 - Train loss: 17.90819 - Test loss: 54.17482\n",
      "Epoch 1133 - lr: 0.05000 - Train loss: 17.90818 - Test loss: 54.17484\n",
      "Epoch 1134 - lr: 0.05000 - Train loss: 17.90816 - Test loss: 54.17485\n",
      "Epoch 1135 - lr: 0.05000 - Train loss: 17.90815 - Test loss: 54.17487\n",
      "Epoch 1136 - lr: 0.05000 - Train loss: 17.90814 - Test loss: 54.17489\n",
      "Epoch 1137 - lr: 0.05000 - Train loss: 17.90812 - Test loss: 54.17490\n",
      "Epoch 1138 - lr: 0.05000 - Train loss: 17.90811 - Test loss: 54.17492\n",
      "Epoch 1139 - lr: 0.05000 - Train loss: 17.90810 - Test loss: 54.17493\n",
      "Epoch 1140 - lr: 0.05000 - Train loss: 17.90808 - Test loss: 54.17495\n",
      "Epoch 1141 - lr: 0.05000 - Train loss: 17.90807 - Test loss: 54.17496\n",
      "Epoch 1142 - lr: 0.05000 - Train loss: 17.90806 - Test loss: 54.17498\n",
      "Epoch 1143 - lr: 0.05000 - Train loss: 17.90804 - Test loss: 54.17499\n",
      "Epoch 1144 - lr: 0.05000 - Train loss: 17.90803 - Test loss: 54.17500\n",
      "Epoch 1145 - lr: 0.05000 - Train loss: 17.90802 - Test loss: 54.17502\n",
      "Epoch 1146 - lr: 0.05000 - Train loss: 17.90800 - Test loss: 54.17503\n",
      "Epoch 1147 - lr: 0.05000 - Train loss: 17.90799 - Test loss: 54.17505\n",
      "Epoch 1148 - lr: 0.05000 - Train loss: 17.90798 - Test loss: 54.17506\n",
      "Epoch 1149 - lr: 0.05000 - Train loss: 17.90796 - Test loss: 54.17507\n",
      "Epoch 1150 - lr: 0.05000 - Train loss: 17.90795 - Test loss: 54.17508\n",
      "Epoch 1151 - lr: 0.05000 - Train loss: 17.90794 - Test loss: 54.17510\n",
      "Epoch 1152 - lr: 0.05000 - Train loss: 17.90792 - Test loss: 54.17511\n",
      "Epoch 1153 - lr: 0.05000 - Train loss: 17.90791 - Test loss: 54.17512\n",
      "Epoch 1154 - lr: 0.05000 - Train loss: 17.90790 - Test loss: 54.17513\n",
      "Epoch 1155 - lr: 0.05000 - Train loss: 17.90789 - Test loss: 54.17515\n",
      "Epoch 1156 - lr: 0.05000 - Train loss: 17.90787 - Test loss: 54.17516\n",
      "Epoch 1157 - lr: 0.05000 - Train loss: 17.90786 - Test loss: 54.17517\n",
      "Epoch 1158 - lr: 0.05000 - Train loss: 17.90785 - Test loss: 54.17518\n",
      "Epoch 1159 - lr: 0.05000 - Train loss: 17.90783 - Test loss: 54.17519\n",
      "Epoch 1160 - lr: 0.05000 - Train loss: 17.90782 - Test loss: 54.17520\n",
      "Epoch 1161 - lr: 0.05000 - Train loss: 17.90781 - Test loss: 54.17521\n",
      "Epoch 1162 - lr: 0.05000 - Train loss: 17.90780 - Test loss: 54.17522\n",
      "Epoch 1163 - lr: 0.05000 - Train loss: 17.90778 - Test loss: 54.17523\n",
      "Epoch 1164 - lr: 0.05000 - Train loss: 17.90777 - Test loss: 54.17524\n",
      "Epoch 1165 - lr: 0.05000 - Train loss: 17.90776 - Test loss: 54.17525\n",
      "Epoch 1166 - lr: 0.05000 - Train loss: 17.90775 - Test loss: 54.17526\n",
      "Epoch 1167 - lr: 0.05000 - Train loss: 17.90773 - Test loss: 54.17527\n",
      "Epoch 1168 - lr: 0.05000 - Train loss: 17.90772 - Test loss: 54.17528\n",
      "Epoch 1169 - lr: 0.05000 - Train loss: 17.90771 - Test loss: 54.17529\n",
      "Epoch 1170 - lr: 0.05000 - Train loss: 17.90770 - Test loss: 54.17530\n",
      "Epoch 1171 - lr: 0.05000 - Train loss: 17.90768 - Test loss: 54.17531\n",
      "Epoch 1172 - lr: 0.05000 - Train loss: 17.90767 - Test loss: 54.17532\n",
      "Epoch 1173 - lr: 0.05000 - Train loss: 17.90766 - Test loss: 54.17532\n",
      "Epoch 1174 - lr: 0.05000 - Train loss: 17.90765 - Test loss: 54.17533\n",
      "Epoch 1175 - lr: 0.05000 - Train loss: 17.90764 - Test loss: 54.17534\n",
      "Epoch 1176 - lr: 0.05000 - Train loss: 17.90762 - Test loss: 54.17535\n",
      "Epoch 1177 - lr: 0.05000 - Train loss: 17.90761 - Test loss: 54.17535\n",
      "Epoch 1178 - lr: 0.05000 - Train loss: 17.90760 - Test loss: 54.17536\n",
      "Epoch 1179 - lr: 0.05000 - Train loss: 17.90759 - Test loss: 54.17537\n",
      "Epoch 1180 - lr: 0.05000 - Train loss: 17.90757 - Test loss: 54.17538\n",
      "Epoch 1181 - lr: 0.05000 - Train loss: 17.90756 - Test loss: 54.17538\n",
      "Epoch 1182 - lr: 0.05000 - Train loss: 17.90755 - Test loss: 54.17539\n",
      "Epoch 1183 - lr: 0.05000 - Train loss: 17.90754 - Test loss: 54.17539\n",
      "Epoch 1184 - lr: 0.05000 - Train loss: 17.90753 - Test loss: 54.17540\n",
      "Epoch 1185 - lr: 0.05000 - Train loss: 17.90751 - Test loss: 54.17541\n",
      "Epoch 1186 - lr: 0.05000 - Train loss: 17.90750 - Test loss: 54.17541\n",
      "Epoch 1187 - lr: 0.05000 - Train loss: 17.90749 - Test loss: 54.17542\n",
      "Epoch 1188 - lr: 0.05000 - Train loss: 17.90748 - Test loss: 54.17542\n",
      "Epoch 1189 - lr: 0.05000 - Train loss: 17.90747 - Test loss: 54.17543\n",
      "Epoch 1190 - lr: 0.05000 - Train loss: 17.90746 - Test loss: 54.17543\n",
      "Epoch 1191 - lr: 0.05000 - Train loss: 17.90744 - Test loss: 54.17544\n",
      "Epoch 1192 - lr: 0.05000 - Train loss: 17.90743 - Test loss: 54.17544\n",
      "Epoch 1193 - lr: 0.05000 - Train loss: 17.90742 - Test loss: 54.17545\n",
      "Epoch 1194 - lr: 0.05000 - Train loss: 17.90741 - Test loss: 54.17545\n",
      "Epoch 1195 - lr: 0.05000 - Train loss: 17.90740 - Test loss: 54.17546\n",
      "Epoch 1196 - lr: 0.05000 - Train loss: 17.90739 - Test loss: 54.17546\n",
      "Epoch 1197 - lr: 0.05000 - Train loss: 17.90737 - Test loss: 54.17546\n",
      "Epoch 1198 - lr: 0.05000 - Train loss: 17.90736 - Test loss: 54.17547\n",
      "Epoch 1199 - lr: 0.05000 - Train loss: 17.90735 - Test loss: 54.17547\n",
      "Epoch 1200 - lr: 0.05000 - Train loss: 17.90734 - Test loss: 54.17547\n",
      "Epoch 1201 - lr: 0.05000 - Train loss: 17.90733 - Test loss: 54.17548\n",
      "Epoch 1202 - lr: 0.05000 - Train loss: 17.90732 - Test loss: 54.17548\n",
      "Epoch 1203 - lr: 0.05000 - Train loss: 17.90731 - Test loss: 54.17548\n",
      "Epoch 1204 - lr: 0.05000 - Train loss: 17.90730 - Test loss: 54.17548\n",
      "Epoch 1205 - lr: 0.05000 - Train loss: 17.90728 - Test loss: 54.17549\n",
      "Epoch 1206 - lr: 0.05000 - Train loss: 17.90727 - Test loss: 54.17549\n",
      "Epoch 1207 - lr: 0.05000 - Train loss: 17.90726 - Test loss: 54.17549\n",
      "Epoch 1208 - lr: 0.05000 - Train loss: 17.90725 - Test loss: 54.17549\n",
      "Epoch 1209 - lr: 0.05000 - Train loss: 17.90724 - Test loss: 54.17549\n",
      "Epoch 1210 - lr: 0.05000 - Train loss: 17.90723 - Test loss: 54.17549\n",
      "Epoch 1211 - lr: 0.05000 - Train loss: 17.90722 - Test loss: 54.17549\n",
      "Epoch 1212 - lr: 0.05000 - Train loss: 17.90721 - Test loss: 54.17550\n",
      "Epoch 1213 - lr: 0.05000 - Train loss: 17.90720 - Test loss: 54.17550\n",
      "Epoch 1214 - lr: 0.05000 - Train loss: 17.90718 - Test loss: 54.17550\n",
      "Epoch 1215 - lr: 0.05000 - Train loss: 17.90717 - Test loss: 54.17550\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1216 - lr: 0.05000 - Train loss: 17.90716 - Test loss: 54.17550\n",
      "Epoch 1217 - lr: 0.05000 - Train loss: 17.90715 - Test loss: 54.17550\n",
      "Epoch 1218 - lr: 0.05000 - Train loss: 17.90714 - Test loss: 54.17550\n",
      "Epoch 1219 - lr: 0.05000 - Train loss: 17.90713 - Test loss: 54.17550\n",
      "Epoch 1220 - lr: 0.05000 - Train loss: 17.90712 - Test loss: 54.17550\n",
      "Epoch 1221 - lr: 0.05000 - Train loss: 17.90711 - Test loss: 54.17550\n",
      "Epoch 1222 - lr: 0.05000 - Train loss: 17.90710 - Test loss: 54.17550\n",
      "Epoch 1223 - lr: 0.05000 - Train loss: 17.90709 - Test loss: 54.17550\n",
      "Epoch 1224 - lr: 0.05000 - Train loss: 17.90708 - Test loss: 54.17549\n",
      "Epoch 1225 - lr: 0.05000 - Train loss: 17.90707 - Test loss: 54.17549\n",
      "Epoch 1226 - lr: 0.05000 - Train loss: 17.90705 - Test loss: 54.17549\n",
      "Epoch 1227 - lr: 0.05000 - Train loss: 17.90704 - Test loss: 54.17549\n",
      "Epoch 1228 - lr: 0.05000 - Train loss: 17.90703 - Test loss: 54.17549\n",
      "Epoch 1229 - lr: 0.05000 - Train loss: 17.90702 - Test loss: 54.17549\n",
      "Epoch 1230 - lr: 0.05000 - Train loss: 17.90701 - Test loss: 54.17548\n",
      "Epoch 1231 - lr: 0.05000 - Train loss: 17.90700 - Test loss: 54.17548\n",
      "Epoch 1232 - lr: 0.05000 - Train loss: 17.90699 - Test loss: 54.17548\n",
      "Epoch 1233 - lr: 0.05000 - Train loss: 17.90698 - Test loss: 54.17548\n",
      "Epoch 1234 - lr: 0.05000 - Train loss: 17.90697 - Test loss: 54.17547\n",
      "Epoch 1235 - lr: 0.05000 - Train loss: 17.90696 - Test loss: 54.17547\n",
      "Epoch 1236 - lr: 0.05000 - Train loss: 17.90695 - Test loss: 54.17547\n",
      "Epoch 1237 - lr: 0.05000 - Train loss: 17.90694 - Test loss: 54.17546\n",
      "Epoch 1238 - lr: 0.05000 - Train loss: 17.90693 - Test loss: 54.17546\n",
      "Epoch 1239 - lr: 0.05000 - Train loss: 17.90692 - Test loss: 54.17546\n",
      "Epoch 1240 - lr: 0.05000 - Train loss: 17.90691 - Test loss: 54.17545\n",
      "Epoch 1241 - lr: 0.05000 - Train loss: 17.90690 - Test loss: 54.17545\n",
      "Epoch 1242 - lr: 0.05000 - Train loss: 17.90689 - Test loss: 54.17545\n",
      "Epoch 1243 - lr: 0.05000 - Train loss: 17.90688 - Test loss: 54.17544\n",
      "Epoch 1244 - lr: 0.05000 - Train loss: 17.90687 - Test loss: 54.17544\n",
      "Epoch 1245 - lr: 0.05000 - Train loss: 17.90686 - Test loss: 54.17543\n",
      "Epoch 1246 - lr: 0.05000 - Train loss: 17.90685 - Test loss: 54.17543\n",
      "Epoch 1247 - lr: 0.05000 - Train loss: 17.90684 - Test loss: 54.17542\n",
      "Epoch 1248 - lr: 0.05000 - Train loss: 17.90683 - Test loss: 54.17542\n",
      "Epoch 1249 - lr: 0.05000 - Train loss: 17.90682 - Test loss: 54.17541\n",
      "Epoch 1250 - lr: 0.05000 - Train loss: 17.90681 - Test loss: 54.17541\n",
      "Epoch 1251 - lr: 0.05000 - Train loss: 17.90680 - Test loss: 54.17540\n",
      "Epoch 1252 - lr: 0.05000 - Train loss: 17.90679 - Test loss: 54.17540\n",
      "Epoch 1253 - lr: 0.05000 - Train loss: 17.90678 - Test loss: 54.17539\n",
      "Epoch 1254 - lr: 0.05000 - Train loss: 17.90677 - Test loss: 54.17538\n",
      "Epoch 1255 - lr: 0.05000 - Train loss: 17.90676 - Test loss: 54.17538\n",
      "Epoch 1256 - lr: 0.05000 - Train loss: 17.90675 - Test loss: 54.17537\n",
      "Epoch 1257 - lr: 0.05000 - Train loss: 17.90674 - Test loss: 54.17537\n",
      "Epoch 1258 - lr: 0.05000 - Train loss: 17.90673 - Test loss: 54.17536\n",
      "Epoch 1259 - lr: 0.05000 - Train loss: 17.90672 - Test loss: 54.17535\n",
      "Epoch 1260 - lr: 0.05000 - Train loss: 17.90671 - Test loss: 54.17534\n",
      "Epoch 1261 - lr: 0.05000 - Train loss: 17.90670 - Test loss: 54.17534\n",
      "Epoch 1262 - lr: 0.05000 - Train loss: 17.90669 - Test loss: 54.17533\n",
      "Epoch 1263 - lr: 0.05000 - Train loss: 17.90668 - Test loss: 54.17532\n",
      "Epoch 1264 - lr: 0.05000 - Train loss: 17.90667 - Test loss: 54.17532\n",
      "Epoch 1265 - lr: 0.05000 - Train loss: 17.90666 - Test loss: 54.17531\n",
      "Epoch 1266 - lr: 0.05000 - Train loss: 17.90665 - Test loss: 54.17530\n",
      "Epoch 1267 - lr: 0.05000 - Train loss: 17.90664 - Test loss: 54.17529\n",
      "Epoch 1268 - lr: 0.05000 - Train loss: 17.90663 - Test loss: 54.17528\n",
      "Epoch 1269 - lr: 0.05000 - Train loss: 17.90662 - Test loss: 54.17528\n",
      "Epoch 1270 - lr: 0.05000 - Train loss: 17.90661 - Test loss: 54.17527\n",
      "Epoch 1271 - lr: 0.05000 - Train loss: 17.90660 - Test loss: 54.17526\n",
      "Epoch 1272 - lr: 0.05000 - Train loss: 17.90660 - Test loss: 54.17525\n",
      "Epoch 1273 - lr: 0.05000 - Train loss: 17.90659 - Test loss: 54.17524\n",
      "Epoch 1274 - lr: 0.05000 - Train loss: 17.90658 - Test loss: 54.17523\n",
      "Epoch 1275 - lr: 0.05000 - Train loss: 17.90657 - Test loss: 54.17522\n",
      "Epoch 1276 - lr: 0.05000 - Train loss: 17.90656 - Test loss: 54.17521\n",
      "Epoch 1277 - lr: 0.05000 - Train loss: 17.90655 - Test loss: 54.17520\n",
      "Epoch 1278 - lr: 0.05000 - Train loss: 17.90654 - Test loss: 54.17519\n",
      "Epoch 1279 - lr: 0.05000 - Train loss: 17.90653 - Test loss: 54.17518\n",
      "Epoch 1280 - lr: 0.05000 - Train loss: 17.90652 - Test loss: 54.17517\n",
      "Epoch 1281 - lr: 0.05000 - Train loss: 17.90651 - Test loss: 54.17516\n",
      "Epoch 1282 - lr: 0.05000 - Train loss: 17.90650 - Test loss: 54.17515\n",
      "Epoch 1283 - lr: 0.05000 - Train loss: 17.90649 - Test loss: 54.17514\n",
      "Epoch 1284 - lr: 0.05000 - Train loss: 17.90648 - Test loss: 54.17513\n",
      "Epoch 1285 - lr: 0.05000 - Train loss: 17.90648 - Test loss: 54.17512\n",
      "Epoch 1286 - lr: 0.05000 - Train loss: 17.90647 - Test loss: 54.17511\n",
      "Epoch 1287 - lr: 0.05000 - Train loss: 17.90646 - Test loss: 54.17510\n",
      "Epoch 1288 - lr: 0.05000 - Train loss: 17.90645 - Test loss: 54.17509\n",
      "Epoch 1289 - lr: 0.05000 - Train loss: 17.90644 - Test loss: 54.17508\n",
      "Epoch 1290 - lr: 0.05000 - Train loss: 17.90643 - Test loss: 54.17507\n",
      "Epoch 1291 - lr: 0.05000 - Train loss: 17.90642 - Test loss: 54.17506\n",
      "Epoch 1292 - lr: 0.05000 - Train loss: 17.90641 - Test loss: 54.17504\n",
      "Epoch 1293 - lr: 0.05000 - Train loss: 17.90640 - Test loss: 54.17503\n",
      "Epoch 1294 - lr: 0.05000 - Train loss: 17.90640 - Test loss: 54.17502\n",
      "Epoch 1295 - lr: 0.05000 - Train loss: 17.90639 - Test loss: 54.17501\n",
      "Epoch 1296 - lr: 0.05000 - Train loss: 17.90638 - Test loss: 54.17500\n",
      "Epoch 1297 - lr: 0.05000 - Train loss: 17.90637 - Test loss: 54.17498\n",
      "Epoch 1298 - lr: 0.05000 - Train loss: 17.90636 - Test loss: 54.17497\n",
      "Epoch 1299 - lr: 0.05000 - Train loss: 17.90635 - Test loss: 54.17496\n",
      "Epoch 1300 - lr: 0.05000 - Train loss: 17.90634 - Test loss: 54.17495\n",
      "Epoch 1301 - lr: 0.05000 - Train loss: 17.90633 - Test loss: 54.17493\n",
      "Epoch 1302 - lr: 0.05000 - Train loss: 17.90633 - Test loss: 54.17492\n",
      "Epoch 1303 - lr: 0.05000 - Train loss: 17.90632 - Test loss: 54.17491\n",
      "Epoch 1304 - lr: 0.05000 - Train loss: 17.90631 - Test loss: 54.17489\n",
      "Epoch 1305 - lr: 0.05000 - Train loss: 17.90630 - Test loss: 54.17488\n",
      "Epoch 1306 - lr: 0.05000 - Train loss: 17.90629 - Test loss: 54.17487\n",
      "Epoch 1307 - lr: 0.05000 - Train loss: 17.90628 - Test loss: 54.17485\n",
      "Epoch 1308 - lr: 0.05000 - Train loss: 17.90627 - Test loss: 54.17484\n",
      "Epoch 1309 - lr: 0.05000 - Train loss: 17.90626 - Test loss: 54.17482\n",
      "Epoch 1310 - lr: 0.05000 - Train loss: 17.90626 - Test loss: 54.17481\n",
      "Epoch 1311 - lr: 0.05000 - Train loss: 17.90625 - Test loss: 54.17480\n",
      "Epoch 1312 - lr: 0.05000 - Train loss: 17.90624 - Test loss: 54.17478\n",
      "Epoch 1313 - lr: 0.05000 - Train loss: 17.90623 - Test loss: 54.17477\n",
      "Epoch 1314 - lr: 0.05000 - Train loss: 17.90622 - Test loss: 54.17475\n",
      "Epoch 1315 - lr: 0.05000 - Train loss: 17.90621 - Test loss: 54.17474\n",
      "Epoch 1316 - lr: 0.05000 - Train loss: 17.90621 - Test loss: 54.17472\n",
      "Epoch 1317 - lr: 0.05000 - Train loss: 17.90620 - Test loss: 54.17471\n",
      "Epoch 1318 - lr: 0.05000 - Train loss: 17.90619 - Test loss: 54.17469\n",
      "Epoch 1319 - lr: 0.05000 - Train loss: 17.90618 - Test loss: 54.17468\n",
      "Epoch 1320 - lr: 0.05000 - Train loss: 17.90617 - Test loss: 54.17466\n",
      "Epoch 1321 - lr: 0.05000 - Train loss: 17.90616 - Test loss: 54.17465\n",
      "Epoch 1322 - lr: 0.05000 - Train loss: 17.90616 - Test loss: 54.17463\n",
      "Epoch 1323 - lr: 0.05000 - Train loss: 17.90615 - Test loss: 54.17461\n",
      "Epoch 1324 - lr: 0.05000 - Train loss: 17.90614 - Test loss: 54.17460\n",
      "Epoch 1325 - lr: 0.05000 - Train loss: 17.90613 - Test loss: 54.17458\n",
      "Epoch 1326 - lr: 0.05000 - Train loss: 17.90612 - Test loss: 54.17457\n",
      "Epoch 1327 - lr: 0.05000 - Train loss: 17.90612 - Test loss: 54.17455\n",
      "Epoch 1328 - lr: 0.05000 - Train loss: 17.90611 - Test loss: 54.17453\n",
      "Epoch 1329 - lr: 0.05000 - Train loss: 17.90610 - Test loss: 54.17452\n",
      "Epoch 1330 - lr: 0.05000 - Train loss: 17.90609 - Test loss: 54.17450\n",
      "Epoch 1331 - lr: 0.05000 - Train loss: 17.90608 - Test loss: 54.17448\n",
      "Epoch 1332 - lr: 0.05000 - Train loss: 17.90608 - Test loss: 54.17447\n",
      "Epoch 1333 - lr: 0.05000 - Train loss: 17.90607 - Test loss: 54.17445\n",
      "Epoch 1334 - lr: 0.05000 - Train loss: 17.90606 - Test loss: 54.17443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1335 - lr: 0.05000 - Train loss: 17.90605 - Test loss: 54.17441\n",
      "Epoch 1336 - lr: 0.05000 - Train loss: 17.90604 - Test loss: 54.17440\n",
      "Epoch 1337 - lr: 0.05000 - Train loss: 17.90604 - Test loss: 54.17438\n",
      "Epoch 1338 - lr: 0.05000 - Train loss: 17.90603 - Test loss: 54.17436\n",
      "Epoch 1339 - lr: 0.05000 - Train loss: 17.90602 - Test loss: 54.17434\n",
      "Epoch 1340 - lr: 0.05000 - Train loss: 17.90601 - Test loss: 54.17432\n",
      "Epoch 1341 - lr: 0.05000 - Train loss: 17.90600 - Test loss: 54.17431\n",
      "Epoch 1342 - lr: 0.05000 - Train loss: 17.90600 - Test loss: 54.17429\n",
      "Epoch 1343 - lr: 0.05000 - Train loss: 17.90599 - Test loss: 54.17427\n",
      "Epoch 1344 - lr: 0.05000 - Train loss: 17.90598 - Test loss: 54.17425\n",
      "Epoch 1345 - lr: 0.05000 - Train loss: 17.90597 - Test loss: 54.17423\n",
      "Epoch 1346 - lr: 0.05000 - Train loss: 17.90597 - Test loss: 54.17421\n",
      "Epoch 1347 - lr: 0.05000 - Train loss: 17.90596 - Test loss: 54.17419\n",
      "Epoch 1348 - lr: 0.05000 - Train loss: 17.90595 - Test loss: 54.17418\n",
      "Epoch 1349 - lr: 0.05000 - Train loss: 17.90594 - Test loss: 54.17416\n",
      "Epoch 1350 - lr: 0.05000 - Train loss: 17.90593 - Test loss: 54.17414\n",
      "Epoch 1351 - lr: 0.05000 - Train loss: 17.90593 - Test loss: 54.17412\n",
      "Epoch 1352 - lr: 0.05000 - Train loss: 17.90592 - Test loss: 54.17410\n",
      "Epoch 1353 - lr: 0.05000 - Train loss: 17.90591 - Test loss: 54.17408\n",
      "Epoch 1354 - lr: 0.05000 - Train loss: 17.90590 - Test loss: 54.17406\n",
      "Epoch 1355 - lr: 0.05000 - Train loss: 17.90590 - Test loss: 54.17404\n",
      "Epoch 1356 - lr: 0.05000 - Train loss: 17.90589 - Test loss: 54.17402\n",
      "Epoch 1357 - lr: 0.05000 - Train loss: 17.90588 - Test loss: 54.17400\n",
      "Epoch 1358 - lr: 0.05000 - Train loss: 17.90587 - Test loss: 54.17398\n",
      "Epoch 1359 - lr: 0.05000 - Train loss: 17.90587 - Test loss: 54.17396\n",
      "Epoch 1360 - lr: 0.05000 - Train loss: 17.90586 - Test loss: 54.17394\n",
      "Epoch 1361 - lr: 0.05000 - Train loss: 17.90585 - Test loss: 54.17392\n",
      "Epoch 1362 - lr: 0.05000 - Train loss: 17.90584 - Test loss: 54.17390\n",
      "Epoch 1363 - lr: 0.05000 - Train loss: 17.90584 - Test loss: 54.17388\n",
      "Epoch 1364 - lr: 0.05000 - Train loss: 17.90583 - Test loss: 54.17385\n",
      "Epoch 1365 - lr: 0.05000 - Train loss: 17.90582 - Test loss: 54.17383\n",
      "Epoch 1366 - lr: 0.05000 - Train loss: 17.90581 - Test loss: 54.17381\n",
      "Epoch 1367 - lr: 0.05000 - Train loss: 17.90581 - Test loss: 54.17379\n",
      "Epoch 1368 - lr: 0.05000 - Train loss: 17.90580 - Test loss: 54.17377\n",
      "Epoch 1369 - lr: 0.05000 - Train loss: 17.90579 - Test loss: 54.17375\n",
      "Epoch 1370 - lr: 0.05000 - Train loss: 17.90579 - Test loss: 54.17373\n",
      "Epoch 1371 - lr: 0.05000 - Train loss: 17.90578 - Test loss: 54.17370\n",
      "Epoch 1372 - lr: 0.05000 - Train loss: 17.90577 - Test loss: 54.17368\n",
      "Epoch 1373 - lr: 0.05000 - Train loss: 17.90576 - Test loss: 54.17366\n",
      "Epoch 1374 - lr: 0.05000 - Train loss: 17.90576 - Test loss: 54.17364\n",
      "Epoch 1375 - lr: 0.05000 - Train loss: 17.90575 - Test loss: 54.17362\n",
      "Epoch 1376 - lr: 0.05000 - Train loss: 17.90574 - Test loss: 54.17359\n",
      "Epoch 1377 - lr: 0.05000 - Train loss: 17.90574 - Test loss: 54.17357\n",
      "Epoch 1378 - lr: 0.05000 - Train loss: 17.90573 - Test loss: 54.17355\n",
      "Epoch 1379 - lr: 0.05000 - Train loss: 17.90572 - Test loss: 54.17353\n",
      "Epoch 1380 - lr: 0.05000 - Train loss: 17.90571 - Test loss: 54.17350\n",
      "Epoch 1381 - lr: 0.05000 - Train loss: 17.90571 - Test loss: 54.17348\n",
      "Epoch 1382 - lr: 0.05000 - Train loss: 17.90570 - Test loss: 54.17346\n",
      "Epoch 1383 - lr: 0.05000 - Train loss: 17.90569 - Test loss: 54.17343\n",
      "Epoch 1384 - lr: 0.05000 - Train loss: 17.90569 - Test loss: 54.17341\n",
      "Epoch 1385 - lr: 0.05000 - Train loss: 17.90568 - Test loss: 54.17339\n",
      "Epoch 1386 - lr: 0.05000 - Train loss: 17.90567 - Test loss: 54.17336\n",
      "Epoch 1387 - lr: 0.05000 - Train loss: 17.90567 - Test loss: 54.17334\n",
      "Epoch 1388 - lr: 0.05000 - Train loss: 17.90566 - Test loss: 54.17332\n",
      "Epoch 1389 - lr: 0.05000 - Train loss: 17.90565 - Test loss: 54.17329\n",
      "Epoch 1390 - lr: 0.05000 - Train loss: 17.90564 - Test loss: 54.17327\n",
      "Epoch 1391 - lr: 0.05000 - Train loss: 17.90564 - Test loss: 54.17324\n",
      "Epoch 1392 - lr: 0.05000 - Train loss: 17.90563 - Test loss: 54.17322\n",
      "Epoch 1393 - lr: 0.05000 - Train loss: 17.90562 - Test loss: 54.17320\n",
      "Epoch 1394 - lr: 0.05000 - Train loss: 17.90562 - Test loss: 54.17317\n",
      "Epoch 1395 - lr: 0.05000 - Train loss: 17.90561 - Test loss: 54.17315\n",
      "Epoch 1396 - lr: 0.05000 - Train loss: 17.90560 - Test loss: 54.17312\n",
      "Epoch 1397 - lr: 0.05000 - Train loss: 17.90560 - Test loss: 54.17310\n",
      "Epoch 1398 - lr: 0.05000 - Train loss: 17.90559 - Test loss: 54.17307\n",
      "Epoch 1399 - lr: 0.05000 - Train loss: 17.90558 - Test loss: 54.17305\n",
      "Epoch 1400 - lr: 0.05000 - Train loss: 17.90558 - Test loss: 54.17302\n",
      "Epoch 1401 - lr: 0.05000 - Train loss: 17.90557 - Test loss: 54.17300\n",
      "Epoch 1402 - lr: 0.05000 - Train loss: 17.90556 - Test loss: 54.17297\n",
      "Epoch 1403 - lr: 0.05000 - Train loss: 17.90556 - Test loss: 54.17295\n",
      "Epoch 1404 - lr: 0.05000 - Train loss: 17.90555 - Test loss: 54.17292\n",
      "Epoch 1405 - lr: 0.05000 - Train loss: 17.90554 - Test loss: 54.17290\n",
      "Epoch 1406 - lr: 0.05000 - Train loss: 17.90554 - Test loss: 54.17287\n",
      "Epoch 1407 - lr: 0.05000 - Train loss: 17.90553 - Test loss: 54.17284\n",
      "Epoch 1408 - lr: 0.05000 - Train loss: 17.90552 - Test loss: 54.17282\n",
      "Epoch 1409 - lr: 0.05000 - Train loss: 17.90552 - Test loss: 54.17279\n",
      "Epoch 1410 - lr: 0.05000 - Train loss: 17.90551 - Test loss: 54.17277\n",
      "Epoch 1411 - lr: 0.05000 - Train loss: 17.90550 - Test loss: 54.17274\n",
      "Epoch 1412 - lr: 0.05000 - Train loss: 17.90550 - Test loss: 54.17271\n",
      "Epoch 1413 - lr: 0.05000 - Train loss: 17.90549 - Test loss: 54.17269\n",
      "Epoch 1414 - lr: 0.05000 - Train loss: 17.90548 - Test loss: 54.17266\n",
      "Epoch 1415 - lr: 0.05000 - Train loss: 17.90548 - Test loss: 54.17263\n",
      "Epoch 1416 - lr: 0.05000 - Train loss: 17.90547 - Test loss: 54.17261\n",
      "Epoch 1417 - lr: 0.05000 - Train loss: 17.90546 - Test loss: 54.17258\n",
      "Epoch 1418 - lr: 0.05000 - Train loss: 17.90546 - Test loss: 54.17255\n",
      "Epoch 1419 - lr: 0.05000 - Train loss: 17.90545 - Test loss: 54.17253\n",
      "Epoch 1420 - lr: 0.05000 - Train loss: 17.90545 - Test loss: 54.17250\n",
      "Epoch 1421 - lr: 0.05000 - Train loss: 17.90544 - Test loss: 54.17247\n",
      "Epoch 1422 - lr: 0.05000 - Train loss: 17.90543 - Test loss: 54.17244\n",
      "Epoch 1423 - lr: 0.05000 - Train loss: 17.90543 - Test loss: 54.17242\n",
      "Epoch 1424 - lr: 0.05000 - Train loss: 17.90542 - Test loss: 54.17239\n",
      "Epoch 1425 - lr: 0.05000 - Train loss: 17.90541 - Test loss: 54.17236\n",
      "Epoch 1426 - lr: 0.05000 - Train loss: 17.90541 - Test loss: 54.17233\n",
      "Epoch 1427 - lr: 0.05000 - Train loss: 17.90540 - Test loss: 54.17231\n",
      "Epoch 1428 - lr: 0.05000 - Train loss: 17.90540 - Test loss: 54.17228\n",
      "Epoch 1429 - lr: 0.05000 - Train loss: 17.90539 - Test loss: 54.17225\n",
      "Epoch 1430 - lr: 0.05000 - Train loss: 17.90538 - Test loss: 54.17222\n",
      "Epoch 1431 - lr: 0.05000 - Train loss: 17.90538 - Test loss: 54.17219\n",
      "Epoch 1432 - lr: 0.05000 - Train loss: 17.90537 - Test loss: 54.17216\n",
      "Epoch 1433 - lr: 0.05000 - Train loss: 17.90536 - Test loss: 54.17214\n",
      "Epoch 1434 - lr: 0.05000 - Train loss: 17.90536 - Test loss: 54.17211\n",
      "Epoch 1435 - lr: 0.05000 - Train loss: 17.90535 - Test loss: 54.17208\n",
      "Epoch 1436 - lr: 0.05000 - Train loss: 17.90535 - Test loss: 54.17205\n",
      "Epoch 1437 - lr: 0.05000 - Train loss: 17.90534 - Test loss: 54.17202\n",
      "Epoch 1438 - lr: 0.05000 - Train loss: 17.90533 - Test loss: 54.17199\n",
      "Epoch 1439 - lr: 0.05000 - Train loss: 17.90533 - Test loss: 54.17196\n",
      "Epoch 1440 - lr: 0.05000 - Train loss: 17.90532 - Test loss: 54.17193\n",
      "Epoch 1441 - lr: 0.05000 - Train loss: 17.90532 - Test loss: 54.17190\n",
      "Epoch 1442 - lr: 0.05000 - Train loss: 17.90531 - Test loss: 54.17187\n",
      "Epoch 1443 - lr: 0.05000 - Train loss: 17.90530 - Test loss: 54.17185\n",
      "Epoch 1444 - lr: 0.05000 - Train loss: 17.90530 - Test loss: 54.17182\n",
      "Epoch 1445 - lr: 0.05000 - Train loss: 17.90529 - Test loss: 54.17179\n",
      "Epoch 1446 - lr: 0.05000 - Train loss: 17.90529 - Test loss: 54.17176\n",
      "Epoch 1447 - lr: 0.05000 - Train loss: 17.90528 - Test loss: 54.17173\n",
      "Epoch 1448 - lr: 0.05000 - Train loss: 17.90527 - Test loss: 54.17170\n",
      "Epoch 1449 - lr: 0.05000 - Train loss: 17.90527 - Test loss: 54.17167\n",
      "Epoch 1450 - lr: 0.05000 - Train loss: 17.90526 - Test loss: 54.17164\n",
      "Epoch 1451 - lr: 0.05000 - Train loss: 17.90526 - Test loss: 54.17161\n",
      "Epoch 1452 - lr: 0.05000 - Train loss: 17.90525 - Test loss: 54.17158\n",
      "Epoch 1453 - lr: 0.05000 - Train loss: 17.90524 - Test loss: 54.17154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1454 - lr: 0.05000 - Train loss: 17.90524 - Test loss: 54.17151\n",
      "Epoch 1455 - lr: 0.05000 - Train loss: 17.90523 - Test loss: 54.17148\n",
      "Epoch 1456 - lr: 0.05000 - Train loss: 17.90523 - Test loss: 54.17145\n",
      "Epoch 1457 - lr: 0.05000 - Train loss: 17.90522 - Test loss: 54.17142\n",
      "Epoch 1458 - lr: 0.05000 - Train loss: 17.90522 - Test loss: 54.17139\n",
      "Epoch 1459 - lr: 0.05000 - Train loss: 17.90521 - Test loss: 54.17136\n",
      "Epoch 1460 - lr: 0.05000 - Train loss: 17.90520 - Test loss: 54.17133\n",
      "Epoch 1461 - lr: 0.05000 - Train loss: 17.90520 - Test loss: 54.17130\n",
      "Epoch 1462 - lr: 0.05000 - Train loss: 17.90519 - Test loss: 54.17127\n",
      "Epoch 1463 - lr: 0.05000 - Train loss: 17.90519 - Test loss: 54.17123\n",
      "Epoch 1464 - lr: 0.05000 - Train loss: 17.90518 - Test loss: 54.17120\n",
      "Epoch 1465 - lr: 0.05000 - Train loss: 17.90518 - Test loss: 54.17117\n",
      "Epoch 1466 - lr: 0.05000 - Train loss: 17.90517 - Test loss: 54.17114\n",
      "Epoch 1467 - lr: 0.05000 - Train loss: 17.90516 - Test loss: 54.17111\n",
      "Epoch 1468 - lr: 0.05000 - Train loss: 17.90516 - Test loss: 54.17108\n",
      "Epoch 1469 - lr: 0.05000 - Train loss: 17.90515 - Test loss: 54.17104\n",
      "Epoch 1470 - lr: 0.05000 - Train loss: 17.90515 - Test loss: 54.17101\n",
      "Epoch 1471 - lr: 0.05000 - Train loss: 17.90514 - Test loss: 54.17098\n",
      "Epoch 1472 - lr: 0.05000 - Train loss: 17.90514 - Test loss: 54.17095\n",
      "Epoch 1473 - lr: 0.05000 - Train loss: 17.90513 - Test loss: 54.17091\n",
      "Epoch 1474 - lr: 0.05000 - Train loss: 17.90513 - Test loss: 54.17088\n",
      "Epoch 1475 - lr: 0.05000 - Train loss: 17.90512 - Test loss: 54.17085\n",
      "Epoch 1476 - lr: 0.05000 - Train loss: 17.90511 - Test loss: 54.17082\n",
      "Epoch 1477 - lr: 0.05000 - Train loss: 17.90511 - Test loss: 54.17078\n",
      "Epoch 1478 - lr: 0.05000 - Train loss: 17.90510 - Test loss: 54.17075\n",
      "Epoch 1479 - lr: 0.05000 - Train loss: 17.90510 - Test loss: 54.17072\n",
      "Epoch 1480 - lr: 0.05000 - Train loss: 17.90509 - Test loss: 54.17069\n",
      "Epoch 1481 - lr: 0.05000 - Train loss: 17.90509 - Test loss: 54.17065\n",
      "Epoch 1482 - lr: 0.05000 - Train loss: 17.90508 - Test loss: 54.17062\n",
      "Epoch 1483 - lr: 0.05000 - Train loss: 17.90508 - Test loss: 54.17059\n",
      "Epoch 1484 - lr: 0.05000 - Train loss: 17.90507 - Test loss: 54.17055\n",
      "Epoch 1485 - lr: 0.05000 - Train loss: 17.90507 - Test loss: 54.17052\n",
      "Epoch 1486 - lr: 0.05000 - Train loss: 17.90506 - Test loss: 54.17049\n",
      "Epoch 1487 - lr: 0.05000 - Train loss: 17.90505 - Test loss: 54.17045\n",
      "Epoch 1488 - lr: 0.05000 - Train loss: 17.90505 - Test loss: 54.17042\n",
      "Epoch 1489 - lr: 0.05000 - Train loss: 17.90504 - Test loss: 54.17038\n",
      "Epoch 1490 - lr: 0.05000 - Train loss: 17.90504 - Test loss: 54.17035\n",
      "Epoch 1491 - lr: 0.05000 - Train loss: 17.90503 - Test loss: 54.17032\n",
      "Epoch 1492 - lr: 0.05000 - Train loss: 17.90503 - Test loss: 54.17028\n",
      "Epoch 1493 - lr: 0.05000 - Train loss: 17.90502 - Test loss: 54.17025\n",
      "Epoch 1494 - lr: 0.05000 - Train loss: 17.90502 - Test loss: 54.17021\n",
      "Epoch 1495 - lr: 0.05000 - Train loss: 17.90501 - Test loss: 54.17018\n",
      "Epoch 1496 - lr: 0.05000 - Train loss: 17.90501 - Test loss: 54.17014\n",
      "Epoch 1497 - lr: 0.05000 - Train loss: 17.90500 - Test loss: 54.17011\n",
      "Epoch 1498 - lr: 0.05000 - Train loss: 17.90500 - Test loss: 54.17008\n",
      "Epoch 1499 - lr: 0.05000 - Train loss: 17.90499 - Test loss: 54.17004\n",
      "Epoch 1500 - lr: 0.05000 - Train loss: 17.90499 - Test loss: 54.17001\n",
      "Epoch 1501 - lr: 0.05000 - Train loss: 17.90498 - Test loss: 54.16997\n",
      "Epoch 1502 - lr: 0.05000 - Train loss: 17.90498 - Test loss: 54.16994\n",
      "Epoch 1503 - lr: 0.05000 - Train loss: 17.90497 - Test loss: 54.16990\n",
      "Epoch 1504 - lr: 0.05000 - Train loss: 17.90497 - Test loss: 54.16987\n",
      "Epoch 1505 - lr: 0.05000 - Train loss: 17.90496 - Test loss: 54.16983\n",
      "Epoch 1506 - lr: 0.05000 - Train loss: 17.90496 - Test loss: 54.16980\n",
      "Epoch 1507 - lr: 0.05000 - Train loss: 17.90495 - Test loss: 54.16976\n",
      "Epoch 1508 - lr: 0.05000 - Train loss: 17.90495 - Test loss: 54.16972\n",
      "Epoch 1509 - lr: 0.05000 - Train loss: 17.90494 - Test loss: 54.16969\n",
      "Epoch 1510 - lr: 0.05000 - Train loss: 17.90494 - Test loss: 54.16965\n",
      "Epoch 1511 - lr: 0.05000 - Train loss: 17.90493 - Test loss: 54.16962\n",
      "Epoch 1512 - lr: 0.05000 - Train loss: 17.90493 - Test loss: 54.16958\n",
      "Epoch 1513 - lr: 0.05000 - Train loss: 17.90492 - Test loss: 54.16955\n",
      "Epoch 1514 - lr: 0.05000 - Train loss: 17.90492 - Test loss: 54.16951\n",
      "Epoch 1515 - lr: 0.05000 - Train loss: 17.90491 - Test loss: 54.16947\n",
      "Epoch 1516 - lr: 0.05000 - Train loss: 17.90491 - Test loss: 54.16944\n",
      "Epoch 1517 - lr: 0.05000 - Train loss: 17.90490 - Test loss: 54.16940\n",
      "Epoch 1518 - lr: 0.05000 - Train loss: 17.90490 - Test loss: 54.16936\n",
      "Epoch 1519 - lr: 0.05000 - Train loss: 17.90489 - Test loss: 54.16933\n",
      "Epoch 1520 - lr: 0.05000 - Train loss: 17.90489 - Test loss: 54.16929\n",
      "Epoch 1521 - lr: 0.05000 - Train loss: 17.90488 - Test loss: 54.16926\n",
      "Epoch 1522 - lr: 0.05000 - Train loss: 17.90488 - Test loss: 54.16922\n",
      "Epoch 1523 - lr: 0.05000 - Train loss: 17.90487 - Test loss: 54.16918\n",
      "Epoch 1524 - lr: 0.05000 - Train loss: 17.90487 - Test loss: 54.16914\n",
      "Epoch 1525 - lr: 0.05000 - Train loss: 17.90486 - Test loss: 54.16911\n",
      "Epoch 1526 - lr: 0.05000 - Train loss: 17.90486 - Test loss: 54.16907\n",
      "Epoch 1527 - lr: 0.05000 - Train loss: 17.90485 - Test loss: 54.16903\n",
      "Epoch 1528 - lr: 0.05000 - Train loss: 17.90485 - Test loss: 54.16900\n",
      "Epoch 1529 - lr: 0.05000 - Train loss: 17.90484 - Test loss: 54.16896\n",
      "Epoch 1530 - lr: 0.05000 - Train loss: 17.90484 - Test loss: 54.16892\n",
      "Epoch 1531 - lr: 0.05000 - Train loss: 17.90483 - Test loss: 54.16888\n",
      "Epoch 1532 - lr: 0.05000 - Train loss: 17.90483 - Test loss: 54.16885\n",
      "Epoch 1533 - lr: 0.05000 - Train loss: 17.90482 - Test loss: 54.16881\n",
      "Epoch 1534 - lr: 0.05000 - Train loss: 17.90482 - Test loss: 54.16877\n",
      "Epoch 1535 - lr: 0.05000 - Train loss: 17.90481 - Test loss: 54.16873\n",
      "Epoch 1536 - lr: 0.05000 - Train loss: 17.90481 - Test loss: 54.16870\n",
      "Epoch 1537 - lr: 0.05000 - Train loss: 17.90480 - Test loss: 54.16866\n",
      "Epoch 1538 - lr: 0.05000 - Train loss: 17.90480 - Test loss: 54.16862\n",
      "Epoch 1539 - lr: 0.05000 - Train loss: 17.90479 - Test loss: 54.16858\n",
      "Epoch 1540 - lr: 0.05000 - Train loss: 17.90479 - Test loss: 54.16854\n",
      "Epoch 1541 - lr: 0.05000 - Train loss: 17.90479 - Test loss: 54.16851\n",
      "Epoch 1542 - lr: 0.05000 - Train loss: 17.90478 - Test loss: 54.16847\n",
      "Epoch 1543 - lr: 0.05000 - Train loss: 17.90478 - Test loss: 54.16843\n",
      "Epoch 1544 - lr: 0.05000 - Train loss: 17.90477 - Test loss: 54.16839\n",
      "Epoch 1545 - lr: 0.05000 - Train loss: 17.90477 - Test loss: 54.16835\n",
      "Epoch 1546 - lr: 0.05000 - Train loss: 17.90476 - Test loss: 54.16831\n",
      "Epoch 1547 - lr: 0.05000 - Train loss: 17.90476 - Test loss: 54.16827\n",
      "Epoch 1548 - lr: 0.05000 - Train loss: 17.90475 - Test loss: 54.16824\n",
      "Epoch 1549 - lr: 0.05000 - Train loss: 17.90475 - Test loss: 54.16820\n",
      "Epoch 1550 - lr: 0.05000 - Train loss: 17.90474 - Test loss: 54.16816\n",
      "Epoch 1551 - lr: 0.05000 - Train loss: 17.90474 - Test loss: 54.16812\n",
      "Epoch 1552 - lr: 0.05000 - Train loss: 17.90474 - Test loss: 54.16808\n",
      "Epoch 1553 - lr: 0.05000 - Train loss: 17.90473 - Test loss: 54.16804\n",
      "Epoch 1554 - lr: 0.05000 - Train loss: 17.90473 - Test loss: 54.16800\n",
      "Epoch 1555 - lr: 0.05000 - Train loss: 17.90472 - Test loss: 54.16796\n",
      "Epoch 1556 - lr: 0.05000 - Train loss: 17.90472 - Test loss: 54.16792\n",
      "Epoch 1557 - lr: 0.05000 - Train loss: 17.90471 - Test loss: 54.16788\n",
      "Epoch 1558 - lr: 0.05000 - Train loss: 17.90471 - Test loss: 54.16784\n",
      "Epoch 1559 - lr: 0.05000 - Train loss: 17.90470 - Test loss: 54.16780\n",
      "Epoch 1560 - lr: 0.05000 - Train loss: 17.90470 - Test loss: 54.16776\n",
      "Epoch 1561 - lr: 0.05000 - Train loss: 17.90469 - Test loss: 54.16772\n",
      "Epoch 1562 - lr: 0.05000 - Train loss: 17.90469 - Test loss: 54.16768\n",
      "Epoch 1563 - lr: 0.05000 - Train loss: 17.90469 - Test loss: 54.16764\n",
      "Epoch 1564 - lr: 0.05000 - Train loss: 17.90468 - Test loss: 54.16760\n",
      "Epoch 1565 - lr: 0.05000 - Train loss: 17.90468 - Test loss: 54.16756\n",
      "Epoch 1566 - lr: 0.05000 - Train loss: 17.90467 - Test loss: 54.16752\n",
      "Epoch 1567 - lr: 0.05000 - Train loss: 17.90467 - Test loss: 54.16748\n",
      "Epoch 1568 - lr: 0.05000 - Train loss: 17.90466 - Test loss: 54.16744\n",
      "Epoch 1569 - lr: 0.05000 - Train loss: 17.90466 - Test loss: 54.16740\n",
      "Epoch 1570 - lr: 0.05000 - Train loss: 17.90466 - Test loss: 54.16736\n",
      "Epoch 1571 - lr: 0.05000 - Train loss: 17.90465 - Test loss: 54.16732\n",
      "Epoch 1572 - lr: 0.05000 - Train loss: 17.90465 - Test loss: 54.16728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1573 - lr: 0.05000 - Train loss: 17.90464 - Test loss: 54.16724\n",
      "Epoch 1574 - lr: 0.05000 - Train loss: 17.90464 - Test loss: 54.16720\n",
      "Epoch 1575 - lr: 0.05000 - Train loss: 17.90463 - Test loss: 54.16716\n",
      "Epoch 1576 - lr: 0.05000 - Train loss: 17.90463 - Test loss: 54.16712\n",
      "Epoch 1577 - lr: 0.05000 - Train loss: 17.90463 - Test loss: 54.16707\n",
      "Epoch 1578 - lr: 0.05000 - Train loss: 17.90462 - Test loss: 54.16703\n",
      "Epoch 1579 - lr: 0.05000 - Train loss: 17.90462 - Test loss: 54.16699\n",
      "Epoch 1580 - lr: 0.05000 - Train loss: 17.90461 - Test loss: 54.16695\n",
      "Epoch 1581 - lr: 0.05000 - Train loss: 17.90461 - Test loss: 54.16691\n",
      "Epoch 1582 - lr: 0.05000 - Train loss: 17.90461 - Test loss: 54.16687\n",
      "Epoch 1583 - lr: 0.05000 - Train loss: 17.90460 - Test loss: 54.16683\n",
      "Epoch 1584 - lr: 0.05000 - Train loss: 17.90460 - Test loss: 54.16679\n",
      "Epoch 1585 - lr: 0.05000 - Train loss: 17.90459 - Test loss: 54.16674\n",
      "Epoch 1586 - lr: 0.05000 - Train loss: 17.90459 - Test loss: 54.16670\n",
      "Epoch 1587 - lr: 0.05000 - Train loss: 17.90458 - Test loss: 54.16666\n",
      "Epoch 1588 - lr: 0.05000 - Train loss: 17.90458 - Test loss: 54.16662\n",
      "Epoch 1589 - lr: 0.05000 - Train loss: 17.90458 - Test loss: 54.16658\n",
      "Epoch 1590 - lr: 0.05000 - Train loss: 17.90457 - Test loss: 54.16653\n",
      "Epoch 1591 - lr: 0.05000 - Train loss: 17.90457 - Test loss: 54.16649\n",
      "Epoch 1592 - lr: 0.05000 - Train loss: 17.90456 - Test loss: 54.16645\n",
      "Epoch 1593 - lr: 0.05000 - Train loss: 17.90456 - Test loss: 54.16641\n",
      "Epoch 1594 - lr: 0.05000 - Train loss: 17.90456 - Test loss: 54.16636\n",
      "Epoch 1595 - lr: 0.05000 - Train loss: 17.90455 - Test loss: 54.16632\n",
      "Epoch 1596 - lr: 0.05000 - Train loss: 17.90455 - Test loss: 54.16628\n",
      "Epoch 1597 - lr: 0.05000 - Train loss: 17.90454 - Test loss: 54.16624\n",
      "Epoch 1598 - lr: 0.05000 - Train loss: 17.90454 - Test loss: 54.16619\n",
      "Epoch 1599 - lr: 0.05000 - Train loss: 17.90454 - Test loss: 54.16615\n",
      "Epoch 1600 - lr: 0.05000 - Train loss: 17.90453 - Test loss: 54.16611\n",
      "Epoch 1601 - lr: 0.05000 - Train loss: 17.90453 - Test loss: 54.16607\n",
      "Epoch 1602 - lr: 0.05000 - Train loss: 17.90452 - Test loss: 54.16602\n",
      "Epoch 1603 - lr: 0.05000 - Train loss: 17.90452 - Test loss: 54.16598\n",
      "Epoch 1604 - lr: 0.05000 - Train loss: 17.90452 - Test loss: 54.16594\n",
      "Epoch 1605 - lr: 0.05000 - Train loss: 17.90451 - Test loss: 54.16589\n",
      "Epoch 1606 - lr: 0.05000 - Train loss: 17.90451 - Test loss: 54.16585\n",
      "Epoch 1607 - lr: 0.05000 - Train loss: 17.90450 - Test loss: 54.16581\n",
      "Epoch 1608 - lr: 0.05000 - Train loss: 17.90450 - Test loss: 54.16576\n",
      "Epoch 1609 - lr: 0.05000 - Train loss: 17.90450 - Test loss: 54.16572\n",
      "Epoch 1610 - lr: 0.05000 - Train loss: 17.90449 - Test loss: 54.16568\n",
      "Epoch 1611 - lr: 0.05000 - Train loss: 17.90449 - Test loss: 54.16563\n",
      "Epoch 1612 - lr: 0.05000 - Train loss: 17.90449 - Test loss: 54.16559\n",
      "Epoch 1613 - lr: 0.05000 - Train loss: 17.90448 - Test loss: 54.16554\n",
      "Epoch 1614 - lr: 0.05000 - Train loss: 17.90448 - Test loss: 54.16550\n",
      "Epoch 1615 - lr: 0.05000 - Train loss: 17.90447 - Test loss: 54.16546\n",
      "Epoch 1616 - lr: 0.05000 - Train loss: 17.90447 - Test loss: 54.16541\n",
      "Epoch 1617 - lr: 0.05000 - Train loss: 17.90447 - Test loss: 54.16537\n",
      "Epoch 1618 - lr: 0.05000 - Train loss: 17.90446 - Test loss: 54.16532\n",
      "Epoch 1619 - lr: 0.05000 - Train loss: 17.90446 - Test loss: 54.16528\n",
      "Epoch 1620 - lr: 0.05000 - Train loss: 17.90446 - Test loss: 54.16524\n",
      "Epoch 1621 - lr: 0.05000 - Train loss: 17.90445 - Test loss: 54.16519\n",
      "Epoch 1622 - lr: 0.05000 - Train loss: 17.90445 - Test loss: 54.16515\n",
      "Epoch 1623 - lr: 0.05000 - Train loss: 17.90444 - Test loss: 54.16510\n",
      "Epoch 1624 - lr: 0.05000 - Train loss: 17.90444 - Test loss: 54.16506\n",
      "Epoch 1625 - lr: 0.05000 - Train loss: 17.90444 - Test loss: 54.16501\n",
      "Epoch 1626 - lr: 0.05000 - Train loss: 17.90443 - Test loss: 54.16497\n",
      "Epoch 1627 - lr: 0.05000 - Train loss: 17.90443 - Test loss: 54.16492\n",
      "Epoch 1628 - lr: 0.05000 - Train loss: 17.90443 - Test loss: 54.16488\n",
      "Epoch 1629 - lr: 0.05000 - Train loss: 17.90442 - Test loss: 54.16483\n",
      "Epoch 1630 - lr: 0.05000 - Train loss: 17.90442 - Test loss: 54.16479\n",
      "Epoch 1631 - lr: 0.05000 - Train loss: 17.90441 - Test loss: 54.16474\n",
      "Epoch 1632 - lr: 0.05000 - Train loss: 17.90441 - Test loss: 54.16470\n",
      "Epoch 1633 - lr: 0.05000 - Train loss: 17.90441 - Test loss: 54.16465\n",
      "Epoch 1634 - lr: 0.05000 - Train loss: 17.90440 - Test loss: 54.16461\n",
      "Epoch 1635 - lr: 0.05000 - Train loss: 17.90440 - Test loss: 54.16456\n",
      "Epoch 1636 - lr: 0.05000 - Train loss: 17.90440 - Test loss: 54.16452\n",
      "Epoch 1637 - lr: 0.05000 - Train loss: 17.90439 - Test loss: 54.16447\n",
      "Epoch 1638 - lr: 0.05000 - Train loss: 17.90439 - Test loss: 54.16443\n",
      "Epoch 1639 - lr: 0.05000 - Train loss: 17.90439 - Test loss: 54.16438\n",
      "Epoch 1640 - lr: 0.05000 - Train loss: 17.90438 - Test loss: 54.16433\n",
      "Epoch 1641 - lr: 0.05000 - Train loss: 17.90438 - Test loss: 54.16429\n",
      "Epoch 1642 - lr: 0.05000 - Train loss: 17.90438 - Test loss: 54.16424\n",
      "Epoch 1643 - lr: 0.05000 - Train loss: 17.90437 - Test loss: 54.16420\n",
      "Epoch 1644 - lr: 0.05000 - Train loss: 17.90437 - Test loss: 54.16415\n",
      "Epoch 1645 - lr: 0.05000 - Train loss: 17.90436 - Test loss: 54.16410\n",
      "Epoch 1646 - lr: 0.05000 - Train loss: 17.90436 - Test loss: 54.16406\n",
      "Epoch 1647 - lr: 0.05000 - Train loss: 17.90436 - Test loss: 54.16401\n",
      "Epoch 1648 - lr: 0.05000 - Train loss: 17.90435 - Test loss: 54.16397\n",
      "Epoch 1649 - lr: 0.05000 - Train loss: 17.90435 - Test loss: 54.16392\n",
      "Epoch 1650 - lr: 0.05000 - Train loss: 17.90435 - Test loss: 54.16387\n",
      "Epoch 1651 - lr: 0.05000 - Train loss: 17.90434 - Test loss: 54.16383\n",
      "Epoch 1652 - lr: 0.05000 - Train loss: 17.90434 - Test loss: 54.16378\n",
      "Epoch 1653 - lr: 0.05000 - Train loss: 17.90434 - Test loss: 54.16373\n",
      "Epoch 1654 - lr: 0.05000 - Train loss: 17.90433 - Test loss: 54.16369\n",
      "Epoch 1655 - lr: 0.05000 - Train loss: 17.90433 - Test loss: 54.16364\n",
      "Epoch 1656 - lr: 0.05000 - Train loss: 17.90433 - Test loss: 54.16359\n",
      "Epoch 1657 - lr: 0.05000 - Train loss: 17.90432 - Test loss: 54.16355\n",
      "Epoch 1658 - lr: 0.05000 - Train loss: 17.90432 - Test loss: 54.16350\n",
      "Epoch 1659 - lr: 0.05000 - Train loss: 17.90432 - Test loss: 54.16345\n",
      "Epoch 1660 - lr: 0.05000 - Train loss: 17.90431 - Test loss: 54.16340\n",
      "Epoch 1661 - lr: 0.05000 - Train loss: 17.90431 - Test loss: 54.16336\n",
      "Epoch 1662 - lr: 0.05000 - Train loss: 17.90431 - Test loss: 54.16331\n",
      "Epoch 1663 - lr: 0.05000 - Train loss: 17.90430 - Test loss: 54.16326\n",
      "Epoch 1664 - lr: 0.05000 - Train loss: 17.90430 - Test loss: 54.16322\n",
      "Epoch 1665 - lr: 0.05000 - Train loss: 17.90430 - Test loss: 54.16317\n",
      "Epoch 1666 - lr: 0.05000 - Train loss: 17.90429 - Test loss: 54.16312\n",
      "Epoch 1667 - lr: 0.05000 - Train loss: 17.90429 - Test loss: 54.16307\n",
      "Epoch 1668 - lr: 0.05000 - Train loss: 17.90429 - Test loss: 54.16302\n",
      "Epoch 1669 - lr: 0.05000 - Train loss: 17.90428 - Test loss: 54.16298\n",
      "Epoch 1670 - lr: 0.05000 - Train loss: 17.90428 - Test loss: 54.16293\n",
      "Epoch 1671 - lr: 0.05000 - Train loss: 17.90428 - Test loss: 54.16288\n",
      "Epoch 1672 - lr: 0.05000 - Train loss: 17.90427 - Test loss: 54.16283\n",
      "Epoch 1673 - lr: 0.05000 - Train loss: 17.90427 - Test loss: 54.16278\n",
      "Epoch 1674 - lr: 0.05000 - Train loss: 17.90427 - Test loss: 54.16274\n",
      "Epoch 1675 - lr: 0.05000 - Train loss: 17.90426 - Test loss: 54.16269\n",
      "Epoch 1676 - lr: 0.05000 - Train loss: 17.90426 - Test loss: 54.16264\n",
      "Epoch 1677 - lr: 0.05000 - Train loss: 17.90426 - Test loss: 54.16259\n",
      "Epoch 1678 - lr: 0.05000 - Train loss: 17.90425 - Test loss: 54.16254\n",
      "Epoch 1679 - lr: 0.05000 - Train loss: 17.90425 - Test loss: 54.16249\n",
      "Epoch 1680 - lr: 0.05000 - Train loss: 17.90425 - Test loss: 54.16245\n",
      "Epoch 1681 - lr: 0.05000 - Train loss: 17.90425 - Test loss: 54.16240\n",
      "Epoch 1682 - lr: 0.05000 - Train loss: 17.90424 - Test loss: 54.16235\n",
      "Epoch 1683 - lr: 0.05000 - Train loss: 17.90424 - Test loss: 54.16230\n",
      "Epoch 1684 - lr: 0.05000 - Train loss: 17.90424 - Test loss: 54.16225\n",
      "Epoch 1685 - lr: 0.05000 - Train loss: 17.90423 - Test loss: 54.16220\n",
      "Epoch 1686 - lr: 0.05000 - Train loss: 17.90423 - Test loss: 54.16215\n",
      "Epoch 1687 - lr: 0.05000 - Train loss: 17.90423 - Test loss: 54.16210\n",
      "Epoch 1688 - lr: 0.05000 - Train loss: 17.90422 - Test loss: 54.16205\n",
      "Epoch 1689 - lr: 0.05000 - Train loss: 17.90422 - Test loss: 54.16201\n",
      "Epoch 1690 - lr: 0.05000 - Train loss: 17.90422 - Test loss: 54.16196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1691 - lr: 0.05000 - Train loss: 17.90421 - Test loss: 54.16191\n",
      "Epoch 1692 - lr: 0.05000 - Train loss: 17.90421 - Test loss: 54.16186\n",
      "Epoch 1693 - lr: 0.05000 - Train loss: 17.90421 - Test loss: 54.16181\n",
      "Epoch 1694 - lr: 0.05000 - Train loss: 17.90421 - Test loss: 54.16176\n",
      "Epoch 1695 - lr: 0.05000 - Train loss: 17.90420 - Test loss: 54.16171\n",
      "Epoch 1696 - lr: 0.05000 - Train loss: 17.90420 - Test loss: 54.16166\n",
      "Epoch 1697 - lr: 0.05000 - Train loss: 17.90420 - Test loss: 54.16161\n",
      "Epoch 1698 - lr: 0.05000 - Train loss: 17.90419 - Test loss: 54.16156\n",
      "Epoch 1699 - lr: 0.05000 - Train loss: 17.90419 - Test loss: 54.16151\n",
      "Epoch 1700 - lr: 0.05000 - Train loss: 17.90419 - Test loss: 54.16146\n",
      "Epoch 1701 - lr: 0.05000 - Train loss: 17.90418 - Test loss: 54.16141\n",
      "Epoch 1702 - lr: 0.05000 - Train loss: 17.90418 - Test loss: 54.16136\n",
      "Epoch 1703 - lr: 0.05000 - Train loss: 17.90418 - Test loss: 54.16131\n",
      "Epoch 1704 - lr: 0.05000 - Train loss: 17.90418 - Test loss: 54.16126\n",
      "Epoch 1705 - lr: 0.05000 - Train loss: 17.90417 - Test loss: 54.16121\n",
      "Epoch 1706 - lr: 0.05000 - Train loss: 17.90417 - Test loss: 54.16116\n",
      "Epoch 1707 - lr: 0.05000 - Train loss: 17.90417 - Test loss: 54.16111\n",
      "Epoch 1708 - lr: 0.05000 - Train loss: 17.90416 - Test loss: 54.16106\n",
      "Epoch 1709 - lr: 0.05000 - Train loss: 17.90416 - Test loss: 54.16101\n",
      "Epoch 1710 - lr: 0.05000 - Train loss: 17.90416 - Test loss: 54.16095\n",
      "Epoch 1711 - lr: 0.05000 - Train loss: 17.90416 - Test loss: 54.16090\n",
      "Epoch 1712 - lr: 0.05000 - Train loss: 17.90415 - Test loss: 54.16085\n",
      "Epoch 1713 - lr: 0.05000 - Train loss: 17.90415 - Test loss: 54.16080\n",
      "Epoch 1714 - lr: 0.05000 - Train loss: 17.90415 - Test loss: 54.16075\n",
      "Epoch 1715 - lr: 0.05000 - Train loss: 17.90414 - Test loss: 54.16070\n",
      "Epoch 1716 - lr: 0.05000 - Train loss: 17.90414 - Test loss: 54.16065\n",
      "Epoch 1717 - lr: 0.05000 - Train loss: 17.90414 - Test loss: 54.16060\n",
      "Epoch 1718 - lr: 0.05000 - Train loss: 17.90414 - Test loss: 54.16055\n",
      "Epoch 1719 - lr: 0.05000 - Train loss: 17.90413 - Test loss: 54.16049\n",
      "Epoch 1720 - lr: 0.05000 - Train loss: 17.90413 - Test loss: 54.16044\n",
      "Epoch 1721 - lr: 0.05000 - Train loss: 17.90413 - Test loss: 54.16039\n",
      "Epoch 1722 - lr: 0.05000 - Train loss: 17.90412 - Test loss: 54.16034\n",
      "Epoch 1723 - lr: 0.05000 - Train loss: 17.90412 - Test loss: 54.16029\n",
      "Epoch 1724 - lr: 0.05000 - Train loss: 17.90412 - Test loss: 54.16024\n",
      "Epoch 1725 - lr: 0.05000 - Train loss: 17.90412 - Test loss: 54.16018\n",
      "Epoch 1726 - lr: 0.05000 - Train loss: 17.90411 - Test loss: 54.16013\n",
      "Epoch 1727 - lr: 0.05000 - Train loss: 17.90411 - Test loss: 54.16008\n",
      "Epoch 1728 - lr: 0.05000 - Train loss: 17.90411 - Test loss: 54.16003\n",
      "Epoch 1729 - lr: 0.05000 - Train loss: 17.90411 - Test loss: 54.15998\n",
      "Epoch 1730 - lr: 0.05000 - Train loss: 17.90410 - Test loss: 54.15992\n",
      "Epoch 1731 - lr: 0.05000 - Train loss: 17.90410 - Test loss: 54.15987\n",
      "Epoch 1732 - lr: 0.05000 - Train loss: 17.90410 - Test loss: 54.15982\n",
      "Epoch 1733 - lr: 0.05000 - Train loss: 17.90409 - Test loss: 54.15977\n",
      "Epoch 1734 - lr: 0.05000 - Train loss: 17.90409 - Test loss: 54.15971\n",
      "Epoch 1735 - lr: 0.05000 - Train loss: 17.90409 - Test loss: 54.15966\n",
      "Epoch 1736 - lr: 0.05000 - Train loss: 17.90409 - Test loss: 54.15961\n",
      "Epoch 1737 - lr: 0.05000 - Train loss: 17.90408 - Test loss: 54.15956\n",
      "Epoch 1738 - lr: 0.05000 - Train loss: 17.90408 - Test loss: 54.15950\n",
      "Epoch 1739 - lr: 0.05000 - Train loss: 17.90408 - Test loss: 54.15945\n",
      "Epoch 1740 - lr: 0.05000 - Train loss: 17.90408 - Test loss: 54.15940\n",
      "Epoch 1741 - lr: 0.05000 - Train loss: 17.90407 - Test loss: 54.15934\n",
      "Epoch 1742 - lr: 0.05000 - Train loss: 17.90407 - Test loss: 54.15929\n",
      "Epoch 1743 - lr: 0.05000 - Train loss: 17.90407 - Test loss: 54.15924\n",
      "Epoch 1744 - lr: 0.05000 - Train loss: 17.90407 - Test loss: 54.15919\n",
      "Epoch 1745 - lr: 0.05000 - Train loss: 17.90406 - Test loss: 54.15913\n",
      "Epoch 1746 - lr: 0.05000 - Train loss: 17.90406 - Test loss: 54.15908\n",
      "Epoch 1747 - lr: 0.05000 - Train loss: 17.90406 - Test loss: 54.15903\n",
      "Epoch 1748 - lr: 0.05000 - Train loss: 17.90406 - Test loss: 54.15897\n",
      "Epoch 1749 - lr: 0.05000 - Train loss: 17.90405 - Test loss: 54.15892\n",
      "Epoch 1750 - lr: 0.05000 - Train loss: 17.90405 - Test loss: 54.15886\n",
      "Epoch 1751 - lr: 0.05000 - Train loss: 17.90405 - Test loss: 54.15881\n",
      "Epoch 1752 - lr: 0.05000 - Train loss: 17.90405 - Test loss: 54.15876\n",
      "Epoch 1753 - lr: 0.05000 - Train loss: 17.90404 - Test loss: 54.15870\n",
      "Epoch 1754 - lr: 0.05000 - Train loss: 17.90404 - Test loss: 54.15865\n",
      "Epoch 1755 - lr: 0.05000 - Train loss: 17.90404 - Test loss: 54.15859\n",
      "Epoch 1756 - lr: 0.05000 - Train loss: 17.90404 - Test loss: 54.15854\n",
      "Epoch 1757 - lr: 0.05000 - Train loss: 17.90403 - Test loss: 54.15849\n",
      "Epoch 1758 - lr: 0.05000 - Train loss: 17.90403 - Test loss: 54.15843\n",
      "Epoch 1759 - lr: 0.05000 - Train loss: 17.90403 - Test loss: 54.15838\n",
      "Epoch 1760 - lr: 0.05000 - Train loss: 17.90403 - Test loss: 54.15832\n",
      "Epoch 1761 - lr: 0.05000 - Train loss: 17.90402 - Test loss: 54.15827\n",
      "Epoch 1762 - lr: 0.05000 - Train loss: 17.90402 - Test loss: 54.15821\n",
      "Epoch 1763 - lr: 0.05000 - Train loss: 17.90402 - Test loss: 54.15816\n",
      "Epoch 1764 - lr: 0.05000 - Train loss: 17.90402 - Test loss: 54.15810\n",
      "Epoch 1765 - lr: 0.05000 - Train loss: 17.90401 - Test loss: 54.15805\n",
      "Epoch 1766 - lr: 0.05000 - Train loss: 17.90401 - Test loss: 54.15799\n",
      "Epoch 1767 - lr: 0.05000 - Train loss: 17.90401 - Test loss: 54.15794\n",
      "Epoch 1768 - lr: 0.05000 - Train loss: 17.90401 - Test loss: 54.15788\n",
      "Epoch 1769 - lr: 0.05000 - Train loss: 17.90400 - Test loss: 54.15783\n",
      "Epoch 1770 - lr: 0.05000 - Train loss: 17.90400 - Test loss: 54.15777\n",
      "Epoch 1771 - lr: 0.05000 - Train loss: 17.90400 - Test loss: 54.15772\n",
      "Epoch 1772 - lr: 0.05000 - Train loss: 17.90400 - Test loss: 54.15766\n",
      "Epoch 1773 - lr: 0.05000 - Train loss: 17.90399 - Test loss: 54.15761\n",
      "Epoch 1774 - lr: 0.05000 - Train loss: 17.90399 - Test loss: 54.15755\n",
      "Epoch 1775 - lr: 0.05000 - Train loss: 17.90399 - Test loss: 54.15750\n",
      "Epoch 1776 - lr: 0.05000 - Train loss: 17.90399 - Test loss: 54.15744\n",
      "Epoch 1777 - lr: 0.05000 - Train loss: 17.90399 - Test loss: 54.15738\n",
      "Epoch 1778 - lr: 0.05000 - Train loss: 17.90398 - Test loss: 54.15733\n",
      "Epoch 1779 - lr: 0.05000 - Train loss: 17.90398 - Test loss: 54.15727\n",
      "Epoch 1780 - lr: 0.05000 - Train loss: 17.90398 - Test loss: 54.15722\n",
      "Epoch 1781 - lr: 0.05000 - Train loss: 17.90398 - Test loss: 54.15716\n",
      "Epoch 1782 - lr: 0.05000 - Train loss: 17.90397 - Test loss: 54.15710\n",
      "Epoch 1783 - lr: 0.05000 - Train loss: 17.90397 - Test loss: 54.15705\n",
      "Epoch 1784 - lr: 0.05000 - Train loss: 17.90397 - Test loss: 54.15699\n",
      "Epoch 1785 - lr: 0.05000 - Train loss: 17.90397 - Test loss: 54.15693\n",
      "Epoch 1786 - lr: 0.05000 - Train loss: 17.90397 - Test loss: 54.15688\n",
      "Epoch 1787 - lr: 0.05000 - Train loss: 17.90396 - Test loss: 54.15682\n",
      "Epoch 1788 - lr: 0.05000 - Train loss: 17.90396 - Test loss: 54.15676\n",
      "Epoch 1789 - lr: 0.05000 - Train loss: 17.90396 - Test loss: 54.15671\n",
      "Epoch 1790 - lr: 0.05000 - Train loss: 17.90396 - Test loss: 54.15665\n",
      "Epoch 1791 - lr: 0.05000 - Train loss: 17.90395 - Test loss: 54.15659\n",
      "Epoch 1792 - lr: 0.05000 - Train loss: 17.90395 - Test loss: 54.15653\n",
      "Epoch 1793 - lr: 0.05000 - Train loss: 17.90395 - Test loss: 54.15648\n",
      "Epoch 1794 - lr: 0.05000 - Train loss: 17.90395 - Test loss: 54.15642\n",
      "Epoch 1795 - lr: 0.05000 - Train loss: 17.90395 - Test loss: 54.15636\n",
      "Epoch 1796 - lr: 0.05000 - Train loss: 17.90394 - Test loss: 54.15630\n",
      "Epoch 1797 - lr: 0.05000 - Train loss: 17.90394 - Test loss: 54.15625\n",
      "Epoch 1798 - lr: 0.05000 - Train loss: 17.90394 - Test loss: 54.15619\n",
      "Epoch 1799 - lr: 0.05000 - Train loss: 17.90394 - Test loss: 54.15613\n",
      "Epoch 1800 - lr: 0.05000 - Train loss: 17.90394 - Test loss: 54.15607\n",
      "Epoch 1801 - lr: 0.05000 - Train loss: 17.90393 - Test loss: 54.15602\n",
      "Epoch 1802 - lr: 0.05000 - Train loss: 17.90393 - Test loss: 54.15596\n",
      "Epoch 1803 - lr: 0.05000 - Train loss: 17.90393 - Test loss: 54.15590\n",
      "Epoch 1804 - lr: 0.05000 - Train loss: 17.90393 - Test loss: 54.15584\n",
      "Epoch 1805 - lr: 0.05000 - Train loss: 17.90393 - Test loss: 54.15578\n",
      "Epoch 1806 - lr: 0.05000 - Train loss: 17.90392 - Test loss: 54.15572\n",
      "Epoch 1807 - lr: 0.05000 - Train loss: 17.90392 - Test loss: 54.15567\n",
      "Epoch 1808 - lr: 0.05000 - Train loss: 17.90392 - Test loss: 54.15561\n",
      "Epoch 1809 - lr: 0.05000 - Train loss: 17.90392 - Test loss: 54.15555\n",
      "Epoch 1810 - lr: 0.05000 - Train loss: 17.90392 - Test loss: 54.15549\n",
      "Epoch 1811 - lr: 0.05000 - Train loss: 17.90391 - Test loss: 54.15543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1812 - lr: 0.05000 - Train loss: 17.90391 - Test loss: 54.15537\n",
      "Epoch 1813 - lr: 0.05000 - Train loss: 17.90391 - Test loss: 54.15531\n",
      "Epoch 1814 - lr: 0.05000 - Train loss: 17.90391 - Test loss: 54.15525\n",
      "Epoch 1815 - lr: 0.05000 - Train loss: 17.90391 - Test loss: 54.15519\n",
      "Epoch 1816 - lr: 0.05000 - Train loss: 17.90390 - Test loss: 54.15513\n",
      "Epoch 1817 - lr: 0.05000 - Train loss: 17.90390 - Test loss: 54.15507\n",
      "Epoch 1818 - lr: 0.05000 - Train loss: 17.90390 - Test loss: 54.15501\n",
      "Epoch 1819 - lr: 0.05000 - Train loss: 17.90390 - Test loss: 54.15495\n",
      "Epoch 1820 - lr: 0.05000 - Train loss: 17.90390 - Test loss: 54.15489\n",
      "Epoch 1821 - lr: 0.05000 - Train loss: 17.90389 - Test loss: 54.15483\n",
      "Epoch 1822 - lr: 0.05000 - Train loss: 17.90389 - Test loss: 54.15477\n",
      "Epoch 1823 - lr: 0.05000 - Train loss: 17.90389 - Test loss: 54.15471\n",
      "Epoch 1824 - lr: 0.05000 - Train loss: 17.90389 - Test loss: 54.15465\n",
      "Epoch 1825 - lr: 0.05000 - Train loss: 17.90389 - Test loss: 54.15459\n",
      "Epoch 1826 - lr: 0.05000 - Train loss: 17.90389 - Test loss: 54.15453\n",
      "Epoch 1827 - lr: 0.05000 - Train loss: 17.90388 - Test loss: 54.15447\n",
      "Epoch 1828 - lr: 0.05000 - Train loss: 17.90388 - Test loss: 54.15441\n",
      "Epoch 1829 - lr: 0.05000 - Train loss: 17.90388 - Test loss: 54.15435\n",
      "Epoch 1830 - lr: 0.05000 - Train loss: 17.90388 - Test loss: 54.15429\n",
      "Epoch 1831 - lr: 0.05000 - Train loss: 17.90388 - Test loss: 54.15423\n",
      "Epoch 1832 - lr: 0.05000 - Train loss: 17.90388 - Test loss: 54.15417\n",
      "Epoch 1833 - lr: 0.05000 - Train loss: 17.90387 - Test loss: 54.15411\n",
      "Epoch 1834 - lr: 0.05000 - Train loss: 17.90387 - Test loss: 54.15404\n",
      "Epoch 1835 - lr: 0.05000 - Train loss: 17.90387 - Test loss: 54.15398\n",
      "Epoch 1836 - lr: 0.05000 - Train loss: 17.90387 - Test loss: 54.15392\n",
      "Epoch 1837 - lr: 0.05000 - Train loss: 17.90387 - Test loss: 54.15386\n",
      "Epoch 1838 - lr: 0.05000 - Train loss: 17.90386 - Test loss: 54.15380\n",
      "Epoch 1839 - lr: 0.05000 - Train loss: 17.90386 - Test loss: 54.15374\n",
      "Epoch 1840 - lr: 0.05000 - Train loss: 17.90386 - Test loss: 54.15367\n",
      "Epoch 1841 - lr: 0.05000 - Train loss: 17.90386 - Test loss: 54.15361\n",
      "Epoch 1842 - lr: 0.05000 - Train loss: 17.90386 - Test loss: 54.15355\n",
      "Epoch 1843 - lr: 0.05000 - Train loss: 17.90386 - Test loss: 54.15349\n",
      "Epoch 1844 - lr: 0.05000 - Train loss: 17.90385 - Test loss: 54.15342\n",
      "Epoch 1845 - lr: 0.05000 - Train loss: 17.90385 - Test loss: 54.15336\n",
      "Epoch 1846 - lr: 0.05000 - Train loss: 17.90385 - Test loss: 54.15330\n",
      "Epoch 1847 - lr: 0.05000 - Train loss: 17.90385 - Test loss: 54.15323\n",
      "Epoch 1848 - lr: 0.05000 - Train loss: 17.90385 - Test loss: 54.15317\n",
      "Epoch 1849 - lr: 0.05000 - Train loss: 17.90385 - Test loss: 54.15311\n",
      "Epoch 1850 - lr: 0.05000 - Train loss: 17.90385 - Test loss: 54.15304\n",
      "Epoch 1851 - lr: 0.05000 - Train loss: 17.90384 - Test loss: 54.15298\n",
      "Epoch 1852 - lr: 0.05000 - Train loss: 17.90384 - Test loss: 54.15292\n",
      "Epoch 1853 - lr: 0.05000 - Train loss: 17.90384 - Test loss: 54.15285\n",
      "Epoch 1854 - lr: 0.05000 - Train loss: 17.90384 - Test loss: 54.15279\n",
      "Epoch 1855 - lr: 0.05000 - Train loss: 17.90384 - Test loss: 54.15273\n",
      "Epoch 1856 - lr: 0.05000 - Train loss: 17.90384 - Test loss: 54.15266\n",
      "Epoch 1857 - lr: 0.05000 - Train loss: 17.90383 - Test loss: 54.15260\n",
      "Epoch 1858 - lr: 0.05000 - Train loss: 17.90383 - Test loss: 54.15253\n",
      "Epoch 1859 - lr: 0.05000 - Train loss: 17.90383 - Test loss: 54.15247\n",
      "Epoch 1860 - lr: 0.05000 - Train loss: 17.90383 - Test loss: 54.15240\n",
      "Epoch 1861 - lr: 0.05000 - Train loss: 17.90383 - Test loss: 54.15234\n",
      "Epoch 1862 - lr: 0.05000 - Train loss: 17.90383 - Test loss: 54.15227\n",
      "Epoch 1863 - lr: 0.05000 - Train loss: 17.90383 - Test loss: 54.15221\n",
      "Epoch 1864 - lr: 0.05000 - Train loss: 17.90382 - Test loss: 54.15214\n",
      "Epoch 1865 - lr: 0.05000 - Train loss: 17.90382 - Test loss: 54.15208\n",
      "Epoch 1866 - lr: 0.05000 - Train loss: 17.90382 - Test loss: 54.15201\n",
      "Epoch 1867 - lr: 0.05000 - Train loss: 17.90382 - Test loss: 54.15194\n",
      "Epoch 1868 - lr: 0.05000 - Train loss: 17.90382 - Test loss: 54.15188\n",
      "Epoch 1869 - lr: 0.05000 - Train loss: 17.90382 - Test loss: 54.15181\n",
      "Epoch 1870 - lr: 0.05000 - Train loss: 17.90382 - Test loss: 54.15175\n",
      "Epoch 1871 - lr: 0.05000 - Train loss: 17.90382 - Test loss: 54.15168\n",
      "Epoch 1872 - lr: 0.05000 - Train loss: 17.90381 - Test loss: 54.15161\n",
      "Epoch 1873 - lr: 0.05000 - Train loss: 17.90381 - Test loss: 54.15155\n",
      "Epoch 1874 - lr: 0.05000 - Train loss: 17.90381 - Test loss: 54.15148\n",
      "Epoch 1875 - lr: 0.05000 - Train loss: 17.90381 - Test loss: 54.15141\n",
      "Epoch 1876 - lr: 0.05000 - Train loss: 17.90381 - Test loss: 54.15134\n",
      "Epoch 1877 - lr: 0.05000 - Train loss: 17.90381 - Test loss: 54.15128\n",
      "Epoch 1878 - lr: 0.05000 - Train loss: 17.90381 - Test loss: 54.15121\n",
      "Epoch 1879 - lr: 0.05000 - Train loss: 17.90381 - Test loss: 54.15114\n",
      "Epoch 1880 - lr: 0.05000 - Train loss: 17.90380 - Test loss: 54.15107\n",
      "Epoch 1881 - lr: 0.05000 - Train loss: 17.90380 - Test loss: 54.15100\n",
      "Epoch 1882 - lr: 0.05000 - Train loss: 17.90380 - Test loss: 54.15094\n",
      "Epoch 1883 - lr: 0.05000 - Train loss: 17.90380 - Test loss: 54.15087\n",
      "Epoch 1884 - lr: 0.05000 - Train loss: 17.90380 - Test loss: 54.15080\n",
      "Epoch 1885 - lr: 0.05000 - Train loss: 17.90380 - Test loss: 54.15073\n",
      "Epoch 1886 - lr: 0.05000 - Train loss: 17.90380 - Test loss: 54.15066\n",
      "Epoch 1887 - lr: 0.05000 - Train loss: 17.90380 - Test loss: 54.15059\n",
      "Epoch 1888 - lr: 0.05000 - Train loss: 17.90379 - Test loss: 54.15052\n",
      "Epoch 1889 - lr: 0.05000 - Train loss: 17.90379 - Test loss: 54.15045\n",
      "Epoch 1890 - lr: 0.05000 - Train loss: 17.90379 - Test loss: 54.15038\n",
      "Epoch 1891 - lr: 0.05000 - Train loss: 17.90379 - Test loss: 54.15031\n",
      "Epoch 1892 - lr: 0.05000 - Train loss: 17.90379 - Test loss: 54.15024\n",
      "Epoch 1893 - lr: 0.05000 - Train loss: 17.90379 - Test loss: 54.15017\n",
      "Epoch 1894 - lr: 0.05000 - Train loss: 17.90379 - Test loss: 54.15010\n",
      "Epoch 1895 - lr: 0.05000 - Train loss: 17.90379 - Test loss: 54.15003\n",
      "Epoch 1896 - lr: 0.05000 - Train loss: 17.90379 - Test loss: 54.14996\n",
      "Epoch 1897 - lr: 0.05000 - Train loss: 17.90379 - Test loss: 54.14989\n",
      "Epoch 1898 - lr: 0.05000 - Train loss: 17.90378 - Test loss: 54.14982\n",
      "Epoch 1899 - lr: 0.05000 - Train loss: 17.90378 - Test loss: 54.14974\n",
      "Epoch 1900 - lr: 0.05000 - Train loss: 17.90378 - Test loss: 54.14967\n",
      "Epoch 1901 - lr: 0.05000 - Train loss: 17.90378 - Test loss: 54.14960\n",
      "Epoch 1902 - lr: 0.05000 - Train loss: 17.90378 - Test loss: 54.14953\n",
      "Epoch 1903 - lr: 0.05000 - Train loss: 17.90378 - Test loss: 54.14945\n",
      "Epoch 1904 - lr: 0.05000 - Train loss: 17.90378 - Test loss: 54.14938\n",
      "Epoch 1905 - lr: 0.05000 - Train loss: 17.90378 - Test loss: 54.14931\n",
      "Epoch 1906 - lr: 0.05000 - Train loss: 17.90378 - Test loss: 54.14923\n",
      "Epoch 1907 - lr: 0.05000 - Train loss: 17.90378 - Test loss: 54.14916\n",
      "Epoch 1908 - lr: 0.05000 - Train loss: 17.90378 - Test loss: 54.14909\n",
      "Epoch 1909 - lr: 0.05000 - Train loss: 17.90378 - Test loss: 54.14901\n",
      "Epoch 1910 - lr: 0.05000 - Train loss: 17.90377 - Test loss: 54.14894\n",
      "Epoch 1911 - lr: 0.05000 - Train loss: 17.90377 - Test loss: 54.14886\n",
      "Epoch 1912 - lr: 0.05000 - Train loss: 17.90377 - Test loss: 54.14879\n",
      "Epoch 1913 - lr: 0.05000 - Train loss: 17.90377 - Test loss: 54.14871\n",
      "Epoch 1914 - lr: 0.05000 - Train loss: 17.90377 - Test loss: 54.14864\n",
      "Epoch 1915 - lr: 0.05000 - Train loss: 17.90377 - Test loss: 54.14856\n",
      "Epoch 1916 - lr: 0.05000 - Train loss: 17.90377 - Test loss: 54.14848\n",
      "Epoch 1917 - lr: 0.05000 - Train loss: 17.90377 - Test loss: 54.14841\n",
      "Epoch 1918 - lr: 0.05000 - Train loss: 17.90377 - Test loss: 54.14833\n",
      "Epoch 1919 - lr: 0.05000 - Train loss: 17.90377 - Test loss: 54.14825\n",
      "Epoch 1920 - lr: 0.05000 - Train loss: 17.90377 - Test loss: 54.14818\n",
      "Epoch 1921 - lr: 0.05000 - Train loss: 17.90377 - Test loss: 54.14810\n",
      "Epoch 1922 - lr: 0.05000 - Train loss: 17.90377 - Test loss: 54.14802\n",
      "Epoch 1923 - lr: 0.05000 - Train loss: 17.90377 - Test loss: 54.14794\n",
      "Epoch 1924 - lr: 0.05000 - Train loss: 17.90377 - Test loss: 54.14786\n",
      "Epoch 1925 - lr: 0.05000 - Train loss: 17.90377 - Test loss: 54.14778\n",
      "Epoch 1926 - lr: 0.05000 - Train loss: 17.90377 - Test loss: 54.14771\n",
      "Epoch 1927 - lr: 0.05000 - Train loss: 17.90376 - Test loss: 54.14763\n",
      "Epoch 1928 - lr: 0.05000 - Train loss: 17.90376 - Test loss: 54.14755\n",
      "Epoch 1929 - lr: 0.05000 - Train loss: 17.90376 - Test loss: 54.14747\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1930 - lr: 0.05000 - Train loss: 17.90376 - Test loss: 54.14738\n",
      "Epoch 1931 - lr: 0.05000 - Train loss: 17.90376 - Test loss: 54.14730\n",
      "Epoch 1932 - lr: 0.05000 - Train loss: 17.90376 - Test loss: 54.14722\n",
      "Epoch 1933 - lr: 0.05000 - Train loss: 17.90376 - Test loss: 54.14714\n",
      "Epoch 1934 - lr: 0.05000 - Train loss: 17.90376 - Test loss: 54.14706\n",
      "Epoch 1935 - lr: 0.05000 - Train loss: 17.90376 - Test loss: 54.14698\n",
      "Epoch 1936 - lr: 0.05000 - Train loss: 17.90376 - Test loss: 54.14689\n",
      "Epoch 1937 - lr: 0.05000 - Train loss: 17.90376 - Test loss: 54.14681\n",
      "Epoch 1938 - lr: 0.05000 - Train loss: 17.90376 - Test loss: 54.14673\n",
      "Epoch 1939 - lr: 0.05000 - Train loss: 17.90376 - Test loss: 54.14664\n",
      "Epoch 1940 - lr: 0.05000 - Train loss: 17.90376 - Test loss: 54.14656\n",
      "Epoch 1941 - lr: 0.05000 - Train loss: 17.90376 - Test loss: 54.14647\n",
      "Epoch 1942 - lr: 0.05000 - Train loss: 17.90376 - Test loss: 54.14639\n",
      "Epoch 1943 - lr: 0.05000 - Train loss: 17.90376 - Test loss: 54.14630\n",
      "Epoch 1944 - lr: 0.05000 - Train loss: 17.90376 - Test loss: 54.14622\n",
      "Epoch 1945 - lr: 0.05000 - Train loss: 17.90376 - Test loss: 54.14613\n",
      "Epoch 1946 - lr: 0.05000 - Train loss: 17.90376 - Test loss: 54.14604\n",
      "Epoch 1947 - lr: 0.05000 - Train loss: 17.90376 - Test loss: 54.14595\n",
      "Epoch 1948 - lr: 0.05000 - Train loss: 17.90376 - Test loss: 54.14587\n",
      "Epoch 1949 - lr: 0.05000 - Train loss: 17.90376 - Test loss: 54.14578\n",
      "Epoch 1950 - lr: 0.05000 - Train loss: 17.90376 - Test loss: 54.14569\n",
      "Epoch 1951 - lr: 0.05000 - Train loss: 17.90376 - Test loss: 54.14560\n",
      "Epoch 1952 - lr: 0.05000 - Train loss: 17.90376 - Test loss: 54.14551\n",
      "Epoch 1953 - lr: 0.05000 - Train loss: 17.90376 - Test loss: 54.14542\n",
      "Epoch 1954 - lr: 0.05000 - Train loss: 17.90376 - Test loss: 54.14533\n",
      "Epoch 1955 - lr: 0.05000 - Train loss: 17.90376 - Test loss: 54.14524\n",
      "Epoch 1956 - lr: 0.05000 - Train loss: 17.90376 - Test loss: 54.14514\n",
      "Epoch 1957 - lr: 0.05000 - Train loss: 17.90376 - Test loss: 54.14505\n",
      "Epoch 1958 - lr: 0.05000 - Train loss: 17.90376 - Test loss: 54.14496\n",
      "Epoch 1959 - lr: 0.05000 - Train loss: 17.90376 - Test loss: 54.14486\n",
      "Epoch 1960 - lr: 0.05000 - Train loss: 17.90376 - Test loss: 54.14477\n",
      "Epoch 1961 - lr: 0.05000 - Train loss: 17.90377 - Test loss: 54.14467\n",
      "Epoch 1962 - lr: 0.05000 - Train loss: 17.90377 - Test loss: 54.14458\n",
      "Epoch 1963 - lr: 0.05000 - Train loss: 17.90377 - Test loss: 54.14448\n",
      "Epoch 1964 - lr: 0.05000 - Train loss: 17.90377 - Test loss: 54.14438\n",
      "Epoch 1965 - lr: 0.05000 - Train loss: 17.90377 - Test loss: 54.14429\n",
      "Epoch 1966 - lr: 0.05000 - Train loss: 17.90377 - Test loss: 54.14419\n",
      "Epoch 1967 - lr: 0.05000 - Train loss: 17.90377 - Test loss: 54.14409\n",
      "Epoch 1968 - lr: 0.05000 - Train loss: 17.90377 - Test loss: 54.14399\n",
      "Epoch 1969 - lr: 0.05000 - Train loss: 17.90377 - Test loss: 54.14389\n",
      "Epoch 1970 - lr: 0.05000 - Train loss: 17.90377 - Test loss: 54.14379\n",
      "Epoch 1971 - lr: 0.05000 - Train loss: 17.90377 - Test loss: 54.14369\n",
      "Epoch 1972 - lr: 0.05000 - Train loss: 17.90377 - Test loss: 54.14358\n",
      "Epoch 1973 - lr: 0.05000 - Train loss: 17.90377 - Test loss: 54.14348\n",
      "Epoch 1974 - lr: 0.05000 - Train loss: 17.90378 - Test loss: 54.14338\n",
      "Epoch 1975 - lr: 0.05000 - Train loss: 17.90378 - Test loss: 54.14327\n",
      "Epoch 1976 - lr: 0.05000 - Train loss: 17.90378 - Test loss: 54.14317\n",
      "Epoch 1977 - lr: 0.05000 - Train loss: 17.90378 - Test loss: 54.14306\n",
      "Epoch 1978 - lr: 0.05000 - Train loss: 17.90378 - Test loss: 54.14295\n",
      "Epoch 1979 - lr: 0.05000 - Train loss: 17.90378 - Test loss: 54.14285\n",
      "Epoch 1980 - lr: 0.05000 - Train loss: 17.90378 - Test loss: 54.14274\n",
      "Epoch 1981 - lr: 0.05000 - Train loss: 17.90378 - Test loss: 54.14263\n",
      "Epoch 1982 - lr: 0.05000 - Train loss: 17.90379 - Test loss: 54.14252\n",
      "Epoch 1983 - lr: 0.05000 - Train loss: 17.90379 - Test loss: 54.14241\n",
      "Epoch 1984 - lr: 0.05000 - Train loss: 17.90379 - Test loss: 54.14229\n",
      "Epoch 1985 - lr: 0.05000 - Train loss: 17.90379 - Test loss: 54.14218\n",
      "Epoch 1986 - lr: 0.05000 - Train loss: 17.90379 - Test loss: 54.14206\n",
      "Epoch 1987 - lr: 0.05000 - Train loss: 17.90379 - Test loss: 54.14195\n",
      "Epoch 1988 - lr: 0.05000 - Train loss: 17.90379 - Test loss: 54.14183\n",
      "Epoch 1989 - lr: 0.05000 - Train loss: 17.90380 - Test loss: 54.14172\n",
      "Epoch 1990 - lr: 0.05000 - Train loss: 17.90380 - Test loss: 54.14160\n",
      "Epoch 1991 - lr: 0.05000 - Train loss: 17.90380 - Test loss: 54.14148\n",
      "Epoch 1992 - lr: 0.05000 - Train loss: 17.90380 - Test loss: 54.14136\n",
      "Epoch 1993 - lr: 0.05000 - Train loss: 17.90380 - Test loss: 54.14124\n",
      "Epoch 1994 - lr: 0.05000 - Train loss: 17.90381 - Test loss: 54.14111\n",
      "Epoch 1995 - lr: 0.05000 - Train loss: 17.90381 - Test loss: 54.14099\n",
      "Epoch 1996 - lr: 0.05000 - Train loss: 17.90381 - Test loss: 54.14087\n",
      "Epoch 1997 - lr: 0.05000 - Train loss: 17.90381 - Test loss: 54.14074\n",
      "Epoch 1998 - lr: 0.05000 - Train loss: 17.90382 - Test loss: 54.14061\n",
      "Epoch 1999 - lr: 0.05000 - Train loss: 17.90382 - Test loss: 54.14048\n",
      "Epoch 2000 - lr: 0.05000 - Train loss: 17.90382 - Test loss: 54.14035\n",
      "Epoch 2001 - lr: 0.05000 - Train loss: 17.90382 - Test loss: 54.14022\n",
      "Epoch 2002 - lr: 0.05000 - Train loss: 17.90383 - Test loss: 54.14009\n",
      "Epoch 2003 - lr: 0.05000 - Train loss: 17.90383 - Test loss: 54.13996\n",
      "Epoch 2004 - lr: 0.05000 - Train loss: 17.90383 - Test loss: 54.13982\n",
      "Epoch 2005 - lr: 0.05000 - Train loss: 17.90383 - Test loss: 54.13969\n",
      "Epoch 2006 - lr: 0.05000 - Train loss: 17.90384 - Test loss: 54.13955\n",
      "Epoch 2007 - lr: 0.05000 - Train loss: 17.90384 - Test loss: 54.13941\n",
      "Epoch 2008 - lr: 0.05000 - Train loss: 17.90384 - Test loss: 54.13927\n",
      "Epoch 2009 - lr: 0.05000 - Train loss: 17.90385 - Test loss: 54.13913\n",
      "Epoch 2010 - lr: 0.05000 - Train loss: 17.90385 - Test loss: 54.13898\n",
      "Epoch 2011 - lr: 0.05000 - Train loss: 17.90385 - Test loss: 54.13884\n",
      "Epoch 2012 - lr: 0.05000 - Train loss: 17.90386 - Test loss: 54.13869\n",
      "Epoch 2013 - lr: 0.05000 - Train loss: 17.90386 - Test loss: 54.13854\n",
      "Epoch 2014 - lr: 0.05000 - Train loss: 17.90386 - Test loss: 54.13839\n",
      "Epoch 2015 - lr: 0.05000 - Train loss: 17.90387 - Test loss: 54.13824\n",
      "Epoch 2016 - lr: 0.05000 - Train loss: 17.90387 - Test loss: 54.13809\n",
      "Epoch 2017 - lr: 0.05000 - Train loss: 17.90387 - Test loss: 54.13794\n",
      "Epoch 2018 - lr: 0.05000 - Train loss: 17.90388 - Test loss: 54.13778\n",
      "Epoch 2019 - lr: 0.05000 - Train loss: 17.90388 - Test loss: 54.13762\n",
      "Epoch 2020 - lr: 0.05000 - Train loss: 17.90389 - Test loss: 54.13746\n",
      "Epoch 2021 - lr: 0.05000 - Train loss: 17.90389 - Test loss: 54.13730\n",
      "Epoch 2022 - lr: 0.05000 - Train loss: 17.90390 - Test loss: 54.13714\n",
      "Epoch 2023 - lr: 0.05000 - Train loss: 17.90390 - Test loss: 54.13697\n",
      "Epoch 2024 - lr: 0.05000 - Train loss: 17.90390 - Test loss: 54.13681\n",
      "Epoch 2025 - lr: 0.05000 - Train loss: 17.90391 - Test loss: 54.13664\n",
      "Epoch 2026 - lr: 0.05000 - Train loss: 17.90391 - Test loss: 54.13647\n",
      "Epoch 2027 - lr: 0.05000 - Train loss: 17.90392 - Test loss: 54.13630\n",
      "Epoch 2028 - lr: 0.05000 - Train loss: 17.90392 - Test loss: 54.13612\n",
      "Epoch 2029 - lr: 0.05000 - Train loss: 17.90393 - Test loss: 54.13595\n",
      "Epoch 2030 - lr: 0.05000 - Train loss: 17.90393 - Test loss: 54.13577\n",
      "Epoch 2031 - lr: 0.05000 - Train loss: 17.90394 - Test loss: 54.13559\n",
      "Epoch 2032 - lr: 0.05000 - Train loss: 17.90395 - Test loss: 54.13541\n",
      "Epoch 2033 - lr: 0.05000 - Train loss: 17.90395 - Test loss: 54.13522\n",
      "Epoch 2034 - lr: 0.05000 - Train loss: 17.90396 - Test loss: 54.13504\n",
      "Epoch 2035 - lr: 0.05000 - Train loss: 17.90396 - Test loss: 54.13485\n",
      "Epoch 2036 - lr: 0.05000 - Train loss: 17.90397 - Test loss: 54.13466\n",
      "Epoch 2037 - lr: 0.05000 - Train loss: 17.90398 - Test loss: 54.13447\n",
      "Epoch 2038 - lr: 0.05000 - Train loss: 17.90398 - Test loss: 54.13427\n",
      "Epoch 2039 - lr: 0.05000 - Train loss: 17.90399 - Test loss: 54.13408\n",
      "Epoch 2040 - lr: 0.05000 - Train loss: 17.90399 - Test loss: 54.13388\n",
      "Epoch 2041 - lr: 0.05000 - Train loss: 17.90400 - Test loss: 54.13368\n",
      "Epoch 2042 - lr: 0.05000 - Train loss: 17.90401 - Test loss: 54.13348\n",
      "Epoch 2043 - lr: 0.05000 - Train loss: 17.90401 - Test loss: 54.13327\n",
      "Epoch 2044 - lr: 0.05000 - Train loss: 17.90402 - Test loss: 54.13307\n",
      "Epoch 2045 - lr: 0.05000 - Train loss: 17.90403 - Test loss: 54.13286\n",
      "Epoch 2046 - lr: 0.05000 - Train loss: 17.90404 - Test loss: 54.13265\n",
      "Epoch 2047 - lr: 0.05000 - Train loss: 17.90404 - Test loss: 54.13243\n",
      "Epoch 2048 - lr: 0.05000 - Train loss: 17.90405 - Test loss: 54.13222\n",
      "Epoch 2049 - lr: 0.05000 - Train loss: 17.90406 - Test loss: 54.13200\n",
      "Epoch 2050 - lr: 0.05000 - Train loss: 17.90407 - Test loss: 54.13178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2051 - lr: 0.05000 - Train loss: 17.90407 - Test loss: 54.13156\n",
      "Epoch 2052 - lr: 0.05000 - Train loss: 17.90408 - Test loss: 54.13134\n",
      "Epoch 2053 - lr: 0.05000 - Train loss: 17.90409 - Test loss: 54.13111\n",
      "Epoch 2054 - lr: 0.05000 - Train loss: 17.90410 - Test loss: 54.13089\n",
      "Epoch 2055 - lr: 0.05000 - Train loss: 17.90411 - Test loss: 54.13066\n",
      "Epoch 2056 - lr: 0.05000 - Train loss: 17.90412 - Test loss: 54.13043\n",
      "Epoch 2057 - lr: 0.05000 - Train loss: 17.90413 - Test loss: 54.13020\n",
      "Epoch 2058 - lr: 0.05000 - Train loss: 17.90413 - Test loss: 54.12996\n",
      "Epoch 2059 - lr: 0.05000 - Train loss: 17.90414 - Test loss: 54.12973\n",
      "Epoch 2060 - lr: 0.05000 - Train loss: 17.90415 - Test loss: 54.12949\n",
      "Epoch 2061 - lr: 0.05000 - Train loss: 17.90416 - Test loss: 54.12925\n",
      "Epoch 2062 - lr: 0.05000 - Train loss: 17.90417 - Test loss: 54.12901\n",
      "Epoch 2063 - lr: 0.05000 - Train loss: 17.90418 - Test loss: 54.12877\n",
      "Epoch 2064 - lr: 0.05000 - Train loss: 17.90419 - Test loss: 54.12852\n",
      "Epoch 2065 - lr: 0.05000 - Train loss: 17.90420 - Test loss: 54.12828\n",
      "Epoch 2066 - lr: 0.05000 - Train loss: 17.90421 - Test loss: 54.12803\n",
      "Epoch 2067 - lr: 0.05000 - Train loss: 17.90422 - Test loss: 54.12778\n",
      "Epoch 2068 - lr: 0.05000 - Train loss: 17.90423 - Test loss: 54.12753\n",
      "Epoch 2069 - lr: 0.05000 - Train loss: 17.90424 - Test loss: 54.12728\n",
      "Epoch 2070 - lr: 0.05000 - Train loss: 17.90425 - Test loss: 54.12703\n",
      "Epoch 2071 - lr: 0.05000 - Train loss: 17.90426 - Test loss: 54.12677\n",
      "Epoch 2072 - lr: 0.05000 - Train loss: 17.90427 - Test loss: 54.12652\n",
      "Epoch 2073 - lr: 0.05000 - Train loss: 17.90428 - Test loss: 54.12627\n",
      "Epoch 2074 - lr: 0.05000 - Train loss: 17.90429 - Test loss: 54.12601\n",
      "Epoch 2075 - lr: 0.05000 - Train loss: 17.90430 - Test loss: 54.12575\n",
      "Epoch 2076 - lr: 0.05000 - Train loss: 17.90431 - Test loss: 54.12550\n",
      "Epoch 2077 - lr: 0.05000 - Train loss: 17.90432 - Test loss: 54.12524\n",
      "Epoch 2078 - lr: 0.05000 - Train loss: 17.90433 - Test loss: 54.12498\n",
      "Epoch 2079 - lr: 0.05000 - Train loss: 17.90435 - Test loss: 54.12472\n",
      "Epoch 2080 - lr: 0.05000 - Train loss: 17.90436 - Test loss: 54.12446\n",
      "Epoch 2081 - lr: 0.05000 - Train loss: 17.90437 - Test loss: 54.12420\n",
      "Epoch 2082 - lr: 0.05000 - Train loss: 17.90438 - Test loss: 54.12394\n",
      "Epoch 2083 - lr: 0.05000 - Train loss: 17.90439 - Test loss: 54.12368\n",
      "Epoch 2084 - lr: 0.05000 - Train loss: 17.90440 - Test loss: 54.12342\n",
      "Epoch 2085 - lr: 0.05000 - Train loss: 17.90441 - Test loss: 54.12316\n",
      "Epoch 2086 - lr: 0.05000 - Train loss: 17.90442 - Test loss: 54.12290\n",
      "Epoch 2087 - lr: 0.05000 - Train loss: 17.90443 - Test loss: 54.12264\n",
      "Epoch 2088 - lr: 0.05000 - Train loss: 17.90444 - Test loss: 54.12238\n",
      "Epoch 2089 - lr: 0.05000 - Train loss: 17.90446 - Test loss: 54.12212\n",
      "Epoch 2090 - lr: 0.05000 - Train loss: 17.90447 - Test loss: 54.12186\n",
      "Epoch 2091 - lr: 0.05000 - Train loss: 17.90448 - Test loss: 54.12161\n",
      "Epoch 2092 - lr: 0.05000 - Train loss: 17.90449 - Test loss: 54.12135\n",
      "Epoch 2093 - lr: 0.05000 - Train loss: 17.90450 - Test loss: 54.12109\n",
      "Epoch 2094 - lr: 0.05000 - Train loss: 17.90451 - Test loss: 54.12084\n",
      "Epoch 2095 - lr: 0.05000 - Train loss: 17.90452 - Test loss: 54.12058\n",
      "Epoch 2096 - lr: 0.05000 - Train loss: 17.90453 - Test loss: 54.12033\n",
      "Epoch 2097 - lr: 0.05000 - Train loss: 17.90454 - Test loss: 54.12007\n",
      "Epoch 2098 - lr: 0.05000 - Train loss: 17.90455 - Test loss: 54.11982\n",
      "Epoch 2099 - lr: 0.05000 - Train loss: 17.90457 - Test loss: 54.11957\n",
      "Epoch 2100 - lr: 0.05000 - Train loss: 17.90458 - Test loss: 54.11932\n",
      "Epoch 2101 - lr: 0.05000 - Train loss: 17.90459 - Test loss: 54.11907\n",
      "Epoch 2102 - lr: 0.05000 - Train loss: 17.90460 - Test loss: 54.11882\n",
      "Epoch 2103 - lr: 0.05000 - Train loss: 17.90461 - Test loss: 54.11858\n",
      "Epoch 2104 - lr: 0.05000 - Train loss: 17.90462 - Test loss: 54.11833\n",
      "Epoch 2105 - lr: 0.05000 - Train loss: 17.90463 - Test loss: 54.11809\n",
      "Epoch 2106 - lr: 0.05000 - Train loss: 17.90464 - Test loss: 54.11785\n",
      "Epoch 2107 - lr: 0.05000 - Train loss: 17.90465 - Test loss: 54.11761\n",
      "Epoch 2108 - lr: 0.05000 - Train loss: 17.90466 - Test loss: 54.11737\n",
      "Epoch 2109 - lr: 0.05000 - Train loss: 17.90467 - Test loss: 54.11713\n",
      "Epoch 2110 - lr: 0.05000 - Train loss: 17.90468 - Test loss: 54.11689\n",
      "Epoch 2111 - lr: 0.05000 - Train loss: 17.90469 - Test loss: 54.11666\n",
      "Epoch 2112 - lr: 0.05000 - Train loss: 17.90470 - Test loss: 54.11643\n",
      "Epoch 2113 - lr: 0.05000 - Train loss: 17.90471 - Test loss: 54.11619\n",
      "Epoch 2114 - lr: 0.05000 - Train loss: 17.90472 - Test loss: 54.11596\n",
      "Epoch 2115 - lr: 0.05000 - Train loss: 17.90473 - Test loss: 54.11574\n",
      "Epoch 2116 - lr: 0.05000 - Train loss: 17.90474 - Test loss: 54.11551\n",
      "Epoch 2117 - lr: 0.05000 - Train loss: 17.90475 - Test loss: 54.11529\n",
      "Epoch 2118 - lr: 0.05000 - Train loss: 17.90476 - Test loss: 54.11506\n",
      "Epoch 2119 - lr: 0.05000 - Train loss: 17.90476 - Test loss: 54.11484\n",
      "Epoch 2120 - lr: 0.05000 - Train loss: 17.90477 - Test loss: 54.11462\n",
      "Epoch 2121 - lr: 0.05000 - Train loss: 17.90478 - Test loss: 54.11440\n",
      "Epoch 2122 - lr: 0.05000 - Train loss: 17.90479 - Test loss: 54.11419\n",
      "Epoch 2123 - lr: 0.05000 - Train loss: 17.90480 - Test loss: 54.11397\n",
      "Epoch 2124 - lr: 0.05000 - Train loss: 17.90481 - Test loss: 54.11376\n",
      "Epoch 2125 - lr: 0.05000 - Train loss: 17.90482 - Test loss: 54.11355\n",
      "Epoch 2126 - lr: 0.05000 - Train loss: 17.90482 - Test loss: 54.11334\n",
      "Epoch 2127 - lr: 0.05000 - Train loss: 17.90483 - Test loss: 54.11313\n",
      "Epoch 2128 - lr: 0.05000 - Train loss: 17.90484 - Test loss: 54.11293\n",
      "Epoch 2129 - lr: 0.05000 - Train loss: 17.90485 - Test loss: 54.11272\n",
      "Epoch 2130 - lr: 0.05000 - Train loss: 17.90486 - Test loss: 54.11252\n",
      "Epoch 2131 - lr: 0.05000 - Train loss: 17.90486 - Test loss: 54.11232\n",
      "Epoch 2132 - lr: 0.05000 - Train loss: 17.90487 - Test loss: 54.11212\n",
      "Epoch 2133 - lr: 0.05000 - Train loss: 17.90488 - Test loss: 54.11192\n",
      "Epoch 2134 - lr: 0.05000 - Train loss: 17.90488 - Test loss: 54.11173\n",
      "Epoch 2135 - lr: 0.05000 - Train loss: 17.90489 - Test loss: 54.11153\n",
      "Epoch 2136 - lr: 0.05000 - Train loss: 17.90490 - Test loss: 54.11134\n",
      "Epoch 2137 - lr: 0.05000 - Train loss: 17.90491 - Test loss: 54.11115\n",
      "Epoch 2138 - lr: 0.05000 - Train loss: 17.90491 - Test loss: 54.11096\n",
      "Epoch 2139 - lr: 0.05000 - Train loss: 17.90492 - Test loss: 54.11077\n",
      "Epoch 2140 - lr: 0.05000 - Train loss: 17.90493 - Test loss: 54.11059\n",
      "Epoch 2141 - lr: 0.05000 - Train loss: 17.90493 - Test loss: 54.11040\n",
      "Epoch 2142 - lr: 0.05000 - Train loss: 17.90494 - Test loss: 54.11022\n",
      "Epoch 2143 - lr: 0.05000 - Train loss: 17.90494 - Test loss: 54.11003\n",
      "Epoch 2144 - lr: 0.05000 - Train loss: 17.90495 - Test loss: 54.10985\n",
      "Epoch 2145 - lr: 0.05000 - Train loss: 17.90496 - Test loss: 54.10967\n",
      "Epoch 2146 - lr: 0.05000 - Train loss: 17.90496 - Test loss: 54.10949\n",
      "Epoch 2147 - lr: 0.05000 - Train loss: 17.90497 - Test loss: 54.10932\n",
      "Epoch 2148 - lr: 0.05000 - Train loss: 17.90497 - Test loss: 54.10914\n",
      "Epoch 2149 - lr: 0.05000 - Train loss: 17.90498 - Test loss: 54.10896\n",
      "Epoch 2150 - lr: 0.05000 - Train loss: 17.90499 - Test loss: 54.10879\n",
      "Epoch 2151 - lr: 0.05000 - Train loss: 17.90499 - Test loss: 54.10862\n",
      "Epoch 2152 - lr: 0.05000 - Train loss: 17.90500 - Test loss: 54.10845\n",
      "Epoch 2153 - lr: 0.05000 - Train loss: 17.90500 - Test loss: 54.10828\n",
      "Epoch 2154 - lr: 0.05000 - Train loss: 17.90501 - Test loss: 54.10811\n",
      "Epoch 2155 - lr: 0.05000 - Train loss: 17.90501 - Test loss: 54.10794\n",
      "Epoch 2156 - lr: 0.05000 - Train loss: 17.90502 - Test loss: 54.10777\n",
      "Epoch 2157 - lr: 0.05000 - Train loss: 17.90502 - Test loss: 54.10761\n",
      "Epoch 2158 - lr: 0.05000 - Train loss: 17.90503 - Test loss: 54.10744\n",
      "Epoch 2159 - lr: 0.05000 - Train loss: 17.90503 - Test loss: 54.10728\n",
      "Epoch 2160 - lr: 0.05000 - Train loss: 17.90504 - Test loss: 54.10711\n",
      "Epoch 2161 - lr: 0.05000 - Train loss: 17.90504 - Test loss: 54.10695\n",
      "Epoch 2162 - lr: 0.05000 - Train loss: 17.90505 - Test loss: 54.10679\n",
      "Epoch 2163 - lr: 0.05000 - Train loss: 17.90505 - Test loss: 54.10663\n",
      "Epoch 2164 - lr: 0.05000 - Train loss: 17.90506 - Test loss: 54.10647\n",
      "Epoch 2165 - lr: 0.05000 - Train loss: 17.90506 - Test loss: 54.10631\n",
      "Epoch 2166 - lr: 0.05000 - Train loss: 17.90506 - Test loss: 54.10615\n",
      "Epoch 2167 - lr: 0.05000 - Train loss: 17.90507 - Test loss: 54.10599\n",
      "Epoch 2168 - lr: 0.05000 - Train loss: 17.90507 - Test loss: 54.10583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2169 - lr: 0.05000 - Train loss: 17.90508 - Test loss: 54.10568\n",
      "Epoch 2170 - lr: 0.05000 - Train loss: 17.90508 - Test loss: 54.10552\n",
      "Epoch 2171 - lr: 0.05000 - Train loss: 17.90509 - Test loss: 54.10536\n",
      "Epoch 2172 - lr: 0.05000 - Train loss: 17.90509 - Test loss: 54.10521\n",
      "Epoch 2173 - lr: 0.05000 - Train loss: 17.90509 - Test loss: 54.10505\n",
      "Epoch 2174 - lr: 0.05000 - Train loss: 17.90510 - Test loss: 54.10490\n",
      "Epoch 2175 - lr: 0.05000 - Train loss: 17.90510 - Test loss: 54.10475\n",
      "Epoch 2176 - lr: 0.05000 - Train loss: 17.90511 - Test loss: 54.10459\n",
      "Epoch 2177 - lr: 0.05000 - Train loss: 17.90511 - Test loss: 54.10444\n",
      "Epoch 2178 - lr: 0.05000 - Train loss: 17.90511 - Test loss: 54.10429\n",
      "Epoch 2179 - lr: 0.05000 - Train loss: 17.90512 - Test loss: 54.10413\n",
      "Epoch 2180 - lr: 0.05000 - Train loss: 17.90512 - Test loss: 54.10398\n",
      "Epoch 2181 - lr: 0.05000 - Train loss: 17.90513 - Test loss: 54.10383\n",
      "Epoch 2182 - lr: 0.05000 - Train loss: 17.90513 - Test loss: 54.10368\n",
      "Epoch 2183 - lr: 0.05000 - Train loss: 17.90513 - Test loss: 54.10353\n",
      "Epoch 2184 - lr: 0.05000 - Train loss: 17.90514 - Test loss: 54.10338\n",
      "Epoch 2185 - lr: 0.05000 - Train loss: 17.90514 - Test loss: 54.10323\n",
      "Epoch 2186 - lr: 0.05000 - Train loss: 17.90514 - Test loss: 54.10308\n",
      "Epoch 2187 - lr: 0.05000 - Train loss: 17.90515 - Test loss: 54.10292\n",
      "Epoch 2188 - lr: 0.05000 - Train loss: 17.90515 - Test loss: 54.10277\n",
      "Epoch 2189 - lr: 0.05000 - Train loss: 17.90516 - Test loss: 54.10262\n",
      "Epoch 2190 - lr: 0.05000 - Train loss: 17.90516 - Test loss: 54.10247\n",
      "Epoch 2191 - lr: 0.05000 - Train loss: 17.90516 - Test loss: 54.10232\n",
      "Epoch 2192 - lr: 0.05000 - Train loss: 17.90517 - Test loss: 54.10217\n",
      "Epoch 2193 - lr: 0.05000 - Train loss: 17.90517 - Test loss: 54.10202\n",
      "Epoch 2194 - lr: 0.05000 - Train loss: 17.90517 - Test loss: 54.10187\n",
      "Epoch 2195 - lr: 0.05000 - Train loss: 17.90518 - Test loss: 54.10172\n",
      "Epoch 2196 - lr: 0.05000 - Train loss: 17.90518 - Test loss: 54.10157\n",
      "Epoch 2197 - lr: 0.05000 - Train loss: 17.90518 - Test loss: 54.10142\n",
      "Epoch 2198 - lr: 0.05000 - Train loss: 17.90519 - Test loss: 54.10127\n",
      "Epoch 2199 - lr: 0.05000 - Train loss: 17.90519 - Test loss: 54.10111\n",
      "Epoch 2200 - lr: 0.05000 - Train loss: 17.90520 - Test loss: 54.10096\n",
      "Epoch 2201 - lr: 0.05000 - Train loss: 17.90520 - Test loss: 54.10081\n",
      "Epoch 2202 - lr: 0.05000 - Train loss: 17.90520 - Test loss: 54.10066\n",
      "Epoch 2203 - lr: 0.05000 - Train loss: 17.90521 - Test loss: 54.10051\n",
      "Epoch 2204 - lr: 0.05000 - Train loss: 17.90521 - Test loss: 54.10035\n",
      "Epoch 2205 - lr: 0.05000 - Train loss: 17.90521 - Test loss: 54.10020\n",
      "Epoch 2206 - lr: 0.05000 - Train loss: 17.90522 - Test loss: 54.10005\n",
      "Epoch 2207 - lr: 0.05000 - Train loss: 17.90522 - Test loss: 54.09989\n",
      "Epoch 2208 - lr: 0.05000 - Train loss: 17.90523 - Test loss: 54.09974\n",
      "Epoch 2209 - lr: 0.05000 - Train loss: 17.90523 - Test loss: 54.09958\n",
      "Epoch 2210 - lr: 0.05000 - Train loss: 17.90523 - Test loss: 54.09943\n",
      "Epoch 2211 - lr: 0.05000 - Train loss: 17.90524 - Test loss: 54.09927\n",
      "Epoch 2212 - lr: 0.05000 - Train loss: 17.90524 - Test loss: 54.09911\n",
      "Epoch 2213 - lr: 0.05000 - Train loss: 17.90524 - Test loss: 54.09896\n",
      "Epoch 2214 - lr: 0.05000 - Train loss: 17.90525 - Test loss: 54.09880\n",
      "Epoch 2215 - lr: 0.05000 - Train loss: 17.90525 - Test loss: 54.09864\n",
      "Epoch 2216 - lr: 0.05000 - Train loss: 17.90526 - Test loss: 54.09848\n",
      "Epoch 2217 - lr: 0.05000 - Train loss: 17.90526 - Test loss: 54.09832\n",
      "Epoch 2218 - lr: 0.05000 - Train loss: 17.90526 - Test loss: 54.09816\n",
      "Epoch 2219 - lr: 0.05000 - Train loss: 17.90527 - Test loss: 54.09800\n",
      "Epoch 2220 - lr: 0.05000 - Train loss: 17.90527 - Test loss: 54.09784\n",
      "Epoch 2221 - lr: 0.05000 - Train loss: 17.90528 - Test loss: 54.09768\n",
      "Epoch 2222 - lr: 0.05000 - Train loss: 17.90528 - Test loss: 54.09752\n",
      "Epoch 2223 - lr: 0.05000 - Train loss: 17.90529 - Test loss: 54.09735\n",
      "Epoch 2224 - lr: 0.05000 - Train loss: 17.90529 - Test loss: 54.09719\n",
      "Epoch 2225 - lr: 0.05000 - Train loss: 17.90529 - Test loss: 54.09702\n",
      "Epoch 2226 - lr: 0.05000 - Train loss: 17.90530 - Test loss: 54.09686\n",
      "Epoch 2227 - lr: 0.05000 - Train loss: 17.90530 - Test loss: 54.09669\n",
      "Epoch 2228 - lr: 0.05000 - Train loss: 17.90531 - Test loss: 54.09652\n",
      "Epoch 2229 - lr: 0.05000 - Train loss: 17.90531 - Test loss: 54.09635\n",
      "Epoch 2230 - lr: 0.05000 - Train loss: 17.90532 - Test loss: 54.09618\n",
      "Epoch 2231 - lr: 0.05000 - Train loss: 17.90532 - Test loss: 54.09601\n",
      "Epoch 2232 - lr: 0.05000 - Train loss: 17.90533 - Test loss: 54.09584\n",
      "Epoch 2233 - lr: 0.05000 - Train loss: 17.90533 - Test loss: 54.09567\n",
      "Epoch 2234 - lr: 0.05000 - Train loss: 17.90534 - Test loss: 54.09550\n",
      "Epoch 2235 - lr: 0.05000 - Train loss: 17.90534 - Test loss: 54.09533\n",
      "Epoch 2236 - lr: 0.05000 - Train loss: 17.90535 - Test loss: 54.09515\n",
      "Epoch 2237 - lr: 0.05000 - Train loss: 17.90535 - Test loss: 54.09498\n",
      "Epoch 2238 - lr: 0.05000 - Train loss: 17.90536 - Test loss: 54.09480\n",
      "Epoch 2239 - lr: 0.05000 - Train loss: 17.90536 - Test loss: 54.09462\n",
      "Epoch 2240 - lr: 0.05000 - Train loss: 17.90537 - Test loss: 54.09444\n",
      "Epoch 2241 - lr: 0.05000 - Train loss: 17.90537 - Test loss: 54.09426\n",
      "Epoch 2242 - lr: 0.05000 - Train loss: 17.90538 - Test loss: 54.09408\n",
      "Epoch 2243 - lr: 0.05000 - Train loss: 17.90539 - Test loss: 54.09390\n",
      "Epoch 2244 - lr: 0.05000 - Train loss: 17.90539 - Test loss: 54.09372\n",
      "Epoch 2245 - lr: 0.05000 - Train loss: 17.90540 - Test loss: 54.09354\n",
      "Epoch 2246 - lr: 0.05000 - Train loss: 17.90540 - Test loss: 54.09335\n",
      "Epoch 2247 - lr: 0.05000 - Train loss: 17.90541 - Test loss: 54.09317\n",
      "Epoch 2248 - lr: 0.05000 - Train loss: 17.90541 - Test loss: 54.09298\n",
      "Epoch 2249 - lr: 0.05000 - Train loss: 17.90542 - Test loss: 54.09280\n",
      "Epoch 2250 - lr: 0.05000 - Train loss: 17.90543 - Test loss: 54.09261\n",
      "Epoch 2251 - lr: 0.05000 - Train loss: 17.90543 - Test loss: 54.09242\n",
      "Epoch 2252 - lr: 0.05000 - Train loss: 17.90544 - Test loss: 54.09223\n",
      "Epoch 2253 - lr: 0.05000 - Train loss: 17.90544 - Test loss: 54.09204\n",
      "Epoch 2254 - lr: 0.05000 - Train loss: 17.90545 - Test loss: 54.09185\n",
      "Epoch 2255 - lr: 0.05000 - Train loss: 17.90546 - Test loss: 54.09166\n",
      "Epoch 2256 - lr: 0.05000 - Train loss: 17.90546 - Test loss: 54.09146\n",
      "Epoch 2257 - lr: 0.05000 - Train loss: 17.90547 - Test loss: 54.09127\n",
      "Epoch 2258 - lr: 0.05000 - Train loss: 17.90548 - Test loss: 54.09107\n",
      "Epoch 2259 - lr: 0.05000 - Train loss: 17.90548 - Test loss: 54.09088\n",
      "Epoch 2260 - lr: 0.05000 - Train loss: 17.90549 - Test loss: 54.09068\n",
      "Epoch 2261 - lr: 0.05000 - Train loss: 17.90550 - Test loss: 54.09049\n",
      "Epoch 2262 - lr: 0.05000 - Train loss: 17.90550 - Test loss: 54.09029\n",
      "Epoch 2263 - lr: 0.05000 - Train loss: 17.90551 - Test loss: 54.09009\n",
      "Epoch 2264 - lr: 0.05000 - Train loss: 17.90552 - Test loss: 54.08989\n",
      "Epoch 2265 - lr: 0.05000 - Train loss: 17.90553 - Test loss: 54.08969\n",
      "Epoch 2266 - lr: 0.05000 - Train loss: 17.90553 - Test loss: 54.08949\n",
      "Epoch 2267 - lr: 0.05000 - Train loss: 17.90554 - Test loss: 54.08929\n",
      "Epoch 2268 - lr: 0.05000 - Train loss: 17.90555 - Test loss: 54.08909\n",
      "Epoch 2269 - lr: 0.05000 - Train loss: 17.90555 - Test loss: 54.08888\n",
      "Epoch 2270 - lr: 0.05000 - Train loss: 17.90556 - Test loss: 54.08868\n",
      "Epoch 2271 - lr: 0.05000 - Train loss: 17.90557 - Test loss: 54.08848\n",
      "Epoch 2272 - lr: 0.05000 - Train loss: 17.90558 - Test loss: 54.08827\n",
      "Epoch 2273 - lr: 0.05000 - Train loss: 17.90558 - Test loss: 54.08807\n",
      "Epoch 2274 - lr: 0.05000 - Train loss: 17.90559 - Test loss: 54.08787\n",
      "Epoch 2275 - lr: 0.05000 - Train loss: 17.90560 - Test loss: 54.08766\n",
      "Epoch 2276 - lr: 0.05000 - Train loss: 17.90561 - Test loss: 54.08746\n",
      "Epoch 2277 - lr: 0.05000 - Train loss: 17.90561 - Test loss: 54.08725\n",
      "Epoch 2278 - lr: 0.05000 - Train loss: 17.90562 - Test loss: 54.08704\n",
      "Epoch 2279 - lr: 0.05000 - Train loss: 17.90563 - Test loss: 54.08684\n",
      "Epoch 2280 - lr: 0.05000 - Train loss: 17.90564 - Test loss: 54.08663\n",
      "Epoch 2281 - lr: 0.05000 - Train loss: 17.90565 - Test loss: 54.08643\n",
      "Epoch 2282 - lr: 0.05000 - Train loss: 17.90565 - Test loss: 54.08622\n",
      "Epoch 2283 - lr: 0.05000 - Train loss: 17.90566 - Test loss: 54.08601\n",
      "Epoch 2284 - lr: 0.05000 - Train loss: 17.90567 - Test loss: 54.08581\n",
      "Epoch 2285 - lr: 0.05000 - Train loss: 17.90568 - Test loss: 54.08560\n",
      "Epoch 2286 - lr: 0.05000 - Train loss: 17.90569 - Test loss: 54.08540\n",
      "Epoch 2287 - lr: 0.05000 - Train loss: 17.90569 - Test loss: 54.08519\n",
      "Epoch 2288 - lr: 0.05000 - Train loss: 17.90570 - Test loss: 54.08499\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2289 - lr: 0.05000 - Train loss: 17.90571 - Test loss: 54.08478\n",
      "Epoch 2290 - lr: 0.05000 - Train loss: 17.90572 - Test loss: 54.08458\n",
      "Epoch 2291 - lr: 0.05000 - Train loss: 17.90572 - Test loss: 54.08437\n",
      "Epoch 2292 - lr: 0.05000 - Train loss: 17.90573 - Test loss: 54.08417\n",
      "Epoch 2293 - lr: 0.05000 - Train loss: 17.90574 - Test loss: 54.08396\n",
      "Epoch 2294 - lr: 0.05000 - Train loss: 17.90575 - Test loss: 54.08376\n",
      "Epoch 2295 - lr: 0.05000 - Train loss: 17.90576 - Test loss: 54.08356\n",
      "Epoch 2296 - lr: 0.05000 - Train loss: 17.90576 - Test loss: 54.08335\n",
      "Epoch 2297 - lr: 0.05000 - Train loss: 17.90577 - Test loss: 54.08315\n",
      "Epoch 2298 - lr: 0.05000 - Train loss: 17.90578 - Test loss: 54.08295\n",
      "Epoch 2299 - lr: 0.05000 - Train loss: 17.90579 - Test loss: 54.08275\n",
      "Epoch 2300 - lr: 0.05000 - Train loss: 17.90580 - Test loss: 54.08255\n",
      "Epoch 2301 - lr: 0.05000 - Train loss: 17.90580 - Test loss: 54.08235\n",
      "Epoch 2302 - lr: 0.05000 - Train loss: 17.90581 - Test loss: 54.08215\n",
      "Epoch 2303 - lr: 0.05000 - Train loss: 17.90582 - Test loss: 54.08196\n",
      "Epoch 2304 - lr: 0.05000 - Train loss: 17.90583 - Test loss: 54.08176\n",
      "Epoch 2305 - lr: 0.05000 - Train loss: 17.90583 - Test loss: 54.08157\n",
      "Epoch 2306 - lr: 0.05000 - Train loss: 17.90584 - Test loss: 54.08137\n",
      "Epoch 2307 - lr: 0.05000 - Train loss: 17.90585 - Test loss: 54.08118\n",
      "Epoch 2308 - lr: 0.05000 - Train loss: 17.90586 - Test loss: 54.08098\n",
      "Epoch 2309 - lr: 0.05000 - Train loss: 17.90586 - Test loss: 54.08079\n",
      "Epoch 2310 - lr: 0.05000 - Train loss: 17.90587 - Test loss: 54.08060\n",
      "Epoch 2311 - lr: 0.05000 - Train loss: 17.90588 - Test loss: 54.08041\n",
      "Epoch 2312 - lr: 0.05000 - Train loss: 17.90588 - Test loss: 54.08022\n",
      "Epoch 2313 - lr: 0.05000 - Train loss: 17.90589 - Test loss: 54.08004\n",
      "Epoch 2314 - lr: 0.05000 - Train loss: 17.90590 - Test loss: 54.07985\n",
      "Epoch 2315 - lr: 0.05000 - Train loss: 17.90591 - Test loss: 54.07967\n",
      "Epoch 2316 - lr: 0.05000 - Train loss: 17.90591 - Test loss: 54.07948\n",
      "Epoch 2317 - lr: 0.05000 - Train loss: 17.90592 - Test loss: 54.07930\n",
      "Epoch 2318 - lr: 0.05000 - Train loss: 17.90593 - Test loss: 54.07912\n",
      "Epoch 2319 - lr: 0.05000 - Train loss: 17.90593 - Test loss: 54.07894\n",
      "Epoch 2320 - lr: 0.05000 - Train loss: 17.90594 - Test loss: 54.07876\n",
      "Epoch 2321 - lr: 0.05000 - Train loss: 17.90595 - Test loss: 54.07858\n",
      "Epoch 2322 - lr: 0.05000 - Train loss: 17.90595 - Test loss: 54.07840\n",
      "Epoch 2323 - lr: 0.05000 - Train loss: 17.90596 - Test loss: 54.07823\n",
      "Epoch 2324 - lr: 0.05000 - Train loss: 17.90596 - Test loss: 54.07806\n",
      "Epoch 2325 - lr: 0.05000 - Train loss: 17.90597 - Test loss: 54.07788\n",
      "Epoch 2326 - lr: 0.05000 - Train loss: 17.90598 - Test loss: 54.07771\n",
      "Epoch 2327 - lr: 0.05000 - Train loss: 17.90598 - Test loss: 54.07754\n",
      "Epoch 2328 - lr: 0.05000 - Train loss: 17.90599 - Test loss: 54.07737\n",
      "Epoch 2329 - lr: 0.05000 - Train loss: 17.90599 - Test loss: 54.07721\n",
      "Epoch 2330 - lr: 0.05000 - Train loss: 17.90600 - Test loss: 54.07704\n",
      "Epoch 2331 - lr: 0.05000 - Train loss: 17.90601 - Test loss: 54.07688\n",
      "Epoch 2332 - lr: 0.05000 - Train loss: 17.90601 - Test loss: 54.07671\n",
      "Epoch 2333 - lr: 0.05000 - Train loss: 17.90602 - Test loss: 54.07655\n",
      "Epoch 2334 - lr: 0.05000 - Train loss: 17.90602 - Test loss: 54.07639\n",
      "Epoch 2335 - lr: 0.05000 - Train loss: 17.90603 - Test loss: 54.07623\n",
      "Epoch 2336 - lr: 0.05000 - Train loss: 17.90603 - Test loss: 54.07608\n",
      "Epoch 2337 - lr: 0.05000 - Train loss: 17.90604 - Test loss: 54.07592\n",
      "Epoch 2338 - lr: 0.05000 - Train loss: 17.90604 - Test loss: 54.07576\n",
      "Epoch 2339 - lr: 0.05000 - Train loss: 17.90605 - Test loss: 54.07561\n",
      "Epoch 2340 - lr: 0.05000 - Train loss: 17.90605 - Test loss: 54.07546\n",
      "Epoch 2341 - lr: 0.05000 - Train loss: 17.90606 - Test loss: 54.07531\n",
      "Epoch 2342 - lr: 0.05000 - Train loss: 17.90606 - Test loss: 54.07516\n",
      "Epoch 2343 - lr: 0.05000 - Train loss: 17.90606 - Test loss: 54.07501\n",
      "Epoch 2344 - lr: 0.05000 - Train loss: 17.90607 - Test loss: 54.07486\n",
      "Epoch 2345 - lr: 0.05000 - Train loss: 17.90607 - Test loss: 54.07472\n",
      "Epoch 2346 - lr: 0.05000 - Train loss: 17.90608 - Test loss: 54.07458\n",
      "Epoch 2347 - lr: 0.05000 - Train loss: 17.90608 - Test loss: 54.07443\n",
      "Epoch 2348 - lr: 0.05000 - Train loss: 17.90608 - Test loss: 54.07429\n",
      "Epoch 2349 - lr: 0.05000 - Train loss: 17.90609 - Test loss: 54.07415\n",
      "Epoch 2350 - lr: 0.05000 - Train loss: 17.90609 - Test loss: 54.07401\n",
      "Epoch 2351 - lr: 0.05000 - Train loss: 17.90610 - Test loss: 54.07388\n",
      "Epoch 2352 - lr: 0.05000 - Train loss: 17.90610 - Test loss: 54.07374\n",
      "Epoch 2353 - lr: 0.05000 - Train loss: 17.90610 - Test loss: 54.07360\n",
      "Epoch 2354 - lr: 0.05000 - Train loss: 17.90611 - Test loss: 54.07347\n",
      "Epoch 2355 - lr: 0.05000 - Train loss: 17.90611 - Test loss: 54.07334\n",
      "Epoch 2356 - lr: 0.05000 - Train loss: 17.90611 - Test loss: 54.07321\n",
      "Epoch 2357 - lr: 0.05000 - Train loss: 17.90612 - Test loss: 54.07308\n",
      "Epoch 2358 - lr: 0.05000 - Train loss: 17.90612 - Test loss: 54.07295\n",
      "Epoch 2359 - lr: 0.05000 - Train loss: 17.90612 - Test loss: 54.07282\n",
      "Epoch 2360 - lr: 0.05000 - Train loss: 17.90612 - Test loss: 54.07269\n",
      "Epoch 2361 - lr: 0.05000 - Train loss: 17.90613 - Test loss: 54.07257\n",
      "Epoch 2362 - lr: 0.05000 - Train loss: 17.90613 - Test loss: 54.07244\n",
      "Epoch 2363 - lr: 0.05000 - Train loss: 17.90613 - Test loss: 54.07232\n",
      "Epoch 2364 - lr: 0.05000 - Train loss: 17.90613 - Test loss: 54.07220\n",
      "Epoch 2365 - lr: 0.05000 - Train loss: 17.90614 - Test loss: 54.07208\n",
      "Epoch 2366 - lr: 0.05000 - Train loss: 17.90614 - Test loss: 54.07196\n",
      "Epoch 2367 - lr: 0.05000 - Train loss: 17.90614 - Test loss: 54.07184\n",
      "Epoch 2368 - lr: 0.05000 - Train loss: 17.90614 - Test loss: 54.07172\n",
      "Epoch 2369 - lr: 0.05000 - Train loss: 17.90615 - Test loss: 54.07161\n",
      "Epoch 2370 - lr: 0.05000 - Train loss: 17.90615 - Test loss: 54.07149\n",
      "Epoch 2371 - lr: 0.05000 - Train loss: 17.90615 - Test loss: 54.07138\n",
      "Epoch 2372 - lr: 0.05000 - Train loss: 17.90615 - Test loss: 54.07126\n",
      "Epoch 2373 - lr: 0.05000 - Train loss: 17.90615 - Test loss: 54.07115\n",
      "Epoch 2374 - lr: 0.05000 - Train loss: 17.90615 - Test loss: 54.07104\n",
      "Epoch 2375 - lr: 0.05000 - Train loss: 17.90616 - Test loss: 54.07093\n",
      "Epoch 2376 - lr: 0.05000 - Train loss: 17.90616 - Test loss: 54.07082\n",
      "Epoch 2377 - lr: 0.05000 - Train loss: 17.90616 - Test loss: 54.07071\n",
      "Epoch 2378 - lr: 0.05000 - Train loss: 17.90616 - Test loss: 54.07061\n",
      "Epoch 2379 - lr: 0.05000 - Train loss: 17.90616 - Test loss: 54.07050\n",
      "Epoch 2380 - lr: 0.05000 - Train loss: 17.90616 - Test loss: 54.07039\n",
      "Epoch 2381 - lr: 0.05000 - Train loss: 17.90616 - Test loss: 54.07029\n",
      "Epoch 2382 - lr: 0.05000 - Train loss: 17.90617 - Test loss: 54.07019\n",
      "Epoch 2383 - lr: 0.05000 - Train loss: 17.90617 - Test loss: 54.07008\n",
      "Epoch 2384 - lr: 0.05000 - Train loss: 17.90617 - Test loss: 54.06998\n",
      "Epoch 2385 - lr: 0.05000 - Train loss: 17.90617 - Test loss: 54.06988\n",
      "Epoch 2386 - lr: 0.05000 - Train loss: 17.90617 - Test loss: 54.06978\n",
      "Epoch 2387 - lr: 0.05000 - Train loss: 17.90617 - Test loss: 54.06968\n",
      "Epoch 2388 - lr: 0.05000 - Train loss: 17.90617 - Test loss: 54.06958\n",
      "Epoch 2389 - lr: 0.05000 - Train loss: 17.90617 - Test loss: 54.06948\n",
      "Epoch 2390 - lr: 0.05000 - Train loss: 17.90617 - Test loss: 54.06939\n",
      "Epoch 2391 - lr: 0.05000 - Train loss: 17.90617 - Test loss: 54.06929\n",
      "Epoch 2392 - lr: 0.05000 - Train loss: 17.90617 - Test loss: 54.06920\n",
      "Epoch 2393 - lr: 0.05000 - Train loss: 17.90617 - Test loss: 54.06910\n",
      "Epoch 2394 - lr: 0.05000 - Train loss: 17.90617 - Test loss: 54.06901\n",
      "Epoch 2395 - lr: 0.05000 - Train loss: 17.90617 - Test loss: 54.06891\n",
      "Epoch 2396 - lr: 0.05000 - Train loss: 17.90617 - Test loss: 54.06882\n",
      "Epoch 2397 - lr: 0.05000 - Train loss: 17.90617 - Test loss: 54.06873\n",
      "Epoch 2398 - lr: 0.05000 - Train loss: 17.90617 - Test loss: 54.06864\n",
      "Epoch 2399 - lr: 0.05000 - Train loss: 17.90617 - Test loss: 54.06855\n",
      "Epoch 2400 - lr: 0.05000 - Train loss: 17.90617 - Test loss: 54.06846\n",
      "Epoch 2401 - lr: 0.05000 - Train loss: 17.90617 - Test loss: 54.06837\n",
      "Epoch 2402 - lr: 0.05000 - Train loss: 17.90617 - Test loss: 54.06828\n",
      "Epoch 2403 - lr: 0.05000 - Train loss: 17.90617 - Test loss: 54.06819\n",
      "Epoch 2404 - lr: 0.05000 - Train loss: 17.90617 - Test loss: 54.06811\n",
      "Epoch 2405 - lr: 0.05000 - Train loss: 17.90617 - Test loss: 54.06802\n",
      "Epoch 2406 - lr: 0.05000 - Train loss: 17.90617 - Test loss: 54.06793\n",
      "Epoch 2407 - lr: 0.05000 - Train loss: 17.90617 - Test loss: 54.06785\n",
      "Epoch 2408 - lr: 0.05000 - Train loss: 17.90617 - Test loss: 54.06776\n",
      "Epoch 2409 - lr: 0.05000 - Train loss: 17.90617 - Test loss: 54.06768\n",
      "Epoch 2410 - lr: 0.05000 - Train loss: 17.90617 - Test loss: 54.06760\n",
      "Epoch 2411 - lr: 0.05000 - Train loss: 17.90617 - Test loss: 54.06751\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2412 - lr: 0.05000 - Train loss: 17.90617 - Test loss: 54.06743\n",
      "Epoch 2413 - lr: 0.05000 - Train loss: 17.90617 - Test loss: 54.06735\n",
      "Epoch 2414 - lr: 0.05000 - Train loss: 17.90617 - Test loss: 54.06727\n",
      "Epoch 2415 - lr: 0.05000 - Train loss: 17.90617 - Test loss: 54.06719\n",
      "Epoch 2416 - lr: 0.05000 - Train loss: 17.90617 - Test loss: 54.06711\n",
      "Epoch 2417 - lr: 0.05000 - Train loss: 17.90617 - Test loss: 54.06703\n",
      "Epoch 2418 - lr: 0.05000 - Train loss: 17.90617 - Test loss: 54.06695\n",
      "Epoch 2419 - lr: 0.05000 - Train loss: 17.90617 - Test loss: 54.06687\n",
      "Epoch 2420 - lr: 0.05000 - Train loss: 17.90617 - Test loss: 54.06679\n",
      "Epoch 2421 - lr: 0.05000 - Train loss: 17.90617 - Test loss: 54.06671\n",
      "Epoch 2422 - lr: 0.05000 - Train loss: 17.90616 - Test loss: 54.06664\n",
      "Epoch 2423 - lr: 0.05000 - Train loss: 17.90616 - Test loss: 54.06656\n",
      "Epoch 2424 - lr: 0.05000 - Train loss: 17.90616 - Test loss: 54.06648\n",
      "Epoch 2425 - lr: 0.05000 - Train loss: 17.90616 - Test loss: 54.06641\n",
      "Epoch 2426 - lr: 0.05000 - Train loss: 17.90616 - Test loss: 54.06633\n",
      "Epoch 2427 - lr: 0.05000 - Train loss: 17.90616 - Test loss: 54.06626\n",
      "Epoch 2428 - lr: 0.05000 - Train loss: 17.90616 - Test loss: 54.06618\n",
      "Epoch 2429 - lr: 0.05000 - Train loss: 17.90616 - Test loss: 54.06611\n",
      "Epoch 2430 - lr: 0.05000 - Train loss: 17.90616 - Test loss: 54.06603\n",
      "Epoch 2431 - lr: 0.05000 - Train loss: 17.90615 - Test loss: 54.06596\n",
      "Epoch 2432 - lr: 0.05000 - Train loss: 17.90615 - Test loss: 54.06589\n",
      "Epoch 2433 - lr: 0.05000 - Train loss: 17.90615 - Test loss: 54.06582\n",
      "Epoch 2434 - lr: 0.05000 - Train loss: 17.90615 - Test loss: 54.06574\n",
      "Epoch 2435 - lr: 0.05000 - Train loss: 17.90615 - Test loss: 54.06567\n",
      "Epoch 2436 - lr: 0.05000 - Train loss: 17.90615 - Test loss: 54.06560\n",
      "Epoch 2437 - lr: 0.05000 - Train loss: 17.90615 - Test loss: 54.06553\n",
      "Epoch 2438 - lr: 0.05000 - Train loss: 17.90615 - Test loss: 54.06546\n",
      "Epoch 2439 - lr: 0.05000 - Train loss: 17.90614 - Test loss: 54.06539\n",
      "Epoch 2440 - lr: 0.05000 - Train loss: 17.90614 - Test loss: 54.06532\n",
      "Epoch 2441 - lr: 0.05000 - Train loss: 17.90614 - Test loss: 54.06525\n",
      "Epoch 2442 - lr: 0.05000 - Train loss: 17.90614 - Test loss: 54.06518\n",
      "Epoch 2443 - lr: 0.05000 - Train loss: 17.90614 - Test loss: 54.06511\n",
      "Epoch 2444 - lr: 0.05000 - Train loss: 17.90614 - Test loss: 54.06504\n",
      "Epoch 2445 - lr: 0.05000 - Train loss: 17.90614 - Test loss: 54.06497\n",
      "Epoch 2446 - lr: 0.05000 - Train loss: 17.90613 - Test loss: 54.06491\n",
      "Epoch 2447 - lr: 0.05000 - Train loss: 17.90613 - Test loss: 54.06484\n",
      "Epoch 2448 - lr: 0.05000 - Train loss: 17.90613 - Test loss: 54.06477\n",
      "Epoch 2449 - lr: 0.05000 - Train loss: 17.90613 - Test loss: 54.06470\n",
      "Epoch 2450 - lr: 0.05000 - Train loss: 17.90613 - Test loss: 54.06464\n",
      "Epoch 2451 - lr: 0.05000 - Train loss: 17.90613 - Test loss: 54.06457\n",
      "Epoch 2452 - lr: 0.05000 - Train loss: 17.90613 - Test loss: 54.06450\n",
      "Epoch 2453 - lr: 0.05000 - Train loss: 17.90612 - Test loss: 54.06444\n",
      "Epoch 2454 - lr: 0.05000 - Train loss: 17.90612 - Test loss: 54.06437\n",
      "Epoch 2455 - lr: 0.05000 - Train loss: 17.90612 - Test loss: 54.06431\n",
      "Epoch 2456 - lr: 0.05000 - Train loss: 17.90612 - Test loss: 54.06424\n",
      "Epoch 2457 - lr: 0.05000 - Train loss: 17.90612 - Test loss: 54.06418\n",
      "Epoch 2458 - lr: 0.05000 - Train loss: 17.90612 - Test loss: 54.06411\n",
      "Epoch 2459 - lr: 0.05000 - Train loss: 17.90611 - Test loss: 54.06405\n",
      "Epoch 2460 - lr: 0.05000 - Train loss: 17.90611 - Test loss: 54.06398\n",
      "Epoch 2461 - lr: 0.05000 - Train loss: 17.90611 - Test loss: 54.06392\n",
      "Epoch 2462 - lr: 0.05000 - Train loss: 17.90611 - Test loss: 54.06386\n",
      "Epoch 2463 - lr: 0.05000 - Train loss: 17.90611 - Test loss: 54.06379\n",
      "Epoch 2464 - lr: 0.05000 - Train loss: 17.90610 - Test loss: 54.06373\n",
      "Epoch 2465 - lr: 0.05000 - Train loss: 17.90610 - Test loss: 54.06367\n",
      "Epoch 2466 - lr: 0.05000 - Train loss: 17.90610 - Test loss: 54.06360\n",
      "Epoch 2467 - lr: 0.05000 - Train loss: 17.90610 - Test loss: 54.06354\n",
      "Epoch 2468 - lr: 0.05000 - Train loss: 17.90610 - Test loss: 54.06348\n",
      "Epoch 2469 - lr: 0.05000 - Train loss: 17.90610 - Test loss: 54.06342\n",
      "Epoch 2470 - lr: 0.05000 - Train loss: 17.90609 - Test loss: 54.06336\n",
      "Epoch 2471 - lr: 0.05000 - Train loss: 17.90609 - Test loss: 54.06329\n",
      "Epoch 2472 - lr: 0.05000 - Train loss: 17.90609 - Test loss: 54.06323\n",
      "Epoch 2473 - lr: 0.05000 - Train loss: 17.90609 - Test loss: 54.06317\n",
      "Epoch 2474 - lr: 0.05000 - Train loss: 17.90609 - Test loss: 54.06311\n",
      "Epoch 2475 - lr: 0.05000 - Train loss: 17.90608 - Test loss: 54.06305\n",
      "Epoch 2476 - lr: 0.05000 - Train loss: 17.90608 - Test loss: 54.06299\n",
      "Epoch 2477 - lr: 0.05000 - Train loss: 17.90608 - Test loss: 54.06293\n",
      "Epoch 2478 - lr: 0.05000 - Train loss: 17.90608 - Test loss: 54.06287\n",
      "Epoch 2479 - lr: 0.05000 - Train loss: 17.90608 - Test loss: 54.06281\n",
      "Epoch 2480 - lr: 0.05000 - Train loss: 17.90607 - Test loss: 54.06275\n",
      "Epoch 2481 - lr: 0.05000 - Train loss: 17.90607 - Test loss: 54.06269\n",
      "Epoch 2482 - lr: 0.05000 - Train loss: 17.90607 - Test loss: 54.06263\n",
      "Epoch 2483 - lr: 0.05000 - Train loss: 17.90607 - Test loss: 54.06257\n",
      "Epoch 2484 - lr: 0.05000 - Train loss: 17.90607 - Test loss: 54.06251\n",
      "Epoch 2485 - lr: 0.05000 - Train loss: 17.90606 - Test loss: 54.06245\n",
      "Epoch 2486 - lr: 0.05000 - Train loss: 17.90606 - Test loss: 54.06239\n",
      "Epoch 2487 - lr: 0.05000 - Train loss: 17.90606 - Test loss: 54.06233\n",
      "Epoch 2488 - lr: 0.05000 - Train loss: 17.90606 - Test loss: 54.06227\n",
      "Epoch 2489 - lr: 0.05000 - Train loss: 17.90606 - Test loss: 54.06221\n",
      "Epoch 2490 - lr: 0.05000 - Train loss: 17.90605 - Test loss: 54.06215\n",
      "Epoch 2491 - lr: 0.05000 - Train loss: 17.90605 - Test loss: 54.06210\n",
      "Epoch 2492 - lr: 0.05000 - Train loss: 17.90605 - Test loss: 54.06204\n",
      "Epoch 2493 - lr: 0.05000 - Train loss: 17.90605 - Test loss: 54.06198\n",
      "Epoch 2494 - lr: 0.05000 - Train loss: 17.90605 - Test loss: 54.06192\n",
      "Epoch 2495 - lr: 0.05000 - Train loss: 17.90604 - Test loss: 54.06186\n",
      "Epoch 2496 - lr: 0.05000 - Train loss: 17.90604 - Test loss: 54.06181\n",
      "Epoch 2497 - lr: 0.05000 - Train loss: 17.90604 - Test loss: 54.06175\n",
      "Epoch 2498 - lr: 0.05000 - Train loss: 17.90604 - Test loss: 54.06169\n",
      "Epoch 2499 - lr: 0.05000 - Train loss: 17.90603 - Test loss: 54.06163\n",
      "Epoch 2500 - lr: 0.05000 - Train loss: 17.90603 - Test loss: 54.06158\n",
      "Epoch 2501 - lr: 0.05000 - Train loss: 17.90603 - Test loss: 54.06152\n",
      "Epoch 2502 - lr: 0.05000 - Train loss: 17.90603 - Test loss: 54.06146\n",
      "Epoch 2503 - lr: 0.05000 - Train loss: 17.90603 - Test loss: 54.06141\n",
      "Epoch 2504 - lr: 0.05000 - Train loss: 17.90602 - Test loss: 54.06135\n",
      "Epoch 2505 - lr: 0.05000 - Train loss: 17.90602 - Test loss: 54.06129\n",
      "Epoch 2506 - lr: 0.05000 - Train loss: 17.90602 - Test loss: 54.06124\n",
      "Epoch 2507 - lr: 0.05000 - Train loss: 17.90602 - Test loss: 54.06118\n",
      "Epoch 2508 - lr: 0.05000 - Train loss: 17.90602 - Test loss: 54.06112\n",
      "Epoch 2509 - lr: 0.05000 - Train loss: 17.90601 - Test loss: 54.06107\n",
      "Epoch 2510 - lr: 0.05000 - Train loss: 17.90601 - Test loss: 54.06101\n",
      "Epoch 2511 - lr: 0.05000 - Train loss: 17.90601 - Test loss: 54.06096\n",
      "Epoch 2512 - lr: 0.05000 - Train loss: 17.90601 - Test loss: 54.06090\n",
      "Epoch 2513 - lr: 0.05000 - Train loss: 17.90600 - Test loss: 54.06084\n",
      "Epoch 2514 - lr: 0.05000 - Train loss: 17.90600 - Test loss: 54.06079\n",
      "Epoch 2515 - lr: 0.05000 - Train loss: 17.90600 - Test loss: 54.06073\n",
      "Epoch 2516 - lr: 0.05000 - Train loss: 17.90600 - Test loss: 54.06068\n",
      "Epoch 2517 - lr: 0.05000 - Train loss: 17.90600 - Test loss: 54.06062\n",
      "Epoch 2518 - lr: 0.05000 - Train loss: 17.90599 - Test loss: 54.06057\n",
      "Epoch 2519 - lr: 0.05000 - Train loss: 17.90599 - Test loss: 54.06051\n",
      "Epoch 2520 - lr: 0.05000 - Train loss: 17.90599 - Test loss: 54.06046\n",
      "Epoch 2521 - lr: 0.05000 - Train loss: 17.90599 - Test loss: 54.06040\n",
      "Epoch 2522 - lr: 0.05000 - Train loss: 17.90599 - Test loss: 54.06035\n",
      "Epoch 2523 - lr: 0.05000 - Train loss: 17.90598 - Test loss: 54.06029\n",
      "Epoch 2524 - lr: 0.05000 - Train loss: 17.90598 - Test loss: 54.06024\n",
      "Epoch 2525 - lr: 0.05000 - Train loss: 17.90598 - Test loss: 54.06018\n",
      "Epoch 2526 - lr: 0.05000 - Train loss: 17.90598 - Test loss: 54.06013\n",
      "Epoch 2527 - lr: 0.05000 - Train loss: 17.90597 - Test loss: 54.06007\n",
      "Epoch 2528 - lr: 0.05000 - Train loss: 17.90597 - Test loss: 54.06002\n",
      "Epoch 2529 - lr: 0.05000 - Train loss: 17.90597 - Test loss: 54.05996\n",
      "Epoch 2530 - lr: 0.05000 - Train loss: 17.90597 - Test loss: 54.05991\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2531 - lr: 0.05000 - Train loss: 17.90597 - Test loss: 54.05985\n",
      "Epoch 2532 - lr: 0.05000 - Train loss: 17.90596 - Test loss: 54.05980\n",
      "Epoch 2533 - lr: 0.05000 - Train loss: 17.90596 - Test loss: 54.05974\n",
      "Epoch 2534 - lr: 0.05000 - Train loss: 17.90596 - Test loss: 54.05969\n",
      "Epoch 2535 - lr: 0.05000 - Train loss: 17.90596 - Test loss: 54.05964\n",
      "Epoch 2536 - lr: 0.05000 - Train loss: 17.90595 - Test loss: 54.05958\n",
      "Epoch 2537 - lr: 0.05000 - Train loss: 17.90595 - Test loss: 54.05953\n",
      "Epoch 2538 - lr: 0.05000 - Train loss: 17.90595 - Test loss: 54.05947\n",
      "Epoch 2539 - lr: 0.05000 - Train loss: 17.90595 - Test loss: 54.05942\n",
      "Epoch 2540 - lr: 0.05000 - Train loss: 17.90595 - Test loss: 54.05937\n",
      "Epoch 2541 - lr: 0.05000 - Train loss: 17.90594 - Test loss: 54.05931\n",
      "Epoch 2542 - lr: 0.05000 - Train loss: 17.90594 - Test loss: 54.05926\n",
      "Epoch 2543 - lr: 0.05000 - Train loss: 17.90594 - Test loss: 54.05920\n",
      "Epoch 2544 - lr: 0.05000 - Train loss: 17.90594 - Test loss: 54.05915\n",
      "Epoch 2545 - lr: 0.05000 - Train loss: 17.90593 - Test loss: 54.05910\n",
      "Epoch 2546 - lr: 0.05000 - Train loss: 17.90593 - Test loss: 54.05904\n",
      "Epoch 2547 - lr: 0.05000 - Train loss: 17.90593 - Test loss: 54.05899\n",
      "Epoch 2548 - lr: 0.05000 - Train loss: 17.90593 - Test loss: 54.05894\n",
      "Epoch 2549 - lr: 0.05000 - Train loss: 17.90593 - Test loss: 54.05888\n",
      "Epoch 2550 - lr: 0.05000 - Train loss: 17.90592 - Test loss: 54.05883\n",
      "Epoch 2551 - lr: 0.05000 - Train loss: 17.90592 - Test loss: 54.05878\n",
      "Epoch 2552 - lr: 0.05000 - Train loss: 17.90592 - Test loss: 54.05872\n",
      "Epoch 2553 - lr: 0.05000 - Train loss: 17.90592 - Test loss: 54.05867\n",
      "Epoch 2554 - lr: 0.05000 - Train loss: 17.90591 - Test loss: 54.05862\n",
      "Epoch 2555 - lr: 0.05000 - Train loss: 17.90591 - Test loss: 54.05856\n",
      "Epoch 2556 - lr: 0.05000 - Train loss: 17.90591 - Test loss: 54.05851\n",
      "Epoch 2557 - lr: 0.05000 - Train loss: 17.90591 - Test loss: 54.05846\n",
      "Epoch 2558 - lr: 0.05000 - Train loss: 17.90591 - Test loss: 54.05840\n",
      "Epoch 2559 - lr: 0.05000 - Train loss: 17.90590 - Test loss: 54.05835\n",
      "Epoch 2560 - lr: 0.05000 - Train loss: 17.90590 - Test loss: 54.05830\n",
      "Epoch 2561 - lr: 0.05000 - Train loss: 17.90590 - Test loss: 54.05825\n",
      "Epoch 2562 - lr: 0.05000 - Train loss: 17.90590 - Test loss: 54.05819\n",
      "Epoch 2563 - lr: 0.05000 - Train loss: 17.90590 - Test loss: 54.05814\n",
      "Epoch 2564 - lr: 0.05000 - Train loss: 17.90589 - Test loss: 54.05809\n",
      "Epoch 2565 - lr: 0.05000 - Train loss: 17.90589 - Test loss: 54.05803\n",
      "Epoch 2566 - lr: 0.05000 - Train loss: 17.90589 - Test loss: 54.05798\n",
      "Epoch 2567 - lr: 0.05000 - Train loss: 17.90589 - Test loss: 54.05793\n",
      "Epoch 2568 - lr: 0.05000 - Train loss: 17.90588 - Test loss: 54.05788\n",
      "Epoch 2569 - lr: 0.05000 - Train loss: 17.90588 - Test loss: 54.05782\n",
      "Epoch 2570 - lr: 0.05000 - Train loss: 17.90588 - Test loss: 54.05777\n",
      "Epoch 2571 - lr: 0.05000 - Train loss: 17.90588 - Test loss: 54.05772\n",
      "Epoch 2572 - lr: 0.05000 - Train loss: 17.90588 - Test loss: 54.05766\n",
      "Epoch 2573 - lr: 0.05000 - Train loss: 17.90587 - Test loss: 54.05761\n",
      "Epoch 2574 - lr: 0.05000 - Train loss: 17.90587 - Test loss: 54.05756\n",
      "Epoch 2575 - lr: 0.05000 - Train loss: 17.90587 - Test loss: 54.05751\n",
      "Epoch 2576 - lr: 0.05000 - Train loss: 17.90587 - Test loss: 54.05745\n",
      "Epoch 2577 - lr: 0.05000 - Train loss: 17.90586 - Test loss: 54.05740\n",
      "Epoch 2578 - lr: 0.05000 - Train loss: 17.90586 - Test loss: 54.05735\n",
      "Epoch 2579 - lr: 0.05000 - Train loss: 17.90586 - Test loss: 54.05730\n",
      "Epoch 2580 - lr: 0.05000 - Train loss: 17.90586 - Test loss: 54.05724\n",
      "Epoch 2581 - lr: 0.05000 - Train loss: 17.90586 - Test loss: 54.05719\n",
      "Epoch 2582 - lr: 0.05000 - Train loss: 17.90585 - Test loss: 54.05714\n",
      "Epoch 2583 - lr: 0.05000 - Train loss: 17.90585 - Test loss: 54.05709\n",
      "Epoch 2584 - lr: 0.05000 - Train loss: 17.90585 - Test loss: 54.05703\n",
      "Epoch 2585 - lr: 0.05000 - Train loss: 17.90585 - Test loss: 54.05698\n",
      "Epoch 2586 - lr: 0.05000 - Train loss: 17.90585 - Test loss: 54.05693\n",
      "Epoch 2587 - lr: 0.05000 - Train loss: 17.90584 - Test loss: 54.05688\n",
      "Epoch 2588 - lr: 0.05000 - Train loss: 17.90584 - Test loss: 54.05683\n",
      "Epoch 2589 - lr: 0.05000 - Train loss: 17.90584 - Test loss: 54.05677\n",
      "Epoch 2590 - lr: 0.05000 - Train loss: 17.90584 - Test loss: 54.05672\n",
      "Epoch 2591 - lr: 0.05000 - Train loss: 17.90583 - Test loss: 54.05667\n",
      "Epoch 2592 - lr: 0.05000 - Train loss: 17.90583 - Test loss: 54.05662\n",
      "Epoch 2593 - lr: 0.05000 - Train loss: 17.90583 - Test loss: 54.05656\n",
      "Epoch 2594 - lr: 0.05000 - Train loss: 17.90583 - Test loss: 54.05651\n",
      "Epoch 2595 - lr: 0.05000 - Train loss: 17.90583 - Test loss: 54.05646\n",
      "Epoch 2596 - lr: 0.05000 - Train loss: 17.90582 - Test loss: 54.05641\n",
      "Epoch 2597 - lr: 0.05000 - Train loss: 17.90582 - Test loss: 54.05636\n",
      "Epoch 2598 - lr: 0.05000 - Train loss: 17.90582 - Test loss: 54.05630\n",
      "Epoch 2599 - lr: 0.05000 - Train loss: 17.90582 - Test loss: 54.05625\n",
      "Epoch 2600 - lr: 0.05000 - Train loss: 17.90582 - Test loss: 54.05620\n",
      "Epoch 2601 - lr: 0.05000 - Train loss: 17.90581 - Test loss: 54.05615\n",
      "Epoch 2602 - lr: 0.05000 - Train loss: 17.90581 - Test loss: 54.05610\n",
      "Epoch 2603 - lr: 0.05000 - Train loss: 17.90581 - Test loss: 54.05604\n",
      "Epoch 2604 - lr: 0.05000 - Train loss: 17.90581 - Test loss: 54.05599\n",
      "Epoch 2605 - lr: 0.05000 - Train loss: 17.90581 - Test loss: 54.05594\n",
      "Epoch 2606 - lr: 0.05000 - Train loss: 17.90580 - Test loss: 54.05589\n",
      "Epoch 2607 - lr: 0.05000 - Train loss: 17.90580 - Test loss: 54.05583\n",
      "Epoch 2608 - lr: 0.05000 - Train loss: 17.90580 - Test loss: 54.05578\n",
      "Epoch 2609 - lr: 0.05000 - Train loss: 17.90580 - Test loss: 54.05573\n",
      "Epoch 2610 - lr: 0.05000 - Train loss: 17.90579 - Test loss: 54.05568\n",
      "Epoch 2611 - lr: 0.05000 - Train loss: 17.90579 - Test loss: 54.05563\n",
      "Epoch 2612 - lr: 0.05000 - Train loss: 17.90579 - Test loss: 54.05557\n",
      "Epoch 2613 - lr: 0.05000 - Train loss: 17.90579 - Test loss: 54.05552\n",
      "Epoch 2614 - lr: 0.05000 - Train loss: 17.90579 - Test loss: 54.05547\n",
      "Epoch 2615 - lr: 0.05000 - Train loss: 17.90578 - Test loss: 54.05542\n",
      "Epoch 2616 - lr: 0.05000 - Train loss: 17.90578 - Test loss: 54.05537\n",
      "Epoch 2617 - lr: 0.05000 - Train loss: 17.90578 - Test loss: 54.05531\n",
      "Epoch 2618 - lr: 0.05000 - Train loss: 17.90578 - Test loss: 54.05526\n",
      "Epoch 2619 - lr: 0.05000 - Train loss: 17.90578 - Test loss: 54.05521\n",
      "Epoch 2620 - lr: 0.05000 - Train loss: 17.90577 - Test loss: 54.05516\n",
      "Epoch 2621 - lr: 0.05000 - Train loss: 17.90577 - Test loss: 54.05511\n",
      "Epoch 2622 - lr: 0.05000 - Train loss: 17.90577 - Test loss: 54.05505\n",
      "Epoch 2623 - lr: 0.05000 - Train loss: 17.90577 - Test loss: 54.05500\n",
      "Epoch 2624 - lr: 0.05000 - Train loss: 17.90577 - Test loss: 54.05495\n",
      "Epoch 2625 - lr: 0.05000 - Train loss: 17.90576 - Test loss: 54.05490\n",
      "Epoch 2626 - lr: 0.05000 - Train loss: 17.90576 - Test loss: 54.05485\n",
      "Epoch 2627 - lr: 0.05000 - Train loss: 17.90576 - Test loss: 54.05480\n",
      "Epoch 2628 - lr: 0.05000 - Train loss: 17.90576 - Test loss: 54.05474\n",
      "Epoch 2629 - lr: 0.05000 - Train loss: 17.90576 - Test loss: 54.05469\n",
      "Epoch 2630 - lr: 0.05000 - Train loss: 17.90575 - Test loss: 54.05464\n",
      "Epoch 2631 - lr: 0.05000 - Train loss: 17.90575 - Test loss: 54.05459\n",
      "Epoch 2632 - lr: 0.05000 - Train loss: 17.90575 - Test loss: 54.05454\n",
      "Epoch 2633 - lr: 0.05000 - Train loss: 17.90575 - Test loss: 54.05448\n",
      "Epoch 2634 - lr: 0.05000 - Train loss: 17.90575 - Test loss: 54.05443\n",
      "Epoch 2635 - lr: 0.05000 - Train loss: 17.90574 - Test loss: 54.05438\n",
      "Epoch 2636 - lr: 0.05000 - Train loss: 17.90574 - Test loss: 54.05433\n",
      "Epoch 2637 - lr: 0.05000 - Train loss: 17.90574 - Test loss: 54.05428\n",
      "Epoch 2638 - lr: 0.05000 - Train loss: 17.90574 - Test loss: 54.05422\n",
      "Epoch 2639 - lr: 0.05000 - Train loss: 17.90574 - Test loss: 54.05417\n",
      "Epoch 2640 - lr: 0.05000 - Train loss: 17.90573 - Test loss: 54.05412\n",
      "Epoch 2641 - lr: 0.05000 - Train loss: 17.90573 - Test loss: 54.05407\n",
      "Epoch 2642 - lr: 0.05000 - Train loss: 17.90573 - Test loss: 54.05402\n",
      "Epoch 2643 - lr: 0.05000 - Train loss: 17.90573 - Test loss: 54.05396\n",
      "Epoch 2644 - lr: 0.05000 - Train loss: 17.90573 - Test loss: 54.05391\n",
      "Epoch 2645 - lr: 0.05000 - Train loss: 17.90572 - Test loss: 54.05386\n",
      "Epoch 2646 - lr: 0.05000 - Train loss: 17.90572 - Test loss: 54.05381\n",
      "Epoch 2647 - lr: 0.05000 - Train loss: 17.90572 - Test loss: 54.05376\n",
      "Epoch 2648 - lr: 0.05000 - Train loss: 17.90572 - Test loss: 54.05371\n",
      "Epoch 2649 - lr: 0.05000 - Train loss: 17.90572 - Test loss: 54.05365\n",
      "Epoch 2650 - lr: 0.05000 - Train loss: 17.90572 - Test loss: 54.05360\n",
      "Epoch 2651 - lr: 0.05000 - Train loss: 17.90571 - Test loss: 54.05355\n",
      "Epoch 2652 - lr: 0.05000 - Train loss: 17.90571 - Test loss: 54.05350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2653 - lr: 0.05000 - Train loss: 17.90571 - Test loss: 54.05345\n",
      "Epoch 2654 - lr: 0.05000 - Train loss: 17.90571 - Test loss: 54.05339\n",
      "Epoch 2655 - lr: 0.05000 - Train loss: 17.90571 - Test loss: 54.05334\n",
      "Epoch 2656 - lr: 0.05000 - Train loss: 17.90570 - Test loss: 54.05329\n",
      "Epoch 2657 - lr: 0.05000 - Train loss: 17.90570 - Test loss: 54.05324\n",
      "Epoch 2658 - lr: 0.05000 - Train loss: 17.90570 - Test loss: 54.05319\n",
      "Epoch 2659 - lr: 0.05000 - Train loss: 17.90570 - Test loss: 54.05313\n",
      "Epoch 2660 - lr: 0.05000 - Train loss: 17.90570 - Test loss: 54.05308\n",
      "Epoch 2661 - lr: 0.05000 - Train loss: 17.90569 - Test loss: 54.05303\n",
      "Epoch 2662 - lr: 0.05000 - Train loss: 17.90569 - Test loss: 54.05298\n",
      "Epoch 2663 - lr: 0.05000 - Train loss: 17.90569 - Test loss: 54.05293\n",
      "Epoch 2664 - lr: 0.05000 - Train loss: 17.90569 - Test loss: 54.05287\n",
      "Epoch 2665 - lr: 0.05000 - Train loss: 17.90569 - Test loss: 54.05282\n",
      "Epoch 2666 - lr: 0.05000 - Train loss: 17.90568 - Test loss: 54.05277\n",
      "Epoch 2667 - lr: 0.05000 - Train loss: 17.90568 - Test loss: 54.05272\n",
      "Epoch 2668 - lr: 0.05000 - Train loss: 17.90568 - Test loss: 54.05267\n",
      "Epoch 2669 - lr: 0.05000 - Train loss: 17.90568 - Test loss: 54.05261\n",
      "Epoch 2670 - lr: 0.05000 - Train loss: 17.90568 - Test loss: 54.05256\n",
      "Epoch 2671 - lr: 0.05000 - Train loss: 17.90568 - Test loss: 54.05251\n",
      "Epoch 2672 - lr: 0.05000 - Train loss: 17.90567 - Test loss: 54.05246\n",
      "Epoch 2673 - lr: 0.05000 - Train loss: 17.90567 - Test loss: 54.05241\n",
      "Epoch 2674 - lr: 0.05000 - Train loss: 17.90567 - Test loss: 54.05236\n",
      "Epoch 2675 - lr: 0.05000 - Train loss: 17.90567 - Test loss: 54.05230\n",
      "Epoch 2676 - lr: 0.05000 - Train loss: 17.90567 - Test loss: 54.05225\n",
      "Epoch 2677 - lr: 0.05000 - Train loss: 17.90566 - Test loss: 54.05220\n",
      "Epoch 2678 - lr: 0.05000 - Train loss: 17.90566 - Test loss: 54.05215\n",
      "Epoch 2679 - lr: 0.05000 - Train loss: 17.90566 - Test loss: 54.05210\n",
      "Epoch 2680 - lr: 0.05000 - Train loss: 17.90566 - Test loss: 54.05204\n",
      "Epoch 2681 - lr: 0.05000 - Train loss: 17.90566 - Test loss: 54.05199\n",
      "Epoch 2682 - lr: 0.05000 - Train loss: 17.90566 - Test loss: 54.05194\n",
      "Epoch 2683 - lr: 0.05000 - Train loss: 17.90565 - Test loss: 54.05189\n",
      "Epoch 2684 - lr: 0.05000 - Train loss: 17.90565 - Test loss: 54.05184\n",
      "Epoch 2685 - lr: 0.05000 - Train loss: 17.90565 - Test loss: 54.05178\n",
      "Epoch 2686 - lr: 0.05000 - Train loss: 17.90565 - Test loss: 54.05173\n",
      "Epoch 2687 - lr: 0.05000 - Train loss: 17.90565 - Test loss: 54.05168\n",
      "Epoch 2688 - lr: 0.05000 - Train loss: 17.90564 - Test loss: 54.05163\n",
      "Epoch 2689 - lr: 0.05000 - Train loss: 17.90564 - Test loss: 54.05158\n",
      "Epoch 2690 - lr: 0.05000 - Train loss: 17.90564 - Test loss: 54.05152\n",
      "Epoch 2691 - lr: 0.05000 - Train loss: 17.90564 - Test loss: 54.05147\n",
      "Epoch 2692 - lr: 0.05000 - Train loss: 17.90564 - Test loss: 54.05142\n",
      "Epoch 2693 - lr: 0.05000 - Train loss: 17.90564 - Test loss: 54.05137\n",
      "Epoch 2694 - lr: 0.05000 - Train loss: 17.90563 - Test loss: 54.05132\n",
      "Epoch 2695 - lr: 0.05000 - Train loss: 17.90563 - Test loss: 54.05126\n",
      "Epoch 2696 - lr: 0.05000 - Train loss: 17.90563 - Test loss: 54.05121\n",
      "Epoch 2697 - lr: 0.05000 - Train loss: 17.90563 - Test loss: 54.05116\n",
      "Epoch 2698 - lr: 0.05000 - Train loss: 17.90563 - Test loss: 54.05111\n",
      "Epoch 2699 - lr: 0.05000 - Train loss: 17.90562 - Test loss: 54.05105\n",
      "Epoch 2700 - lr: 0.05000 - Train loss: 17.90562 - Test loss: 54.05100\n",
      "Epoch 2701 - lr: 0.05000 - Train loss: 17.90562 - Test loss: 54.05095\n",
      "Epoch 2702 - lr: 0.05000 - Train loss: 17.90562 - Test loss: 54.05090\n",
      "Epoch 2703 - lr: 0.05000 - Train loss: 17.90562 - Test loss: 54.05085\n",
      "Epoch 2704 - lr: 0.05000 - Train loss: 17.90562 - Test loss: 54.05079\n",
      "Epoch 2705 - lr: 0.05000 - Train loss: 17.90561 - Test loss: 54.05074\n",
      "Epoch 2706 - lr: 0.05000 - Train loss: 17.90561 - Test loss: 54.05069\n",
      "Epoch 2707 - lr: 0.05000 - Train loss: 17.90561 - Test loss: 54.05064\n",
      "Epoch 2708 - lr: 0.05000 - Train loss: 17.90561 - Test loss: 54.05059\n",
      "Epoch 2709 - lr: 0.05000 - Train loss: 17.90561 - Test loss: 54.05053\n",
      "Epoch 2710 - lr: 0.05000 - Train loss: 17.90561 - Test loss: 54.05048\n",
      "Epoch 2711 - lr: 0.05000 - Train loss: 17.90560 - Test loss: 54.05043\n",
      "Epoch 2712 - lr: 0.05000 - Train loss: 17.90560 - Test loss: 54.05038\n",
      "Epoch 2713 - lr: 0.05000 - Train loss: 17.90560 - Test loss: 54.05033\n",
      "Epoch 2714 - lr: 0.05000 - Train loss: 17.90560 - Test loss: 54.05027\n",
      "Epoch 2715 - lr: 0.05000 - Train loss: 17.90560 - Test loss: 54.05022\n",
      "Epoch 2716 - lr: 0.05000 - Train loss: 17.90559 - Test loss: 54.05017\n",
      "Epoch 2717 - lr: 0.05000 - Train loss: 17.90559 - Test loss: 54.05012\n",
      "Epoch 2718 - lr: 0.05000 - Train loss: 17.90559 - Test loss: 54.05007\n",
      "Epoch 2719 - lr: 0.05000 - Train loss: 17.90559 - Test loss: 54.05001\n",
      "Epoch 2720 - lr: 0.05000 - Train loss: 17.90559 - Test loss: 54.04996\n",
      "Epoch 2721 - lr: 0.05000 - Train loss: 17.90559 - Test loss: 54.04991\n",
      "Epoch 2722 - lr: 0.05000 - Train loss: 17.90558 - Test loss: 54.04986\n",
      "Epoch 2723 - lr: 0.05000 - Train loss: 17.90558 - Test loss: 54.04980\n",
      "Epoch 2724 - lr: 0.05000 - Train loss: 17.90558 - Test loss: 54.04975\n",
      "Epoch 2725 - lr: 0.05000 - Train loss: 17.90558 - Test loss: 54.04970\n",
      "Epoch 2726 - lr: 0.05000 - Train loss: 17.90558 - Test loss: 54.04965\n",
      "Epoch 2727 - lr: 0.05000 - Train loss: 17.90558 - Test loss: 54.04960\n",
      "Epoch 2728 - lr: 0.05000 - Train loss: 17.90557 - Test loss: 54.04954\n",
      "Epoch 2729 - lr: 0.05000 - Train loss: 17.90557 - Test loss: 54.04949\n",
      "Epoch 2730 - lr: 0.05000 - Train loss: 17.90557 - Test loss: 54.04944\n",
      "Epoch 2731 - lr: 0.05000 - Train loss: 17.90557 - Test loss: 54.04939\n",
      "Epoch 2732 - lr: 0.05000 - Train loss: 17.90557 - Test loss: 54.04934\n",
      "Epoch 2733 - lr: 0.05000 - Train loss: 17.90557 - Test loss: 54.04928\n",
      "Epoch 2734 - lr: 0.05000 - Train loss: 17.90556 - Test loss: 54.04923\n",
      "Epoch 2735 - lr: 0.05000 - Train loss: 17.90556 - Test loss: 54.04918\n",
      "Epoch 2736 - lr: 0.05000 - Train loss: 17.90556 - Test loss: 54.04913\n",
      "Epoch 2737 - lr: 0.05000 - Train loss: 17.90556 - Test loss: 54.04907\n",
      "Epoch 2738 - lr: 0.05000 - Train loss: 17.90556 - Test loss: 54.04902\n",
      "Epoch 2739 - lr: 0.05000 - Train loss: 17.90556 - Test loss: 54.04897\n",
      "Epoch 2740 - lr: 0.05000 - Train loss: 17.90555 - Test loss: 54.04892\n",
      "Epoch 2741 - lr: 0.05000 - Train loss: 17.90555 - Test loss: 54.04887\n",
      "Epoch 2742 - lr: 0.05000 - Train loss: 17.90555 - Test loss: 54.04881\n",
      "Epoch 2743 - lr: 0.05000 - Train loss: 17.90555 - Test loss: 54.04876\n",
      "Epoch 2744 - lr: 0.05000 - Train loss: 17.90555 - Test loss: 54.04871\n",
      "Epoch 2745 - lr: 0.05000 - Train loss: 17.90555 - Test loss: 54.04866\n",
      "Epoch 2746 - lr: 0.05000 - Train loss: 17.90554 - Test loss: 54.04861\n",
      "Epoch 2747 - lr: 0.05000 - Train loss: 17.90554 - Test loss: 54.04855\n",
      "Epoch 2748 - lr: 0.05000 - Train loss: 17.90554 - Test loss: 54.04850\n",
      "Epoch 2749 - lr: 0.05000 - Train loss: 17.90554 - Test loss: 54.04845\n",
      "Epoch 2750 - lr: 0.05000 - Train loss: 17.90554 - Test loss: 54.04840\n",
      "Epoch 2751 - lr: 0.05000 - Train loss: 17.90554 - Test loss: 54.04834\n",
      "Epoch 2752 - lr: 0.05000 - Train loss: 17.90553 - Test loss: 54.04829\n",
      "Epoch 2753 - lr: 0.05000 - Train loss: 17.90553 - Test loss: 54.04824\n",
      "Epoch 2754 - lr: 0.05000 - Train loss: 17.90553 - Test loss: 54.04819\n",
      "Epoch 2755 - lr: 0.05000 - Train loss: 17.90553 - Test loss: 54.04814\n",
      "Epoch 2756 - lr: 0.05000 - Train loss: 17.90553 - Test loss: 54.04808\n",
      "Epoch 2757 - lr: 0.05000 - Train loss: 17.90553 - Test loss: 54.04803\n",
      "Epoch 2758 - lr: 0.05000 - Train loss: 17.90553 - Test loss: 54.04798\n",
      "Epoch 2759 - lr: 0.05000 - Train loss: 17.90552 - Test loss: 54.04793\n",
      "Epoch 2760 - lr: 0.05000 - Train loss: 17.90552 - Test loss: 54.04788\n",
      "Epoch 2761 - lr: 0.05000 - Train loss: 17.90552 - Test loss: 54.04782\n",
      "Epoch 2762 - lr: 0.05000 - Train loss: 17.90552 - Test loss: 54.04777\n",
      "Epoch 2763 - lr: 0.05000 - Train loss: 17.90552 - Test loss: 54.04772\n",
      "Epoch 2764 - lr: 0.05000 - Train loss: 17.90552 - Test loss: 54.04767\n",
      "Epoch 2765 - lr: 0.05000 - Train loss: 17.90551 - Test loss: 54.04761\n",
      "Epoch 2766 - lr: 0.05000 - Train loss: 17.90551 - Test loss: 54.04756\n",
      "Epoch 2767 - lr: 0.05000 - Train loss: 17.90551 - Test loss: 54.04751\n",
      "Epoch 2768 - lr: 0.05000 - Train loss: 17.90551 - Test loss: 54.04746\n",
      "Epoch 2769 - lr: 0.05000 - Train loss: 17.90551 - Test loss: 54.04741\n",
      "Epoch 2770 - lr: 0.05000 - Train loss: 17.90551 - Test loss: 54.04735\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2771 - lr: 0.05000 - Train loss: 17.90550 - Test loss: 54.04730\n",
      "Epoch 2772 - lr: 0.05000 - Train loss: 17.90550 - Test loss: 54.04725\n",
      "Epoch 2773 - lr: 0.05000 - Train loss: 17.90550 - Test loss: 54.04720\n",
      "Epoch 2774 - lr: 0.05000 - Train loss: 17.90550 - Test loss: 54.04715\n",
      "Epoch 2775 - lr: 0.05000 - Train loss: 17.90550 - Test loss: 54.04709\n",
      "Epoch 2776 - lr: 0.05000 - Train loss: 17.90550 - Test loss: 54.04704\n",
      "Epoch 2777 - lr: 0.05000 - Train loss: 17.90549 - Test loss: 54.04699\n",
      "Epoch 2778 - lr: 0.05000 - Train loss: 17.90549 - Test loss: 54.04694\n",
      "Epoch 2779 - lr: 0.05000 - Train loss: 17.90549 - Test loss: 54.04689\n",
      "Epoch 2780 - lr: 0.05000 - Train loss: 17.90549 - Test loss: 54.04683\n",
      "Epoch 2781 - lr: 0.05000 - Train loss: 17.90549 - Test loss: 54.04678\n",
      "Epoch 2782 - lr: 0.05000 - Train loss: 17.90549 - Test loss: 54.04673\n",
      "Epoch 2783 - lr: 0.05000 - Train loss: 17.90549 - Test loss: 54.04668\n",
      "Epoch 2784 - lr: 0.05000 - Train loss: 17.90548 - Test loss: 54.04663\n",
      "Epoch 2785 - lr: 0.05000 - Train loss: 17.90548 - Test loss: 54.04657\n",
      "Epoch 2786 - lr: 0.05000 - Train loss: 17.90548 - Test loss: 54.04652\n",
      "Epoch 2787 - lr: 0.05000 - Train loss: 17.90548 - Test loss: 54.04647\n",
      "Epoch 2788 - lr: 0.05000 - Train loss: 17.90548 - Test loss: 54.04642\n",
      "Epoch 2789 - lr: 0.05000 - Train loss: 17.90548 - Test loss: 54.04637\n",
      "Epoch 2790 - lr: 0.05000 - Train loss: 17.90547 - Test loss: 54.04631\n",
      "Epoch 2791 - lr: 0.05000 - Train loss: 17.90547 - Test loss: 54.04626\n",
      "Epoch 2792 - lr: 0.05000 - Train loss: 17.90547 - Test loss: 54.04621\n",
      "Epoch 2793 - lr: 0.05000 - Train loss: 17.90547 - Test loss: 54.04616\n",
      "Epoch 2794 - lr: 0.05000 - Train loss: 17.90547 - Test loss: 54.04611\n",
      "Epoch 2795 - lr: 0.05000 - Train loss: 17.90547 - Test loss: 54.04605\n",
      "Epoch 2796 - lr: 0.05000 - Train loss: 17.90546 - Test loss: 54.04600\n",
      "Epoch 2797 - lr: 0.05000 - Train loss: 17.90546 - Test loss: 54.04595\n",
      "Epoch 2798 - lr: 0.05000 - Train loss: 17.90546 - Test loss: 54.04590\n",
      "Epoch 2799 - lr: 0.05000 - Train loss: 17.90546 - Test loss: 54.04585\n",
      "Epoch 2800 - lr: 0.05000 - Train loss: 17.90546 - Test loss: 54.04580\n",
      "Epoch 2801 - lr: 0.05000 - Train loss: 17.90546 - Test loss: 54.04574\n",
      "Epoch 2802 - lr: 0.05000 - Train loss: 17.90546 - Test loss: 54.04569\n",
      "Epoch 2803 - lr: 0.05000 - Train loss: 17.90545 - Test loss: 54.04564\n",
      "Epoch 2804 - lr: 0.05000 - Train loss: 17.90545 - Test loss: 54.04559\n",
      "Epoch 2805 - lr: 0.05000 - Train loss: 17.90545 - Test loss: 54.04554\n",
      "Epoch 2806 - lr: 0.05000 - Train loss: 17.90545 - Test loss: 54.04548\n",
      "Epoch 2807 - lr: 0.05000 - Train loss: 17.90545 - Test loss: 54.04543\n",
      "Epoch 2808 - lr: 0.05000 - Train loss: 17.90545 - Test loss: 54.04538\n",
      "Epoch 2809 - lr: 0.05000 - Train loss: 17.90544 - Test loss: 54.04533\n",
      "Epoch 2810 - lr: 0.05000 - Train loss: 17.90544 - Test loss: 54.04528\n",
      "Epoch 2811 - lr: 0.05000 - Train loss: 17.90544 - Test loss: 54.04523\n",
      "Epoch 2812 - lr: 0.05000 - Train loss: 17.90544 - Test loss: 54.04517\n",
      "Epoch 2813 - lr: 0.05000 - Train loss: 17.90544 - Test loss: 54.04512\n",
      "Epoch 2814 - lr: 0.05000 - Train loss: 17.90544 - Test loss: 54.04507\n",
      "Epoch 2815 - lr: 0.05000 - Train loss: 17.90544 - Test loss: 54.04502\n",
      "Epoch 2816 - lr: 0.05000 - Train loss: 17.90543 - Test loss: 54.04497\n",
      "Epoch 2817 - lr: 0.05000 - Train loss: 17.90543 - Test loss: 54.04492\n",
      "Epoch 2818 - lr: 0.05000 - Train loss: 17.90543 - Test loss: 54.04487\n",
      "Epoch 2819 - lr: 0.05000 - Train loss: 17.90543 - Test loss: 54.04481\n",
      "Epoch 2820 - lr: 0.05000 - Train loss: 17.90543 - Test loss: 54.04476\n",
      "Epoch 2821 - lr: 0.05000 - Train loss: 17.90543 - Test loss: 54.04471\n",
      "Epoch 2822 - lr: 0.05000 - Train loss: 17.90542 - Test loss: 54.04466\n",
      "Epoch 2823 - lr: 0.05000 - Train loss: 17.90542 - Test loss: 54.04461\n",
      "Epoch 2824 - lr: 0.05000 - Train loss: 17.90542 - Test loss: 54.04456\n",
      "Epoch 2825 - lr: 0.05000 - Train loss: 17.90542 - Test loss: 54.04451\n",
      "Epoch 2826 - lr: 0.05000 - Train loss: 17.90542 - Test loss: 54.04445\n",
      "Epoch 2827 - lr: 0.05000 - Train loss: 17.90542 - Test loss: 54.04440\n",
      "Epoch 2828 - lr: 0.05000 - Train loss: 17.90541 - Test loss: 54.04435\n",
      "Epoch 2829 - lr: 0.05000 - Train loss: 17.90541 - Test loss: 54.04430\n",
      "Epoch 2830 - lr: 0.05000 - Train loss: 17.90541 - Test loss: 54.04425\n",
      "Epoch 2831 - lr: 0.05000 - Train loss: 17.90541 - Test loss: 54.04420\n",
      "Epoch 2832 - lr: 0.05000 - Train loss: 17.90541 - Test loss: 54.04415\n",
      "Epoch 2833 - lr: 0.05000 - Train loss: 17.90541 - Test loss: 54.04410\n",
      "Epoch 2834 - lr: 0.05000 - Train loss: 17.90540 - Test loss: 54.04405\n",
      "Epoch 2835 - lr: 0.05000 - Train loss: 17.90540 - Test loss: 54.04399\n",
      "Epoch 2836 - lr: 0.05000 - Train loss: 17.90540 - Test loss: 54.04394\n",
      "Epoch 2837 - lr: 0.05000 - Train loss: 17.90540 - Test loss: 54.04389\n",
      "Epoch 2838 - lr: 0.05000 - Train loss: 17.90540 - Test loss: 54.04384\n",
      "Epoch 2839 - lr: 0.05000 - Train loss: 17.90540 - Test loss: 54.04379\n",
      "Epoch 2840 - lr: 0.05000 - Train loss: 17.90540 - Test loss: 54.04374\n",
      "Epoch 2841 - lr: 0.05000 - Train loss: 17.90539 - Test loss: 54.04369\n",
      "Epoch 2842 - lr: 0.05000 - Train loss: 17.90539 - Test loss: 54.04364\n",
      "Epoch 2843 - lr: 0.05000 - Train loss: 17.90539 - Test loss: 54.04359\n",
      "Epoch 2844 - lr: 0.05000 - Train loss: 17.90539 - Test loss: 54.04354\n",
      "Epoch 2845 - lr: 0.05000 - Train loss: 17.90539 - Test loss: 54.04349\n",
      "Epoch 2846 - lr: 0.05000 - Train loss: 17.90539 - Test loss: 54.04344\n",
      "Epoch 2847 - lr: 0.05000 - Train loss: 17.90538 - Test loss: 54.04339\n",
      "Epoch 2848 - lr: 0.05000 - Train loss: 17.90538 - Test loss: 54.04334\n",
      "Epoch 2849 - lr: 0.05000 - Train loss: 17.90538 - Test loss: 54.04329\n",
      "Epoch 2850 - lr: 0.05000 - Train loss: 17.90538 - Test loss: 54.04324\n",
      "Epoch 2851 - lr: 0.05000 - Train loss: 17.90538 - Test loss: 54.04319\n",
      "Epoch 2852 - lr: 0.05000 - Train loss: 17.90538 - Test loss: 54.04314\n",
      "Epoch 2853 - lr: 0.05000 - Train loss: 17.90537 - Test loss: 54.04309\n",
      "Epoch 2854 - lr: 0.05000 - Train loss: 17.90537 - Test loss: 54.04304\n",
      "Epoch 2855 - lr: 0.05000 - Train loss: 17.90537 - Test loss: 54.04299\n",
      "Epoch 2856 - lr: 0.05000 - Train loss: 17.90537 - Test loss: 54.04294\n",
      "Epoch 2857 - lr: 0.05000 - Train loss: 17.90537 - Test loss: 54.04289\n",
      "Epoch 2858 - lr: 0.05000 - Train loss: 17.90536 - Test loss: 54.04284\n",
      "Epoch 2859 - lr: 0.05000 - Train loss: 17.90536 - Test loss: 54.04279\n",
      "Epoch 2860 - lr: 0.05000 - Train loss: 17.90536 - Test loss: 54.04274\n",
      "Epoch 2861 - lr: 0.05000 - Train loss: 17.90536 - Test loss: 54.04269\n",
      "Epoch 2862 - lr: 0.05000 - Train loss: 17.90536 - Test loss: 54.04264\n",
      "Epoch 2863 - lr: 0.05000 - Train loss: 17.90536 - Test loss: 54.04259\n",
      "Epoch 2864 - lr: 0.05000 - Train loss: 17.90535 - Test loss: 54.04254\n",
      "Epoch 2865 - lr: 0.05000 - Train loss: 17.90535 - Test loss: 54.04249\n",
      "Epoch 2866 - lr: 0.05000 - Train loss: 17.90535 - Test loss: 54.04244\n",
      "Epoch 2867 - lr: 0.05000 - Train loss: 17.90535 - Test loss: 54.04239\n",
      "Epoch 2868 - lr: 0.05000 - Train loss: 17.90535 - Test loss: 54.04234\n",
      "Epoch 2869 - lr: 0.05000 - Train loss: 17.90535 - Test loss: 54.04229\n",
      "Epoch 2870 - lr: 0.05000 - Train loss: 17.90534 - Test loss: 54.04224\n",
      "Epoch 2871 - lr: 0.05000 - Train loss: 17.90534 - Test loss: 54.04220\n",
      "Epoch 2872 - lr: 0.05000 - Train loss: 17.90534 - Test loss: 54.04215\n",
      "Epoch 2873 - lr: 0.05000 - Train loss: 17.90534 - Test loss: 54.04210\n",
      "Epoch 2874 - lr: 0.05000 - Train loss: 17.90534 - Test loss: 54.04205\n",
      "Epoch 2875 - lr: 0.05000 - Train loss: 17.90533 - Test loss: 54.04200\n",
      "Epoch 2876 - lr: 0.05000 - Train loss: 17.90533 - Test loss: 54.04195\n",
      "Epoch 2877 - lr: 0.05000 - Train loss: 17.90533 - Test loss: 54.04190\n",
      "Epoch 2878 - lr: 0.05000 - Train loss: 17.90533 - Test loss: 54.04186\n",
      "Epoch 2879 - lr: 0.05000 - Train loss: 17.90533 - Test loss: 54.04181\n",
      "Epoch 2880 - lr: 0.05000 - Train loss: 17.90532 - Test loss: 54.04176\n",
      "Epoch 2881 - lr: 0.05000 - Train loss: 17.90532 - Test loss: 54.04171\n",
      "Epoch 2882 - lr: 0.05000 - Train loss: 17.90532 - Test loss: 54.04167\n",
      "Epoch 2883 - lr: 0.05000 - Train loss: 17.90532 - Test loss: 54.04162\n",
      "Epoch 2884 - lr: 0.05000 - Train loss: 17.90532 - Test loss: 54.04157\n",
      "Epoch 2885 - lr: 0.05000 - Train loss: 17.90531 - Test loss: 54.04152\n",
      "Epoch 2886 - lr: 0.05000 - Train loss: 17.90531 - Test loss: 54.04148\n",
      "Epoch 2887 - lr: 0.05000 - Train loss: 17.90531 - Test loss: 54.04143\n",
      "Epoch 2888 - lr: 0.05000 - Train loss: 17.90531 - Test loss: 54.04138\n",
      "Epoch 2889 - lr: 0.05000 - Train loss: 17.90531 - Test loss: 54.04134\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2890 - lr: 0.05000 - Train loss: 17.90530 - Test loss: 54.04129\n",
      "Epoch 2891 - lr: 0.05000 - Train loss: 17.90530 - Test loss: 54.04124\n",
      "Epoch 2892 - lr: 0.05000 - Train loss: 17.90530 - Test loss: 54.04120\n",
      "Epoch 2893 - lr: 0.05000 - Train loss: 17.90530 - Test loss: 54.04115\n",
      "Epoch 2894 - lr: 0.05000 - Train loss: 17.90530 - Test loss: 54.04111\n",
      "Epoch 2895 - lr: 0.05000 - Train loss: 17.90529 - Test loss: 54.04106\n",
      "Epoch 2896 - lr: 0.05000 - Train loss: 17.90529 - Test loss: 54.04102\n",
      "Epoch 2897 - lr: 0.05000 - Train loss: 17.90529 - Test loss: 54.04097\n",
      "Epoch 2898 - lr: 0.05000 - Train loss: 17.90529 - Test loss: 54.04092\n",
      "Epoch 2899 - lr: 0.05000 - Train loss: 17.90528 - Test loss: 54.04088\n",
      "Epoch 2900 - lr: 0.05000 - Train loss: 17.90528 - Test loss: 54.04084\n",
      "Epoch 2901 - lr: 0.05000 - Train loss: 17.90528 - Test loss: 54.04079\n",
      "Epoch 2902 - lr: 0.05000 - Train loss: 17.90528 - Test loss: 54.04075\n",
      "Epoch 2903 - lr: 0.05000 - Train loss: 17.90527 - Test loss: 54.04070\n",
      "Epoch 2904 - lr: 0.05000 - Train loss: 17.90527 - Test loss: 54.04066\n",
      "Epoch 2905 - lr: 0.05000 - Train loss: 17.90527 - Test loss: 54.04062\n",
      "Epoch 2906 - lr: 0.05000 - Train loss: 17.90527 - Test loss: 54.04057\n",
      "Epoch 2907 - lr: 0.05000 - Train loss: 17.90526 - Test loss: 54.04053\n",
      "Epoch 2908 - lr: 0.05000 - Train loss: 17.90526 - Test loss: 54.04049\n",
      "Epoch 2909 - lr: 0.05000 - Train loss: 17.90526 - Test loss: 54.04045\n",
      "Epoch 2910 - lr: 0.05000 - Train loss: 17.90526 - Test loss: 54.04040\n",
      "Epoch 2911 - lr: 0.05000 - Train loss: 17.90525 - Test loss: 54.04036\n",
      "Epoch 2912 - lr: 0.05000 - Train loss: 17.90525 - Test loss: 54.04032\n",
      "Epoch 2913 - lr: 0.05000 - Train loss: 17.90525 - Test loss: 54.04028\n",
      "Epoch 2914 - lr: 0.05000 - Train loss: 17.90525 - Test loss: 54.04024\n",
      "Epoch 2915 - lr: 0.05000 - Train loss: 17.90524 - Test loss: 54.04020\n",
      "Epoch 2916 - lr: 0.05000 - Train loss: 17.90524 - Test loss: 54.04016\n",
      "Epoch 2917 - lr: 0.05000 - Train loss: 17.90524 - Test loss: 54.04012\n",
      "Epoch 2918 - lr: 0.05000 - Train loss: 17.90523 - Test loss: 54.04008\n",
      "Epoch 2919 - lr: 0.05000 - Train loss: 17.90523 - Test loss: 54.04004\n",
      "Epoch 2920 - lr: 0.05000 - Train loss: 17.90523 - Test loss: 54.04000\n",
      "Epoch 2921 - lr: 0.05000 - Train loss: 17.90522 - Test loss: 54.03996\n",
      "Epoch 2922 - lr: 0.05000 - Train loss: 17.90522 - Test loss: 54.03993\n",
      "Epoch 2923 - lr: 0.05000 - Train loss: 17.90522 - Test loss: 54.03989\n",
      "Epoch 2924 - lr: 0.05000 - Train loss: 17.90521 - Test loss: 54.03985\n",
      "Epoch 2925 - lr: 0.05000 - Train loss: 17.90521 - Test loss: 54.03982\n",
      "Epoch 2926 - lr: 0.05000 - Train loss: 17.90521 - Test loss: 54.03978\n",
      "Epoch 2927 - lr: 0.05000 - Train loss: 17.90520 - Test loss: 54.03975\n",
      "Epoch 2928 - lr: 0.05000 - Train loss: 17.90520 - Test loss: 54.03971\n",
      "Epoch 2929 - lr: 0.05000 - Train loss: 17.90520 - Test loss: 54.03968\n",
      "Epoch 2930 - lr: 0.05000 - Train loss: 17.90519 - Test loss: 54.03965\n",
      "Epoch 2931 - lr: 0.05000 - Train loss: 17.90519 - Test loss: 54.03961\n",
      "Epoch 2932 - lr: 0.05000 - Train loss: 17.90519 - Test loss: 54.03958\n",
      "Epoch 2933 - lr: 0.05000 - Train loss: 17.90518 - Test loss: 54.03955\n",
      "Epoch 2934 - lr: 0.05000 - Train loss: 17.90518 - Test loss: 54.03952\n",
      "Epoch 2935 - lr: 0.05000 - Train loss: 17.90517 - Test loss: 54.03949\n",
      "Epoch 2936 - lr: 0.05000 - Train loss: 17.90517 - Test loss: 54.03946\n",
      "Epoch 2937 - lr: 0.05000 - Train loss: 17.90516 - Test loss: 54.03944\n",
      "Epoch 2938 - lr: 0.05000 - Train loss: 17.90516 - Test loss: 54.03941\n",
      "Epoch 2939 - lr: 0.05000 - Train loss: 17.90515 - Test loss: 54.03938\n",
      "Epoch 2940 - lr: 0.05000 - Train loss: 17.90515 - Test loss: 54.03936\n",
      "Epoch 2941 - lr: 0.05000 - Train loss: 17.90514 - Test loss: 54.03933\n",
      "Epoch 2942 - lr: 0.05000 - Train loss: 17.90514 - Test loss: 54.03931\n",
      "Epoch 2943 - lr: 0.05000 - Train loss: 17.90513 - Test loss: 54.03929\n",
      "Epoch 2944 - lr: 0.05000 - Train loss: 17.90513 - Test loss: 54.03927\n",
      "Epoch 2945 - lr: 0.05000 - Train loss: 17.90512 - Test loss: 54.03925\n",
      "Epoch 2946 - lr: 0.05000 - Train loss: 17.90512 - Test loss: 54.03923\n",
      "Epoch 2947 - lr: 0.05000 - Train loss: 17.90511 - Test loss: 54.03922\n",
      "Epoch 2948 - lr: 0.05000 - Train loss: 17.90510 - Test loss: 54.03920\n",
      "Epoch 2949 - lr: 0.05000 - Train loss: 17.90510 - Test loss: 54.03919\n",
      "Epoch 2950 - lr: 0.05000 - Train loss: 17.90509 - Test loss: 54.03918\n",
      "Epoch 2951 - lr: 0.05000 - Train loss: 17.90508 - Test loss: 54.03917\n",
      "Epoch 2952 - lr: 0.05000 - Train loss: 17.90508 - Test loss: 54.03916\n",
      "Epoch 2953 - lr: 0.05000 - Train loss: 17.90507 - Test loss: 54.03916\n",
      "Epoch 2954 - lr: 0.05000 - Train loss: 17.90506 - Test loss: 54.03916\n",
      "Epoch 2955 - lr: 0.05000 - Train loss: 17.90505 - Test loss: 54.03916\n",
      "Epoch 2956 - lr: 0.05000 - Train loss: 17.90505 - Test loss: 54.03916\n",
      "Epoch 2957 - lr: 0.05000 - Train loss: 17.90504 - Test loss: 54.03916\n",
      "Epoch 2958 - lr: 0.05000 - Train loss: 17.90503 - Test loss: 54.03917\n",
      "Epoch 2959 - lr: 0.05000 - Train loss: 17.90502 - Test loss: 54.03918\n",
      "Epoch 2960 - lr: 0.05000 - Train loss: 17.90501 - Test loss: 54.03919\n",
      "Epoch 2961 - lr: 0.05000 - Train loss: 17.90500 - Test loss: 54.03921\n",
      "Epoch 2962 - lr: 0.05000 - Train loss: 17.90499 - Test loss: 54.03923\n",
      "Epoch 2963 - lr: 0.05000 - Train loss: 17.90497 - Test loss: 54.03926\n",
      "Epoch 2964 - lr: 0.05000 - Train loss: 17.90496 - Test loss: 54.03929\n",
      "Epoch 2965 - lr: 0.05000 - Train loss: 17.90495 - Test loss: 54.03932\n",
      "Epoch 2966 - lr: 0.05000 - Train loss: 17.90494 - Test loss: 54.03936\n",
      "Epoch 2967 - lr: 0.05000 - Train loss: 17.90492 - Test loss: 54.03941\n",
      "Epoch 2968 - lr: 0.05000 - Train loss: 17.90491 - Test loss: 54.03946\n",
      "Epoch 2969 - lr: 0.05000 - Train loss: 17.90489 - Test loss: 54.03952\n",
      "Epoch 2970 - lr: 0.05000 - Train loss: 17.90487 - Test loss: 54.03958\n",
      "Epoch 2971 - lr: 0.05000 - Train loss: 17.90486 - Test loss: 54.03966\n",
      "Epoch 2972 - lr: 0.05000 - Train loss: 17.90484 - Test loss: 54.03974\n",
      "Epoch 2973 - lr: 0.05000 - Train loss: 17.90482 - Test loss: 54.03983\n",
      "Epoch 2974 - lr: 0.05000 - Train loss: 17.90479 - Test loss: 54.03994\n",
      "Epoch 2975 - lr: 0.05000 - Train loss: 17.90477 - Test loss: 54.04005\n",
      "Epoch 2976 - lr: 0.05000 - Train loss: 17.90474 - Test loss: 54.04018\n",
      "Epoch 2977 - lr: 0.05000 - Train loss: 17.90472 - Test loss: 54.04033\n",
      "Epoch 2978 - lr: 0.05000 - Train loss: 17.90469 - Test loss: 54.04049\n",
      "Epoch 2979 - lr: 0.05000 - Train loss: 17.90465 - Test loss: 54.04067\n",
      "Epoch 2980 - lr: 0.05000 - Train loss: 17.90462 - Test loss: 54.04087\n",
      "Epoch 2981 - lr: 0.05000 - Train loss: 17.90458 - Test loss: 54.04110\n",
      "Epoch 2982 - lr: 0.05000 - Train loss: 17.90453 - Test loss: 54.04135\n",
      "Epoch 2983 - lr: 0.05000 - Train loss: 17.90448 - Test loss: 54.04164\n",
      "Epoch 2984 - lr: 0.05000 - Train loss: 17.90443 - Test loss: 54.04197\n",
      "Epoch 2985 - lr: 0.05000 - Train loss: 17.90437 - Test loss: 54.04234\n",
      "Epoch 2986 - lr: 0.05000 - Train loss: 17.90430 - Test loss: 54.04276\n",
      "Epoch 2987 - lr: 0.05000 - Train loss: 17.90422 - Test loss: 54.04325\n",
      "Epoch 2988 - lr: 0.05000 - Train loss: 17.90414 - Test loss: 54.04381\n",
      "Epoch 2989 - lr: 0.05000 - Train loss: 17.90403 - Test loss: 54.04447\n",
      "Epoch 2990 - lr: 0.05000 - Train loss: 17.90391 - Test loss: 54.04524\n",
      "Epoch 2991 - lr: 0.05000 - Train loss: 17.90377 - Test loss: 54.04615\n",
      "Epoch 2992 - lr: 0.05000 - Train loss: 17.90360 - Test loss: 54.04725\n",
      "Epoch 2993 - lr: 0.05000 - Train loss: 17.90339 - Test loss: 54.04860\n",
      "Epoch 2994 - lr: 0.05000 - Train loss: 17.90312 - Test loss: 54.05028\n",
      "Epoch 2995 - lr: 0.05000 - Train loss: 17.90278 - Test loss: 54.05242\n",
      "Epoch 2996 - lr: 0.05000 - Train loss: 17.90231 - Test loss: 54.05525\n",
      "Epoch 2997 - lr: 0.05000 - Train loss: 17.90164 - Test loss: 54.05917\n",
      "Epoch 2998 - lr: 0.05000 - Train loss: 17.90058 - Test loss: 54.06505\n",
      "Epoch 2999 - lr: 0.05000 - Train loss: 17.89848 - Test loss: 54.07555\n",
      "Epoch 3000 - lr: 0.05000 - Train loss: 17.87988 - Test loss: 54.15380\n",
      "Epoch 3001 - lr: 0.05000 - Train loss: 12.58794 - Test loss: 90.51397\n",
      "Epoch 3002 - lr: 0.05000 - Train loss: 18.64631 - Test loss: 54.04495\n",
      "Epoch 3003 - lr: 0.05000 - Train loss: 17.90860 - Test loss: 54.03482\n",
      "Epoch 3004 - lr: 0.05000 - Train loss: 17.90552 - Test loss: 54.03410\n",
      "Epoch 3005 - lr: 0.05000 - Train loss: 17.90534 - Test loss: 54.03401\n",
      "Epoch 3006 - lr: 0.05000 - Train loss: 17.90532 - Test loss: 54.03396\n",
      "Epoch 3007 - lr: 0.05000 - Train loss: 17.90532 - Test loss: 54.03390\n",
      "Epoch 3008 - lr: 0.05000 - Train loss: 17.90532 - Test loss: 54.03385\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3009 - lr: 0.05000 - Train loss: 17.90532 - Test loss: 54.03380\n",
      "Epoch 3010 - lr: 0.05000 - Train loss: 17.90532 - Test loss: 54.03375\n",
      "Epoch 3011 - lr: 0.05000 - Train loss: 17.90532 - Test loss: 54.03370\n",
      "Epoch 3012 - lr: 0.05000 - Train loss: 17.90532 - Test loss: 54.03364\n",
      "Epoch 3013 - lr: 0.05000 - Train loss: 17.90532 - Test loss: 54.03359\n",
      "Epoch 3014 - lr: 0.05000 - Train loss: 17.90532 - Test loss: 54.03354\n",
      "Epoch 3015 - lr: 0.05000 - Train loss: 17.90531 - Test loss: 54.03349\n",
      "Epoch 3016 - lr: 0.05000 - Train loss: 17.90531 - Test loss: 54.03344\n",
      "Epoch 3017 - lr: 0.05000 - Train loss: 17.90531 - Test loss: 54.03338\n",
      "Epoch 3018 - lr: 0.05000 - Train loss: 17.90531 - Test loss: 54.03333\n",
      "Epoch 3019 - lr: 0.05000 - Train loss: 17.90531 - Test loss: 54.03328\n",
      "Epoch 3020 - lr: 0.05000 - Train loss: 17.90531 - Test loss: 54.03323\n",
      "Epoch 3021 - lr: 0.05000 - Train loss: 17.90531 - Test loss: 54.03317\n",
      "Epoch 3022 - lr: 0.05000 - Train loss: 17.90531 - Test loss: 54.03312\n",
      "Epoch 3023 - lr: 0.05000 - Train loss: 17.90531 - Test loss: 54.03307\n",
      "Epoch 3024 - lr: 0.05000 - Train loss: 17.90530 - Test loss: 54.03302\n",
      "Epoch 3025 - lr: 0.05000 - Train loss: 17.90530 - Test loss: 54.03297\n",
      "Epoch 3026 - lr: 0.05000 - Train loss: 17.90530 - Test loss: 54.03291\n",
      "Epoch 3027 - lr: 0.05000 - Train loss: 17.90530 - Test loss: 54.03286\n",
      "Epoch 3028 - lr: 0.05000 - Train loss: 17.90530 - Test loss: 54.03281\n",
      "Epoch 3029 - lr: 0.05000 - Train loss: 17.90530 - Test loss: 54.03276\n",
      "Epoch 3030 - lr: 0.05000 - Train loss: 17.90530 - Test loss: 54.03271\n",
      "Epoch 3031 - lr: 0.05000 - Train loss: 17.90530 - Test loss: 54.03265\n",
      "Epoch 3032 - lr: 0.05000 - Train loss: 17.90530 - Test loss: 54.03260\n",
      "Epoch 3033 - lr: 0.05000 - Train loss: 17.90530 - Test loss: 54.03255\n",
      "Epoch 3034 - lr: 0.05000 - Train loss: 17.90529 - Test loss: 54.03250\n",
      "Epoch 3035 - lr: 0.05000 - Train loss: 17.90529 - Test loss: 54.03245\n",
      "Epoch 3036 - lr: 0.05000 - Train loss: 17.90529 - Test loss: 54.03239\n",
      "Epoch 3037 - lr: 0.05000 - Train loss: 17.90529 - Test loss: 54.03234\n",
      "Epoch 3038 - lr: 0.05000 - Train loss: 17.90529 - Test loss: 54.03229\n",
      "Epoch 3039 - lr: 0.05000 - Train loss: 17.90529 - Test loss: 54.03224\n",
      "Epoch 3040 - lr: 0.05000 - Train loss: 17.90529 - Test loss: 54.03219\n",
      "Epoch 3041 - lr: 0.05000 - Train loss: 17.90529 - Test loss: 54.03213\n",
      "Epoch 3042 - lr: 0.05000 - Train loss: 17.90529 - Test loss: 54.03208\n",
      "Epoch 3043 - lr: 0.05000 - Train loss: 17.90529 - Test loss: 54.03203\n",
      "Epoch 3044 - lr: 0.05000 - Train loss: 17.90528 - Test loss: 54.03198\n",
      "Epoch 3045 - lr: 0.05000 - Train loss: 17.90528 - Test loss: 54.03192\n",
      "Epoch 3046 - lr: 0.05000 - Train loss: 17.90528 - Test loss: 54.03187\n",
      "Epoch 3047 - lr: 0.05000 - Train loss: 17.90528 - Test loss: 54.03182\n",
      "Epoch 3048 - lr: 0.05000 - Train loss: 17.90528 - Test loss: 54.03177\n",
      "Epoch 3049 - lr: 0.05000 - Train loss: 17.90528 - Test loss: 54.03172\n",
      "Epoch 3050 - lr: 0.05000 - Train loss: 17.90528 - Test loss: 54.03166\n",
      "Epoch 3051 - lr: 0.05000 - Train loss: 17.90528 - Test loss: 54.03161\n",
      "Epoch 3052 - lr: 0.05000 - Train loss: 17.90528 - Test loss: 54.03156\n",
      "Epoch 3053 - lr: 0.05000 - Train loss: 17.90528 - Test loss: 54.03151\n",
      "Epoch 3054 - lr: 0.05000 - Train loss: 17.90527 - Test loss: 54.03146\n",
      "Epoch 3055 - lr: 0.05000 - Train loss: 17.90527 - Test loss: 54.03140\n",
      "Epoch 3056 - lr: 0.05000 - Train loss: 17.90527 - Test loss: 54.03135\n",
      "Epoch 3057 - lr: 0.05000 - Train loss: 17.90527 - Test loss: 54.03130\n",
      "Epoch 3058 - lr: 0.05000 - Train loss: 17.90527 - Test loss: 54.03125\n",
      "Epoch 3059 - lr: 0.05000 - Train loss: 17.90527 - Test loss: 54.03120\n",
      "Epoch 3060 - lr: 0.05000 - Train loss: 17.90527 - Test loss: 54.03114\n",
      "Epoch 3061 - lr: 0.05000 - Train loss: 17.90527 - Test loss: 54.03109\n",
      "Epoch 3062 - lr: 0.05000 - Train loss: 17.90527 - Test loss: 54.03104\n",
      "Epoch 3063 - lr: 0.05000 - Train loss: 17.90527 - Test loss: 54.03099\n",
      "Epoch 3064 - lr: 0.05000 - Train loss: 17.90526 - Test loss: 54.03093\n",
      "Epoch 3065 - lr: 0.05000 - Train loss: 17.90526 - Test loss: 54.03088\n",
      "Epoch 3066 - lr: 0.05000 - Train loss: 17.90526 - Test loss: 54.03083\n",
      "Epoch 3067 - lr: 0.05000 - Train loss: 17.90526 - Test loss: 54.03078\n",
      "Epoch 3068 - lr: 0.05000 - Train loss: 17.90526 - Test loss: 54.03073\n",
      "Epoch 3069 - lr: 0.05000 - Train loss: 17.90526 - Test loss: 54.03067\n",
      "Epoch 3070 - lr: 0.05000 - Train loss: 17.90526 - Test loss: 54.03062\n",
      "Epoch 3071 - lr: 0.05000 - Train loss: 17.90526 - Test loss: 54.03057\n",
      "Epoch 3072 - lr: 0.05000 - Train loss: 17.90526 - Test loss: 54.03052\n",
      "Epoch 3073 - lr: 0.05000 - Train loss: 17.90526 - Test loss: 54.03047\n",
      "Epoch 3074 - lr: 0.05000 - Train loss: 17.90525 - Test loss: 54.03041\n",
      "Epoch 3075 - lr: 0.05000 - Train loss: 17.90525 - Test loss: 54.03036\n",
      "Epoch 3076 - lr: 0.05000 - Train loss: 17.90525 - Test loss: 54.03031\n",
      "Epoch 3077 - lr: 0.05000 - Train loss: 17.90525 - Test loss: 54.03026\n",
      "Epoch 3078 - lr: 0.05000 - Train loss: 17.90525 - Test loss: 54.03021\n",
      "Epoch 3079 - lr: 0.05000 - Train loss: 17.90525 - Test loss: 54.03016\n",
      "Epoch 3080 - lr: 0.05000 - Train loss: 17.90525 - Test loss: 54.03010\n",
      "Epoch 3081 - lr: 0.05000 - Train loss: 17.90525 - Test loss: 54.03005\n",
      "Epoch 3082 - lr: 0.05000 - Train loss: 17.90525 - Test loss: 54.03000\n",
      "Epoch 3083 - lr: 0.05000 - Train loss: 17.90525 - Test loss: 54.02995\n",
      "Epoch 3084 - lr: 0.05000 - Train loss: 17.90524 - Test loss: 54.02990\n",
      "Epoch 3085 - lr: 0.05000 - Train loss: 17.90524 - Test loss: 54.02984\n",
      "Epoch 3086 - lr: 0.05000 - Train loss: 17.90524 - Test loss: 54.02979\n",
      "Epoch 3087 - lr: 0.05000 - Train loss: 17.90524 - Test loss: 54.02974\n",
      "Epoch 3088 - lr: 0.05000 - Train loss: 17.90524 - Test loss: 54.02969\n",
      "Epoch 3089 - lr: 0.05000 - Train loss: 17.90524 - Test loss: 54.02964\n",
      "Epoch 3090 - lr: 0.05000 - Train loss: 17.90524 - Test loss: 54.02958\n",
      "Epoch 3091 - lr: 0.05000 - Train loss: 17.90524 - Test loss: 54.02953\n",
      "Epoch 3092 - lr: 0.05000 - Train loss: 17.90524 - Test loss: 54.02948\n",
      "Epoch 3093 - lr: 0.05000 - Train loss: 17.90524 - Test loss: 54.02943\n",
      "Epoch 3094 - lr: 0.05000 - Train loss: 17.90523 - Test loss: 54.02938\n",
      "Epoch 3095 - lr: 0.05000 - Train loss: 17.90523 - Test loss: 54.02933\n",
      "Epoch 3096 - lr: 0.05000 - Train loss: 17.90523 - Test loss: 54.02927\n",
      "Epoch 3097 - lr: 0.05000 - Train loss: 17.90523 - Test loss: 54.02922\n",
      "Epoch 3098 - lr: 0.05000 - Train loss: 17.90523 - Test loss: 54.02917\n",
      "Epoch 3099 - lr: 0.05000 - Train loss: 17.90523 - Test loss: 54.02912\n",
      "Epoch 3100 - lr: 0.05000 - Train loss: 17.90523 - Test loss: 54.02907\n",
      "Epoch 3101 - lr: 0.05000 - Train loss: 17.90523 - Test loss: 54.02902\n",
      "Epoch 3102 - lr: 0.05000 - Train loss: 17.90523 - Test loss: 54.02896\n",
      "Epoch 3103 - lr: 0.05000 - Train loss: 17.90523 - Test loss: 54.02891\n",
      "Epoch 3104 - lr: 0.05000 - Train loss: 17.90522 - Test loss: 54.02886\n",
      "Epoch 3105 - lr: 0.05000 - Train loss: 17.90522 - Test loss: 54.02881\n",
      "Epoch 3106 - lr: 0.05000 - Train loss: 17.90522 - Test loss: 54.02876\n",
      "Epoch 3107 - lr: 0.05000 - Train loss: 17.90522 - Test loss: 54.02871\n",
      "Epoch 3108 - lr: 0.05000 - Train loss: 17.90522 - Test loss: 54.02865\n",
      "Epoch 3109 - lr: 0.05000 - Train loss: 17.90522 - Test loss: 54.02860\n",
      "Epoch 3110 - lr: 0.05000 - Train loss: 17.90522 - Test loss: 54.02855\n",
      "Epoch 3111 - lr: 0.05000 - Train loss: 17.90522 - Test loss: 54.02850\n",
      "Epoch 3112 - lr: 0.05000 - Train loss: 17.90522 - Test loss: 54.02845\n",
      "Epoch 3113 - lr: 0.05000 - Train loss: 17.90522 - Test loss: 54.02840\n",
      "Epoch 3114 - lr: 0.05000 - Train loss: 17.90521 - Test loss: 54.02835\n",
      "Epoch 3115 - lr: 0.05000 - Train loss: 17.90521 - Test loss: 54.02829\n",
      "Epoch 3116 - lr: 0.05000 - Train loss: 17.90521 - Test loss: 54.02824\n",
      "Epoch 3117 - lr: 0.05000 - Train loss: 17.90521 - Test loss: 54.02819\n",
      "Epoch 3118 - lr: 0.05000 - Train loss: 17.90521 - Test loss: 54.02814\n",
      "Epoch 3119 - lr: 0.05000 - Train loss: 17.90521 - Test loss: 54.02809\n",
      "Epoch 3120 - lr: 0.05000 - Train loss: 17.90521 - Test loss: 54.02804\n",
      "Epoch 3121 - lr: 0.05000 - Train loss: 17.90521 - Test loss: 54.02799\n",
      "Epoch 3122 - lr: 0.05000 - Train loss: 17.90521 - Test loss: 54.02794\n",
      "Epoch 3123 - lr: 0.05000 - Train loss: 17.90521 - Test loss: 54.02788\n",
      "Epoch 3124 - lr: 0.05000 - Train loss: 17.90520 - Test loss: 54.02783\n",
      "Epoch 3125 - lr: 0.05000 - Train loss: 17.90520 - Test loss: 54.02778\n",
      "Epoch 3126 - lr: 0.05000 - Train loss: 17.90520 - Test loss: 54.02773\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3127 - lr: 0.05000 - Train loss: 17.90520 - Test loss: 54.02768\n",
      "Epoch 3128 - lr: 0.05000 - Train loss: 17.90520 - Test loss: 54.02763\n",
      "Epoch 3129 - lr: 0.05000 - Train loss: 17.90520 - Test loss: 54.02758\n",
      "Epoch 3130 - lr: 0.05000 - Train loss: 17.90520 - Test loss: 54.02753\n",
      "Epoch 3131 - lr: 0.05000 - Train loss: 17.90520 - Test loss: 54.02748\n",
      "Epoch 3132 - lr: 0.05000 - Train loss: 17.90520 - Test loss: 54.02742\n",
      "Epoch 3133 - lr: 0.05000 - Train loss: 17.90519 - Test loss: 54.02737\n",
      "Epoch 3134 - lr: 0.05000 - Train loss: 17.90519 - Test loss: 54.02732\n",
      "Epoch 3135 - lr: 0.05000 - Train loss: 17.90519 - Test loss: 54.02727\n",
      "Epoch 3136 - lr: 0.05000 - Train loss: 17.90519 - Test loss: 54.02722\n",
      "Epoch 3137 - lr: 0.05000 - Train loss: 17.90519 - Test loss: 54.02717\n",
      "Epoch 3138 - lr: 0.05000 - Train loss: 17.90519 - Test loss: 54.02712\n",
      "Epoch 3139 - lr: 0.05000 - Train loss: 17.90519 - Test loss: 54.02707\n",
      "Epoch 3140 - lr: 0.05000 - Train loss: 17.90519 - Test loss: 54.02702\n",
      "Epoch 3141 - lr: 0.05000 - Train loss: 17.90519 - Test loss: 54.02697\n",
      "Epoch 3142 - lr: 0.05000 - Train loss: 17.90518 - Test loss: 54.02692\n",
      "Epoch 3143 - lr: 0.05000 - Train loss: 17.90518 - Test loss: 54.02687\n",
      "Epoch 3144 - lr: 0.05000 - Train loss: 17.90518 - Test loss: 54.02682\n",
      "Epoch 3145 - lr: 0.05000 - Train loss: 17.90518 - Test loss: 54.02677\n",
      "Epoch 3146 - lr: 0.05000 - Train loss: 17.90518 - Test loss: 54.02672\n",
      "Epoch 3147 - lr: 0.05000 - Train loss: 17.90518 - Test loss: 54.02667\n",
      "Epoch 3148 - lr: 0.05000 - Train loss: 17.90518 - Test loss: 54.02662\n",
      "Epoch 3149 - lr: 0.05000 - Train loss: 17.90518 - Test loss: 54.02657\n",
      "Epoch 3150 - lr: 0.05000 - Train loss: 17.90518 - Test loss: 54.02652\n",
      "Epoch 3151 - lr: 0.05000 - Train loss: 17.90517 - Test loss: 54.02646\n",
      "Epoch 3152 - lr: 0.05000 - Train loss: 17.90517 - Test loss: 54.02641\n",
      "Epoch 3153 - lr: 0.05000 - Train loss: 17.90517 - Test loss: 54.02636\n",
      "Epoch 3154 - lr: 0.05000 - Train loss: 17.90517 - Test loss: 54.02631\n",
      "Epoch 3155 - lr: 0.05000 - Train loss: 17.90517 - Test loss: 54.02626\n",
      "Epoch 3156 - lr: 0.05000 - Train loss: 17.90517 - Test loss: 54.02622\n",
      "Epoch 3157 - lr: 0.05000 - Train loss: 17.90517 - Test loss: 54.02617\n",
      "Epoch 3158 - lr: 0.05000 - Train loss: 17.90517 - Test loss: 54.02612\n",
      "Epoch 3159 - lr: 0.05000 - Train loss: 17.90517 - Test loss: 54.02607\n",
      "Epoch 3160 - lr: 0.05000 - Train loss: 17.90516 - Test loss: 54.02602\n",
      "Epoch 3161 - lr: 0.05000 - Train loss: 17.90516 - Test loss: 54.02597\n",
      "Epoch 3162 - lr: 0.05000 - Train loss: 17.90516 - Test loss: 54.02592\n",
      "Epoch 3163 - lr: 0.05000 - Train loss: 17.90516 - Test loss: 54.02587\n",
      "Epoch 3164 - lr: 0.05000 - Train loss: 17.90516 - Test loss: 54.02582\n",
      "Epoch 3165 - lr: 0.05000 - Train loss: 17.90516 - Test loss: 54.02577\n",
      "Epoch 3166 - lr: 0.05000 - Train loss: 17.90516 - Test loss: 54.02572\n",
      "Epoch 3167 - lr: 0.05000 - Train loss: 17.90516 - Test loss: 54.02567\n",
      "Epoch 3168 - lr: 0.05000 - Train loss: 17.90515 - Test loss: 54.02562\n",
      "Epoch 3169 - lr: 0.05000 - Train loss: 17.90515 - Test loss: 54.02557\n",
      "Epoch 3170 - lr: 0.05000 - Train loss: 17.90515 - Test loss: 54.02552\n",
      "Epoch 3171 - lr: 0.05000 - Train loss: 17.90515 - Test loss: 54.02547\n",
      "Epoch 3172 - lr: 0.05000 - Train loss: 17.90515 - Test loss: 54.02543\n",
      "Epoch 3173 - lr: 0.05000 - Train loss: 17.90515 - Test loss: 54.02538\n",
      "Epoch 3174 - lr: 0.05000 - Train loss: 17.90515 - Test loss: 54.02533\n",
      "Epoch 3175 - lr: 0.05000 - Train loss: 17.90515 - Test loss: 54.02528\n",
      "Epoch 3176 - lr: 0.05000 - Train loss: 17.90514 - Test loss: 54.02523\n",
      "Epoch 3177 - lr: 0.05000 - Train loss: 17.90514 - Test loss: 54.02518\n",
      "Epoch 3178 - lr: 0.05000 - Train loss: 17.90514 - Test loss: 54.02513\n",
      "Epoch 3179 - lr: 0.05000 - Train loss: 17.90514 - Test loss: 54.02509\n",
      "Epoch 3180 - lr: 0.05000 - Train loss: 17.90514 - Test loss: 54.02504\n",
      "Epoch 3181 - lr: 0.05000 - Train loss: 17.90514 - Test loss: 54.02499\n",
      "Epoch 3182 - lr: 0.05000 - Train loss: 17.90514 - Test loss: 54.02494\n",
      "Epoch 3183 - lr: 0.05000 - Train loss: 17.90513 - Test loss: 54.02489\n",
      "Epoch 3184 - lr: 0.05000 - Train loss: 17.90513 - Test loss: 54.02485\n",
      "Epoch 3185 - lr: 0.05000 - Train loss: 17.90513 - Test loss: 54.02480\n",
      "Epoch 3186 - lr: 0.05000 - Train loss: 17.90513 - Test loss: 54.02475\n",
      "Epoch 3187 - lr: 0.05000 - Train loss: 17.90513 - Test loss: 54.02470\n",
      "Epoch 3188 - lr: 0.05000 - Train loss: 17.90513 - Test loss: 54.02466\n",
      "Epoch 3189 - lr: 0.05000 - Train loss: 17.90513 - Test loss: 54.02461\n",
      "Epoch 3190 - lr: 0.05000 - Train loss: 17.90512 - Test loss: 54.02456\n",
      "Epoch 3191 - lr: 0.05000 - Train loss: 17.90512 - Test loss: 54.02452\n",
      "Epoch 3192 - lr: 0.05000 - Train loss: 17.90512 - Test loss: 54.02447\n",
      "Epoch 3193 - lr: 0.05000 - Train loss: 17.90512 - Test loss: 54.02442\n",
      "Epoch 3194 - lr: 0.05000 - Train loss: 17.90512 - Test loss: 54.02438\n",
      "Epoch 3195 - lr: 0.05000 - Train loss: 17.90512 - Test loss: 54.02433\n",
      "Epoch 3196 - lr: 0.05000 - Train loss: 17.90511 - Test loss: 54.02428\n",
      "Epoch 3197 - lr: 0.05000 - Train loss: 17.90511 - Test loss: 54.02424\n",
      "Epoch 3198 - lr: 0.05000 - Train loss: 17.90511 - Test loss: 54.02419\n",
      "Epoch 3199 - lr: 0.05000 - Train loss: 17.90511 - Test loss: 54.02415\n",
      "Epoch 3200 - lr: 0.05000 - Train loss: 17.90511 - Test loss: 54.02410\n",
      "Epoch 3201 - lr: 0.05000 - Train loss: 17.90511 - Test loss: 54.02406\n",
      "Epoch 3202 - lr: 0.05000 - Train loss: 17.90511 - Test loss: 54.02401\n",
      "Epoch 3203 - lr: 0.05000 - Train loss: 17.90510 - Test loss: 54.02397\n",
      "Epoch 3204 - lr: 0.05000 - Train loss: 17.90510 - Test loss: 54.02392\n",
      "Epoch 3205 - lr: 0.05000 - Train loss: 17.90510 - Test loss: 54.02388\n",
      "Epoch 3206 - lr: 0.05000 - Train loss: 17.90510 - Test loss: 54.02383\n",
      "Epoch 3207 - lr: 0.05000 - Train loss: 17.90510 - Test loss: 54.02379\n",
      "Epoch 3208 - lr: 0.05000 - Train loss: 17.90509 - Test loss: 54.02374\n",
      "Epoch 3209 - lr: 0.05000 - Train loss: 17.90509 - Test loss: 54.02370\n",
      "Epoch 3210 - lr: 0.05000 - Train loss: 17.90509 - Test loss: 54.02366\n",
      "Epoch 3211 - lr: 0.05000 - Train loss: 17.90509 - Test loss: 54.02361\n",
      "Epoch 3212 - lr: 0.05000 - Train loss: 17.90509 - Test loss: 54.02357\n",
      "Epoch 3213 - lr: 0.05000 - Train loss: 17.90508 - Test loss: 54.02353\n",
      "Epoch 3214 - lr: 0.05000 - Train loss: 17.90508 - Test loss: 54.02348\n",
      "Epoch 3215 - lr: 0.05000 - Train loss: 17.90508 - Test loss: 54.02344\n",
      "Epoch 3216 - lr: 0.05000 - Train loss: 17.90508 - Test loss: 54.02340\n",
      "Epoch 3217 - lr: 0.05000 - Train loss: 17.90508 - Test loss: 54.02336\n",
      "Epoch 3218 - lr: 0.05000 - Train loss: 17.90507 - Test loss: 54.02331\n",
      "Epoch 3219 - lr: 0.05000 - Train loss: 17.90507 - Test loss: 54.02327\n",
      "Epoch 3220 - lr: 0.05000 - Train loss: 17.90507 - Test loss: 54.02323\n",
      "Epoch 3221 - lr: 0.05000 - Train loss: 17.90507 - Test loss: 54.02319\n",
      "Epoch 3222 - lr: 0.05000 - Train loss: 17.90507 - Test loss: 54.02315\n",
      "Epoch 3223 - lr: 0.05000 - Train loss: 17.90506 - Test loss: 54.02311\n",
      "Epoch 3224 - lr: 0.05000 - Train loss: 17.90506 - Test loss: 54.02307\n",
      "Epoch 3225 - lr: 0.05000 - Train loss: 17.90506 - Test loss: 54.02303\n",
      "Epoch 3226 - lr: 0.05000 - Train loss: 17.90506 - Test loss: 54.02299\n",
      "Epoch 3227 - lr: 0.05000 - Train loss: 17.90505 - Test loss: 54.02295\n",
      "Epoch 3228 - lr: 0.05000 - Train loss: 17.90505 - Test loss: 54.02292\n",
      "Epoch 3229 - lr: 0.05000 - Train loss: 17.90505 - Test loss: 54.02288\n",
      "Epoch 3230 - lr: 0.05000 - Train loss: 17.90505 - Test loss: 54.02284\n",
      "Epoch 3231 - lr: 0.05000 - Train loss: 17.90504 - Test loss: 54.02280\n",
      "Epoch 3232 - lr: 0.05000 - Train loss: 17.90504 - Test loss: 54.02277\n",
      "Epoch 3233 - lr: 0.05000 - Train loss: 17.90504 - Test loss: 54.02273\n",
      "Epoch 3234 - lr: 0.05000 - Train loss: 17.90503 - Test loss: 54.02270\n",
      "Epoch 3235 - lr: 0.05000 - Train loss: 17.90503 - Test loss: 54.02266\n",
      "Epoch 3236 - lr: 0.05000 - Train loss: 17.90503 - Test loss: 54.02263\n",
      "Epoch 3237 - lr: 0.05000 - Train loss: 17.90503 - Test loss: 54.02259\n",
      "Epoch 3238 - lr: 0.05000 - Train loss: 17.90502 - Test loss: 54.02256\n",
      "Epoch 3239 - lr: 0.05000 - Train loss: 17.90502 - Test loss: 54.02253\n",
      "Epoch 3240 - lr: 0.05000 - Train loss: 17.90502 - Test loss: 54.02249\n",
      "Epoch 3241 - lr: 0.05000 - Train loss: 17.90501 - Test loss: 54.02246\n",
      "Epoch 3242 - lr: 0.05000 - Train loss: 17.90501 - Test loss: 54.02243\n",
      "Epoch 3243 - lr: 0.05000 - Train loss: 17.90501 - Test loss: 54.02240\n",
      "Epoch 3244 - lr: 0.05000 - Train loss: 17.90500 - Test loss: 54.02237\n",
      "Epoch 3245 - lr: 0.05000 - Train loss: 17.90500 - Test loss: 54.02234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3246 - lr: 0.05000 - Train loss: 17.90499 - Test loss: 54.02232\n",
      "Epoch 3247 - lr: 0.05000 - Train loss: 17.90499 - Test loss: 54.02229\n",
      "Epoch 3248 - lr: 0.05000 - Train loss: 17.90499 - Test loss: 54.02226\n",
      "Epoch 3249 - lr: 0.05000 - Train loss: 17.90498 - Test loss: 54.02224\n",
      "Epoch 3250 - lr: 0.05000 - Train loss: 17.90498 - Test loss: 54.02222\n",
      "Epoch 3251 - lr: 0.05000 - Train loss: 17.90497 - Test loss: 54.02219\n",
      "Epoch 3252 - lr: 0.05000 - Train loss: 17.90497 - Test loss: 54.02217\n",
      "Epoch 3253 - lr: 0.05000 - Train loss: 17.90496 - Test loss: 54.02215\n",
      "Epoch 3254 - lr: 0.05000 - Train loss: 17.90496 - Test loss: 54.02213\n",
      "Epoch 3255 - lr: 0.05000 - Train loss: 17.90495 - Test loss: 54.02211\n",
      "Epoch 3256 - lr: 0.05000 - Train loss: 17.90495 - Test loss: 54.02210\n",
      "Epoch 3257 - lr: 0.05000 - Train loss: 17.90494 - Test loss: 54.02208\n",
      "Epoch 3258 - lr: 0.05000 - Train loss: 17.90494 - Test loss: 54.02207\n",
      "Epoch 3259 - lr: 0.05000 - Train loss: 17.90493 - Test loss: 54.02206\n",
      "Epoch 3260 - lr: 0.05000 - Train loss: 17.90493 - Test loss: 54.02205\n",
      "Epoch 3261 - lr: 0.05000 - Train loss: 17.90492 - Test loss: 54.02204\n",
      "Epoch 3262 - lr: 0.05000 - Train loss: 17.90491 - Test loss: 54.02203\n",
      "Epoch 3263 - lr: 0.05000 - Train loss: 17.90491 - Test loss: 54.02203\n",
      "Epoch 3264 - lr: 0.05000 - Train loss: 17.90490 - Test loss: 54.02202\n",
      "Epoch 3265 - lr: 0.05000 - Train loss: 17.90489 - Test loss: 54.02202\n",
      "Epoch 3266 - lr: 0.05000 - Train loss: 17.90488 - Test loss: 54.02203\n",
      "Epoch 3267 - lr: 0.05000 - Train loss: 17.90488 - Test loss: 54.02203\n",
      "Epoch 3268 - lr: 0.05000 - Train loss: 17.90487 - Test loss: 54.02204\n",
      "Epoch 3269 - lr: 0.05000 - Train loss: 17.90486 - Test loss: 54.02205\n",
      "Epoch 3270 - lr: 0.05000 - Train loss: 17.90485 - Test loss: 54.02206\n",
      "Epoch 3271 - lr: 0.05000 - Train loss: 17.90484 - Test loss: 54.02208\n",
      "Epoch 3272 - lr: 0.05000 - Train loss: 17.90483 - Test loss: 54.02210\n",
      "Epoch 3273 - lr: 0.05000 - Train loss: 17.90482 - Test loss: 54.02213\n",
      "Epoch 3274 - lr: 0.05000 - Train loss: 17.90481 - Test loss: 54.02216\n",
      "Epoch 3275 - lr: 0.05000 - Train loss: 17.90480 - Test loss: 54.02219\n",
      "Epoch 3276 - lr: 0.05000 - Train loss: 17.90478 - Test loss: 54.02223\n",
      "Epoch 3277 - lr: 0.05000 - Train loss: 17.90477 - Test loss: 54.02228\n",
      "Epoch 3278 - lr: 0.05000 - Train loss: 17.90476 - Test loss: 54.02233\n",
      "Epoch 3279 - lr: 0.05000 - Train loss: 17.90474 - Test loss: 54.02239\n",
      "Epoch 3280 - lr: 0.05000 - Train loss: 17.90473 - Test loss: 54.02245\n",
      "Epoch 3281 - lr: 0.05000 - Train loss: 17.90471 - Test loss: 54.02252\n",
      "Epoch 3282 - lr: 0.05000 - Train loss: 17.90469 - Test loss: 54.02260\n",
      "Epoch 3283 - lr: 0.05000 - Train loss: 17.90467 - Test loss: 54.02270\n",
      "Epoch 3284 - lr: 0.05000 - Train loss: 17.90465 - Test loss: 54.02280\n",
      "Epoch 3285 - lr: 0.05000 - Train loss: 17.90463 - Test loss: 54.02291\n",
      "Epoch 3286 - lr: 0.05000 - Train loss: 17.90461 - Test loss: 54.02304\n",
      "Epoch 3287 - lr: 0.05000 - Train loss: 17.90458 - Test loss: 54.02318\n",
      "Epoch 3288 - lr: 0.05000 - Train loss: 17.90456 - Test loss: 54.02333\n",
      "Epoch 3289 - lr: 0.05000 - Train loss: 17.90453 - Test loss: 54.02351\n",
      "Epoch 3290 - lr: 0.05000 - Train loss: 17.90449 - Test loss: 54.02370\n",
      "Epoch 3291 - lr: 0.05000 - Train loss: 17.90446 - Test loss: 54.02392\n",
      "Epoch 3292 - lr: 0.05000 - Train loss: 17.90442 - Test loss: 54.02416\n",
      "Epoch 3293 - lr: 0.05000 - Train loss: 17.90437 - Test loss: 54.02444\n",
      "Epoch 3294 - lr: 0.05000 - Train loss: 17.90433 - Test loss: 54.02475\n",
      "Epoch 3295 - lr: 0.05000 - Train loss: 17.90427 - Test loss: 54.02510\n",
      "Epoch 3296 - lr: 0.05000 - Train loss: 17.90421 - Test loss: 54.02550\n",
      "Epoch 3297 - lr: 0.05000 - Train loss: 17.90414 - Test loss: 54.02596\n",
      "Epoch 3298 - lr: 0.05000 - Train loss: 17.90407 - Test loss: 54.02649\n",
      "Epoch 3299 - lr: 0.05000 - Train loss: 17.90398 - Test loss: 54.02710\n",
      "Epoch 3300 - lr: 0.05000 - Train loss: 17.90387 - Test loss: 54.02781\n",
      "Epoch 3301 - lr: 0.05000 - Train loss: 17.90375 - Test loss: 54.02865\n",
      "Epoch 3302 - lr: 0.05000 - Train loss: 17.90361 - Test loss: 54.02964\n",
      "Epoch 3303 - lr: 0.05000 - Train loss: 17.90343 - Test loss: 54.03084\n",
      "Epoch 3304 - lr: 0.05000 - Train loss: 17.90322 - Test loss: 54.03231\n",
      "Epoch 3305 - lr: 0.05000 - Train loss: 17.90294 - Test loss: 54.03414\n",
      "Epoch 3306 - lr: 0.05000 - Train loss: 17.90259 - Test loss: 54.03648\n",
      "Epoch 3307 - lr: 0.05000 - Train loss: 17.90212 - Test loss: 54.03957\n",
      "Epoch 3308 - lr: 0.05000 - Train loss: 17.90143 - Test loss: 54.04385\n",
      "Epoch 3309 - lr: 0.05000 - Train loss: 17.90036 - Test loss: 54.05021\n",
      "Epoch 3310 - lr: 0.05000 - Train loss: 17.89830 - Test loss: 54.06116\n",
      "Epoch 3311 - lr: 0.05000 - Train loss: 17.88815 - Test loss: 54.10487\n",
      "Epoch 3312 - lr: 0.05000 - Train loss: 12.62557 - Test loss: 91.04937\n",
      "Epoch 3313 - lr: 0.05000 - Train loss: 18.51773 - Test loss: 54.02084\n",
      "Epoch 3314 - lr: 0.05000 - Train loss: 17.90703 - Test loss: 54.01724\n",
      "Epoch 3315 - lr: 0.05000 - Train loss: 17.90532 - Test loss: 54.01682\n",
      "Epoch 3316 - lr: 0.05000 - Train loss: 17.90518 - Test loss: 54.01675\n",
      "Epoch 3317 - lr: 0.05000 - Train loss: 17.90517 - Test loss: 54.01670\n",
      "Epoch 3318 - lr: 0.05000 - Train loss: 17.90517 - Test loss: 54.01665\n",
      "Epoch 3319 - lr: 0.05000 - Train loss: 17.90517 - Test loss: 54.01660\n",
      "Epoch 3320 - lr: 0.05000 - Train loss: 17.90517 - Test loss: 54.01655\n",
      "Epoch 3321 - lr: 0.05000 - Train loss: 17.90517 - Test loss: 54.01650\n",
      "Epoch 3322 - lr: 0.05000 - Train loss: 17.90516 - Test loss: 54.01645\n",
      "Epoch 3323 - lr: 0.05000 - Train loss: 17.90516 - Test loss: 54.01640\n",
      "Epoch 3324 - lr: 0.05000 - Train loss: 17.90516 - Test loss: 54.01635\n",
      "Epoch 3325 - lr: 0.05000 - Train loss: 17.90516 - Test loss: 54.01630\n",
      "Epoch 3326 - lr: 0.05000 - Train loss: 17.90516 - Test loss: 54.01625\n",
      "Epoch 3327 - lr: 0.05000 - Train loss: 17.90516 - Test loss: 54.01620\n",
      "Epoch 3328 - lr: 0.05000 - Train loss: 17.90516 - Test loss: 54.01615\n",
      "Epoch 3329 - lr: 0.05000 - Train loss: 17.90516 - Test loss: 54.01611\n",
      "Epoch 3330 - lr: 0.05000 - Train loss: 17.90516 - Test loss: 54.01606\n",
      "Epoch 3331 - lr: 0.05000 - Train loss: 17.90516 - Test loss: 54.01601\n",
      "Epoch 3332 - lr: 0.05000 - Train loss: 17.90516 - Test loss: 54.01596\n",
      "Epoch 3333 - lr: 0.05000 - Train loss: 17.90515 - Test loss: 54.01591\n",
      "Epoch 3334 - lr: 0.05000 - Train loss: 17.90515 - Test loss: 54.01586\n",
      "Epoch 3335 - lr: 0.05000 - Train loss: 17.90515 - Test loss: 54.01581\n",
      "Epoch 3336 - lr: 0.05000 - Train loss: 17.90515 - Test loss: 54.01576\n",
      "Epoch 3337 - lr: 0.05000 - Train loss: 17.90515 - Test loss: 54.01571\n",
      "Epoch 3338 - lr: 0.05000 - Train loss: 17.90515 - Test loss: 54.01566\n",
      "Epoch 3339 - lr: 0.05000 - Train loss: 17.90515 - Test loss: 54.01561\n",
      "Epoch 3340 - lr: 0.05000 - Train loss: 17.90515 - Test loss: 54.01556\n",
      "Epoch 3341 - lr: 0.05000 - Train loss: 17.90515 - Test loss: 54.01552\n",
      "Epoch 3342 - lr: 0.05000 - Train loss: 17.90515 - Test loss: 54.01547\n",
      "Epoch 3343 - lr: 0.05000 - Train loss: 17.90515 - Test loss: 54.01542\n",
      "Epoch 3344 - lr: 0.05000 - Train loss: 17.90514 - Test loss: 54.01537\n",
      "Epoch 3345 - lr: 0.05000 - Train loss: 17.90514 - Test loss: 54.01532\n",
      "Epoch 3346 - lr: 0.05000 - Train loss: 17.90514 - Test loss: 54.01527\n",
      "Epoch 3347 - lr: 0.05000 - Train loss: 17.90514 - Test loss: 54.01522\n",
      "Epoch 3348 - lr: 0.05000 - Train loss: 17.90514 - Test loss: 54.01517\n",
      "Epoch 3349 - lr: 0.05000 - Train loss: 17.90514 - Test loss: 54.01512\n",
      "Epoch 3350 - lr: 0.05000 - Train loss: 17.90514 - Test loss: 54.01507\n",
      "Epoch 3351 - lr: 0.05000 - Train loss: 17.90514 - Test loss: 54.01502\n",
      "Epoch 3352 - lr: 0.05000 - Train loss: 17.90514 - Test loss: 54.01497\n",
      "Epoch 3353 - lr: 0.05000 - Train loss: 17.90514 - Test loss: 54.01492\n",
      "Epoch 3354 - lr: 0.05000 - Train loss: 17.90514 - Test loss: 54.01487\n",
      "Epoch 3355 - lr: 0.05000 - Train loss: 17.90514 - Test loss: 54.01483\n",
      "Epoch 3356 - lr: 0.05000 - Train loss: 17.90513 - Test loss: 54.01478\n",
      "Epoch 3357 - lr: 0.05000 - Train loss: 17.90513 - Test loss: 54.01473\n",
      "Epoch 3358 - lr: 0.05000 - Train loss: 17.90513 - Test loss: 54.01468\n",
      "Epoch 3359 - lr: 0.05000 - Train loss: 17.90513 - Test loss: 54.01463\n",
      "Epoch 3360 - lr: 0.05000 - Train loss: 17.90513 - Test loss: 54.01458\n",
      "Epoch 3361 - lr: 0.05000 - Train loss: 17.90513 - Test loss: 54.01453\n",
      "Epoch 3362 - lr: 0.05000 - Train loss: 17.90513 - Test loss: 54.01448\n",
      "Epoch 3363 - lr: 0.05000 - Train loss: 17.90513 - Test loss: 54.01443\n",
      "Epoch 3364 - lr: 0.05000 - Train loss: 17.90513 - Test loss: 54.01438\n",
      "Epoch 3365 - lr: 0.05000 - Train loss: 17.90513 - Test loss: 54.01433\n",
      "Epoch 3366 - lr: 0.05000 - Train loss: 17.90513 - Test loss: 54.01428\n",
      "Epoch 3367 - lr: 0.05000 - Train loss: 17.90512 - Test loss: 54.01423\n",
      "Epoch 3368 - lr: 0.05000 - Train loss: 17.90512 - Test loss: 54.01418\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3369 - lr: 0.05000 - Train loss: 17.90512 - Test loss: 54.01414\n",
      "Epoch 3370 - lr: 0.05000 - Train loss: 17.90512 - Test loss: 54.01409\n",
      "Epoch 3371 - lr: 0.05000 - Train loss: 17.90512 - Test loss: 54.01404\n",
      "Epoch 3372 - lr: 0.05000 - Train loss: 17.90512 - Test loss: 54.01399\n",
      "Epoch 3373 - lr: 0.05000 - Train loss: 17.90512 - Test loss: 54.01394\n",
      "Epoch 3374 - lr: 0.05000 - Train loss: 17.90512 - Test loss: 54.01389\n",
      "Epoch 3375 - lr: 0.05000 - Train loss: 17.90512 - Test loss: 54.01384\n",
      "Epoch 3376 - lr: 0.05000 - Train loss: 17.90512 - Test loss: 54.01379\n",
      "Epoch 3377 - lr: 0.05000 - Train loss: 17.90512 - Test loss: 54.01374\n",
      "Epoch 3378 - lr: 0.05000 - Train loss: 17.90512 - Test loss: 54.01369\n",
      "Epoch 3379 - lr: 0.05000 - Train loss: 17.90511 - Test loss: 54.01364\n",
      "Epoch 3380 - lr: 0.05000 - Train loss: 17.90511 - Test loss: 54.01359\n",
      "Epoch 3381 - lr: 0.05000 - Train loss: 17.90511 - Test loss: 54.01354\n",
      "Epoch 3382 - lr: 0.05000 - Train loss: 17.90511 - Test loss: 54.01349\n",
      "Epoch 3383 - lr: 0.05000 - Train loss: 17.90511 - Test loss: 54.01345\n",
      "Epoch 3384 - lr: 0.05000 - Train loss: 17.90511 - Test loss: 54.01340\n",
      "Epoch 3385 - lr: 0.05000 - Train loss: 17.90511 - Test loss: 54.01335\n",
      "Epoch 3386 - lr: 0.05000 - Train loss: 17.90511 - Test loss: 54.01330\n",
      "Epoch 3387 - lr: 0.05000 - Train loss: 17.90511 - Test loss: 54.01325\n",
      "Epoch 3388 - lr: 0.05000 - Train loss: 17.90511 - Test loss: 54.01320\n",
      "Epoch 3389 - lr: 0.05000 - Train loss: 17.90511 - Test loss: 54.01315\n",
      "Epoch 3390 - lr: 0.05000 - Train loss: 17.90510 - Test loss: 54.01310\n",
      "Epoch 3391 - lr: 0.05000 - Train loss: 17.90510 - Test loss: 54.01305\n",
      "Epoch 3392 - lr: 0.05000 - Train loss: 17.90510 - Test loss: 54.01300\n",
      "Epoch 3393 - lr: 0.05000 - Train loss: 17.90510 - Test loss: 54.01295\n",
      "Epoch 3394 - lr: 0.05000 - Train loss: 17.90510 - Test loss: 54.01290\n",
      "Epoch 3395 - lr: 0.05000 - Train loss: 17.90510 - Test loss: 54.01285\n",
      "Epoch 3396 - lr: 0.05000 - Train loss: 17.90510 - Test loss: 54.01281\n",
      "Epoch 3397 - lr: 0.05000 - Train loss: 17.90510 - Test loss: 54.01276\n",
      "Epoch 3398 - lr: 0.05000 - Train loss: 17.90510 - Test loss: 54.01271\n",
      "Epoch 3399 - lr: 0.05000 - Train loss: 17.90510 - Test loss: 54.01266\n",
      "Epoch 3400 - lr: 0.05000 - Train loss: 17.90510 - Test loss: 54.01261\n",
      "Epoch 3401 - lr: 0.05000 - Train loss: 17.90510 - Test loss: 54.01256\n",
      "Epoch 3402 - lr: 0.05000 - Train loss: 17.90509 - Test loss: 54.01251\n",
      "Epoch 3403 - lr: 0.05000 - Train loss: 17.90509 - Test loss: 54.01246\n",
      "Epoch 3404 - lr: 0.05000 - Train loss: 17.90509 - Test loss: 54.01241\n",
      "Epoch 3405 - lr: 0.05000 - Train loss: 17.90509 - Test loss: 54.01236\n",
      "Epoch 3406 - lr: 0.05000 - Train loss: 17.90509 - Test loss: 54.01231\n",
      "Epoch 3407 - lr: 0.05000 - Train loss: 17.90509 - Test loss: 54.01226\n",
      "Epoch 3408 - lr: 0.05000 - Train loss: 17.90509 - Test loss: 54.01221\n",
      "Epoch 3409 - lr: 0.05000 - Train loss: 17.90509 - Test loss: 54.01217\n",
      "Epoch 3410 - lr: 0.05000 - Train loss: 17.90509 - Test loss: 54.01212\n",
      "Epoch 3411 - lr: 0.05000 - Train loss: 17.90509 - Test loss: 54.01207\n",
      "Epoch 3412 - lr: 0.05000 - Train loss: 17.90509 - Test loss: 54.01202\n",
      "Epoch 3413 - lr: 0.05000 - Train loss: 17.90508 - Test loss: 54.01197\n",
      "Epoch 3414 - lr: 0.05000 - Train loss: 17.90508 - Test loss: 54.01192\n",
      "Epoch 3415 - lr: 0.05000 - Train loss: 17.90508 - Test loss: 54.01187\n",
      "Epoch 3416 - lr: 0.05000 - Train loss: 17.90508 - Test loss: 54.01182\n",
      "Epoch 3417 - lr: 0.05000 - Train loss: 17.90508 - Test loss: 54.01177\n",
      "Epoch 3418 - lr: 0.05000 - Train loss: 17.90508 - Test loss: 54.01172\n",
      "Epoch 3419 - lr: 0.05000 - Train loss: 17.90508 - Test loss: 54.01167\n",
      "Epoch 3420 - lr: 0.05000 - Train loss: 17.90508 - Test loss: 54.01162\n",
      "Epoch 3421 - lr: 0.05000 - Train loss: 17.90508 - Test loss: 54.01157\n",
      "Epoch 3422 - lr: 0.05000 - Train loss: 17.90508 - Test loss: 54.01153\n",
      "Epoch 3423 - lr: 0.05000 - Train loss: 17.90508 - Test loss: 54.01148\n",
      "Epoch 3424 - lr: 0.05000 - Train loss: 17.90508 - Test loss: 54.01143\n",
      "Epoch 3425 - lr: 0.05000 - Train loss: 17.90507 - Test loss: 54.01138\n",
      "Epoch 3426 - lr: 0.05000 - Train loss: 17.90507 - Test loss: 54.01133\n",
      "Epoch 3427 - lr: 0.05000 - Train loss: 17.90507 - Test loss: 54.01128\n",
      "Epoch 3428 - lr: 0.05000 - Train loss: 17.90507 - Test loss: 54.01123\n",
      "Epoch 3429 - lr: 0.05000 - Train loss: 17.90507 - Test loss: 54.01118\n",
      "Epoch 3430 - lr: 0.05000 - Train loss: 17.90507 - Test loss: 54.01113\n",
      "Epoch 3431 - lr: 0.05000 - Train loss: 17.90507 - Test loss: 54.01108\n",
      "Epoch 3432 - lr: 0.05000 - Train loss: 17.90507 - Test loss: 54.01103\n",
      "Epoch 3433 - lr: 0.05000 - Train loss: 17.90507 - Test loss: 54.01099\n",
      "Epoch 3434 - lr: 0.05000 - Train loss: 17.90507 - Test loss: 54.01094\n",
      "Epoch 3435 - lr: 0.05000 - Train loss: 17.90507 - Test loss: 54.01089\n",
      "Epoch 3436 - lr: 0.05000 - Train loss: 17.90507 - Test loss: 54.01084\n",
      "Epoch 3437 - lr: 0.05000 - Train loss: 17.90506 - Test loss: 54.01079\n",
      "Epoch 3438 - lr: 0.05000 - Train loss: 17.90506 - Test loss: 54.01074\n",
      "Epoch 3439 - lr: 0.05000 - Train loss: 17.90506 - Test loss: 54.01069\n",
      "Epoch 3440 - lr: 0.05000 - Train loss: 17.90506 - Test loss: 54.01064\n",
      "Epoch 3441 - lr: 0.05000 - Train loss: 17.90506 - Test loss: 54.01059\n",
      "Epoch 3442 - lr: 0.05000 - Train loss: 17.90506 - Test loss: 54.01054\n",
      "Epoch 3443 - lr: 0.05000 - Train loss: 17.90506 - Test loss: 54.01049\n",
      "Epoch 3444 - lr: 0.05000 - Train loss: 17.90506 - Test loss: 54.01045\n",
      "Epoch 3445 - lr: 0.05000 - Train loss: 17.90506 - Test loss: 54.01040\n",
      "Epoch 3446 - lr: 0.05000 - Train loss: 17.90506 - Test loss: 54.01035\n",
      "Epoch 3447 - lr: 0.05000 - Train loss: 17.90506 - Test loss: 54.01030\n",
      "Epoch 3448 - lr: 0.05000 - Train loss: 17.90505 - Test loss: 54.01025\n",
      "Epoch 3449 - lr: 0.05000 - Train loss: 17.90505 - Test loss: 54.01020\n",
      "Epoch 3450 - lr: 0.05000 - Train loss: 17.90505 - Test loss: 54.01015\n",
      "Epoch 3451 - lr: 0.05000 - Train loss: 17.90505 - Test loss: 54.01010\n",
      "Epoch 3452 - lr: 0.05000 - Train loss: 17.90505 - Test loss: 54.01005\n",
      "Epoch 3453 - lr: 0.05000 - Train loss: 17.90505 - Test loss: 54.01001\n",
      "Epoch 3454 - lr: 0.05000 - Train loss: 17.90505 - Test loss: 54.00996\n",
      "Epoch 3455 - lr: 0.05000 - Train loss: 17.90505 - Test loss: 54.00991\n",
      "Epoch 3456 - lr: 0.05000 - Train loss: 17.90505 - Test loss: 54.00986\n",
      "Epoch 3457 - lr: 0.05000 - Train loss: 17.90505 - Test loss: 54.00981\n",
      "Epoch 3458 - lr: 0.05000 - Train loss: 17.90505 - Test loss: 54.00976\n",
      "Epoch 3459 - lr: 0.05000 - Train loss: 17.90505 - Test loss: 54.00971\n",
      "Epoch 3460 - lr: 0.05000 - Train loss: 17.90504 - Test loss: 54.00966\n",
      "Epoch 3461 - lr: 0.05000 - Train loss: 17.90504 - Test loss: 54.00961\n",
      "Epoch 3462 - lr: 0.05000 - Train loss: 17.90504 - Test loss: 54.00957\n",
      "Epoch 3463 - lr: 0.05000 - Train loss: 17.90504 - Test loss: 54.00952\n",
      "Epoch 3464 - lr: 0.05000 - Train loss: 17.90504 - Test loss: 54.00947\n",
      "Epoch 3465 - lr: 0.05000 - Train loss: 17.90504 - Test loss: 54.00942\n",
      "Epoch 3466 - lr: 0.05000 - Train loss: 17.90504 - Test loss: 54.00937\n",
      "Epoch 3467 - lr: 0.05000 - Train loss: 17.90504 - Test loss: 54.00932\n",
      "Epoch 3468 - lr: 0.05000 - Train loss: 17.90504 - Test loss: 54.00927\n",
      "Epoch 3469 - lr: 0.05000 - Train loss: 17.90504 - Test loss: 54.00922\n",
      "Epoch 3470 - lr: 0.05000 - Train loss: 17.90504 - Test loss: 54.00917\n",
      "Epoch 3471 - lr: 0.05000 - Train loss: 17.90503 - Test loss: 54.00913\n",
      "Epoch 3472 - lr: 0.05000 - Train loss: 17.90503 - Test loss: 54.00908\n",
      "Epoch 3473 - lr: 0.05000 - Train loss: 17.90503 - Test loss: 54.00903\n",
      "Epoch 3474 - lr: 0.05000 - Train loss: 17.90503 - Test loss: 54.00898\n",
      "Epoch 3475 - lr: 0.05000 - Train loss: 17.90503 - Test loss: 54.00893\n",
      "Epoch 3476 - lr: 0.05000 - Train loss: 17.90503 - Test loss: 54.00888\n",
      "Epoch 3477 - lr: 0.05000 - Train loss: 17.90503 - Test loss: 54.00883\n",
      "Epoch 3478 - lr: 0.05000 - Train loss: 17.90503 - Test loss: 54.00878\n",
      "Epoch 3479 - lr: 0.05000 - Train loss: 17.90503 - Test loss: 54.00874\n",
      "Epoch 3480 - lr: 0.05000 - Train loss: 17.90503 - Test loss: 54.00869\n",
      "Epoch 3481 - lr: 0.05000 - Train loss: 17.90503 - Test loss: 54.00864\n",
      "Epoch 3482 - lr: 0.05000 - Train loss: 17.90503 - Test loss: 54.00859\n",
      "Epoch 3483 - lr: 0.05000 - Train loss: 17.90502 - Test loss: 54.00854\n",
      "Epoch 3484 - lr: 0.05000 - Train loss: 17.90502 - Test loss: 54.00849\n",
      "Epoch 3485 - lr: 0.05000 - Train loss: 17.90502 - Test loss: 54.00844\n",
      "Epoch 3486 - lr: 0.05000 - Train loss: 17.90502 - Test loss: 54.00840\n",
      "Epoch 3487 - lr: 0.05000 - Train loss: 17.90502 - Test loss: 54.00835\n",
      "Epoch 3488 - lr: 0.05000 - Train loss: 17.90502 - Test loss: 54.00830\n",
      "Epoch 3489 - lr: 0.05000 - Train loss: 17.90502 - Test loss: 54.00825\n",
      "Epoch 3490 - lr: 0.05000 - Train loss: 17.90502 - Test loss: 54.00820\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3491 - lr: 0.05000 - Train loss: 17.90502 - Test loss: 54.00815\n",
      "Epoch 3492 - lr: 0.05000 - Train loss: 17.90502 - Test loss: 54.00810\n",
      "Epoch 3493 - lr: 0.05000 - Train loss: 17.90502 - Test loss: 54.00806\n",
      "Epoch 3494 - lr: 0.05000 - Train loss: 17.90501 - Test loss: 54.00801\n",
      "Epoch 3495 - lr: 0.05000 - Train loss: 17.90501 - Test loss: 54.00796\n",
      "Epoch 3496 - lr: 0.05000 - Train loss: 17.90501 - Test loss: 54.00791\n",
      "Epoch 3497 - lr: 0.05000 - Train loss: 17.90501 - Test loss: 54.00786\n",
      "Epoch 3498 - lr: 0.05000 - Train loss: 17.90501 - Test loss: 54.00781\n",
      "Epoch 3499 - lr: 0.05000 - Train loss: 17.90501 - Test loss: 54.00776\n",
      "Epoch 3500 - lr: 0.05000 - Train loss: 17.90501 - Test loss: 54.00772\n",
      "Epoch 3501 - lr: 0.05000 - Train loss: 17.90501 - Test loss: 54.00767\n",
      "Epoch 3502 - lr: 0.05000 - Train loss: 17.90501 - Test loss: 54.00762\n",
      "Epoch 3503 - lr: 0.05000 - Train loss: 17.90501 - Test loss: 54.00757\n",
      "Epoch 3504 - lr: 0.05000 - Train loss: 17.90501 - Test loss: 54.00752\n",
      "Epoch 3505 - lr: 0.05000 - Train loss: 17.90500 - Test loss: 54.00747\n",
      "Epoch 3506 - lr: 0.05000 - Train loss: 17.90500 - Test loss: 54.00743\n",
      "Epoch 3507 - lr: 0.05000 - Train loss: 17.90500 - Test loss: 54.00738\n",
      "Epoch 3508 - lr: 0.05000 - Train loss: 17.90500 - Test loss: 54.00733\n",
      "Epoch 3509 - lr: 0.05000 - Train loss: 17.90500 - Test loss: 54.00728\n",
      "Epoch 3510 - lr: 0.05000 - Train loss: 17.90500 - Test loss: 54.00723\n",
      "Epoch 3511 - lr: 0.05000 - Train loss: 17.90500 - Test loss: 54.00718\n",
      "Epoch 3512 - lr: 0.05000 - Train loss: 17.90500 - Test loss: 54.00714\n",
      "Epoch 3513 - lr: 0.05000 - Train loss: 17.90500 - Test loss: 54.00709\n",
      "Epoch 3514 - lr: 0.05000 - Train loss: 17.90500 - Test loss: 54.00704\n",
      "Epoch 3515 - lr: 0.05000 - Train loss: 17.90500 - Test loss: 54.00699\n",
      "Epoch 3516 - lr: 0.05000 - Train loss: 17.90499 - Test loss: 54.00694\n",
      "Epoch 3517 - lr: 0.05000 - Train loss: 17.90499 - Test loss: 54.00690\n",
      "Epoch 3518 - lr: 0.05000 - Train loss: 17.90499 - Test loss: 54.00685\n",
      "Epoch 3519 - lr: 0.05000 - Train loss: 17.90499 - Test loss: 54.00680\n",
      "Epoch 3520 - lr: 0.05000 - Train loss: 17.90499 - Test loss: 54.00675\n",
      "Epoch 3521 - lr: 0.05000 - Train loss: 17.90499 - Test loss: 54.00670\n",
      "Epoch 3522 - lr: 0.05000 - Train loss: 17.90499 - Test loss: 54.00665\n",
      "Epoch 3523 - lr: 0.05000 - Train loss: 17.90499 - Test loss: 54.00661\n",
      "Epoch 3524 - lr: 0.05000 - Train loss: 17.90499 - Test loss: 54.00656\n",
      "Epoch 3525 - lr: 0.05000 - Train loss: 17.90499 - Test loss: 54.00651\n",
      "Epoch 3526 - lr: 0.05000 - Train loss: 17.90499 - Test loss: 54.00646\n",
      "Epoch 3527 - lr: 0.05000 - Train loss: 17.90498 - Test loss: 54.00641\n",
      "Epoch 3528 - lr: 0.05000 - Train loss: 17.90498 - Test loss: 54.00637\n",
      "Epoch 3529 - lr: 0.05000 - Train loss: 17.90498 - Test loss: 54.00632\n",
      "Epoch 3530 - lr: 0.05000 - Train loss: 17.90498 - Test loss: 54.00627\n",
      "Epoch 3531 - lr: 0.05000 - Train loss: 17.90498 - Test loss: 54.00622\n",
      "Epoch 3532 - lr: 0.05000 - Train loss: 17.90498 - Test loss: 54.00617\n",
      "Epoch 3533 - lr: 0.05000 - Train loss: 17.90498 - Test loss: 54.00613\n",
      "Epoch 3534 - lr: 0.05000 - Train loss: 17.90498 - Test loss: 54.00608\n",
      "Epoch 3535 - lr: 0.05000 - Train loss: 17.90498 - Test loss: 54.00603\n",
      "Epoch 3536 - lr: 0.05000 - Train loss: 17.90498 - Test loss: 54.00598\n",
      "Epoch 3537 - lr: 0.05000 - Train loss: 17.90497 - Test loss: 54.00594\n",
      "Epoch 3538 - lr: 0.05000 - Train loss: 17.90497 - Test loss: 54.00589\n",
      "Epoch 3539 - lr: 0.05000 - Train loss: 17.90497 - Test loss: 54.00584\n",
      "Epoch 3540 - lr: 0.05000 - Train loss: 17.90497 - Test loss: 54.00579\n",
      "Epoch 3541 - lr: 0.05000 - Train loss: 17.90497 - Test loss: 54.00574\n",
      "Epoch 3542 - lr: 0.05000 - Train loss: 17.90497 - Test loss: 54.00570\n",
      "Epoch 3543 - lr: 0.05000 - Train loss: 17.90497 - Test loss: 54.00565\n",
      "Epoch 3544 - lr: 0.05000 - Train loss: 17.90497 - Test loss: 54.00560\n",
      "Epoch 3545 - lr: 0.05000 - Train loss: 17.90497 - Test loss: 54.00555\n",
      "Epoch 3546 - lr: 0.05000 - Train loss: 17.90497 - Test loss: 54.00551\n",
      "Epoch 3547 - lr: 0.05000 - Train loss: 17.90497 - Test loss: 54.00546\n",
      "Epoch 3548 - lr: 0.05000 - Train loss: 17.90496 - Test loss: 54.00541\n",
      "Epoch 3549 - lr: 0.05000 - Train loss: 17.90496 - Test loss: 54.00536\n",
      "Epoch 3550 - lr: 0.05000 - Train loss: 17.90496 - Test loss: 54.00532\n",
      "Epoch 3551 - lr: 0.05000 - Train loss: 17.90496 - Test loss: 54.00527\n",
      "Epoch 3552 - lr: 0.05000 - Train loss: 17.90496 - Test loss: 54.00522\n",
      "Epoch 3553 - lr: 0.05000 - Train loss: 17.90496 - Test loss: 54.00517\n",
      "Epoch 3554 - lr: 0.05000 - Train loss: 17.90496 - Test loss: 54.00513\n",
      "Epoch 3555 - lr: 0.05000 - Train loss: 17.90496 - Test loss: 54.00508\n",
      "Epoch 3556 - lr: 0.05000 - Train loss: 17.90496 - Test loss: 54.00503\n",
      "Epoch 3557 - lr: 0.05000 - Train loss: 17.90496 - Test loss: 54.00498\n",
      "Epoch 3558 - lr: 0.05000 - Train loss: 17.90495 - Test loss: 54.00494\n",
      "Epoch 3559 - lr: 0.05000 - Train loss: 17.90495 - Test loss: 54.00489\n",
      "Epoch 3560 - lr: 0.05000 - Train loss: 17.90495 - Test loss: 54.00484\n",
      "Epoch 3561 - lr: 0.05000 - Train loss: 17.90495 - Test loss: 54.00479\n",
      "Epoch 3562 - lr: 0.05000 - Train loss: 17.90495 - Test loss: 54.00475\n",
      "Epoch 3563 - lr: 0.05000 - Train loss: 17.90495 - Test loss: 54.00470\n",
      "Epoch 3564 - lr: 0.05000 - Train loss: 17.90495 - Test loss: 54.00465\n",
      "Epoch 3565 - lr: 0.05000 - Train loss: 17.90495 - Test loss: 54.00461\n",
      "Epoch 3566 - lr: 0.05000 - Train loss: 17.90495 - Test loss: 54.00456\n",
      "Epoch 3567 - lr: 0.05000 - Train loss: 17.90495 - Test loss: 54.00451\n",
      "Epoch 3568 - lr: 0.05000 - Train loss: 17.90494 - Test loss: 54.00446\n",
      "Epoch 3569 - lr: 0.05000 - Train loss: 17.90494 - Test loss: 54.00442\n",
      "Epoch 3570 - lr: 0.05000 - Train loss: 17.90494 - Test loss: 54.00437\n",
      "Epoch 3571 - lr: 0.05000 - Train loss: 17.90494 - Test loss: 54.00432\n",
      "Epoch 3572 - lr: 0.05000 - Train loss: 17.90494 - Test loss: 54.00428\n",
      "Epoch 3573 - lr: 0.05000 - Train loss: 17.90494 - Test loss: 54.00423\n",
      "Epoch 3574 - lr: 0.05000 - Train loss: 17.90494 - Test loss: 54.00418\n",
      "Epoch 3575 - lr: 0.05000 - Train loss: 17.90494 - Test loss: 54.00414\n",
      "Epoch 3576 - lr: 0.05000 - Train loss: 17.90494 - Test loss: 54.00409\n",
      "Epoch 3577 - lr: 0.05000 - Train loss: 17.90493 - Test loss: 54.00404\n",
      "Epoch 3578 - lr: 0.05000 - Train loss: 17.90493 - Test loss: 54.00399\n",
      "Epoch 3579 - lr: 0.05000 - Train loss: 17.90493 - Test loss: 54.00395\n",
      "Epoch 3580 - lr: 0.05000 - Train loss: 17.90493 - Test loss: 54.00390\n",
      "Epoch 3581 - lr: 0.05000 - Train loss: 17.90493 - Test loss: 54.00385\n",
      "Epoch 3582 - lr: 0.05000 - Train loss: 17.90493 - Test loss: 54.00381\n",
      "Epoch 3583 - lr: 0.05000 - Train loss: 17.90493 - Test loss: 54.00376\n",
      "Epoch 3584 - lr: 0.05000 - Train loss: 17.90493 - Test loss: 54.00371\n",
      "Epoch 3585 - lr: 0.05000 - Train loss: 17.90493 - Test loss: 54.00367\n",
      "Epoch 3586 - lr: 0.05000 - Train loss: 17.90493 - Test loss: 54.00362\n",
      "Epoch 3587 - lr: 0.05000 - Train loss: 17.90492 - Test loss: 54.00357\n",
      "Epoch 3588 - lr: 0.05000 - Train loss: 17.90492 - Test loss: 54.00353\n",
      "Epoch 3589 - lr: 0.05000 - Train loss: 17.90492 - Test loss: 54.00348\n",
      "Epoch 3590 - lr: 0.05000 - Train loss: 17.90492 - Test loss: 54.00344\n",
      "Epoch 3591 - lr: 0.05000 - Train loss: 17.90492 - Test loss: 54.00339\n",
      "Epoch 3592 - lr: 0.05000 - Train loss: 17.90492 - Test loss: 54.00334\n",
      "Epoch 3593 - lr: 0.05000 - Train loss: 17.90492 - Test loss: 54.00330\n",
      "Epoch 3594 - lr: 0.05000 - Train loss: 17.90492 - Test loss: 54.00325\n",
      "Epoch 3595 - lr: 0.05000 - Train loss: 17.90492 - Test loss: 54.00320\n",
      "Epoch 3596 - lr: 0.05000 - Train loss: 17.90491 - Test loss: 54.00316\n",
      "Epoch 3597 - lr: 0.05000 - Train loss: 17.90491 - Test loss: 54.00311\n",
      "Epoch 3598 - lr: 0.05000 - Train loss: 17.90491 - Test loss: 54.00306\n",
      "Epoch 3599 - lr: 0.05000 - Train loss: 17.90491 - Test loss: 54.00302\n",
      "Epoch 3600 - lr: 0.05000 - Train loss: 17.90491 - Test loss: 54.00297\n",
      "Epoch 3601 - lr: 0.05000 - Train loss: 17.90491 - Test loss: 54.00293\n",
      "Epoch 3602 - lr: 0.05000 - Train loss: 17.90491 - Test loss: 54.00288\n",
      "Epoch 3603 - lr: 0.05000 - Train loss: 17.90491 - Test loss: 54.00283\n",
      "Epoch 3604 - lr: 0.05000 - Train loss: 17.90491 - Test loss: 54.00279\n",
      "Epoch 3605 - lr: 0.05000 - Train loss: 17.90490 - Test loss: 54.00274\n",
      "Epoch 3606 - lr: 0.05000 - Train loss: 17.90490 - Test loss: 54.00270\n",
      "Epoch 3607 - lr: 0.05000 - Train loss: 17.90490 - Test loss: 54.00265\n",
      "Epoch 3608 - lr: 0.05000 - Train loss: 17.90490 - Test loss: 54.00261\n",
      "Epoch 3609 - lr: 0.05000 - Train loss: 17.90490 - Test loss: 54.00256\n",
      "Epoch 3610 - lr: 0.05000 - Train loss: 17.90490 - Test loss: 54.00251\n",
      "Epoch 3611 - lr: 0.05000 - Train loss: 17.90490 - Test loss: 54.00247\n",
      "Epoch 3612 - lr: 0.05000 - Train loss: 17.90490 - Test loss: 54.00242\n",
      "Epoch 3613 - lr: 0.05000 - Train loss: 17.90489 - Test loss: 54.00238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3614 - lr: 0.05000 - Train loss: 17.90489 - Test loss: 54.00233\n",
      "Epoch 3615 - lr: 0.05000 - Train loss: 17.90489 - Test loss: 54.00229\n",
      "Epoch 3616 - lr: 0.05000 - Train loss: 17.90489 - Test loss: 54.00224\n",
      "Epoch 3617 - lr: 0.05000 - Train loss: 17.90489 - Test loss: 54.00220\n",
      "Epoch 3618 - lr: 0.05000 - Train loss: 17.90489 - Test loss: 54.00215\n",
      "Epoch 3619 - lr: 0.05000 - Train loss: 17.90489 - Test loss: 54.00211\n",
      "Epoch 3620 - lr: 0.05000 - Train loss: 17.90489 - Test loss: 54.00206\n",
      "Epoch 3621 - lr: 0.05000 - Train loss: 17.90489 - Test loss: 54.00202\n",
      "Epoch 3622 - lr: 0.05000 - Train loss: 17.90488 - Test loss: 54.00197\n",
      "Epoch 3623 - lr: 0.05000 - Train loss: 17.90488 - Test loss: 54.00193\n",
      "Epoch 3624 - lr: 0.05000 - Train loss: 17.90488 - Test loss: 54.00188\n",
      "Epoch 3625 - lr: 0.05000 - Train loss: 17.90488 - Test loss: 54.00184\n",
      "Epoch 3626 - lr: 0.05000 - Train loss: 17.90488 - Test loss: 54.00179\n",
      "Epoch 3627 - lr: 0.05000 - Train loss: 17.90488 - Test loss: 54.00175\n",
      "Epoch 3628 - lr: 0.05000 - Train loss: 17.90488 - Test loss: 54.00170\n",
      "Epoch 3629 - lr: 0.05000 - Train loss: 17.90488 - Test loss: 54.00166\n",
      "Epoch 3630 - lr: 0.05000 - Train loss: 17.90487 - Test loss: 54.00161\n",
      "Epoch 3631 - lr: 0.05000 - Train loss: 17.90487 - Test loss: 54.00157\n",
      "Epoch 3632 - lr: 0.05000 - Train loss: 17.90487 - Test loss: 54.00152\n",
      "Epoch 3633 - lr: 0.05000 - Train loss: 17.90487 - Test loss: 54.00148\n",
      "Epoch 3634 - lr: 0.05000 - Train loss: 17.90487 - Test loss: 54.00143\n",
      "Epoch 3635 - lr: 0.05000 - Train loss: 17.90487 - Test loss: 54.00139\n",
      "Epoch 3636 - lr: 0.05000 - Train loss: 17.90487 - Test loss: 54.00135\n",
      "Epoch 3637 - lr: 0.05000 - Train loss: 17.90486 - Test loss: 54.00130\n",
      "Epoch 3638 - lr: 0.05000 - Train loss: 17.90486 - Test loss: 54.00126\n",
      "Epoch 3639 - lr: 0.05000 - Train loss: 17.90486 - Test loss: 54.00121\n",
      "Epoch 3640 - lr: 0.05000 - Train loss: 17.90486 - Test loss: 54.00117\n",
      "Epoch 3641 - lr: 0.05000 - Train loss: 17.90486 - Test loss: 54.00112\n",
      "Epoch 3642 - lr: 0.05000 - Train loss: 17.90486 - Test loss: 54.00108\n",
      "Epoch 3643 - lr: 0.05000 - Train loss: 17.90486 - Test loss: 54.00104\n",
      "Epoch 3644 - lr: 0.05000 - Train loss: 17.90485 - Test loss: 54.00099\n",
      "Epoch 3645 - lr: 0.05000 - Train loss: 17.90485 - Test loss: 54.00095\n",
      "Epoch 3646 - lr: 0.05000 - Train loss: 17.90485 - Test loss: 54.00091\n",
      "Epoch 3647 - lr: 0.05000 - Train loss: 17.90485 - Test loss: 54.00086\n",
      "Epoch 3648 - lr: 0.05000 - Train loss: 17.90485 - Test loss: 54.00082\n",
      "Epoch 3649 - lr: 0.05000 - Train loss: 17.90485 - Test loss: 54.00078\n",
      "Epoch 3650 - lr: 0.05000 - Train loss: 17.90485 - Test loss: 54.00073\n",
      "Epoch 3651 - lr: 0.05000 - Train loss: 17.90484 - Test loss: 54.00069\n",
      "Epoch 3652 - lr: 0.05000 - Train loss: 17.90484 - Test loss: 54.00065\n",
      "Epoch 3653 - lr: 0.05000 - Train loss: 17.90484 - Test loss: 54.00060\n",
      "Epoch 3654 - lr: 0.05000 - Train loss: 17.90484 - Test loss: 54.00056\n",
      "Epoch 3655 - lr: 0.05000 - Train loss: 17.90484 - Test loss: 54.00052\n",
      "Epoch 3656 - lr: 0.05000 - Train loss: 17.90484 - Test loss: 54.00048\n",
      "Epoch 3657 - lr: 0.05000 - Train loss: 17.90484 - Test loss: 54.00043\n",
      "Epoch 3658 - lr: 0.05000 - Train loss: 17.90483 - Test loss: 54.00039\n",
      "Epoch 3659 - lr: 0.05000 - Train loss: 17.90483 - Test loss: 54.00035\n",
      "Epoch 3660 - lr: 0.05000 - Train loss: 17.90483 - Test loss: 54.00031\n",
      "Epoch 3661 - lr: 0.05000 - Train loss: 17.90483 - Test loss: 54.00026\n",
      "Epoch 3662 - lr: 0.05000 - Train loss: 17.90483 - Test loss: 54.00022\n",
      "Epoch 3663 - lr: 0.05000 - Train loss: 17.90483 - Test loss: 54.00018\n",
      "Epoch 3664 - lr: 0.05000 - Train loss: 17.90483 - Test loss: 54.00014\n",
      "Epoch 3665 - lr: 0.05000 - Train loss: 17.90482 - Test loss: 54.00010\n",
      "Epoch 3666 - lr: 0.05000 - Train loss: 17.90482 - Test loss: 54.00006\n",
      "Epoch 3667 - lr: 0.05000 - Train loss: 17.90482 - Test loss: 54.00001\n",
      "Epoch 3668 - lr: 0.05000 - Train loss: 17.90482 - Test loss: 53.99997\n",
      "Epoch 3669 - lr: 0.05000 - Train loss: 17.90482 - Test loss: 53.99993\n",
      "Epoch 3670 - lr: 0.05000 - Train loss: 17.90482 - Test loss: 53.99989\n",
      "Epoch 3671 - lr: 0.05000 - Train loss: 17.90481 - Test loss: 53.99985\n",
      "Epoch 3672 - lr: 0.05000 - Train loss: 17.90481 - Test loss: 53.99981\n",
      "Epoch 3673 - lr: 0.05000 - Train loss: 17.90481 - Test loss: 53.99977\n",
      "Epoch 3674 - lr: 0.05000 - Train loss: 17.90481 - Test loss: 53.99973\n",
      "Epoch 3675 - lr: 0.05000 - Train loss: 17.90481 - Test loss: 53.99969\n",
      "Epoch 3676 - lr: 0.05000 - Train loss: 17.90480 - Test loss: 53.99965\n",
      "Epoch 3677 - lr: 0.05000 - Train loss: 17.90480 - Test loss: 53.99961\n",
      "Epoch 3678 - lr: 0.05000 - Train loss: 17.90480 - Test loss: 53.99957\n",
      "Epoch 3679 - lr: 0.05000 - Train loss: 17.90480 - Test loss: 53.99953\n",
      "Epoch 3680 - lr: 0.05000 - Train loss: 17.90480 - Test loss: 53.99949\n",
      "Epoch 3681 - lr: 0.05000 - Train loss: 17.90480 - Test loss: 53.99945\n",
      "Epoch 3682 - lr: 0.05000 - Train loss: 17.90479 - Test loss: 53.99941\n",
      "Epoch 3683 - lr: 0.05000 - Train loss: 17.90479 - Test loss: 53.99937\n",
      "Epoch 3684 - lr: 0.05000 - Train loss: 17.90479 - Test loss: 53.99933\n",
      "Epoch 3685 - lr: 0.05000 - Train loss: 17.90479 - Test loss: 53.99929\n",
      "Epoch 3686 - lr: 0.05000 - Train loss: 17.90479 - Test loss: 53.99925\n",
      "Epoch 3687 - lr: 0.05000 - Train loss: 17.90478 - Test loss: 53.99922\n",
      "Epoch 3688 - lr: 0.05000 - Train loss: 17.90478 - Test loss: 53.99918\n",
      "Epoch 3689 - lr: 0.05000 - Train loss: 17.90478 - Test loss: 53.99914\n",
      "Epoch 3690 - lr: 0.05000 - Train loss: 17.90478 - Test loss: 53.99910\n",
      "Epoch 3691 - lr: 0.05000 - Train loss: 17.90478 - Test loss: 53.99906\n",
      "Epoch 3692 - lr: 0.05000 - Train loss: 17.90477 - Test loss: 53.99903\n",
      "Epoch 3693 - lr: 0.05000 - Train loss: 17.90477 - Test loss: 53.99899\n",
      "Epoch 3694 - lr: 0.05000 - Train loss: 17.90477 - Test loss: 53.99895\n",
      "Epoch 3695 - lr: 0.05000 - Train loss: 17.90477 - Test loss: 53.99892\n",
      "Epoch 3696 - lr: 0.05000 - Train loss: 17.90476 - Test loss: 53.99888\n",
      "Epoch 3697 - lr: 0.05000 - Train loss: 17.90476 - Test loss: 53.99885\n",
      "Epoch 3698 - lr: 0.05000 - Train loss: 17.90476 - Test loss: 53.99881\n",
      "Epoch 3699 - lr: 0.05000 - Train loss: 17.90476 - Test loss: 53.99877\n",
      "Epoch 3700 - lr: 0.05000 - Train loss: 17.90475 - Test loss: 53.99874\n",
      "Epoch 3701 - lr: 0.05000 - Train loss: 17.90475 - Test loss: 53.99871\n",
      "Epoch 3702 - lr: 0.05000 - Train loss: 17.90475 - Test loss: 53.99867\n",
      "Epoch 3703 - lr: 0.05000 - Train loss: 17.90475 - Test loss: 53.99864\n",
      "Epoch 3704 - lr: 0.05000 - Train loss: 17.90474 - Test loss: 53.99860\n",
      "Epoch 3705 - lr: 0.05000 - Train loss: 17.90474 - Test loss: 53.99857\n",
      "Epoch 3706 - lr: 0.05000 - Train loss: 17.90474 - Test loss: 53.99854\n",
      "Epoch 3707 - lr: 0.05000 - Train loss: 17.90474 - Test loss: 53.99850\n",
      "Epoch 3708 - lr: 0.05000 - Train loss: 17.90473 - Test loss: 53.99847\n",
      "Epoch 3709 - lr: 0.05000 - Train loss: 17.90473 - Test loss: 53.99844\n",
      "Epoch 3710 - lr: 0.05000 - Train loss: 17.90473 - Test loss: 53.99841\n",
      "Epoch 3711 - lr: 0.05000 - Train loss: 17.90473 - Test loss: 53.99838\n",
      "Epoch 3712 - lr: 0.05000 - Train loss: 17.90472 - Test loss: 53.99835\n",
      "Epoch 3713 - lr: 0.05000 - Train loss: 17.90472 - Test loss: 53.99832\n",
      "Epoch 3714 - lr: 0.05000 - Train loss: 17.90472 - Test loss: 53.99829\n",
      "Epoch 3715 - lr: 0.05000 - Train loss: 17.90471 - Test loss: 53.99826\n",
      "Epoch 3716 - lr: 0.05000 - Train loss: 17.90471 - Test loss: 53.99823\n",
      "Epoch 3717 - lr: 0.05000 - Train loss: 17.90471 - Test loss: 53.99820\n",
      "Epoch 3718 - lr: 0.05000 - Train loss: 17.90470 - Test loss: 53.99817\n",
      "Epoch 3719 - lr: 0.05000 - Train loss: 17.90470 - Test loss: 53.99815\n",
      "Epoch 3720 - lr: 0.05000 - Train loss: 17.90470 - Test loss: 53.99812\n",
      "Epoch 3721 - lr: 0.05000 - Train loss: 17.90469 - Test loss: 53.99810\n",
      "Epoch 3722 - lr: 0.05000 - Train loss: 17.90469 - Test loss: 53.99807\n",
      "Epoch 3723 - lr: 0.05000 - Train loss: 17.90469 - Test loss: 53.99805\n",
      "Epoch 3724 - lr: 0.05000 - Train loss: 17.90468 - Test loss: 53.99802\n",
      "Epoch 3725 - lr: 0.05000 - Train loss: 17.90468 - Test loss: 53.99800\n",
      "Epoch 3726 - lr: 0.05000 - Train loss: 17.90467 - Test loss: 53.99798\n",
      "Epoch 3727 - lr: 0.05000 - Train loss: 17.90467 - Test loss: 53.99796\n",
      "Epoch 3728 - lr: 0.05000 - Train loss: 17.90467 - Test loss: 53.99794\n",
      "Epoch 3729 - lr: 0.05000 - Train loss: 17.90466 - Test loss: 53.99792\n",
      "Epoch 3730 - lr: 0.05000 - Train loss: 17.90466 - Test loss: 53.99790\n",
      "Epoch 3731 - lr: 0.05000 - Train loss: 17.90465 - Test loss: 53.99788\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3732 - lr: 0.05000 - Train loss: 17.90465 - Test loss: 53.99787\n",
      "Epoch 3733 - lr: 0.05000 - Train loss: 17.90464 - Test loss: 53.99785\n",
      "Epoch 3734 - lr: 0.05000 - Train loss: 17.90464 - Test loss: 53.99784\n",
      "Epoch 3735 - lr: 0.05000 - Train loss: 17.90463 - Test loss: 53.99783\n",
      "Epoch 3736 - lr: 0.05000 - Train loss: 17.90463 - Test loss: 53.99781\n",
      "Epoch 3737 - lr: 0.05000 - Train loss: 17.90462 - Test loss: 53.99780\n",
      "Epoch 3738 - lr: 0.05000 - Train loss: 17.90462 - Test loss: 53.99780\n",
      "Epoch 3739 - lr: 0.05000 - Train loss: 17.90461 - Test loss: 53.99779\n",
      "Epoch 3740 - lr: 0.05000 - Train loss: 17.90461 - Test loss: 53.99778\n",
      "Epoch 3741 - lr: 0.05000 - Train loss: 17.90460 - Test loss: 53.99778\n",
      "Epoch 3742 - lr: 0.05000 - Train loss: 17.90459 - Test loss: 53.99778\n",
      "Epoch 3743 - lr: 0.05000 - Train loss: 17.90459 - Test loss: 53.99778\n",
      "Epoch 3744 - lr: 0.05000 - Train loss: 17.90458 - Test loss: 53.99778\n",
      "Epoch 3745 - lr: 0.05000 - Train loss: 17.90457 - Test loss: 53.99779\n",
      "Epoch 3746 - lr: 0.05000 - Train loss: 17.90456 - Test loss: 53.99779\n",
      "Epoch 3747 - lr: 0.05000 - Train loss: 17.90456 - Test loss: 53.99780\n",
      "Epoch 3748 - lr: 0.05000 - Train loss: 17.90455 - Test loss: 53.99781\n",
      "Epoch 3749 - lr: 0.05000 - Train loss: 17.90454 - Test loss: 53.99783\n",
      "Epoch 3750 - lr: 0.05000 - Train loss: 17.90453 - Test loss: 53.99785\n",
      "Epoch 3751 - lr: 0.05000 - Train loss: 17.90452 - Test loss: 53.99787\n",
      "Epoch 3752 - lr: 0.05000 - Train loss: 17.90451 - Test loss: 53.99789\n",
      "Epoch 3753 - lr: 0.05000 - Train loss: 17.90450 - Test loss: 53.99792\n",
      "Epoch 3754 - lr: 0.05000 - Train loss: 17.90449 - Test loss: 53.99796\n",
      "Epoch 3755 - lr: 0.05000 - Train loss: 17.90448 - Test loss: 53.99799\n",
      "Epoch 3756 - lr: 0.05000 - Train loss: 17.90447 - Test loss: 53.99804\n",
      "Epoch 3757 - lr: 0.05000 - Train loss: 17.90446 - Test loss: 53.99808\n",
      "Epoch 3758 - lr: 0.05000 - Train loss: 17.90444 - Test loss: 53.99814\n",
      "Epoch 3759 - lr: 0.05000 - Train loss: 17.90443 - Test loss: 53.99819\n",
      "Epoch 3760 - lr: 0.05000 - Train loss: 17.90442 - Test loss: 53.99826\n",
      "Epoch 3761 - lr: 0.05000 - Train loss: 17.90440 - Test loss: 53.99833\n",
      "Epoch 3762 - lr: 0.05000 - Train loss: 17.90438 - Test loss: 53.99841\n",
      "Epoch 3763 - lr: 0.05000 - Train loss: 17.90437 - Test loss: 53.99850\n",
      "Epoch 3764 - lr: 0.05000 - Train loss: 17.90435 - Test loss: 53.99860\n",
      "Epoch 3765 - lr: 0.05000 - Train loss: 17.90433 - Test loss: 53.99871\n",
      "Epoch 3766 - lr: 0.05000 - Train loss: 17.90431 - Test loss: 53.99884\n",
      "Epoch 3767 - lr: 0.05000 - Train loss: 17.90428 - Test loss: 53.99897\n",
      "Epoch 3768 - lr: 0.05000 - Train loss: 17.90426 - Test loss: 53.99912\n",
      "Epoch 3769 - lr: 0.05000 - Train loss: 17.90423 - Test loss: 53.99929\n",
      "Epoch 3770 - lr: 0.05000 - Train loss: 17.90420 - Test loss: 53.99947\n",
      "Epoch 3771 - lr: 0.05000 - Train loss: 17.90417 - Test loss: 53.99968\n",
      "Epoch 3772 - lr: 0.05000 - Train loss: 17.90413 - Test loss: 53.99991\n",
      "Epoch 3773 - lr: 0.05000 - Train loss: 17.90409 - Test loss: 54.00017\n",
      "Epoch 3774 - lr: 0.05000 - Train loss: 17.90405 - Test loss: 54.00046\n",
      "Epoch 3775 - lr: 0.05000 - Train loss: 17.90400 - Test loss: 54.00079\n",
      "Epoch 3776 - lr: 0.05000 - Train loss: 17.90395 - Test loss: 54.00116\n",
      "Epoch 3777 - lr: 0.05000 - Train loss: 17.90389 - Test loss: 54.00158\n",
      "Epoch 3778 - lr: 0.05000 - Train loss: 17.90382 - Test loss: 54.00206\n",
      "Epoch 3779 - lr: 0.05000 - Train loss: 17.90374 - Test loss: 54.00261\n",
      "Epoch 3780 - lr: 0.05000 - Train loss: 17.90365 - Test loss: 54.00325\n",
      "Epoch 3781 - lr: 0.05000 - Train loss: 17.90355 - Test loss: 54.00400\n",
      "Epoch 3782 - lr: 0.05000 - Train loss: 17.90343 - Test loss: 54.00487\n",
      "Epoch 3783 - lr: 0.05000 - Train loss: 17.90328 - Test loss: 54.00591\n",
      "Epoch 3784 - lr: 0.05000 - Train loss: 17.90311 - Test loss: 54.00717\n",
      "Epoch 3785 - lr: 0.05000 - Train loss: 17.90290 - Test loss: 54.00870\n",
      "Epoch 3786 - lr: 0.05000 - Train loss: 17.90263 - Test loss: 54.01061\n",
      "Epoch 3787 - lr: 0.05000 - Train loss: 17.90228 - Test loss: 54.01305\n",
      "Epoch 3788 - lr: 0.05000 - Train loss: 17.90181 - Test loss: 54.01624\n",
      "Epoch 3789 - lr: 0.05000 - Train loss: 17.90114 - Test loss: 54.02062\n",
      "Epoch 3790 - lr: 0.05000 - Train loss: 17.90012 - Test loss: 54.02699\n",
      "Epoch 3791 - lr: 0.05000 - Train loss: 17.89829 - Test loss: 54.03733\n",
      "Epoch 3792 - lr: 0.05000 - Train loss: 17.89305 - Test loss: 54.06171\n",
      "Epoch 3793 - lr: 0.05000 - Train loss: 14.34050 - Test loss: 64.62278\n",
      "Epoch 3794 - lr: 0.05000 - Train loss: 19.05992 - Test loss: 54.01149\n",
      "Epoch 3795 - lr: 0.05000 - Train loss: 17.91003 - Test loss: 54.00333\n",
      "Epoch 3796 - lr: 0.05000 - Train loss: 17.90256 - Test loss: 54.01042\n",
      "Epoch 3797 - lr: 0.05000 - Train loss: 17.89396 - Test loss: 54.04212\n",
      "Epoch 3798 - lr: 0.05000 - Train loss: 13.18406 - Test loss: 90.47187\n",
      "Epoch 3799 - lr: 0.05000 - Train loss: 17.79661 - Test loss: 53.98034\n",
      "Epoch 3800 - lr: 0.05000 - Train loss: 17.90429 - Test loss: 53.98948\n",
      "Epoch 3801 - lr: 0.05000 - Train loss: 17.90556 - Test loss: 53.98970\n",
      "Epoch 3802 - lr: 0.05000 - Train loss: 17.90555 - Test loss: 53.98984\n",
      "Epoch 3803 - lr: 0.05000 - Train loss: 17.90551 - Test loss: 53.98995\n",
      "Epoch 3804 - lr: 0.05000 - Train loss: 17.90546 - Test loss: 53.99006\n",
      "Epoch 3805 - lr: 0.05000 - Train loss: 17.90542 - Test loss: 53.99015\n",
      "Epoch 3806 - lr: 0.05000 - Train loss: 17.90539 - Test loss: 53.99023\n",
      "Epoch 3807 - lr: 0.05000 - Train loss: 17.90535 - Test loss: 53.99031\n",
      "Epoch 3808 - lr: 0.05000 - Train loss: 17.90532 - Test loss: 53.99037\n",
      "Epoch 3809 - lr: 0.05000 - Train loss: 17.90530 - Test loss: 53.99043\n",
      "Epoch 3810 - lr: 0.05000 - Train loss: 17.90527 - Test loss: 53.99048\n",
      "Epoch 3811 - lr: 0.05000 - Train loss: 17.90525 - Test loss: 53.99052\n",
      "Epoch 3812 - lr: 0.05000 - Train loss: 17.90522 - Test loss: 53.99056\n",
      "Epoch 3813 - lr: 0.05000 - Train loss: 17.90520 - Test loss: 53.99060\n",
      "Epoch 3814 - lr: 0.05000 - Train loss: 17.90518 - Test loss: 53.99063\n",
      "Epoch 3815 - lr: 0.05000 - Train loss: 17.90517 - Test loss: 53.99065\n",
      "Epoch 3816 - lr: 0.05000 - Train loss: 17.90515 - Test loss: 53.99068\n",
      "Epoch 3817 - lr: 0.05000 - Train loss: 17.90513 - Test loss: 53.99070\n",
      "Epoch 3818 - lr: 0.05000 - Train loss: 17.90512 - Test loss: 53.99071\n",
      "Epoch 3819 - lr: 0.05000 - Train loss: 17.90510 - Test loss: 53.99072\n",
      "Epoch 3820 - lr: 0.05000 - Train loss: 17.90509 - Test loss: 53.99074\n",
      "Epoch 3821 - lr: 0.05000 - Train loss: 17.90508 - Test loss: 53.99074\n",
      "Epoch 3822 - lr: 0.05000 - Train loss: 17.90506 - Test loss: 53.99075\n",
      "Epoch 3823 - lr: 0.05000 - Train loss: 17.90505 - Test loss: 53.99075\n",
      "Epoch 3824 - lr: 0.05000 - Train loss: 17.90504 - Test loss: 53.99076\n",
      "Epoch 3825 - lr: 0.05000 - Train loss: 17.90503 - Test loss: 53.99076\n",
      "Epoch 3826 - lr: 0.05000 - Train loss: 17.90502 - Test loss: 53.99075\n",
      "Epoch 3827 - lr: 0.05000 - Train loss: 17.90501 - Test loss: 53.99075\n",
      "Epoch 3828 - lr: 0.05000 - Train loss: 17.90500 - Test loss: 53.99075\n",
      "Epoch 3829 - lr: 0.05000 - Train loss: 17.90499 - Test loss: 53.99074\n",
      "Epoch 3830 - lr: 0.05000 - Train loss: 17.90498 - Test loss: 53.99073\n",
      "Epoch 3831 - lr: 0.05000 - Train loss: 17.90497 - Test loss: 53.99073\n",
      "Epoch 3832 - lr: 0.05000 - Train loss: 17.90496 - Test loss: 53.99072\n",
      "Epoch 3833 - lr: 0.05000 - Train loss: 17.90496 - Test loss: 53.99071\n",
      "Epoch 3834 - lr: 0.05000 - Train loss: 17.90495 - Test loss: 53.99069\n",
      "Epoch 3835 - lr: 0.05000 - Train loss: 17.90494 - Test loss: 53.99068\n",
      "Epoch 3836 - lr: 0.05000 - Train loss: 17.90493 - Test loss: 53.99067\n",
      "Epoch 3837 - lr: 0.05000 - Train loss: 17.90493 - Test loss: 53.99065\n",
      "Epoch 3838 - lr: 0.05000 - Train loss: 17.90492 - Test loss: 53.99064\n",
      "Epoch 3839 - lr: 0.05000 - Train loss: 17.90491 - Test loss: 53.99062\n",
      "Epoch 3840 - lr: 0.05000 - Train loss: 17.90491 - Test loss: 53.99060\n",
      "Epoch 3841 - lr: 0.05000 - Train loss: 17.90490 - Test loss: 53.99059\n",
      "Epoch 3842 - lr: 0.05000 - Train loss: 17.90489 - Test loss: 53.99057\n",
      "Epoch 3843 - lr: 0.05000 - Train loss: 17.90489 - Test loss: 53.99055\n",
      "Epoch 3844 - lr: 0.05000 - Train loss: 17.90488 - Test loss: 53.99053\n",
      "Epoch 3845 - lr: 0.05000 - Train loss: 17.90488 - Test loss: 53.99051\n",
      "Epoch 3846 - lr: 0.05000 - Train loss: 17.90487 - Test loss: 53.99049\n",
      "Epoch 3847 - lr: 0.05000 - Train loss: 17.90487 - Test loss: 53.99047\n",
      "Epoch 3848 - lr: 0.05000 - Train loss: 17.90486 - Test loss: 53.99045\n",
      "Epoch 3849 - lr: 0.05000 - Train loss: 17.90486 - Test loss: 53.99043\n",
      "Epoch 3850 - lr: 0.05000 - Train loss: 17.90485 - Test loss: 53.99040\n",
      "Epoch 3851 - lr: 0.05000 - Train loss: 17.90485 - Test loss: 53.99038\n",
      "Epoch 3852 - lr: 0.05000 - Train loss: 17.90484 - Test loss: 53.99036\n",
      "Epoch 3853 - lr: 0.05000 - Train loss: 17.90484 - Test loss: 53.99033\n",
      "Epoch 3854 - lr: 0.05000 - Train loss: 17.90483 - Test loss: 53.99031\n",
      "Epoch 3855 - lr: 0.05000 - Train loss: 17.90483 - Test loss: 53.99029\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3856 - lr: 0.05000 - Train loss: 17.90482 - Test loss: 53.99026\n",
      "Epoch 3857 - lr: 0.05000 - Train loss: 17.90482 - Test loss: 53.99024\n",
      "Epoch 3858 - lr: 0.05000 - Train loss: 17.90481 - Test loss: 53.99021\n",
      "Epoch 3859 - lr: 0.05000 - Train loss: 17.90481 - Test loss: 53.99018\n",
      "Epoch 3860 - lr: 0.05000 - Train loss: 17.90480 - Test loss: 53.99016\n",
      "Epoch 3861 - lr: 0.05000 - Train loss: 17.90480 - Test loss: 53.99013\n",
      "Epoch 3862 - lr: 0.05000 - Train loss: 17.90480 - Test loss: 53.99011\n",
      "Epoch 3863 - lr: 0.05000 - Train loss: 17.90479 - Test loss: 53.99008\n",
      "Epoch 3864 - lr: 0.05000 - Train loss: 17.90479 - Test loss: 53.99005\n",
      "Epoch 3865 - lr: 0.05000 - Train loss: 17.90478 - Test loss: 53.99003\n",
      "Epoch 3866 - lr: 0.05000 - Train loss: 17.90478 - Test loss: 53.99000\n",
      "Epoch 3867 - lr: 0.05000 - Train loss: 17.90478 - Test loss: 53.98997\n",
      "Epoch 3868 - lr: 0.05000 - Train loss: 17.90477 - Test loss: 53.98994\n",
      "Epoch 3869 - lr: 0.05000 - Train loss: 17.90477 - Test loss: 53.98992\n",
      "Epoch 3870 - lr: 0.05000 - Train loss: 17.90477 - Test loss: 53.98989\n",
      "Epoch 3871 - lr: 0.05000 - Train loss: 17.90476 - Test loss: 53.98986\n",
      "Epoch 3872 - lr: 0.05000 - Train loss: 17.90476 - Test loss: 53.98983\n",
      "Epoch 3873 - lr: 0.05000 - Train loss: 17.90475 - Test loss: 53.98980\n",
      "Epoch 3874 - lr: 0.05000 - Train loss: 17.90475 - Test loss: 53.98977\n",
      "Epoch 3875 - lr: 0.05000 - Train loss: 17.90475 - Test loss: 53.98974\n",
      "Epoch 3876 - lr: 0.05000 - Train loss: 17.90474 - Test loss: 53.98972\n",
      "Epoch 3877 - lr: 0.05000 - Train loss: 17.90474 - Test loss: 53.98969\n",
      "Epoch 3878 - lr: 0.05000 - Train loss: 17.90474 - Test loss: 53.98966\n",
      "Epoch 3879 - lr: 0.05000 - Train loss: 17.90473 - Test loss: 53.98963\n",
      "Epoch 3880 - lr: 0.05000 - Train loss: 17.90473 - Test loss: 53.98960\n",
      "Epoch 3881 - lr: 0.05000 - Train loss: 17.90473 - Test loss: 53.98957\n",
      "Epoch 3882 - lr: 0.05000 - Train loss: 17.90472 - Test loss: 53.98954\n",
      "Epoch 3883 - lr: 0.05000 - Train loss: 17.90472 - Test loss: 53.98951\n",
      "Epoch 3884 - lr: 0.05000 - Train loss: 17.90472 - Test loss: 53.98948\n",
      "Epoch 3885 - lr: 0.05000 - Train loss: 17.90471 - Test loss: 53.98945\n",
      "Epoch 3886 - lr: 0.05000 - Train loss: 17.90471 - Test loss: 53.98942\n",
      "Epoch 3887 - lr: 0.05000 - Train loss: 17.90471 - Test loss: 53.98939\n",
      "Epoch 3888 - lr: 0.05000 - Train loss: 17.90470 - Test loss: 53.98936\n",
      "Epoch 3889 - lr: 0.05000 - Train loss: 17.90470 - Test loss: 53.98933\n",
      "Epoch 3890 - lr: 0.05000 - Train loss: 17.90470 - Test loss: 53.98930\n",
      "Epoch 3891 - lr: 0.05000 - Train loss: 17.90470 - Test loss: 53.98927\n",
      "Epoch 3892 - lr: 0.05000 - Train loss: 17.90469 - Test loss: 53.98924\n",
      "Epoch 3893 - lr: 0.05000 - Train loss: 17.90469 - Test loss: 53.98921\n",
      "Epoch 3894 - lr: 0.05000 - Train loss: 17.90469 - Test loss: 53.98918\n",
      "Epoch 3895 - lr: 0.05000 - Train loss: 17.90468 - Test loss: 53.98915\n",
      "Epoch 3896 - lr: 0.05000 - Train loss: 17.90468 - Test loss: 53.98912\n",
      "Epoch 3897 - lr: 0.05000 - Train loss: 17.90468 - Test loss: 53.98909\n",
      "Epoch 3898 - lr: 0.05000 - Train loss: 17.90467 - Test loss: 53.98906\n",
      "Epoch 3899 - lr: 0.05000 - Train loss: 17.90467 - Test loss: 53.98903\n",
      "Epoch 3900 - lr: 0.05000 - Train loss: 17.90467 - Test loss: 53.98900\n",
      "Epoch 3901 - lr: 0.05000 - Train loss: 17.90466 - Test loss: 53.98897\n",
      "Epoch 3902 - lr: 0.05000 - Train loss: 17.90466 - Test loss: 53.98894\n",
      "Epoch 3903 - lr: 0.05000 - Train loss: 17.90466 - Test loss: 53.98891\n",
      "Epoch 3904 - lr: 0.05000 - Train loss: 17.90466 - Test loss: 53.98888\n",
      "Epoch 3905 - lr: 0.05000 - Train loss: 17.90465 - Test loss: 53.98885\n",
      "Epoch 3906 - lr: 0.05000 - Train loss: 17.90465 - Test loss: 53.98882\n",
      "Epoch 3907 - lr: 0.05000 - Train loss: 17.90465 - Test loss: 53.98879\n",
      "Epoch 3908 - lr: 0.05000 - Train loss: 17.90464 - Test loss: 53.98876\n",
      "Epoch 3909 - lr: 0.05000 - Train loss: 17.90464 - Test loss: 53.98873\n",
      "Epoch 3910 - lr: 0.05000 - Train loss: 17.90464 - Test loss: 53.98870\n",
      "Epoch 3911 - lr: 0.05000 - Train loss: 17.90463 - Test loss: 53.98867\n",
      "Epoch 3912 - lr: 0.05000 - Train loss: 17.90463 - Test loss: 53.98864\n",
      "Epoch 3913 - lr: 0.05000 - Train loss: 17.90463 - Test loss: 53.98861\n",
      "Epoch 3914 - lr: 0.05000 - Train loss: 17.90462 - Test loss: 53.98858\n",
      "Epoch 3915 - lr: 0.05000 - Train loss: 17.90462 - Test loss: 53.98855\n",
      "Epoch 3916 - lr: 0.05000 - Train loss: 17.90462 - Test loss: 53.98852\n",
      "Epoch 3917 - lr: 0.05000 - Train loss: 17.90462 - Test loss: 53.98849\n",
      "Epoch 3918 - lr: 0.05000 - Train loss: 17.90461 - Test loss: 53.98847\n",
      "Epoch 3919 - lr: 0.05000 - Train loss: 17.90461 - Test loss: 53.98844\n",
      "Epoch 3920 - lr: 0.05000 - Train loss: 17.90461 - Test loss: 53.98841\n",
      "Epoch 3921 - lr: 0.05000 - Train loss: 17.90460 - Test loss: 53.98838\n",
      "Epoch 3922 - lr: 0.05000 - Train loss: 17.90460 - Test loss: 53.98835\n",
      "Epoch 3923 - lr: 0.05000 - Train loss: 17.90460 - Test loss: 53.98832\n",
      "Epoch 3924 - lr: 0.05000 - Train loss: 17.90459 - Test loss: 53.98829\n",
      "Epoch 3925 - lr: 0.05000 - Train loss: 17.90459 - Test loss: 53.98826\n",
      "Epoch 3926 - lr: 0.05000 - Train loss: 17.90459 - Test loss: 53.98824\n",
      "Epoch 3927 - lr: 0.05000 - Train loss: 17.90458 - Test loss: 53.98821\n",
      "Epoch 3928 - lr: 0.05000 - Train loss: 17.90458 - Test loss: 53.98818\n",
      "Epoch 3929 - lr: 0.05000 - Train loss: 17.90458 - Test loss: 53.98815\n",
      "Epoch 3930 - lr: 0.05000 - Train loss: 17.90457 - Test loss: 53.98812\n",
      "Epoch 3931 - lr: 0.05000 - Train loss: 17.90457 - Test loss: 53.98810\n",
      "Epoch 3932 - lr: 0.05000 - Train loss: 17.90457 - Test loss: 53.98807\n",
      "Epoch 3933 - lr: 0.05000 - Train loss: 17.90457 - Test loss: 53.98804\n",
      "Epoch 3934 - lr: 0.05000 - Train loss: 17.90456 - Test loss: 53.98802\n",
      "Epoch 3935 - lr: 0.05000 - Train loss: 17.90456 - Test loss: 53.98799\n",
      "Epoch 3936 - lr: 0.05000 - Train loss: 17.90456 - Test loss: 53.98796\n",
      "Epoch 3937 - lr: 0.05000 - Train loss: 17.90455 - Test loss: 53.98794\n",
      "Epoch 3938 - lr: 0.05000 - Train loss: 17.90455 - Test loss: 53.98791\n",
      "Epoch 3939 - lr: 0.05000 - Train loss: 17.90455 - Test loss: 53.98788\n",
      "Epoch 3940 - lr: 0.05000 - Train loss: 17.90454 - Test loss: 53.98786\n",
      "Epoch 3941 - lr: 0.05000 - Train loss: 17.90454 - Test loss: 53.98783\n",
      "Epoch 3942 - lr: 0.05000 - Train loss: 17.90454 - Test loss: 53.98781\n",
      "Epoch 3943 - lr: 0.05000 - Train loss: 17.90453 - Test loss: 53.98778\n",
      "Epoch 3944 - lr: 0.05000 - Train loss: 17.90453 - Test loss: 53.98776\n",
      "Epoch 3945 - lr: 0.05000 - Train loss: 17.90452 - Test loss: 53.98773\n",
      "Epoch 3946 - lr: 0.05000 - Train loss: 17.90452 - Test loss: 53.98771\n",
      "Epoch 3947 - lr: 0.05000 - Train loss: 17.90452 - Test loss: 53.98769\n",
      "Epoch 3948 - lr: 0.05000 - Train loss: 17.90451 - Test loss: 53.98766\n",
      "Epoch 3949 - lr: 0.05000 - Train loss: 17.90451 - Test loss: 53.98764\n",
      "Epoch 3950 - lr: 0.05000 - Train loss: 17.90451 - Test loss: 53.98762\n",
      "Epoch 3951 - lr: 0.05000 - Train loss: 17.90450 - Test loss: 53.98759\n",
      "Epoch 3952 - lr: 0.05000 - Train loss: 17.90450 - Test loss: 53.98757\n",
      "Epoch 3953 - lr: 0.05000 - Train loss: 17.90450 - Test loss: 53.98755\n",
      "Epoch 3954 - lr: 0.05000 - Train loss: 17.90449 - Test loss: 53.98753\n",
      "Epoch 3955 - lr: 0.05000 - Train loss: 17.90449 - Test loss: 53.98751\n",
      "Epoch 3956 - lr: 0.05000 - Train loss: 17.90448 - Test loss: 53.98749\n",
      "Epoch 3957 - lr: 0.05000 - Train loss: 17.90448 - Test loss: 53.98747\n",
      "Epoch 3958 - lr: 0.05000 - Train loss: 17.90448 - Test loss: 53.98745\n",
      "Epoch 3959 - lr: 0.05000 - Train loss: 17.90447 - Test loss: 53.98743\n",
      "Epoch 3960 - lr: 0.05000 - Train loss: 17.90447 - Test loss: 53.98741\n",
      "Epoch 3961 - lr: 0.05000 - Train loss: 17.90446 - Test loss: 53.98739\n",
      "Epoch 3962 - lr: 0.05000 - Train loss: 17.90446 - Test loss: 53.98737\n",
      "Epoch 3963 - lr: 0.05000 - Train loss: 17.90445 - Test loss: 53.98736\n",
      "Epoch 3964 - lr: 0.05000 - Train loss: 17.90445 - Test loss: 53.98734\n",
      "Epoch 3965 - lr: 0.05000 - Train loss: 17.90445 - Test loss: 53.98732\n",
      "Epoch 3966 - lr: 0.05000 - Train loss: 17.90444 - Test loss: 53.98731\n",
      "Epoch 3967 - lr: 0.05000 - Train loss: 17.90444 - Test loss: 53.98729\n",
      "Epoch 3968 - lr: 0.05000 - Train loss: 17.90443 - Test loss: 53.98728\n",
      "Epoch 3969 - lr: 0.05000 - Train loss: 17.90443 - Test loss: 53.98727\n",
      "Epoch 3970 - lr: 0.05000 - Train loss: 17.90442 - Test loss: 53.98725\n",
      "Epoch 3971 - lr: 0.05000 - Train loss: 17.90442 - Test loss: 53.98724\n",
      "Epoch 3972 - lr: 0.05000 - Train loss: 17.90441 - Test loss: 53.98723\n",
      "Epoch 3973 - lr: 0.05000 - Train loss: 17.90441 - Test loss: 53.98722\n",
      "Epoch 3974 - lr: 0.05000 - Train loss: 17.90440 - Test loss: 53.98721\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3975 - lr: 0.05000 - Train loss: 17.90440 - Test loss: 53.98720\n",
      "Epoch 3976 - lr: 0.05000 - Train loss: 17.90439 - Test loss: 53.98719\n",
      "Epoch 3977 - lr: 0.05000 - Train loss: 17.90439 - Test loss: 53.98719\n",
      "Epoch 3978 - lr: 0.05000 - Train loss: 17.90438 - Test loss: 53.98718\n",
      "Epoch 3979 - lr: 0.05000 - Train loss: 17.90437 - Test loss: 53.98717\n",
      "Epoch 3980 - lr: 0.05000 - Train loss: 17.90437 - Test loss: 53.98717\n",
      "Epoch 3981 - lr: 0.05000 - Train loss: 17.90436 - Test loss: 53.98717\n",
      "Epoch 3982 - lr: 0.05000 - Train loss: 17.90436 - Test loss: 53.98717\n",
      "Epoch 3983 - lr: 0.05000 - Train loss: 17.90435 - Test loss: 53.98717\n",
      "Epoch 3984 - lr: 0.05000 - Train loss: 17.90434 - Test loss: 53.98717\n",
      "Epoch 3985 - lr: 0.05000 - Train loss: 17.90434 - Test loss: 53.98717\n",
      "Epoch 3986 - lr: 0.05000 - Train loss: 17.90433 - Test loss: 53.98717\n",
      "Epoch 3987 - lr: 0.05000 - Train loss: 17.90432 - Test loss: 53.98718\n",
      "Epoch 3988 - lr: 0.05000 - Train loss: 17.90432 - Test loss: 53.98719\n",
      "Epoch 3989 - lr: 0.05000 - Train loss: 17.90431 - Test loss: 53.98719\n",
      "Epoch 3990 - lr: 0.05000 - Train loss: 17.90430 - Test loss: 53.98720\n",
      "Epoch 3991 - lr: 0.05000 - Train loss: 17.90429 - Test loss: 53.98722\n",
      "Epoch 3992 - lr: 0.05000 - Train loss: 17.90429 - Test loss: 53.98723\n",
      "Epoch 3993 - lr: 0.05000 - Train loss: 17.90428 - Test loss: 53.98725\n",
      "Epoch 3994 - lr: 0.05000 - Train loss: 17.90427 - Test loss: 53.98727\n",
      "Epoch 3995 - lr: 0.05000 - Train loss: 17.90426 - Test loss: 53.98729\n",
      "Epoch 3996 - lr: 0.05000 - Train loss: 17.90425 - Test loss: 53.98731\n",
      "Epoch 3997 - lr: 0.05000 - Train loss: 17.90424 - Test loss: 53.98734\n",
      "Epoch 3998 - lr: 0.05000 - Train loss: 17.90423 - Test loss: 53.98736\n",
      "Epoch 3999 - lr: 0.05000 - Train loss: 17.90422 - Test loss: 53.98740\n",
      "Epoch 4000 - lr: 0.05000 - Train loss: 17.90421 - Test loss: 53.98743\n",
      "Epoch 4001 - lr: 0.05000 - Train loss: 17.90420 - Test loss: 53.98747\n",
      "Epoch 4002 - lr: 0.05000 - Train loss: 17.90419 - Test loss: 53.98751\n",
      "Epoch 4003 - lr: 0.05000 - Train loss: 17.90418 - Test loss: 53.98756\n",
      "Epoch 4004 - lr: 0.05000 - Train loss: 17.90417 - Test loss: 53.98761\n",
      "Epoch 4005 - lr: 0.05000 - Train loss: 17.90415 - Test loss: 53.98766\n",
      "Epoch 4006 - lr: 0.05000 - Train loss: 17.90414 - Test loss: 53.98772\n",
      "Epoch 4007 - lr: 0.05000 - Train loss: 17.90413 - Test loss: 53.98778\n",
      "Epoch 4008 - lr: 0.05000 - Train loss: 17.90411 - Test loss: 53.98785\n",
      "Epoch 4009 - lr: 0.05000 - Train loss: 17.90410 - Test loss: 53.98793\n",
      "Epoch 4010 - lr: 0.05000 - Train loss: 17.90408 - Test loss: 53.98801\n",
      "Epoch 4011 - lr: 0.05000 - Train loss: 17.90407 - Test loss: 53.98810\n",
      "Epoch 4012 - lr: 0.05000 - Train loss: 17.90405 - Test loss: 53.98820\n",
      "Epoch 4013 - lr: 0.05000 - Train loss: 17.90403 - Test loss: 53.98831\n",
      "Epoch 4014 - lr: 0.05000 - Train loss: 17.90401 - Test loss: 53.98842\n",
      "Epoch 4015 - lr: 0.05000 - Train loss: 17.90399 - Test loss: 53.98855\n",
      "Epoch 4016 - lr: 0.05000 - Train loss: 17.90397 - Test loss: 53.98869\n",
      "Epoch 4017 - lr: 0.05000 - Train loss: 17.90394 - Test loss: 53.98884\n",
      "Epoch 4018 - lr: 0.05000 - Train loss: 17.90392 - Test loss: 53.98900\n",
      "Epoch 4019 - lr: 0.05000 - Train loss: 17.90389 - Test loss: 53.98918\n",
      "Epoch 4020 - lr: 0.05000 - Train loss: 17.90386 - Test loss: 53.98938\n",
      "Epoch 4021 - lr: 0.05000 - Train loss: 17.90383 - Test loss: 53.98959\n",
      "Epoch 4022 - lr: 0.05000 - Train loss: 17.90379 - Test loss: 53.98983\n",
      "Epoch 4023 - lr: 0.05000 - Train loss: 17.90376 - Test loss: 53.99009\n",
      "Epoch 4024 - lr: 0.05000 - Train loss: 17.90371 - Test loss: 53.99039\n",
      "Epoch 4025 - lr: 0.05000 - Train loss: 17.90367 - Test loss: 53.99071\n",
      "Epoch 4026 - lr: 0.05000 - Train loss: 17.90362 - Test loss: 53.99107\n",
      "Epoch 4027 - lr: 0.05000 - Train loss: 17.90357 - Test loss: 53.99147\n",
      "Epoch 4028 - lr: 0.05000 - Train loss: 17.90350 - Test loss: 53.99193\n",
      "Epoch 4029 - lr: 0.05000 - Train loss: 17.90344 - Test loss: 53.99244\n",
      "Epoch 4030 - lr: 0.05000 - Train loss: 17.90336 - Test loss: 53.99303\n",
      "Epoch 4031 - lr: 0.05000 - Train loss: 17.90327 - Test loss: 53.99370\n",
      "Epoch 4032 - lr: 0.05000 - Train loss: 17.90317 - Test loss: 53.99448\n",
      "Epoch 4033 - lr: 0.05000 - Train loss: 17.90305 - Test loss: 53.99538\n",
      "Epoch 4034 - lr: 0.05000 - Train loss: 17.90291 - Test loss: 53.99645\n",
      "Epoch 4035 - lr: 0.05000 - Train loss: 17.90274 - Test loss: 53.99772\n",
      "Epoch 4036 - lr: 0.05000 - Train loss: 17.90254 - Test loss: 53.99926\n",
      "Epoch 4037 - lr: 0.05000 - Train loss: 17.90229 - Test loss: 54.00115\n",
      "Epoch 4038 - lr: 0.05000 - Train loss: 17.90197 - Test loss: 54.00353\n",
      "Epoch 4039 - lr: 0.05000 - Train loss: 17.90154 - Test loss: 54.00659\n",
      "Epoch 4040 - lr: 0.05000 - Train loss: 17.90096 - Test loss: 54.01069\n",
      "Epoch 4041 - lr: 0.05000 - Train loss: 17.90011 - Test loss: 54.01644\n",
      "Epoch 4042 - lr: 0.05000 - Train loss: 17.89872 - Test loss: 54.02511\n",
      "Epoch 4043 - lr: 0.05000 - Train loss: 17.89592 - Test loss: 54.04041\n",
      "Epoch 4044 - lr: 0.05000 - Train loss: 14.13177 - Test loss: 89.76044\n",
      "Epoch 4045 - lr: 0.05000 - Train loss: 16.86373 - Test loss: 53.95862\n",
      "Epoch 4046 - lr: 0.05000 - Train loss: 17.89667 - Test loss: 53.98557\n",
      "Epoch 4047 - lr: 0.05000 - Train loss: 17.90359 - Test loss: 53.98596\n",
      "Epoch 4048 - lr: 0.05000 - Train loss: 17.90407 - Test loss: 53.98602\n",
      "Epoch 4049 - lr: 0.05000 - Train loss: 17.90412 - Test loss: 53.98605\n",
      "Epoch 4050 - lr: 0.05000 - Train loss: 17.90412 - Test loss: 53.98608\n",
      "Epoch 4051 - lr: 0.05000 - Train loss: 17.90411 - Test loss: 53.98612\n",
      "Epoch 4052 - lr: 0.05000 - Train loss: 17.90410 - Test loss: 53.98616\n",
      "Epoch 4053 - lr: 0.05000 - Train loss: 17.90409 - Test loss: 53.98620\n",
      "Epoch 4054 - lr: 0.05000 - Train loss: 17.90407 - Test loss: 53.98624\n",
      "Epoch 4055 - lr: 0.05000 - Train loss: 17.90406 - Test loss: 53.98630\n",
      "Epoch 4056 - lr: 0.05000 - Train loss: 17.90405 - Test loss: 53.98635\n",
      "Epoch 4057 - lr: 0.05000 - Train loss: 17.90403 - Test loss: 53.98642\n",
      "Epoch 4058 - lr: 0.05000 - Train loss: 17.90402 - Test loss: 53.98649\n",
      "Epoch 4059 - lr: 0.05000 - Train loss: 17.90400 - Test loss: 53.98656\n",
      "Epoch 4060 - lr: 0.05000 - Train loss: 17.90398 - Test loss: 53.98664\n",
      "Epoch 4061 - lr: 0.05000 - Train loss: 17.90397 - Test loss: 53.98674\n",
      "Epoch 4062 - lr: 0.05000 - Train loss: 17.90395 - Test loss: 53.98684\n",
      "Epoch 4063 - lr: 0.05000 - Train loss: 17.90393 - Test loss: 53.98695\n",
      "Epoch 4064 - lr: 0.05000 - Train loss: 17.90391 - Test loss: 53.98707\n",
      "Epoch 4065 - lr: 0.05000 - Train loss: 17.90388 - Test loss: 53.98720\n",
      "Epoch 4066 - lr: 0.05000 - Train loss: 17.90386 - Test loss: 53.98735\n",
      "Epoch 4067 - lr: 0.05000 - Train loss: 17.90383 - Test loss: 53.98751\n",
      "Epoch 4068 - lr: 0.05000 - Train loss: 17.90380 - Test loss: 53.98769\n",
      "Epoch 4069 - lr: 0.05000 - Train loss: 17.90377 - Test loss: 53.98789\n",
      "Epoch 4070 - lr: 0.05000 - Train loss: 17.90374 - Test loss: 53.98811\n",
      "Epoch 4071 - lr: 0.05000 - Train loss: 17.90370 - Test loss: 53.98835\n",
      "Epoch 4072 - lr: 0.05000 - Train loss: 17.90366 - Test loss: 53.98862\n",
      "Epoch 4073 - lr: 0.05000 - Train loss: 17.90362 - Test loss: 53.98892\n",
      "Epoch 4074 - lr: 0.05000 - Train loss: 17.90357 - Test loss: 53.98926\n",
      "Epoch 4075 - lr: 0.05000 - Train loss: 17.90351 - Test loss: 53.98964\n",
      "Epoch 4076 - lr: 0.05000 - Train loss: 17.90345 - Test loss: 53.99007\n",
      "Epoch 4077 - lr: 0.05000 - Train loss: 17.90338 - Test loss: 53.99056\n",
      "Epoch 4078 - lr: 0.05000 - Train loss: 17.90330 - Test loss: 53.99112\n",
      "Epoch 4079 - lr: 0.05000 - Train loss: 17.90322 - Test loss: 53.99177\n",
      "Epoch 4080 - lr: 0.05000 - Train loss: 17.90311 - Test loss: 53.99252\n",
      "Epoch 4081 - lr: 0.05000 - Train loss: 17.90299 - Test loss: 53.99339\n",
      "Epoch 4082 - lr: 0.05000 - Train loss: 17.90285 - Test loss: 53.99443\n",
      "Epoch 4083 - lr: 0.05000 - Train loss: 17.90267 - Test loss: 53.99567\n",
      "Epoch 4084 - lr: 0.05000 - Train loss: 17.90246 - Test loss: 53.99717\n",
      "Epoch 4085 - lr: 0.05000 - Train loss: 17.90220 - Test loss: 53.99904\n",
      "Epoch 4086 - lr: 0.05000 - Train loss: 17.90186 - Test loss: 54.00139\n",
      "Epoch 4087 - lr: 0.05000 - Train loss: 17.90141 - Test loss: 54.00445\n",
      "Epoch 4088 - lr: 0.05000 - Train loss: 17.90079 - Test loss: 54.00857\n",
      "Epoch 4089 - lr: 0.05000 - Train loss: 17.89986 - Test loss: 54.01441\n",
      "Epoch 4090 - lr: 0.05000 - Train loss: 17.89831 - Test loss: 54.02343\n",
      "Epoch 4091 - lr: 0.05000 - Train loss: 17.89489 - Test loss: 54.04045\n",
      "Epoch 4092 - lr: 0.05000 - Train loss: 15.68559 - Test loss: 60.21565\n",
      "Epoch 4093 - lr: 0.05000 - Train loss: 18.58158 - Test loss: 53.98579\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4094 - lr: 0.05000 - Train loss: 17.90836 - Test loss: 53.98041\n",
      "Epoch 4095 - lr: 0.05000 - Train loss: 17.90543 - Test loss: 53.98197\n",
      "Epoch 4096 - lr: 0.05000 - Train loss: 17.90478 - Test loss: 53.98349\n",
      "Epoch 4097 - lr: 0.05000 - Train loss: 17.90437 - Test loss: 53.98503\n",
      "Epoch 4098 - lr: 0.05000 - Train loss: 17.90399 - Test loss: 53.98664\n",
      "Epoch 4099 - lr: 0.05000 - Train loss: 17.90359 - Test loss: 53.98843\n",
      "Epoch 4100 - lr: 0.05000 - Train loss: 17.90315 - Test loss: 53.99050\n",
      "Epoch 4101 - lr: 0.05000 - Train loss: 17.90262 - Test loss: 53.99302\n",
      "Epoch 4102 - lr: 0.05000 - Train loss: 17.90193 - Test loss: 53.99626\n",
      "Epoch 4103 - lr: 0.05000 - Train loss: 17.90093 - Test loss: 54.00072\n",
      "Epoch 4104 - lr: 0.05000 - Train loss: 17.89925 - Test loss: 54.00766\n",
      "Epoch 4105 - lr: 0.05000 - Train loss: 17.89490 - Test loss: 54.02326\n",
      "Epoch 4106 - lr: 0.05000 - Train loss: 15.29468 - Test loss: 61.47167\n",
      "Epoch 4107 - lr: 0.05000 - Train loss: 13.75658 - Test loss: 90.43458\n",
      "Epoch 4108 - lr: 0.05000 - Train loss: 18.05459 - Test loss: 53.96098\n",
      "Epoch 4109 - lr: 0.05000 - Train loss: 17.90821 - Test loss: 53.96936\n",
      "Epoch 4110 - lr: 0.05000 - Train loss: 17.90721 - Test loss: 53.97083\n",
      "Epoch 4111 - lr: 0.05000 - Train loss: 17.90664 - Test loss: 53.97199\n",
      "Epoch 4112 - lr: 0.05000 - Train loss: 17.90630 - Test loss: 53.97291\n",
      "Epoch 4113 - lr: 0.05000 - Train loss: 17.90604 - Test loss: 53.97368\n",
      "Epoch 4114 - lr: 0.05000 - Train loss: 17.90584 - Test loss: 53.97432\n",
      "Epoch 4115 - lr: 0.05000 - Train loss: 17.90567 - Test loss: 53.97488\n",
      "Epoch 4116 - lr: 0.05000 - Train loss: 17.90553 - Test loss: 53.97537\n",
      "Epoch 4117 - lr: 0.05000 - Train loss: 17.90541 - Test loss: 53.97581\n",
      "Epoch 4118 - lr: 0.05000 - Train loss: 17.90530 - Test loss: 53.97620\n",
      "Epoch 4119 - lr: 0.05000 - Train loss: 17.90521 - Test loss: 53.97656\n",
      "Epoch 4120 - lr: 0.05000 - Train loss: 17.90512 - Test loss: 53.97690\n",
      "Epoch 4121 - lr: 0.05000 - Train loss: 17.90504 - Test loss: 53.97722\n",
      "Epoch 4122 - lr: 0.05000 - Train loss: 17.90497 - Test loss: 53.97752\n",
      "Epoch 4123 - lr: 0.05000 - Train loss: 17.90491 - Test loss: 53.97781\n",
      "Epoch 4124 - lr: 0.05000 - Train loss: 17.90485 - Test loss: 53.97808\n",
      "Epoch 4125 - lr: 0.05000 - Train loss: 17.90479 - Test loss: 53.97836\n",
      "Epoch 4126 - lr: 0.05000 - Train loss: 17.90473 - Test loss: 53.97863\n",
      "Epoch 4127 - lr: 0.05000 - Train loss: 17.90468 - Test loss: 53.97889\n",
      "Epoch 4128 - lr: 0.05000 - Train loss: 17.90463 - Test loss: 53.97916\n",
      "Epoch 4129 - lr: 0.05000 - Train loss: 17.90457 - Test loss: 53.97944\n",
      "Epoch 4130 - lr: 0.05000 - Train loss: 17.90452 - Test loss: 53.97972\n",
      "Epoch 4131 - lr: 0.05000 - Train loss: 17.90447 - Test loss: 53.98000\n",
      "Epoch 4132 - lr: 0.05000 - Train loss: 17.90442 - Test loss: 53.98030\n",
      "Epoch 4133 - lr: 0.05000 - Train loss: 17.90437 - Test loss: 53.98062\n",
      "Epoch 4134 - lr: 0.05000 - Train loss: 17.90432 - Test loss: 53.98095\n",
      "Epoch 4135 - lr: 0.05000 - Train loss: 17.90426 - Test loss: 53.98130\n",
      "Epoch 4136 - lr: 0.05000 - Train loss: 17.90420 - Test loss: 53.98168\n",
      "Epoch 4137 - lr: 0.05000 - Train loss: 17.90414 - Test loss: 53.98210\n",
      "Epoch 4138 - lr: 0.05000 - Train loss: 17.90408 - Test loss: 53.98254\n",
      "Epoch 4139 - lr: 0.05000 - Train loss: 17.90401 - Test loss: 53.98303\n",
      "Epoch 4140 - lr: 0.05000 - Train loss: 17.90393 - Test loss: 53.98358\n",
      "Epoch 4141 - lr: 0.05000 - Train loss: 17.90385 - Test loss: 53.98419\n",
      "Epoch 4142 - lr: 0.05000 - Train loss: 17.90375 - Test loss: 53.98488\n",
      "Epoch 4143 - lr: 0.05000 - Train loss: 17.90365 - Test loss: 53.98566\n",
      "Epoch 4144 - lr: 0.05000 - Train loss: 17.90353 - Test loss: 53.98656\n",
      "Epoch 4145 - lr: 0.05000 - Train loss: 17.90339 - Test loss: 53.98761\n",
      "Epoch 4146 - lr: 0.05000 - Train loss: 17.90323 - Test loss: 53.98885\n",
      "Epoch 4147 - lr: 0.05000 - Train loss: 17.90303 - Test loss: 53.99033\n",
      "Epoch 4148 - lr: 0.05000 - Train loss: 17.90279 - Test loss: 53.99214\n",
      "Epoch 4149 - lr: 0.05000 - Train loss: 17.90249 - Test loss: 53.99439\n",
      "Epoch 4150 - lr: 0.05000 - Train loss: 17.90209 - Test loss: 53.99727\n",
      "Epoch 4151 - lr: 0.05000 - Train loss: 17.90156 - Test loss: 54.00106\n",
      "Epoch 4152 - lr: 0.05000 - Train loss: 17.90080 - Test loss: 54.00630\n",
      "Epoch 4153 - lr: 0.05000 - Train loss: 17.89961 - Test loss: 54.01399\n",
      "Epoch 4154 - lr: 0.05000 - Train loss: 17.89742 - Test loss: 54.02666\n",
      "Epoch 4155 - lr: 0.05000 - Train loss: 17.89022 - Test loss: 54.05929\n",
      "Epoch 4156 - lr: 0.05000 - Train loss: 12.88586 - Test loss: 90.07830\n",
      "Epoch 4157 - lr: 0.05000 - Train loss: 18.48234 - Test loss: 53.97449\n",
      "Epoch 4158 - lr: 0.05000 - Train loss: 17.90586 - Test loss: 53.97658\n",
      "Epoch 4159 - lr: 0.05000 - Train loss: 17.90477 - Test loss: 53.97657\n",
      "Epoch 4160 - lr: 0.05000 - Train loss: 17.90460 - Test loss: 53.97658\n",
      "Epoch 4161 - lr: 0.05000 - Train loss: 17.90457 - Test loss: 53.97658\n",
      "Epoch 4162 - lr: 0.05000 - Train loss: 17.90456 - Test loss: 53.97659\n",
      "Epoch 4163 - lr: 0.05000 - Train loss: 17.90455 - Test loss: 53.97659\n",
      "Epoch 4164 - lr: 0.05000 - Train loss: 17.90454 - Test loss: 53.97660\n",
      "Epoch 4165 - lr: 0.05000 - Train loss: 17.90454 - Test loss: 53.97660\n",
      "Epoch 4166 - lr: 0.05000 - Train loss: 17.90453 - Test loss: 53.97661\n",
      "Epoch 4167 - lr: 0.05000 - Train loss: 17.90452 - Test loss: 53.97661\n",
      "Epoch 4168 - lr: 0.05000 - Train loss: 17.90451 - Test loss: 53.97661\n",
      "Epoch 4169 - lr: 0.05000 - Train loss: 17.90451 - Test loss: 53.97661\n",
      "Epoch 4170 - lr: 0.05000 - Train loss: 17.90450 - Test loss: 53.97662\n",
      "Epoch 4171 - lr: 0.05000 - Train loss: 17.90449 - Test loss: 53.97662\n",
      "Epoch 4172 - lr: 0.05000 - Train loss: 17.90449 - Test loss: 53.97662\n",
      "Epoch 4173 - lr: 0.05000 - Train loss: 17.90448 - Test loss: 53.97662\n",
      "Epoch 4174 - lr: 0.05000 - Train loss: 17.90447 - Test loss: 53.97662\n",
      "Epoch 4175 - lr: 0.05000 - Train loss: 17.90447 - Test loss: 53.97662\n",
      "Epoch 4176 - lr: 0.05000 - Train loss: 17.90446 - Test loss: 53.97662\n",
      "Epoch 4177 - lr: 0.05000 - Train loss: 17.90445 - Test loss: 53.97662\n",
      "Epoch 4178 - lr: 0.05000 - Train loss: 17.90445 - Test loss: 53.97662\n",
      "Epoch 4179 - lr: 0.05000 - Train loss: 17.90444 - Test loss: 53.97662\n",
      "Epoch 4180 - lr: 0.05000 - Train loss: 17.90443 - Test loss: 53.97662\n",
      "Epoch 4181 - lr: 0.05000 - Train loss: 17.90443 - Test loss: 53.97662\n",
      "Epoch 4182 - lr: 0.05000 - Train loss: 17.90442 - Test loss: 53.97662\n",
      "Epoch 4183 - lr: 0.05000 - Train loss: 17.90442 - Test loss: 53.97662\n",
      "Epoch 4184 - lr: 0.05000 - Train loss: 17.90441 - Test loss: 53.97662\n",
      "Epoch 4185 - lr: 0.05000 - Train loss: 17.90440 - Test loss: 53.97662\n",
      "Epoch 4186 - lr: 0.05000 - Train loss: 17.90440 - Test loss: 53.97662\n",
      "Epoch 4187 - lr: 0.05000 - Train loss: 17.90439 - Test loss: 53.97662\n",
      "Epoch 4188 - lr: 0.05000 - Train loss: 17.90438 - Test loss: 53.97663\n",
      "Epoch 4189 - lr: 0.05000 - Train loss: 17.90438 - Test loss: 53.97663\n",
      "Epoch 4190 - lr: 0.05000 - Train loss: 17.90437 - Test loss: 53.97663\n",
      "Epoch 4191 - lr: 0.05000 - Train loss: 17.90436 - Test loss: 53.97663\n",
      "Epoch 4192 - lr: 0.05000 - Train loss: 17.90436 - Test loss: 53.97663\n",
      "Epoch 4193 - lr: 0.05000 - Train loss: 17.90435 - Test loss: 53.97663\n",
      "Epoch 4194 - lr: 0.05000 - Train loss: 17.90435 - Test loss: 53.97664\n",
      "Epoch 4195 - lr: 0.05000 - Train loss: 17.90434 - Test loss: 53.97664\n",
      "Epoch 4196 - lr: 0.05000 - Train loss: 17.90433 - Test loss: 53.97664\n",
      "Epoch 4197 - lr: 0.05000 - Train loss: 17.90433 - Test loss: 53.97665\n",
      "Epoch 4198 - lr: 0.05000 - Train loss: 17.90432 - Test loss: 53.97665\n",
      "Epoch 4199 - lr: 0.05000 - Train loss: 17.90431 - Test loss: 53.97666\n",
      "Epoch 4200 - lr: 0.05000 - Train loss: 17.90431 - Test loss: 53.97666\n",
      "Epoch 4201 - lr: 0.05000 - Train loss: 17.90430 - Test loss: 53.97667\n",
      "Epoch 4202 - lr: 0.05000 - Train loss: 17.90429 - Test loss: 53.97668\n",
      "Epoch 4203 - lr: 0.05000 - Train loss: 17.90429 - Test loss: 53.97669\n",
      "Epoch 4204 - lr: 0.05000 - Train loss: 17.90428 - Test loss: 53.97670\n",
      "Epoch 4205 - lr: 0.05000 - Train loss: 17.90427 - Test loss: 53.97671\n",
      "Epoch 4206 - lr: 0.05000 - Train loss: 17.90426 - Test loss: 53.97672\n",
      "Epoch 4207 - lr: 0.05000 - Train loss: 17.90426 - Test loss: 53.97673\n",
      "Epoch 4208 - lr: 0.05000 - Train loss: 17.90425 - Test loss: 53.97674\n",
      "Epoch 4209 - lr: 0.05000 - Train loss: 17.90424 - Test loss: 53.97676\n",
      "Epoch 4210 - lr: 0.05000 - Train loss: 17.90423 - Test loss: 53.97677\n",
      "Epoch 4211 - lr: 0.05000 - Train loss: 17.90423 - Test loss: 53.97679\n",
      "Epoch 4212 - lr: 0.05000 - Train loss: 17.90422 - Test loss: 53.97681\n",
      "Epoch 4213 - lr: 0.05000 - Train loss: 17.90421 - Test loss: 53.97683\n",
      "Epoch 4214 - lr: 0.05000 - Train loss: 17.90420 - Test loss: 53.97686\n",
      "Epoch 4215 - lr: 0.05000 - Train loss: 17.90419 - Test loss: 53.97688\n",
      "Epoch 4216 - lr: 0.05000 - Train loss: 17.90418 - Test loss: 53.97691\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4217 - lr: 0.05000 - Train loss: 17.90417 - Test loss: 53.97694\n",
      "Epoch 4218 - lr: 0.05000 - Train loss: 17.90416 - Test loss: 53.97697\n",
      "Epoch 4219 - lr: 0.05000 - Train loss: 17.90415 - Test loss: 53.97700\n",
      "Epoch 4220 - lr: 0.05000 - Train loss: 17.90414 - Test loss: 53.97704\n",
      "Epoch 4221 - lr: 0.05000 - Train loss: 17.90413 - Test loss: 53.97708\n",
      "Epoch 4222 - lr: 0.05000 - Train loss: 17.90412 - Test loss: 53.97712\n",
      "Epoch 4223 - lr: 0.05000 - Train loss: 17.90411 - Test loss: 53.97717\n",
      "Epoch 4224 - lr: 0.05000 - Train loss: 17.90410 - Test loss: 53.97722\n",
      "Epoch 4225 - lr: 0.05000 - Train loss: 17.90409 - Test loss: 53.97727\n",
      "Epoch 4226 - lr: 0.05000 - Train loss: 17.90407 - Test loss: 53.97733\n",
      "Epoch 4227 - lr: 0.05000 - Train loss: 17.90406 - Test loss: 53.97740\n",
      "Epoch 4228 - lr: 0.05000 - Train loss: 17.90405 - Test loss: 53.97746\n",
      "Epoch 4229 - lr: 0.05000 - Train loss: 17.90403 - Test loss: 53.97754\n",
      "Epoch 4230 - lr: 0.05000 - Train loss: 17.90402 - Test loss: 53.97762\n",
      "Epoch 4231 - lr: 0.05000 - Train loss: 17.90400 - Test loss: 53.97771\n",
      "Epoch 4232 - lr: 0.05000 - Train loss: 17.90398 - Test loss: 53.97780\n",
      "Epoch 4233 - lr: 0.05000 - Train loss: 17.90397 - Test loss: 53.97791\n",
      "Epoch 4234 - lr: 0.05000 - Train loss: 17.90395 - Test loss: 53.97802\n",
      "Epoch 4235 - lr: 0.05000 - Train loss: 17.90393 - Test loss: 53.97814\n",
      "Epoch 4236 - lr: 0.05000 - Train loss: 17.90390 - Test loss: 53.97828\n",
      "Epoch 4237 - lr: 0.05000 - Train loss: 17.90388 - Test loss: 53.97843\n",
      "Epoch 4238 - lr: 0.05000 - Train loss: 17.90386 - Test loss: 53.97859\n",
      "Epoch 4239 - lr: 0.05000 - Train loss: 17.90383 - Test loss: 53.97876\n",
      "Epoch 4240 - lr: 0.05000 - Train loss: 17.90380 - Test loss: 53.97896\n",
      "Epoch 4241 - lr: 0.05000 - Train loss: 17.90377 - Test loss: 53.97917\n",
      "Epoch 4242 - lr: 0.05000 - Train loss: 17.90373 - Test loss: 53.97941\n",
      "Epoch 4243 - lr: 0.05000 - Train loss: 17.90370 - Test loss: 53.97967\n",
      "Epoch 4244 - lr: 0.05000 - Train loss: 17.90366 - Test loss: 53.97997\n",
      "Epoch 4245 - lr: 0.05000 - Train loss: 17.90361 - Test loss: 53.98029\n",
      "Epoch 4246 - lr: 0.05000 - Train loss: 17.90356 - Test loss: 53.98066\n",
      "Epoch 4247 - lr: 0.05000 - Train loss: 17.90350 - Test loss: 53.98107\n",
      "Epoch 4248 - lr: 0.05000 - Train loss: 17.90344 - Test loss: 53.98154\n",
      "Epoch 4249 - lr: 0.05000 - Train loss: 17.90337 - Test loss: 53.98208\n",
      "Epoch 4250 - lr: 0.05000 - Train loss: 17.90329 - Test loss: 53.98269\n",
      "Epoch 4251 - lr: 0.05000 - Train loss: 17.90319 - Test loss: 53.98340\n",
      "Epoch 4252 - lr: 0.05000 - Train loss: 17.90308 - Test loss: 53.98422\n",
      "Epoch 4253 - lr: 0.05000 - Train loss: 17.90295 - Test loss: 53.98520\n",
      "Epoch 4254 - lr: 0.05000 - Train loss: 17.90280 - Test loss: 53.98635\n",
      "Epoch 4255 - lr: 0.05000 - Train loss: 17.90261 - Test loss: 53.98775\n",
      "Epoch 4256 - lr: 0.05000 - Train loss: 17.90238 - Test loss: 53.98947\n",
      "Epoch 4257 - lr: 0.05000 - Train loss: 17.90209 - Test loss: 53.99161\n",
      "Epoch 4258 - lr: 0.05000 - Train loss: 17.90171 - Test loss: 53.99436\n",
      "Epoch 4259 - lr: 0.05000 - Train loss: 17.90119 - Test loss: 53.99799\n",
      "Epoch 4260 - lr: 0.05000 - Train loss: 17.90045 - Test loss: 54.00299\n",
      "Epoch 4261 - lr: 0.05000 - Train loss: 17.89930 - Test loss: 54.01029\n",
      "Epoch 4262 - lr: 0.05000 - Train loss: 17.89723 - Test loss: 54.02216\n",
      "Epoch 4263 - lr: 0.05000 - Train loss: 17.89122 - Test loss: 54.04975\n",
      "Epoch 4264 - lr: 0.05000 - Train loss: 14.07217 - Test loss: 65.37790\n",
      "Epoch 4265 - lr: 0.05000 - Train loss: 19.14124 - Test loss: 53.98175\n",
      "Epoch 4266 - lr: 0.05000 - Train loss: 17.91026 - Test loss: 53.97181\n",
      "Epoch 4267 - lr: 0.05000 - Train loss: 17.90499 - Test loss: 53.97200\n",
      "Epoch 4268 - lr: 0.05000 - Train loss: 17.90443 - Test loss: 53.97203\n",
      "Epoch 4269 - lr: 0.05000 - Train loss: 17.90435 - Test loss: 53.97204\n",
      "Epoch 4270 - lr: 0.05000 - Train loss: 17.90433 - Test loss: 53.97205\n",
      "Epoch 4271 - lr: 0.05000 - Train loss: 17.90432 - Test loss: 53.97207\n",
      "Epoch 4272 - lr: 0.05000 - Train loss: 17.90432 - Test loss: 53.97208\n",
      "Epoch 4273 - lr: 0.05000 - Train loss: 17.90431 - Test loss: 53.97210\n",
      "Epoch 4274 - lr: 0.05000 - Train loss: 17.90430 - Test loss: 53.97211\n",
      "Epoch 4275 - lr: 0.05000 - Train loss: 17.90430 - Test loss: 53.97213\n",
      "Epoch 4276 - lr: 0.05000 - Train loss: 17.90429 - Test loss: 53.97215\n",
      "Epoch 4277 - lr: 0.05000 - Train loss: 17.90428 - Test loss: 53.97217\n",
      "Epoch 4278 - lr: 0.05000 - Train loss: 17.90428 - Test loss: 53.97219\n",
      "Epoch 4279 - lr: 0.05000 - Train loss: 17.90427 - Test loss: 53.97221\n",
      "Epoch 4280 - lr: 0.05000 - Train loss: 17.90426 - Test loss: 53.97224\n",
      "Epoch 4281 - lr: 0.05000 - Train loss: 17.90426 - Test loss: 53.97226\n",
      "Epoch 4282 - lr: 0.05000 - Train loss: 17.90425 - Test loss: 53.97229\n",
      "Epoch 4283 - lr: 0.05000 - Train loss: 17.90424 - Test loss: 53.97232\n",
      "Epoch 4284 - lr: 0.05000 - Train loss: 17.90423 - Test loss: 53.97235\n",
      "Epoch 4285 - lr: 0.05000 - Train loss: 17.90423 - Test loss: 53.97238\n",
      "Epoch 4286 - lr: 0.05000 - Train loss: 17.90422 - Test loss: 53.97242\n",
      "Epoch 4287 - lr: 0.05000 - Train loss: 17.90421 - Test loss: 53.97245\n",
      "Epoch 4288 - lr: 0.05000 - Train loss: 17.90420 - Test loss: 53.97249\n",
      "Epoch 4289 - lr: 0.05000 - Train loss: 17.90419 - Test loss: 53.97254\n",
      "Epoch 4290 - lr: 0.05000 - Train loss: 17.90418 - Test loss: 53.97258\n",
      "Epoch 4291 - lr: 0.05000 - Train loss: 17.90417 - Test loss: 53.97263\n",
      "Epoch 4292 - lr: 0.05000 - Train loss: 17.90416 - Test loss: 53.97268\n",
      "Epoch 4293 - lr: 0.05000 - Train loss: 17.90415 - Test loss: 53.97273\n",
      "Epoch 4294 - lr: 0.05000 - Train loss: 17.90414 - Test loss: 53.97279\n",
      "Epoch 4295 - lr: 0.05000 - Train loss: 17.90413 - Test loss: 53.97286\n",
      "Epoch 4296 - lr: 0.05000 - Train loss: 17.90412 - Test loss: 53.97292\n",
      "Epoch 4297 - lr: 0.05000 - Train loss: 17.90411 - Test loss: 53.97299\n",
      "Epoch 4298 - lr: 0.05000 - Train loss: 17.90410 - Test loss: 53.97307\n",
      "Epoch 4299 - lr: 0.05000 - Train loss: 17.90409 - Test loss: 53.97315\n",
      "Epoch 4300 - lr: 0.05000 - Train loss: 17.90407 - Test loss: 53.97324\n",
      "Epoch 4301 - lr: 0.05000 - Train loss: 17.90406 - Test loss: 53.97334\n",
      "Epoch 4302 - lr: 0.05000 - Train loss: 17.90405 - Test loss: 53.97344\n",
      "Epoch 4303 - lr: 0.05000 - Train loss: 17.90403 - Test loss: 53.97356\n",
      "Epoch 4304 - lr: 0.05000 - Train loss: 17.90401 - Test loss: 53.97368\n",
      "Epoch 4305 - lr: 0.05000 - Train loss: 17.90400 - Test loss: 53.97381\n",
      "Epoch 4306 - lr: 0.05000 - Train loss: 17.90398 - Test loss: 53.97395\n",
      "Epoch 4307 - lr: 0.05000 - Train loss: 17.90396 - Test loss: 53.97410\n",
      "Epoch 4308 - lr: 0.05000 - Train loss: 17.90394 - Test loss: 53.97427\n",
      "Epoch 4309 - lr: 0.05000 - Train loss: 17.90392 - Test loss: 53.97445\n",
      "Epoch 4310 - lr: 0.05000 - Train loss: 17.90389 - Test loss: 53.97465\n",
      "Epoch 4311 - lr: 0.05000 - Train loss: 17.90387 - Test loss: 53.97487\n",
      "Epoch 4312 - lr: 0.05000 - Train loss: 17.90384 - Test loss: 53.97511\n",
      "Epoch 4313 - lr: 0.05000 - Train loss: 17.90381 - Test loss: 53.97538\n",
      "Epoch 4314 - lr: 0.05000 - Train loss: 17.90377 - Test loss: 53.97567\n",
      "Epoch 4315 - lr: 0.05000 - Train loss: 17.90374 - Test loss: 53.97600\n",
      "Epoch 4316 - lr: 0.05000 - Train loss: 17.90370 - Test loss: 53.97637\n",
      "Epoch 4317 - lr: 0.05000 - Train loss: 17.90365 - Test loss: 53.97678\n",
      "Epoch 4318 - lr: 0.05000 - Train loss: 17.90360 - Test loss: 53.97724\n",
      "Epoch 4319 - lr: 0.05000 - Train loss: 17.90354 - Test loss: 53.97776\n",
      "Epoch 4320 - lr: 0.05000 - Train loss: 17.90348 - Test loss: 53.97835\n",
      "Epoch 4321 - lr: 0.05000 - Train loss: 17.90341 - Test loss: 53.97903\n",
      "Epoch 4322 - lr: 0.05000 - Train loss: 17.90332 - Test loss: 53.97981\n",
      "Epoch 4323 - lr: 0.05000 - Train loss: 17.90322 - Test loss: 53.98073\n",
      "Epoch 4324 - lr: 0.05000 - Train loss: 17.90311 - Test loss: 53.98181\n",
      "Epoch 4325 - lr: 0.05000 - Train loss: 17.90297 - Test loss: 53.98309\n",
      "Epoch 4326 - lr: 0.05000 - Train loss: 17.90280 - Test loss: 53.98464\n",
      "Epoch 4327 - lr: 0.05000 - Train loss: 17.90259 - Test loss: 53.98654\n",
      "Epoch 4328 - lr: 0.05000 - Train loss: 17.90232 - Test loss: 53.98892\n",
      "Epoch 4329 - lr: 0.05000 - Train loss: 17.90197 - Test loss: 53.99196\n",
      "Epoch 4330 - lr: 0.05000 - Train loss: 17.90150 - Test loss: 53.99597\n",
      "Epoch 4331 - lr: 0.05000 - Train loss: 17.90083 - Test loss: 54.00146\n",
      "Epoch 4332 - lr: 0.05000 - Train loss: 17.89980 - Test loss: 54.00936\n",
      "Epoch 4333 - lr: 0.05000 - Train loss: 17.89804 - Test loss: 54.02163\n",
      "Epoch 4334 - lr: 0.05000 - Train loss: 17.89422 - Test loss: 54.04362\n",
      "Epoch 4335 - lr: 0.05000 - Train loss: 15.74539 - Test loss: 59.77002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4336 - lr: 0.05000 - Train loss: 13.19070 - Test loss: 69.20318\n",
      "Epoch 4337 - lr: 0.05000 - Train loss: 19.55238 - Test loss: 53.97380\n",
      "Epoch 4338 - lr: 0.05000 - Train loss: 17.91299 - Test loss: 53.96188\n",
      "Epoch 4339 - lr: 0.05000 - Train loss: 17.90593 - Test loss: 53.96225\n",
      "Epoch 4340 - lr: 0.05000 - Train loss: 17.90514 - Test loss: 53.96232\n",
      "Epoch 4341 - lr: 0.05000 - Train loss: 17.90502 - Test loss: 53.96238\n",
      "Epoch 4342 - lr: 0.05000 - Train loss: 17.90499 - Test loss: 53.96243\n",
      "Epoch 4343 - lr: 0.05000 - Train loss: 17.90497 - Test loss: 53.96248\n",
      "Epoch 4344 - lr: 0.05000 - Train loss: 17.90496 - Test loss: 53.96253\n",
      "Epoch 4345 - lr: 0.05000 - Train loss: 17.90494 - Test loss: 53.96258\n",
      "Epoch 4346 - lr: 0.05000 - Train loss: 17.90493 - Test loss: 53.96264\n",
      "Epoch 4347 - lr: 0.05000 - Train loss: 17.90492 - Test loss: 53.96269\n",
      "Epoch 4348 - lr: 0.05000 - Train loss: 17.90491 - Test loss: 53.96274\n",
      "Epoch 4349 - lr: 0.05000 - Train loss: 17.90489 - Test loss: 53.96279\n",
      "Epoch 4350 - lr: 0.05000 - Train loss: 17.90488 - Test loss: 53.96284\n",
      "Epoch 4351 - lr: 0.05000 - Train loss: 17.90487 - Test loss: 53.96290\n",
      "Epoch 4352 - lr: 0.05000 - Train loss: 17.90486 - Test loss: 53.96295\n",
      "Epoch 4353 - lr: 0.05000 - Train loss: 17.90485 - Test loss: 53.96300\n",
      "Epoch 4354 - lr: 0.05000 - Train loss: 17.90483 - Test loss: 53.96305\n",
      "Epoch 4355 - lr: 0.05000 - Train loss: 17.90482 - Test loss: 53.96310\n",
      "Epoch 4356 - lr: 0.05000 - Train loss: 17.90481 - Test loss: 53.96316\n",
      "Epoch 4357 - lr: 0.05000 - Train loss: 17.90480 - Test loss: 53.96321\n",
      "Epoch 4358 - lr: 0.05000 - Train loss: 17.90479 - Test loss: 53.96326\n",
      "Epoch 4359 - lr: 0.05000 - Train loss: 17.90478 - Test loss: 53.96331\n",
      "Epoch 4360 - lr: 0.05000 - Train loss: 17.90477 - Test loss: 53.96336\n",
      "Epoch 4361 - lr: 0.05000 - Train loss: 17.90476 - Test loss: 53.96342\n",
      "Epoch 4362 - lr: 0.05000 - Train loss: 17.90475 - Test loss: 53.96347\n",
      "Epoch 4363 - lr: 0.05000 - Train loss: 17.90474 - Test loss: 53.96352\n",
      "Epoch 4364 - lr: 0.05000 - Train loss: 17.90473 - Test loss: 53.96357\n",
      "Epoch 4365 - lr: 0.05000 - Train loss: 17.90472 - Test loss: 53.96362\n",
      "Epoch 4366 - lr: 0.05000 - Train loss: 17.90471 - Test loss: 53.96367\n",
      "Epoch 4367 - lr: 0.05000 - Train loss: 17.90470 - Test loss: 53.96372\n",
      "Epoch 4368 - lr: 0.05000 - Train loss: 17.90469 - Test loss: 53.96377\n",
      "Epoch 4369 - lr: 0.05000 - Train loss: 17.90468 - Test loss: 53.96382\n",
      "Epoch 4370 - lr: 0.05000 - Train loss: 17.90467 - Test loss: 53.96387\n",
      "Epoch 4371 - lr: 0.05000 - Train loss: 17.90466 - Test loss: 53.96392\n",
      "Epoch 4372 - lr: 0.05000 - Train loss: 17.90465 - Test loss: 53.96397\n",
      "Epoch 4373 - lr: 0.05000 - Train loss: 17.90464 - Test loss: 53.96402\n",
      "Epoch 4374 - lr: 0.05000 - Train loss: 17.90463 - Test loss: 53.96407\n",
      "Epoch 4375 - lr: 0.05000 - Train loss: 17.90462 - Test loss: 53.96412\n",
      "Epoch 4376 - lr: 0.05000 - Train loss: 17.90461 - Test loss: 53.96417\n",
      "Epoch 4377 - lr: 0.05000 - Train loss: 17.90460 - Test loss: 53.96422\n",
      "Epoch 4378 - lr: 0.05000 - Train loss: 17.90459 - Test loss: 53.96427\n",
      "Epoch 4379 - lr: 0.05000 - Train loss: 17.90458 - Test loss: 53.96432\n",
      "Epoch 4380 - lr: 0.05000 - Train loss: 17.90457 - Test loss: 53.96437\n",
      "Epoch 4381 - lr: 0.05000 - Train loss: 17.90457 - Test loss: 53.96442\n",
      "Epoch 4382 - lr: 0.05000 - Train loss: 17.90456 - Test loss: 53.96447\n",
      "Epoch 4383 - lr: 0.05000 - Train loss: 17.90455 - Test loss: 53.96452\n",
      "Epoch 4384 - lr: 0.05000 - Train loss: 17.90454 - Test loss: 53.96457\n",
      "Epoch 4385 - lr: 0.05000 - Train loss: 17.90453 - Test loss: 53.96462\n",
      "Epoch 4386 - lr: 0.05000 - Train loss: 17.90452 - Test loss: 53.96466\n",
      "Epoch 4387 - lr: 0.05000 - Train loss: 17.90451 - Test loss: 53.96471\n",
      "Epoch 4388 - lr: 0.05000 - Train loss: 17.90451 - Test loss: 53.96476\n",
      "Epoch 4389 - lr: 0.05000 - Train loss: 17.90450 - Test loss: 53.96481\n",
      "Epoch 4390 - lr: 0.05000 - Train loss: 17.90449 - Test loss: 53.96486\n",
      "Epoch 4391 - lr: 0.05000 - Train loss: 17.90448 - Test loss: 53.96491\n",
      "Epoch 4392 - lr: 0.05000 - Train loss: 17.90447 - Test loss: 53.96496\n",
      "Epoch 4393 - lr: 0.05000 - Train loss: 17.90447 - Test loss: 53.96501\n",
      "Epoch 4394 - lr: 0.05000 - Train loss: 17.90446 - Test loss: 53.96506\n",
      "Epoch 4395 - lr: 0.05000 - Train loss: 17.90445 - Test loss: 53.96511\n",
      "Epoch 4396 - lr: 0.05000 - Train loss: 17.90444 - Test loss: 53.96515\n",
      "Epoch 4397 - lr: 0.05000 - Train loss: 17.90443 - Test loss: 53.96520\n",
      "Epoch 4398 - lr: 0.05000 - Train loss: 17.90443 - Test loss: 53.96525\n",
      "Epoch 4399 - lr: 0.05000 - Train loss: 17.90442 - Test loss: 53.96530\n",
      "Epoch 4400 - lr: 0.05000 - Train loss: 17.90441 - Test loss: 53.96535\n",
      "Epoch 4401 - lr: 0.05000 - Train loss: 17.90440 - Test loss: 53.96540\n",
      "Epoch 4402 - lr: 0.05000 - Train loss: 17.90439 - Test loss: 53.96545\n",
      "Epoch 4403 - lr: 0.05000 - Train loss: 17.90439 - Test loss: 53.96550\n",
      "Epoch 4404 - lr: 0.05000 - Train loss: 17.90438 - Test loss: 53.96555\n",
      "Epoch 4405 - lr: 0.05000 - Train loss: 17.90437 - Test loss: 53.96560\n",
      "Epoch 4406 - lr: 0.05000 - Train loss: 17.90436 - Test loss: 53.96566\n",
      "Epoch 4407 - lr: 0.05000 - Train loss: 17.90436 - Test loss: 53.96571\n",
      "Epoch 4408 - lr: 0.05000 - Train loss: 17.90435 - Test loss: 53.96576\n",
      "Epoch 4409 - lr: 0.05000 - Train loss: 17.90434 - Test loss: 53.96581\n",
      "Epoch 4410 - lr: 0.05000 - Train loss: 17.90433 - Test loss: 53.96586\n",
      "Epoch 4411 - lr: 0.05000 - Train loss: 17.90433 - Test loss: 53.96592\n",
      "Epoch 4412 - lr: 0.05000 - Train loss: 17.90432 - Test loss: 53.96597\n",
      "Epoch 4413 - lr: 0.05000 - Train loss: 17.90431 - Test loss: 53.96603\n",
      "Epoch 4414 - lr: 0.05000 - Train loss: 17.90430 - Test loss: 53.96608\n",
      "Epoch 4415 - lr: 0.05000 - Train loss: 17.90430 - Test loss: 53.96614\n",
      "Epoch 4416 - lr: 0.05000 - Train loss: 17.90429 - Test loss: 53.96619\n",
      "Epoch 4417 - lr: 0.05000 - Train loss: 17.90428 - Test loss: 53.96625\n",
      "Epoch 4418 - lr: 0.05000 - Train loss: 17.90427 - Test loss: 53.96630\n",
      "Epoch 4419 - lr: 0.05000 - Train loss: 17.90427 - Test loss: 53.96636\n",
      "Epoch 4420 - lr: 0.05000 - Train loss: 17.90426 - Test loss: 53.96642\n",
      "Epoch 4421 - lr: 0.05000 - Train loss: 17.90425 - Test loss: 53.96648\n",
      "Epoch 4422 - lr: 0.05000 - Train loss: 17.90424 - Test loss: 53.96654\n",
      "Epoch 4423 - lr: 0.05000 - Train loss: 17.90424 - Test loss: 53.96660\n",
      "Epoch 4424 - lr: 0.05000 - Train loss: 17.90423 - Test loss: 53.96667\n",
      "Epoch 4425 - lr: 0.05000 - Train loss: 17.90422 - Test loss: 53.96673\n",
      "Epoch 4426 - lr: 0.05000 - Train loss: 17.90421 - Test loss: 53.96679\n",
      "Epoch 4427 - lr: 0.05000 - Train loss: 17.90421 - Test loss: 53.96686\n",
      "Epoch 4428 - lr: 0.05000 - Train loss: 17.90420 - Test loss: 53.96693\n",
      "Epoch 4429 - lr: 0.05000 - Train loss: 17.90419 - Test loss: 53.96700\n",
      "Epoch 4430 - lr: 0.05000 - Train loss: 17.90418 - Test loss: 53.96707\n",
      "Epoch 4431 - lr: 0.05000 - Train loss: 17.90417 - Test loss: 53.96714\n",
      "Epoch 4432 - lr: 0.05000 - Train loss: 17.90417 - Test loss: 53.96721\n",
      "Epoch 4433 - lr: 0.05000 - Train loss: 17.90416 - Test loss: 53.96728\n",
      "Epoch 4434 - lr: 0.05000 - Train loss: 17.90415 - Test loss: 53.96736\n",
      "Epoch 4435 - lr: 0.05000 - Train loss: 17.90414 - Test loss: 53.96744\n",
      "Epoch 4436 - lr: 0.05000 - Train loss: 17.90413 - Test loss: 53.96752\n",
      "Epoch 4437 - lr: 0.05000 - Train loss: 17.90413 - Test loss: 53.96760\n",
      "Epoch 4438 - lr: 0.05000 - Train loss: 17.90412 - Test loss: 53.96769\n",
      "Epoch 4439 - lr: 0.05000 - Train loss: 17.90411 - Test loss: 53.96778\n",
      "Epoch 4440 - lr: 0.05000 - Train loss: 17.90410 - Test loss: 53.96787\n",
      "Epoch 4441 - lr: 0.05000 - Train loss: 17.90409 - Test loss: 53.96796\n",
      "Epoch 4442 - lr: 0.05000 - Train loss: 17.90408 - Test loss: 53.96805\n",
      "Epoch 4443 - lr: 0.05000 - Train loss: 17.90408 - Test loss: 53.96815\n",
      "Epoch 4444 - lr: 0.05000 - Train loss: 17.90407 - Test loss: 53.96826\n",
      "Epoch 4445 - lr: 0.05000 - Train loss: 17.90406 - Test loss: 53.96836\n",
      "Epoch 4446 - lr: 0.05000 - Train loss: 17.90405 - Test loss: 53.96847\n",
      "Epoch 4447 - lr: 0.05000 - Train loss: 17.90404 - Test loss: 53.96859\n",
      "Epoch 4448 - lr: 0.05000 - Train loss: 17.90403 - Test loss: 53.96870\n",
      "Epoch 4449 - lr: 0.05000 - Train loss: 17.90402 - Test loss: 53.96883\n",
      "Epoch 4450 - lr: 0.05000 - Train loss: 17.90401 - Test loss: 53.96895\n",
      "Epoch 4451 - lr: 0.05000 - Train loss: 17.90400 - Test loss: 53.96909\n",
      "Epoch 4452 - lr: 0.05000 - Train loss: 17.90399 - Test loss: 53.96923\n",
      "Epoch 4453 - lr: 0.05000 - Train loss: 17.90398 - Test loss: 53.96937\n",
      "Epoch 4454 - lr: 0.05000 - Train loss: 17.90397 - Test loss: 53.96952\n",
      "Epoch 4455 - lr: 0.05000 - Train loss: 17.90396 - Test loss: 53.96968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4456 - lr: 0.05000 - Train loss: 17.90395 - Test loss: 53.96985\n",
      "Epoch 4457 - lr: 0.05000 - Train loss: 17.90394 - Test loss: 53.97002\n",
      "Epoch 4458 - lr: 0.05000 - Train loss: 17.90392 - Test loss: 53.97021\n",
      "Epoch 4459 - lr: 0.05000 - Train loss: 17.90391 - Test loss: 53.97040\n",
      "Epoch 4460 - lr: 0.05000 - Train loss: 17.90390 - Test loss: 53.97061\n",
      "Epoch 4461 - lr: 0.05000 - Train loss: 17.90389 - Test loss: 53.97083\n",
      "Epoch 4462 - lr: 0.05000 - Train loss: 17.90387 - Test loss: 53.97106\n",
      "Epoch 4463 - lr: 0.05000 - Train loss: 17.90386 - Test loss: 53.97130\n",
      "Epoch 4464 - lr: 0.05000 - Train loss: 17.90384 - Test loss: 53.97156\n",
      "Epoch 4465 - lr: 0.05000 - Train loss: 17.90383 - Test loss: 53.97184\n",
      "Epoch 4466 - lr: 0.05000 - Train loss: 17.90381 - Test loss: 53.97213\n",
      "Epoch 4467 - lr: 0.05000 - Train loss: 17.90379 - Test loss: 53.97245\n",
      "Epoch 4468 - lr: 0.05000 - Train loss: 17.90378 - Test loss: 53.97279\n",
      "Epoch 4469 - lr: 0.05000 - Train loss: 17.90376 - Test loss: 53.97315\n",
      "Epoch 4470 - lr: 0.05000 - Train loss: 17.90374 - Test loss: 53.97355\n",
      "Epoch 4471 - lr: 0.05000 - Train loss: 17.90371 - Test loss: 53.97397\n",
      "Epoch 4472 - lr: 0.05000 - Train loss: 17.90369 - Test loss: 53.97443\n",
      "Epoch 4473 - lr: 0.05000 - Train loss: 17.90367 - Test loss: 53.97494\n",
      "Epoch 4474 - lr: 0.05000 - Train loss: 17.90364 - Test loss: 53.97549\n",
      "Epoch 4475 - lr: 0.05000 - Train loss: 17.90361 - Test loss: 53.97609\n",
      "Epoch 4476 - lr: 0.05000 - Train loss: 17.90358 - Test loss: 53.97675\n",
      "Epoch 4477 - lr: 0.05000 - Train loss: 17.90354 - Test loss: 53.97749\n",
      "Epoch 4478 - lr: 0.05000 - Train loss: 17.90350 - Test loss: 53.97831\n",
      "Epoch 4479 - lr: 0.05000 - Train loss: 17.90346 - Test loss: 53.97922\n",
      "Epoch 4480 - lr: 0.05000 - Train loss: 17.90342 - Test loss: 53.98025\n",
      "Epoch 4481 - lr: 0.05000 - Train loss: 17.90337 - Test loss: 53.98141\n",
      "Epoch 4482 - lr: 0.05000 - Train loss: 17.90331 - Test loss: 53.98274\n",
      "Epoch 4483 - lr: 0.05000 - Train loss: 17.90324 - Test loss: 53.98427\n",
      "Epoch 4484 - lr: 0.05000 - Train loss: 17.90317 - Test loss: 53.98604\n",
      "Epoch 4485 - lr: 0.05000 - Train loss: 17.90308 - Test loss: 53.98812\n",
      "Epoch 4486 - lr: 0.05000 - Train loss: 17.90298 - Test loss: 53.99060\n",
      "Epoch 4487 - lr: 0.05000 - Train loss: 17.90286 - Test loss: 53.99358\n",
      "Epoch 4488 - lr: 0.05000 - Train loss: 17.90271 - Test loss: 53.99724\n",
      "Epoch 4489 - lr: 0.05000 - Train loss: 17.90253 - Test loss: 54.00182\n",
      "Epoch 4490 - lr: 0.05000 - Train loss: 17.90232 - Test loss: 54.00771\n",
      "Epoch 4491 - lr: 0.05000 - Train loss: 17.90204 - Test loss: 54.01551\n",
      "Epoch 4492 - lr: 0.05000 - Train loss: 17.90167 - Test loss: 54.02631\n",
      "Epoch 4493 - lr: 0.05000 - Train loss: 17.90118 - Test loss: 54.04219\n",
      "Epoch 4494 - lr: 0.05000 - Train loss: 17.90050 - Test loss: 54.06784\n",
      "Epoch 4495 - lr: 0.05000 - Train loss: 17.89995 - Test loss: 54.11815\n",
      "Epoch 4496 - lr: 0.05000 - Train loss: 13.38266 - Test loss: 67.12891\n",
      "Epoch 4497 - lr: 0.05000 - Train loss: 19.31929 - Test loss: 53.96939\n",
      "Epoch 4498 - lr: 0.05000 - Train loss: 17.90954 - Test loss: 53.95803\n",
      "Epoch 4499 - lr: 0.05000 - Train loss: 17.90472 - Test loss: 53.95863\n",
      "Epoch 4500 - lr: 0.05000 - Train loss: 17.90421 - Test loss: 53.95872\n",
      "Epoch 4501 - lr: 0.05000 - Train loss: 17.90413 - Test loss: 53.95874\n",
      "Epoch 4502 - lr: 0.05000 - Train loss: 17.90412 - Test loss: 53.95874\n",
      "Epoch 4503 - lr: 0.05000 - Train loss: 17.90411 - Test loss: 53.95875\n",
      "Epoch 4504 - lr: 0.05000 - Train loss: 17.90410 - Test loss: 53.95875\n",
      "Epoch 4505 - lr: 0.05000 - Train loss: 17.90410 - Test loss: 53.95876\n",
      "Epoch 4506 - lr: 0.05000 - Train loss: 17.90410 - Test loss: 53.95876\n",
      "Epoch 4507 - lr: 0.05000 - Train loss: 17.90409 - Test loss: 53.95876\n",
      "Epoch 4508 - lr: 0.05000 - Train loss: 17.90409 - Test loss: 53.95877\n",
      "Epoch 4509 - lr: 0.05000 - Train loss: 17.90408 - Test loss: 53.95877\n",
      "Epoch 4510 - lr: 0.05000 - Train loss: 17.90408 - Test loss: 53.95877\n",
      "Epoch 4511 - lr: 0.05000 - Train loss: 17.90407 - Test loss: 53.95877\n",
      "Epoch 4512 - lr: 0.05000 - Train loss: 17.90407 - Test loss: 53.95878\n",
      "Epoch 4513 - lr: 0.05000 - Train loss: 17.90406 - Test loss: 53.95878\n",
      "Epoch 4514 - lr: 0.05000 - Train loss: 17.90406 - Test loss: 53.95878\n",
      "Epoch 4515 - lr: 0.05000 - Train loss: 17.90405 - Test loss: 53.95878\n",
      "Epoch 4516 - lr: 0.05000 - Train loss: 17.90405 - Test loss: 53.95879\n",
      "Epoch 4517 - lr: 0.05000 - Train loss: 17.90404 - Test loss: 53.95879\n",
      "Epoch 4518 - lr: 0.05000 - Train loss: 17.90404 - Test loss: 53.95879\n",
      "Epoch 4519 - lr: 0.05000 - Train loss: 17.90404 - Test loss: 53.95879\n",
      "Epoch 4520 - lr: 0.05000 - Train loss: 17.90403 - Test loss: 53.95879\n",
      "Epoch 4521 - lr: 0.05000 - Train loss: 17.90403 - Test loss: 53.95879\n",
      "Epoch 4522 - lr: 0.05000 - Train loss: 17.90402 - Test loss: 53.95880\n",
      "Epoch 4523 - lr: 0.05000 - Train loss: 17.90402 - Test loss: 53.95880\n",
      "Epoch 4524 - lr: 0.05000 - Train loss: 17.90401 - Test loss: 53.95880\n",
      "Epoch 4525 - lr: 0.05000 - Train loss: 17.90401 - Test loss: 53.95880\n",
      "Epoch 4526 - lr: 0.05000 - Train loss: 17.90400 - Test loss: 53.95880\n",
      "Epoch 4527 - lr: 0.05000 - Train loss: 17.90400 - Test loss: 53.95880\n",
      "Epoch 4528 - lr: 0.05000 - Train loss: 17.90399 - Test loss: 53.95880\n",
      "Epoch 4529 - lr: 0.05000 - Train loss: 17.90399 - Test loss: 53.95880\n",
      "Epoch 4530 - lr: 0.05000 - Train loss: 17.90398 - Test loss: 53.95880\n",
      "Epoch 4531 - lr: 0.05000 - Train loss: 17.90398 - Test loss: 53.95880\n",
      "Epoch 4532 - lr: 0.05000 - Train loss: 17.90398 - Test loss: 53.95880\n",
      "Epoch 4533 - lr: 0.05000 - Train loss: 17.90397 - Test loss: 53.95880\n",
      "Epoch 4534 - lr: 0.05000 - Train loss: 17.90397 - Test loss: 53.95880\n",
      "Epoch 4535 - lr: 0.05000 - Train loss: 17.90396 - Test loss: 53.95880\n",
      "Epoch 4536 - lr: 0.05000 - Train loss: 17.90396 - Test loss: 53.95880\n",
      "Epoch 4537 - lr: 0.05000 - Train loss: 17.90395 - Test loss: 53.95880\n",
      "Epoch 4538 - lr: 0.05000 - Train loss: 17.90395 - Test loss: 53.95880\n",
      "Epoch 4539 - lr: 0.05000 - Train loss: 17.90394 - Test loss: 53.95880\n",
      "Epoch 4540 - lr: 0.05000 - Train loss: 17.90394 - Test loss: 53.95880\n",
      "Epoch 4541 - lr: 0.05000 - Train loss: 17.90393 - Test loss: 53.95880\n",
      "Epoch 4542 - lr: 0.05000 - Train loss: 17.90393 - Test loss: 53.95880\n",
      "Epoch 4543 - lr: 0.05000 - Train loss: 17.90392 - Test loss: 53.95880\n",
      "Epoch 4544 - lr: 0.05000 - Train loss: 17.90392 - Test loss: 53.95880\n",
      "Epoch 4545 - lr: 0.05000 - Train loss: 17.90392 - Test loss: 53.95880\n",
      "Epoch 4546 - lr: 0.05000 - Train loss: 17.90391 - Test loss: 53.95880\n",
      "Epoch 4547 - lr: 0.05000 - Train loss: 17.90391 - Test loss: 53.95880\n",
      "Epoch 4548 - lr: 0.05000 - Train loss: 17.90390 - Test loss: 53.95880\n",
      "Epoch 4549 - lr: 0.05000 - Train loss: 17.90390 - Test loss: 53.95880\n",
      "Epoch 4550 - lr: 0.05000 - Train loss: 17.90389 - Test loss: 53.95880\n",
      "Epoch 4551 - lr: 0.05000 - Train loss: 17.90389 - Test loss: 53.95880\n",
      "Epoch 4552 - lr: 0.05000 - Train loss: 17.90388 - Test loss: 53.95880\n",
      "Epoch 4553 - lr: 0.05000 - Train loss: 17.90388 - Test loss: 53.95880\n",
      "Epoch 4554 - lr: 0.05000 - Train loss: 17.90387 - Test loss: 53.95880\n",
      "Epoch 4555 - lr: 0.05000 - Train loss: 17.90387 - Test loss: 53.95880\n",
      "Epoch 4556 - lr: 0.05000 - Train loss: 17.90386 - Test loss: 53.95880\n",
      "Epoch 4557 - lr: 0.05000 - Train loss: 17.90386 - Test loss: 53.95880\n",
      "Epoch 4558 - lr: 0.05000 - Train loss: 17.90385 - Test loss: 53.95880\n",
      "Epoch 4559 - lr: 0.05000 - Train loss: 17.90385 - Test loss: 53.95880\n",
      "Epoch 4560 - lr: 0.05000 - Train loss: 17.90384 - Test loss: 53.95880\n",
      "Epoch 4561 - lr: 0.05000 - Train loss: 17.90384 - Test loss: 53.95880\n",
      "Epoch 4562 - lr: 0.05000 - Train loss: 17.90383 - Test loss: 53.95880\n",
      "Epoch 4563 - lr: 0.05000 - Train loss: 17.90383 - Test loss: 53.95880\n",
      "Epoch 4564 - lr: 0.05000 - Train loss: 17.90382 - Test loss: 53.95880\n",
      "Epoch 4565 - lr: 0.05000 - Train loss: 17.90382 - Test loss: 53.95880\n",
      "Epoch 4566 - lr: 0.05000 - Train loss: 17.90381 - Test loss: 53.95880\n",
      "Epoch 4567 - lr: 0.05000 - Train loss: 17.90381 - Test loss: 53.95880\n",
      "Epoch 4568 - lr: 0.05000 - Train loss: 17.90380 - Test loss: 53.95880\n",
      "Epoch 4569 - lr: 0.05000 - Train loss: 17.90380 - Test loss: 53.95880\n",
      "Epoch 4570 - lr: 0.05000 - Train loss: 17.90379 - Test loss: 53.95880\n",
      "Epoch 4571 - lr: 0.05000 - Train loss: 17.90379 - Test loss: 53.95880\n",
      "Epoch 4572 - lr: 0.05000 - Train loss: 17.90378 - Test loss: 53.95880\n",
      "Epoch 4573 - lr: 0.05000 - Train loss: 17.90378 - Test loss: 53.95880\n",
      "Epoch 4574 - lr: 0.05000 - Train loss: 17.90377 - Test loss: 53.95880\n",
      "Epoch 4575 - lr: 0.05000 - Train loss: 17.90377 - Test loss: 53.95881\n",
      "Epoch 4576 - lr: 0.05000 - Train loss: 17.90376 - Test loss: 53.95881\n",
      "Epoch 4577 - lr: 0.05000 - Train loss: 17.90376 - Test loss: 53.95881\n",
      "Epoch 4578 - lr: 0.05000 - Train loss: 17.90375 - Test loss: 53.95881\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4579 - lr: 0.05000 - Train loss: 17.90375 - Test loss: 53.95881\n",
      "Epoch 4580 - lr: 0.05000 - Train loss: 17.90374 - Test loss: 53.95882\n",
      "Epoch 4581 - lr: 0.05000 - Train loss: 17.90374 - Test loss: 53.95882\n",
      "Epoch 4582 - lr: 0.05000 - Train loss: 17.90373 - Test loss: 53.95882\n",
      "Epoch 4583 - lr: 0.05000 - Train loss: 17.90372 - Test loss: 53.95882\n",
      "Epoch 4584 - lr: 0.05000 - Train loss: 17.90372 - Test loss: 53.95883\n",
      "Epoch 4585 - lr: 0.05000 - Train loss: 17.90371 - Test loss: 53.95883\n",
      "Epoch 4586 - lr: 0.05000 - Train loss: 17.90371 - Test loss: 53.95884\n",
      "Epoch 4587 - lr: 0.05000 - Train loss: 17.90370 - Test loss: 53.95884\n",
      "Epoch 4588 - lr: 0.05000 - Train loss: 17.90370 - Test loss: 53.95885\n",
      "Epoch 4589 - lr: 0.05000 - Train loss: 17.90369 - Test loss: 53.95885\n",
      "Epoch 4590 - lr: 0.05000 - Train loss: 17.90368 - Test loss: 53.95886\n",
      "Epoch 4591 - lr: 0.05000 - Train loss: 17.90368 - Test loss: 53.95886\n",
      "Epoch 4592 - lr: 0.05000 - Train loss: 17.90367 - Test loss: 53.95887\n",
      "Epoch 4593 - lr: 0.05000 - Train loss: 17.90367 - Test loss: 53.95887\n",
      "Epoch 4594 - lr: 0.05000 - Train loss: 17.90366 - Test loss: 53.95888\n",
      "Epoch 4595 - lr: 0.05000 - Train loss: 17.90365 - Test loss: 53.95889\n",
      "Epoch 4596 - lr: 0.05000 - Train loss: 17.90365 - Test loss: 53.95890\n",
      "Epoch 4597 - lr: 0.05000 - Train loss: 17.90364 - Test loss: 53.95891\n",
      "Epoch 4598 - lr: 0.05000 - Train loss: 17.90364 - Test loss: 53.95892\n",
      "Epoch 4599 - lr: 0.05000 - Train loss: 17.90363 - Test loss: 53.95893\n",
      "Epoch 4600 - lr: 0.05000 - Train loss: 17.90362 - Test loss: 53.95894\n",
      "Epoch 4601 - lr: 0.05000 - Train loss: 17.90362 - Test loss: 53.95895\n",
      "Epoch 4602 - lr: 0.05000 - Train loss: 17.90361 - Test loss: 53.95896\n",
      "Epoch 4603 - lr: 0.05000 - Train loss: 17.90360 - Test loss: 53.95897\n",
      "Epoch 4604 - lr: 0.05000 - Train loss: 17.90359 - Test loss: 53.95899\n",
      "Epoch 4605 - lr: 0.05000 - Train loss: 17.90359 - Test loss: 53.95900\n",
      "Epoch 4606 - lr: 0.05000 - Train loss: 17.90358 - Test loss: 53.95902\n",
      "Epoch 4607 - lr: 0.05000 - Train loss: 17.90357 - Test loss: 53.95903\n",
      "Epoch 4608 - lr: 0.05000 - Train loss: 17.90357 - Test loss: 53.95905\n",
      "Epoch 4609 - lr: 0.05000 - Train loss: 17.90356 - Test loss: 53.95907\n",
      "Epoch 4610 - lr: 0.05000 - Train loss: 17.90355 - Test loss: 53.95909\n",
      "Epoch 4611 - lr: 0.05000 - Train loss: 17.90354 - Test loss: 53.95911\n",
      "Epoch 4612 - lr: 0.05000 - Train loss: 17.90353 - Test loss: 53.95913\n",
      "Epoch 4613 - lr: 0.05000 - Train loss: 17.90353 - Test loss: 53.95915\n",
      "Epoch 4614 - lr: 0.05000 - Train loss: 17.90352 - Test loss: 53.95917\n",
      "Epoch 4615 - lr: 0.05000 - Train loss: 17.90351 - Test loss: 53.95920\n",
      "Epoch 4616 - lr: 0.05000 - Train loss: 17.90350 - Test loss: 53.95923\n",
      "Epoch 4617 - lr: 0.05000 - Train loss: 17.90349 - Test loss: 53.95926\n",
      "Epoch 4618 - lr: 0.05000 - Train loss: 17.90348 - Test loss: 53.95929\n",
      "Epoch 4619 - lr: 0.05000 - Train loss: 17.90347 - Test loss: 53.95932\n",
      "Epoch 4620 - lr: 0.05000 - Train loss: 17.90346 - Test loss: 53.95935\n",
      "Epoch 4621 - lr: 0.05000 - Train loss: 17.90345 - Test loss: 53.95939\n",
      "Epoch 4622 - lr: 0.05000 - Train loss: 17.90344 - Test loss: 53.95942\n",
      "Epoch 4623 - lr: 0.05000 - Train loss: 17.90343 - Test loss: 53.95946\n",
      "Epoch 4624 - lr: 0.05000 - Train loss: 17.90342 - Test loss: 53.95951\n",
      "Epoch 4625 - lr: 0.05000 - Train loss: 17.90341 - Test loss: 53.95955\n",
      "Epoch 4626 - lr: 0.05000 - Train loss: 17.90340 - Test loss: 53.95960\n",
      "Epoch 4627 - lr: 0.05000 - Train loss: 17.90339 - Test loss: 53.95965\n",
      "Epoch 4628 - lr: 0.05000 - Train loss: 17.90338 - Test loss: 53.95970\n",
      "Epoch 4629 - lr: 0.05000 - Train loss: 17.90337 - Test loss: 53.95976\n",
      "Epoch 4630 - lr: 0.05000 - Train loss: 17.90335 - Test loss: 53.95982\n",
      "Epoch 4631 - lr: 0.05000 - Train loss: 17.90334 - Test loss: 53.95988\n",
      "Epoch 4632 - lr: 0.05000 - Train loss: 17.90333 - Test loss: 53.95995\n",
      "Epoch 4633 - lr: 0.05000 - Train loss: 17.90331 - Test loss: 53.96002\n",
      "Epoch 4634 - lr: 0.05000 - Train loss: 17.90330 - Test loss: 53.96010\n",
      "Epoch 4635 - lr: 0.05000 - Train loss: 17.90328 - Test loss: 53.96018\n",
      "Epoch 4636 - lr: 0.05000 - Train loss: 17.90327 - Test loss: 53.96027\n",
      "Epoch 4637 - lr: 0.05000 - Train loss: 17.90325 - Test loss: 53.96036\n",
      "Epoch 4638 - lr: 0.05000 - Train loss: 17.90323 - Test loss: 53.96046\n",
      "Epoch 4639 - lr: 0.05000 - Train loss: 17.90321 - Test loss: 53.96057\n",
      "Epoch 4640 - lr: 0.05000 - Train loss: 17.90320 - Test loss: 53.96069\n",
      "Epoch 4641 - lr: 0.05000 - Train loss: 17.90318 - Test loss: 53.96081\n",
      "Epoch 4642 - lr: 0.05000 - Train loss: 17.90315 - Test loss: 53.96094\n",
      "Epoch 4643 - lr: 0.05000 - Train loss: 17.90313 - Test loss: 53.96109\n",
      "Epoch 4644 - lr: 0.05000 - Train loss: 17.90311 - Test loss: 53.96124\n",
      "Epoch 4645 - lr: 0.05000 - Train loss: 17.90308 - Test loss: 53.96141\n",
      "Epoch 4646 - lr: 0.05000 - Train loss: 17.90306 - Test loss: 53.96159\n",
      "Epoch 4647 - lr: 0.05000 - Train loss: 17.90303 - Test loss: 53.96178\n",
      "Epoch 4648 - lr: 0.05000 - Train loss: 17.90300 - Test loss: 53.96199\n",
      "Epoch 4649 - lr: 0.05000 - Train loss: 17.90296 - Test loss: 53.96223\n",
      "Epoch 4650 - lr: 0.05000 - Train loss: 17.90293 - Test loss: 53.96248\n",
      "Epoch 4651 - lr: 0.05000 - Train loss: 17.90289 - Test loss: 53.96276\n",
      "Epoch 4652 - lr: 0.05000 - Train loss: 17.90285 - Test loss: 53.96306\n",
      "Epoch 4653 - lr: 0.05000 - Train loss: 17.90280 - Test loss: 53.96340\n",
      "Epoch 4654 - lr: 0.05000 - Train loss: 17.90275 - Test loss: 53.96378\n",
      "Epoch 4655 - lr: 0.05000 - Train loss: 17.90270 - Test loss: 53.96420\n",
      "Epoch 4656 - lr: 0.05000 - Train loss: 17.90264 - Test loss: 53.96467\n",
      "Epoch 4657 - lr: 0.05000 - Train loss: 17.90257 - Test loss: 53.96519\n",
      "Epoch 4658 - lr: 0.05000 - Train loss: 17.90249 - Test loss: 53.96579\n",
      "Epoch 4659 - lr: 0.05000 - Train loss: 17.90240 - Test loss: 53.96647\n",
      "Epoch 4660 - lr: 0.05000 - Train loss: 17.90230 - Test loss: 53.96726\n",
      "Epoch 4661 - lr: 0.05000 - Train loss: 17.90219 - Test loss: 53.96817\n",
      "Epoch 4662 - lr: 0.05000 - Train loss: 17.90205 - Test loss: 53.96924\n",
      "Epoch 4663 - lr: 0.05000 - Train loss: 17.90189 - Test loss: 53.97050\n",
      "Epoch 4664 - lr: 0.05000 - Train loss: 17.90169 - Test loss: 53.97203\n",
      "Epoch 4665 - lr: 0.05000 - Train loss: 17.90145 - Test loss: 53.97389\n",
      "Epoch 4666 - lr: 0.05000 - Train loss: 17.90115 - Test loss: 53.97621\n",
      "Epoch 4667 - lr: 0.05000 - Train loss: 17.90076 - Test loss: 53.97916\n",
      "Epoch 4668 - lr: 0.05000 - Train loss: 17.90023 - Test loss: 53.98305\n",
      "Epoch 4669 - lr: 0.05000 - Train loss: 17.89947 - Test loss: 53.98837\n",
      "Epoch 4670 - lr: 0.05000 - Train loss: 17.89832 - Test loss: 53.99603\n",
      "Epoch 4671 - lr: 0.05000 - Train loss: 17.89634 - Test loss: 54.00806\n",
      "Epoch 4672 - lr: 0.05000 - Train loss: 17.89164 - Test loss: 54.03138\n",
      "Epoch 4673 - lr: 0.05000 - Train loss: 14.82852 - Test loss: 62.67194\n",
      "Epoch 4674 - lr: 0.05000 - Train loss: 18.84897 - Test loss: 53.96336\n",
      "Epoch 4675 - lr: 0.05000 - Train loss: 17.90752 - Test loss: 53.95693\n",
      "Epoch 4676 - lr: 0.05000 - Train loss: 17.90371 - Test loss: 53.95773\n",
      "Epoch 4677 - lr: 0.05000 - Train loss: 17.90325 - Test loss: 53.95803\n",
      "Epoch 4678 - lr: 0.05000 - Train loss: 17.90316 - Test loss: 53.95826\n",
      "Epoch 4679 - lr: 0.05000 - Train loss: 17.90312 - Test loss: 53.95849\n",
      "Epoch 4680 - lr: 0.05000 - Train loss: 17.90309 - Test loss: 53.95873\n",
      "Epoch 4681 - lr: 0.05000 - Train loss: 17.90306 - Test loss: 53.95899\n",
      "Epoch 4682 - lr: 0.05000 - Train loss: 17.90303 - Test loss: 53.95926\n",
      "Epoch 4683 - lr: 0.05000 - Train loss: 17.90300 - Test loss: 53.95955\n",
      "Epoch 4684 - lr: 0.05000 - Train loss: 17.90296 - Test loss: 53.95986\n",
      "Epoch 4685 - lr: 0.05000 - Train loss: 17.90293 - Test loss: 53.96019\n",
      "Epoch 4686 - lr: 0.05000 - Train loss: 17.90289 - Test loss: 53.96055\n",
      "Epoch 4687 - lr: 0.05000 - Train loss: 17.90285 - Test loss: 53.96092\n",
      "Epoch 4688 - lr: 0.05000 - Train loss: 17.90280 - Test loss: 53.96133\n",
      "Epoch 4689 - lr: 0.05000 - Train loss: 17.90276 - Test loss: 53.96177\n",
      "Epoch 4690 - lr: 0.05000 - Train loss: 17.90271 - Test loss: 53.96224\n",
      "Epoch 4691 - lr: 0.05000 - Train loss: 17.90265 - Test loss: 53.96276\n",
      "Epoch 4692 - lr: 0.05000 - Train loss: 17.90260 - Test loss: 53.96331\n",
      "Epoch 4693 - lr: 0.05000 - Train loss: 17.90253 - Test loss: 53.96392\n",
      "Epoch 4694 - lr: 0.05000 - Train loss: 17.90246 - Test loss: 53.96458\n",
      "Epoch 4695 - lr: 0.05000 - Train loss: 17.90239 - Test loss: 53.96531\n",
      "Epoch 4696 - lr: 0.05000 - Train loss: 17.90231 - Test loss: 53.96612\n",
      "Epoch 4697 - lr: 0.05000 - Train loss: 17.90222 - Test loss: 53.96701\n",
      "Epoch 4698 - lr: 0.05000 - Train loss: 17.90212 - Test loss: 53.96800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4699 - lr: 0.05000 - Train loss: 17.90201 - Test loss: 53.96911\n",
      "Epoch 4700 - lr: 0.05000 - Train loss: 17.90189 - Test loss: 53.97036\n",
      "Epoch 4701 - lr: 0.05000 - Train loss: 17.90175 - Test loss: 53.97178\n",
      "Epoch 4702 - lr: 0.05000 - Train loss: 17.90159 - Test loss: 53.97341\n",
      "Epoch 4703 - lr: 0.05000 - Train loss: 17.90141 - Test loss: 53.97528\n",
      "Epoch 4704 - lr: 0.05000 - Train loss: 17.90119 - Test loss: 53.97746\n",
      "Epoch 4705 - lr: 0.05000 - Train loss: 17.90094 - Test loss: 53.98004\n",
      "Epoch 4706 - lr: 0.05000 - Train loss: 17.90064 - Test loss: 53.98313\n",
      "Epoch 4707 - lr: 0.05000 - Train loss: 17.90027 - Test loss: 53.98688\n",
      "Epoch 4708 - lr: 0.05000 - Train loss: 17.89981 - Test loss: 53.99155\n",
      "Epoch 4709 - lr: 0.05000 - Train loss: 17.89921 - Test loss: 53.99751\n",
      "Epoch 4710 - lr: 0.05000 - Train loss: 17.89840 - Test loss: 54.00537\n",
      "Epoch 4711 - lr: 0.05000 - Train loss: 17.89724 - Test loss: 54.01620\n",
      "Epoch 4712 - lr: 0.05000 - Train loss: 17.89539 - Test loss: 54.03204\n",
      "Epoch 4713 - lr: 0.05000 - Train loss: 17.89196 - Test loss: 54.05726\n",
      "Epoch 4714 - lr: 0.05000 - Train loss: 17.87900 - Test loss: 54.11405\n",
      "Epoch 4715 - lr: 0.05000 - Train loss: 12.74624 - Test loss: 67.17175\n",
      "Epoch 4716 - lr: 0.05000 - Train loss: 14.69014 - Test loss: 92.32330\n",
      "Epoch 4717 - lr: 0.05000 - Train loss: 13.36599 - Test loss: 92.17464\n",
      "Epoch 4718 - lr: 0.05000 - Train loss: 14.50236 - Test loss: 63.87434\n",
      "Epoch 4719 - lr: 0.05000 - Train loss: 18.97401 - Test loss: 53.98576\n",
      "Epoch 4720 - lr: 0.05000 - Train loss: 17.90302 - Test loss: 53.98663\n",
      "Epoch 4721 - lr: 0.05000 - Train loss: 17.89463 - Test loss: 54.00534\n",
      "Epoch 4722 - lr: 0.05000 - Train loss: 16.33525 - Test loss: 58.67533\n",
      "Epoch 4723 - lr: 0.05000 - Train loss: 13.78645 - Test loss: 67.71276\n",
      "Epoch 4724 - lr: 0.05000 - Train loss: 14.02846 - Test loss: 71.00322\n",
      "Epoch 4725 - lr: 0.05000 - Train loss: 19.75545 - Test loss: 53.99715\n",
      "Epoch 4726 - lr: 0.05000 - Train loss: 17.90691 - Test loss: 54.00308\n",
      "Epoch 4727 - lr: 0.05000 - Train loss: 17.88681 - Test loss: 54.03692\n",
      "Epoch 4728 - lr: 0.05000 - Train loss: 13.43056 - Test loss: 66.04946\n",
      "Epoch 4729 - lr: 0.05000 - Train loss: 19.20977 - Test loss: 53.96455\n",
      "Epoch 4730 - lr: 0.05000 - Train loss: 17.90513 - Test loss: 53.96612\n",
      "Epoch 4731 - lr: 0.05000 - Train loss: 17.89536 - Test loss: 53.98797\n",
      "Epoch 4732 - lr: 0.05000 - Train loss: 15.70091 - Test loss: 59.80958\n",
      "Epoch 4733 - lr: 0.05000 - Train loss: 15.84865 - Test loss: 61.01551\n",
      "Epoch 4734 - lr: 0.05000 - Train loss: 18.67838 - Test loss: 53.97250\n",
      "Epoch 4735 - lr: 0.05000 - Train loss: 17.90564 - Test loss: 53.98230\n",
      "Epoch 4736 - lr: 0.05000 - Train loss: 17.89799 - Test loss: 54.01011\n",
      "Epoch 4737 - lr: 0.05000 - Train loss: 17.88997 - Test loss: 54.06007\n",
      "Epoch 4738 - lr: 0.05000 - Train loss: 12.85432 - Test loss: 92.16032\n",
      "Epoch 4739 - lr: 0.05000 - Train loss: 13.53573 - Test loss: 90.74261\n",
      "Epoch 4740 - lr: 0.05000 - Train loss: 18.45751 - Test loss: 53.84430\n",
      "Epoch 4741 - lr: 0.05000 - Train loss: 17.90970 - Test loss: 53.88072\n",
      "Epoch 4742 - lr: 0.05000 - Train loss: 17.91200 - Test loss: 53.89162\n",
      "Epoch 4743 - lr: 0.05000 - Train loss: 17.91043 - Test loss: 53.89774\n",
      "Epoch 4744 - lr: 0.05000 - Train loss: 17.90923 - Test loss: 53.90168\n",
      "Epoch 4745 - lr: 0.05000 - Train loss: 17.90842 - Test loss: 53.90449\n",
      "Epoch 4746 - lr: 0.05000 - Train loss: 17.90785 - Test loss: 53.90663\n",
      "Epoch 4747 - lr: 0.05000 - Train loss: 17.90742 - Test loss: 53.90836\n",
      "Epoch 4748 - lr: 0.05000 - Train loss: 17.90709 - Test loss: 53.90981\n",
      "Epoch 4749 - lr: 0.05000 - Train loss: 17.90682 - Test loss: 53.91106\n",
      "Epoch 4750 - lr: 0.05000 - Train loss: 17.90659 - Test loss: 53.91217\n",
      "Epoch 4751 - lr: 0.05000 - Train loss: 17.90640 - Test loss: 53.91317\n",
      "Epoch 4752 - lr: 0.05000 - Train loss: 17.90624 - Test loss: 53.91408\n",
      "Epoch 4753 - lr: 0.05000 - Train loss: 17.90609 - Test loss: 53.91492\n",
      "Epoch 4754 - lr: 0.05000 - Train loss: 17.90596 - Test loss: 53.91571\n",
      "Epoch 4755 - lr: 0.05000 - Train loss: 17.90584 - Test loss: 53.91645\n",
      "Epoch 4756 - lr: 0.05000 - Train loss: 17.90573 - Test loss: 53.91715\n",
      "Epoch 4757 - lr: 0.05000 - Train loss: 17.90563 - Test loss: 53.91782\n",
      "Epoch 4758 - lr: 0.05000 - Train loss: 17.90553 - Test loss: 53.91847\n",
      "Epoch 4759 - lr: 0.05000 - Train loss: 17.90545 - Test loss: 53.91909\n",
      "Epoch 4760 - lr: 0.05000 - Train loss: 17.90536 - Test loss: 53.91970\n",
      "Epoch 4761 - lr: 0.05000 - Train loss: 17.90528 - Test loss: 53.92028\n",
      "Epoch 4762 - lr: 0.05000 - Train loss: 17.90521 - Test loss: 53.92086\n",
      "Epoch 4763 - lr: 0.05000 - Train loss: 17.90513 - Test loss: 53.92142\n",
      "Epoch 4764 - lr: 0.05000 - Train loss: 17.90506 - Test loss: 53.92197\n",
      "Epoch 4765 - lr: 0.05000 - Train loss: 17.90500 - Test loss: 53.92252\n",
      "Epoch 4766 - lr: 0.05000 - Train loss: 17.90493 - Test loss: 53.92306\n",
      "Epoch 4767 - lr: 0.05000 - Train loss: 17.90486 - Test loss: 53.92360\n",
      "Epoch 4768 - lr: 0.05000 - Train loss: 17.90480 - Test loss: 53.92413\n",
      "Epoch 4769 - lr: 0.05000 - Train loss: 17.90474 - Test loss: 53.92466\n",
      "Epoch 4770 - lr: 0.05000 - Train loss: 17.90467 - Test loss: 53.92519\n",
      "Epoch 4771 - lr: 0.05000 - Train loss: 17.90461 - Test loss: 53.92573\n",
      "Epoch 4772 - lr: 0.05000 - Train loss: 17.90455 - Test loss: 53.92627\n",
      "Epoch 4773 - lr: 0.05000 - Train loss: 17.90448 - Test loss: 53.92682\n",
      "Epoch 4774 - lr: 0.05000 - Train loss: 17.90442 - Test loss: 53.92737\n",
      "Epoch 4775 - lr: 0.05000 - Train loss: 17.90436 - Test loss: 53.92794\n",
      "Epoch 4776 - lr: 0.05000 - Train loss: 17.90429 - Test loss: 53.92852\n",
      "Epoch 4777 - lr: 0.05000 - Train loss: 17.90422 - Test loss: 53.92912\n",
      "Epoch 4778 - lr: 0.05000 - Train loss: 17.90415 - Test loss: 53.92973\n",
      "Epoch 4779 - lr: 0.05000 - Train loss: 17.90408 - Test loss: 53.93037\n",
      "Epoch 4780 - lr: 0.05000 - Train loss: 17.90400 - Test loss: 53.93104\n",
      "Epoch 4781 - lr: 0.05000 - Train loss: 17.90392 - Test loss: 53.93174\n",
      "Epoch 4782 - lr: 0.05000 - Train loss: 17.90383 - Test loss: 53.93249\n",
      "Epoch 4783 - lr: 0.05000 - Train loss: 17.90374 - Test loss: 53.93328\n",
      "Epoch 4784 - lr: 0.05000 - Train loss: 17.90364 - Test loss: 53.93413\n",
      "Epoch 4785 - lr: 0.05000 - Train loss: 17.90353 - Test loss: 53.93505\n",
      "Epoch 4786 - lr: 0.05000 - Train loss: 17.90342 - Test loss: 53.93605\n",
      "Epoch 4787 - lr: 0.05000 - Train loss: 17.90328 - Test loss: 53.93716\n",
      "Epoch 4788 - lr: 0.05000 - Train loss: 17.90313 - Test loss: 53.93840\n",
      "Epoch 4789 - lr: 0.05000 - Train loss: 17.90297 - Test loss: 53.93980\n",
      "Epoch 4790 - lr: 0.05000 - Train loss: 17.90277 - Test loss: 53.94140\n",
      "Epoch 4791 - lr: 0.05000 - Train loss: 17.90254 - Test loss: 53.94327\n",
      "Epoch 4792 - lr: 0.05000 - Train loss: 17.90225 - Test loss: 53.94550\n",
      "Epoch 4793 - lr: 0.05000 - Train loss: 17.90190 - Test loss: 53.94821\n",
      "Epoch 4794 - lr: 0.05000 - Train loss: 17.90146 - Test loss: 53.95159\n",
      "Epoch 4795 - lr: 0.05000 - Train loss: 17.90086 - Test loss: 53.95596\n",
      "Epoch 4796 - lr: 0.05000 - Train loss: 17.90003 - Test loss: 53.96184\n",
      "Epoch 4797 - lr: 0.05000 - Train loss: 17.89877 - Test loss: 53.97020\n",
      "Epoch 4798 - lr: 0.05000 - Train loss: 17.89661 - Test loss: 53.98314\n",
      "Epoch 4799 - lr: 0.05000 - Train loss: 17.89167 - Test loss: 54.00722\n",
      "Epoch 4800 - lr: 0.05000 - Train loss: 14.84503 - Test loss: 62.16440\n",
      "Epoch 4801 - lr: 0.05000 - Train loss: 18.80034 - Test loss: 53.96838\n",
      "Epoch 4802 - lr: 0.05000 - Train loss: 17.90640 - Test loss: 53.96959\n",
      "Epoch 4803 - lr: 0.05000 - Train loss: 17.90084 - Test loss: 53.98016\n",
      "Epoch 4804 - lr: 0.05000 - Train loss: 17.89879 - Test loss: 53.99536\n",
      "Epoch 4805 - lr: 0.05000 - Train loss: 17.89603 - Test loss: 54.01988\n",
      "Epoch 4806 - lr: 0.05000 - Train loss: 17.88989 - Test loss: 54.06455\n",
      "Epoch 4807 - lr: 0.05000 - Train loss: 13.83507 - Test loss: 64.47479\n",
      "Epoch 4808 - lr: 0.05000 - Train loss: 19.04826 - Test loss: 53.92200\n",
      "Epoch 4809 - lr: 0.05000 - Train loss: 17.90982 - Test loss: 53.91536\n",
      "Epoch 4810 - lr: 0.05000 - Train loss: 17.90503 - Test loss: 53.91822\n",
      "Epoch 4811 - lr: 0.05000 - Train loss: 17.90415 - Test loss: 53.92085\n",
      "Epoch 4812 - lr: 0.05000 - Train loss: 17.90364 - Test loss: 53.92391\n",
      "Epoch 4813 - lr: 0.05000 - Train loss: 17.90310 - Test loss: 53.92766\n",
      "Epoch 4814 - lr: 0.05000 - Train loss: 17.90239 - Test loss: 53.93250\n",
      "Epoch 4815 - lr: 0.05000 - Train loss: 17.90141 - Test loss: 53.93902\n",
      "Epoch 4816 - lr: 0.05000 - Train loss: 17.89989 - Test loss: 53.94842\n",
      "Epoch 4817 - lr: 0.05000 - Train loss: 17.89716 - Test loss: 53.96338\n",
      "Epoch 4818 - lr: 0.05000 - Train loss: 17.88969 - Test loss: 53.99479\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4819 - lr: 0.05000 - Train loss: 13.69889 - Test loss: 65.82180\n",
      "Epoch 4820 - lr: 0.05000 - Train loss: 19.20299 - Test loss: 53.93767\n",
      "Epoch 4821 - lr: 0.05000 - Train loss: 17.91204 - Test loss: 53.93471\n",
      "Epoch 4822 - lr: 0.05000 - Train loss: 17.90498 - Test loss: 53.94123\n",
      "Epoch 4823 - lr: 0.05000 - Train loss: 17.90359 - Test loss: 53.94907\n",
      "Epoch 4824 - lr: 0.05000 - Train loss: 17.90269 - Test loss: 53.95986\n",
      "Epoch 4825 - lr: 0.05000 - Train loss: 17.90145 - Test loss: 53.97586\n",
      "Epoch 4826 - lr: 0.05000 - Train loss: 17.89919 - Test loss: 54.00177\n",
      "Epoch 4827 - lr: 0.05000 - Train loss: 17.89424 - Test loss: 54.04817\n",
      "Epoch 4828 - lr: 0.05000 - Train loss: 14.84756 - Test loss: 61.24978\n",
      "Epoch 4829 - lr: 0.05000 - Train loss: 10.32381 - Test loss: 75.31991\n",
      "Epoch 4830 - lr: 0.05000 - Train loss: 20.18207 - Test loss: 53.88738\n",
      "Epoch 4831 - lr: 0.05000 - Train loss: 17.91332 - Test loss: 53.86936\n",
      "Epoch 4832 - lr: 0.05000 - Train loss: 17.90728 - Test loss: 53.87208\n",
      "Epoch 4833 - lr: 0.05000 - Train loss: 17.90659 - Test loss: 53.87338\n",
      "Epoch 4834 - lr: 0.05000 - Train loss: 17.90639 - Test loss: 53.87439\n",
      "Epoch 4835 - lr: 0.05000 - Train loss: 17.90628 - Test loss: 53.87531\n",
      "Epoch 4836 - lr: 0.05000 - Train loss: 17.90619 - Test loss: 53.87620\n",
      "Epoch 4837 - lr: 0.05000 - Train loss: 17.90611 - Test loss: 53.87706\n",
      "Epoch 4838 - lr: 0.05000 - Train loss: 17.90604 - Test loss: 53.87789\n",
      "Epoch 4839 - lr: 0.05000 - Train loss: 17.90596 - Test loss: 53.87870\n",
      "Epoch 4840 - lr: 0.05000 - Train loss: 17.90589 - Test loss: 53.87949\n",
      "Epoch 4841 - lr: 0.05000 - Train loss: 17.90582 - Test loss: 53.88025\n",
      "Epoch 4842 - lr: 0.05000 - Train loss: 17.90574 - Test loss: 53.88100\n",
      "Epoch 4843 - lr: 0.05000 - Train loss: 17.90567 - Test loss: 53.88173\n",
      "Epoch 4844 - lr: 0.05000 - Train loss: 17.90560 - Test loss: 53.88244\n",
      "Epoch 4845 - lr: 0.05000 - Train loss: 17.90553 - Test loss: 53.88314\n",
      "Epoch 4846 - lr: 0.05000 - Train loss: 17.90546 - Test loss: 53.88383\n",
      "Epoch 4847 - lr: 0.05000 - Train loss: 17.90539 - Test loss: 53.88451\n",
      "Epoch 4848 - lr: 0.05000 - Train loss: 17.90532 - Test loss: 53.88518\n",
      "Epoch 4849 - lr: 0.05000 - Train loss: 17.90525 - Test loss: 53.88585\n",
      "Epoch 4850 - lr: 0.05000 - Train loss: 17.90517 - Test loss: 53.88652\n",
      "Epoch 4851 - lr: 0.05000 - Train loss: 17.90509 - Test loss: 53.88719\n",
      "Epoch 4852 - lr: 0.05000 - Train loss: 17.90501 - Test loss: 53.88787\n",
      "Epoch 4853 - lr: 0.05000 - Train loss: 17.90493 - Test loss: 53.88857\n",
      "Epoch 4854 - lr: 0.05000 - Train loss: 17.90484 - Test loss: 53.88928\n",
      "Epoch 4855 - lr: 0.05000 - Train loss: 17.90475 - Test loss: 53.89002\n",
      "Epoch 4856 - lr: 0.05000 - Train loss: 17.90465 - Test loss: 53.89081\n",
      "Epoch 4857 - lr: 0.05000 - Train loss: 17.90454 - Test loss: 53.89164\n",
      "Epoch 4858 - lr: 0.05000 - Train loss: 17.90441 - Test loss: 53.89254\n",
      "Epoch 4859 - lr: 0.05000 - Train loss: 17.90428 - Test loss: 53.89354\n",
      "Epoch 4860 - lr: 0.05000 - Train loss: 17.90412 - Test loss: 53.89465\n",
      "Epoch 4861 - lr: 0.05000 - Train loss: 17.90394 - Test loss: 53.89592\n",
      "Epoch 4862 - lr: 0.05000 - Train loss: 17.90372 - Test loss: 53.89740\n",
      "Epoch 4863 - lr: 0.05000 - Train loss: 17.90346 - Test loss: 53.89918\n",
      "Epoch 4864 - lr: 0.05000 - Train loss: 17.90313 - Test loss: 53.90136\n",
      "Epoch 4865 - lr: 0.05000 - Train loss: 17.90271 - Test loss: 53.90414\n",
      "Epoch 4866 - lr: 0.05000 - Train loss: 17.90213 - Test loss: 53.90778\n",
      "Epoch 4867 - lr: 0.05000 - Train loss: 17.90132 - Test loss: 53.91280\n",
      "Epoch 4868 - lr: 0.05000 - Train loss: 17.90008 - Test loss: 53.92011\n",
      "Epoch 4869 - lr: 0.05000 - Train loss: 17.89794 - Test loss: 53.93164\n",
      "Epoch 4870 - lr: 0.05000 - Train loss: 17.89329 - Test loss: 53.95281\n",
      "Epoch 4871 - lr: 0.05000 - Train loss: 15.31341 - Test loss: 60.70910\n",
      "Epoch 4872 - lr: 0.05000 - Train loss: 13.40684 - Test loss: 91.44980\n",
      "Epoch 4873 - lr: 0.05000 - Train loss: 18.55392 - Test loss: 53.83981\n",
      "Epoch 4874 - lr: 0.05000 - Train loss: 17.90905 - Test loss: 53.85504\n",
      "Epoch 4875 - lr: 0.05000 - Train loss: 17.91005 - Test loss: 53.86187\n",
      "Epoch 4876 - lr: 0.05000 - Train loss: 17.90904 - Test loss: 53.86630\n",
      "Epoch 4877 - lr: 0.05000 - Train loss: 17.90820 - Test loss: 53.86933\n",
      "Epoch 4878 - lr: 0.05000 - Train loss: 17.90760 - Test loss: 53.87151\n",
      "Epoch 4879 - lr: 0.05000 - Train loss: 17.90716 - Test loss: 53.87314\n",
      "Epoch 4880 - lr: 0.05000 - Train loss: 17.90682 - Test loss: 53.87442\n",
      "Epoch 4881 - lr: 0.05000 - Train loss: 17.90656 - Test loss: 53.87545\n",
      "Epoch 4882 - lr: 0.05000 - Train loss: 17.90635 - Test loss: 53.87628\n",
      "Epoch 4883 - lr: 0.05000 - Train loss: 17.90617 - Test loss: 53.87696\n",
      "Epoch 4884 - lr: 0.05000 - Train loss: 17.90602 - Test loss: 53.87753\n",
      "Epoch 4885 - lr: 0.05000 - Train loss: 17.90590 - Test loss: 53.87800\n",
      "Epoch 4886 - lr: 0.05000 - Train loss: 17.90578 - Test loss: 53.87839\n",
      "Epoch 4887 - lr: 0.05000 - Train loss: 17.90568 - Test loss: 53.87870\n",
      "Epoch 4888 - lr: 0.05000 - Train loss: 17.90559 - Test loss: 53.87896\n",
      "Epoch 4889 - lr: 0.05000 - Train loss: 17.90551 - Test loss: 53.87917\n",
      "Epoch 4890 - lr: 0.05000 - Train loss: 17.90544 - Test loss: 53.87933\n",
      "Epoch 4891 - lr: 0.05000 - Train loss: 17.90537 - Test loss: 53.87944\n",
      "Epoch 4892 - lr: 0.05000 - Train loss: 17.90531 - Test loss: 53.87952\n",
      "Epoch 4893 - lr: 0.05000 - Train loss: 17.90525 - Test loss: 53.87957\n",
      "Epoch 4894 - lr: 0.05000 - Train loss: 17.90520 - Test loss: 53.87958\n",
      "Epoch 4895 - lr: 0.05000 - Train loss: 17.90515 - Test loss: 53.87956\n",
      "Epoch 4896 - lr: 0.05000 - Train loss: 17.90511 - Test loss: 53.87951\n",
      "Epoch 4897 - lr: 0.05000 - Train loss: 17.90506 - Test loss: 53.87944\n",
      "Epoch 4898 - lr: 0.05000 - Train loss: 17.90502 - Test loss: 53.87934\n",
      "Epoch 4899 - lr: 0.05000 - Train loss: 17.90498 - Test loss: 53.87923\n",
      "Epoch 4900 - lr: 0.05000 - Train loss: 17.90494 - Test loss: 53.87908\n",
      "Epoch 4901 - lr: 0.05000 - Train loss: 17.90491 - Test loss: 53.87892\n",
      "Epoch 4902 - lr: 0.05000 - Train loss: 17.90487 - Test loss: 53.87875\n",
      "Epoch 4903 - lr: 0.05000 - Train loss: 17.90484 - Test loss: 53.87855\n",
      "Epoch 4904 - lr: 0.05000 - Train loss: 17.90481 - Test loss: 53.87834\n",
      "Epoch 4905 - lr: 0.05000 - Train loss: 17.90478 - Test loss: 53.87811\n",
      "Epoch 4906 - lr: 0.05000 - Train loss: 17.90475 - Test loss: 53.87786\n",
      "Epoch 4907 - lr: 0.05000 - Train loss: 17.90472 - Test loss: 53.87760\n",
      "Epoch 4908 - lr: 0.05000 - Train loss: 17.90469 - Test loss: 53.87733\n",
      "Epoch 4909 - lr: 0.05000 - Train loss: 17.90466 - Test loss: 53.87704\n",
      "Epoch 4910 - lr: 0.05000 - Train loss: 17.90464 - Test loss: 53.87675\n",
      "Epoch 4911 - lr: 0.05000 - Train loss: 17.90461 - Test loss: 53.87644\n",
      "Epoch 4912 - lr: 0.05000 - Train loss: 17.90458 - Test loss: 53.87612\n",
      "Epoch 4913 - lr: 0.05000 - Train loss: 17.90456 - Test loss: 53.87579\n",
      "Epoch 4914 - lr: 0.05000 - Train loss: 17.90453 - Test loss: 53.87546\n",
      "Epoch 4915 - lr: 0.05000 - Train loss: 17.90451 - Test loss: 53.87511\n",
      "Epoch 4916 - lr: 0.05000 - Train loss: 17.90449 - Test loss: 53.87476\n",
      "Epoch 4917 - lr: 0.05000 - Train loss: 17.90446 - Test loss: 53.87440\n",
      "Epoch 4918 - lr: 0.05000 - Train loss: 17.90444 - Test loss: 53.87403\n",
      "Epoch 4919 - lr: 0.05000 - Train loss: 17.90442 - Test loss: 53.87366\n",
      "Epoch 4920 - lr: 0.05000 - Train loss: 17.90440 - Test loss: 53.87328\n",
      "Epoch 4921 - lr: 0.05000 - Train loss: 17.90437 - Test loss: 53.87290\n",
      "Epoch 4922 - lr: 0.05000 - Train loss: 17.90435 - Test loss: 53.87251\n",
      "Epoch 4923 - lr: 0.05000 - Train loss: 17.90433 - Test loss: 53.87212\n",
      "Epoch 4924 - lr: 0.05000 - Train loss: 17.90431 - Test loss: 53.87173\n",
      "Epoch 4925 - lr: 0.05000 - Train loss: 17.90429 - Test loss: 53.87134\n",
      "Epoch 4926 - lr: 0.05000 - Train loss: 17.90426 - Test loss: 53.87094\n",
      "Epoch 4927 - lr: 0.05000 - Train loss: 17.90424 - Test loss: 53.87054\n",
      "Epoch 4928 - lr: 0.05000 - Train loss: 17.90422 - Test loss: 53.87014\n",
      "Epoch 4929 - lr: 0.05000 - Train loss: 17.90420 - Test loss: 53.86974\n",
      "Epoch 4930 - lr: 0.05000 - Train loss: 17.90418 - Test loss: 53.86934\n",
      "Epoch 4931 - lr: 0.05000 - Train loss: 17.90416 - Test loss: 53.86894\n",
      "Epoch 4932 - lr: 0.05000 - Train loss: 17.90414 - Test loss: 53.86854\n",
      "Epoch 4933 - lr: 0.05000 - Train loss: 17.90411 - Test loss: 53.86814\n",
      "Epoch 4934 - lr: 0.05000 - Train loss: 17.90409 - Test loss: 53.86774\n",
      "Epoch 4935 - lr: 0.05000 - Train loss: 17.90407 - Test loss: 53.86734\n",
      "Epoch 4936 - lr: 0.05000 - Train loss: 17.90405 - Test loss: 53.86695\n",
      "Epoch 4937 - lr: 0.05000 - Train loss: 17.90403 - Test loss: 53.86655\n",
      "Epoch 4938 - lr: 0.05000 - Train loss: 17.90401 - Test loss: 53.86616\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4939 - lr: 0.05000 - Train loss: 17.90398 - Test loss: 53.86577\n",
      "Epoch 4940 - lr: 0.05000 - Train loss: 17.90396 - Test loss: 53.86539\n",
      "Epoch 4941 - lr: 0.05000 - Train loss: 17.90394 - Test loss: 53.86500\n",
      "Epoch 4942 - lr: 0.05000 - Train loss: 17.90392 - Test loss: 53.86462\n",
      "Epoch 4943 - lr: 0.05000 - Train loss: 17.90390 - Test loss: 53.86425\n",
      "Epoch 4944 - lr: 0.05000 - Train loss: 17.90387 - Test loss: 53.86387\n",
      "Epoch 4945 - lr: 0.05000 - Train loss: 17.90385 - Test loss: 53.86350\n",
      "Epoch 4946 - lr: 0.05000 - Train loss: 17.90383 - Test loss: 53.86313\n",
      "Epoch 4947 - lr: 0.05000 - Train loss: 17.90381 - Test loss: 53.86277\n",
      "Epoch 4948 - lr: 0.05000 - Train loss: 17.90378 - Test loss: 53.86241\n",
      "Epoch 4949 - lr: 0.05000 - Train loss: 17.90376 - Test loss: 53.86205\n",
      "Epoch 4950 - lr: 0.05000 - Train loss: 17.90374 - Test loss: 53.86169\n",
      "Epoch 4951 - lr: 0.05000 - Train loss: 17.90372 - Test loss: 53.86134\n",
      "Epoch 4952 - lr: 0.05000 - Train loss: 17.90369 - Test loss: 53.86100\n",
      "Epoch 4953 - lr: 0.05000 - Train loss: 17.90367 - Test loss: 53.86065\n",
      "Epoch 4954 - lr: 0.05000 - Train loss: 17.90365 - Test loss: 53.86031\n",
      "Epoch 4955 - lr: 0.05000 - Train loss: 17.90362 - Test loss: 53.85997\n",
      "Epoch 4956 - lr: 0.05000 - Train loss: 17.90360 - Test loss: 53.85964\n",
      "Epoch 4957 - lr: 0.05000 - Train loss: 17.90357 - Test loss: 53.85931\n",
      "Epoch 4958 - lr: 0.05000 - Train loss: 17.90355 - Test loss: 53.85898\n",
      "Epoch 4959 - lr: 0.05000 - Train loss: 17.90352 - Test loss: 53.85866\n",
      "Epoch 4960 - lr: 0.05000 - Train loss: 17.90350 - Test loss: 53.85833\n",
      "Epoch 4961 - lr: 0.05000 - Train loss: 17.90348 - Test loss: 53.85801\n",
      "Epoch 4962 - lr: 0.05000 - Train loss: 17.90345 - Test loss: 53.85770\n",
      "Epoch 4963 - lr: 0.05000 - Train loss: 17.90343 - Test loss: 53.85739\n",
      "Epoch 4964 - lr: 0.05000 - Train loss: 17.90340 - Test loss: 53.85708\n",
      "Epoch 4965 - lr: 0.05000 - Train loss: 17.90337 - Test loss: 53.85677\n",
      "Epoch 4966 - lr: 0.05000 - Train loss: 17.90335 - Test loss: 53.85646\n",
      "Epoch 4967 - lr: 0.05000 - Train loss: 17.90332 - Test loss: 53.85616\n",
      "Epoch 4968 - lr: 0.05000 - Train loss: 17.90330 - Test loss: 53.85586\n",
      "Epoch 4969 - lr: 0.05000 - Train loss: 17.90327 - Test loss: 53.85556\n",
      "Epoch 4970 - lr: 0.05000 - Train loss: 17.90324 - Test loss: 53.85527\n",
      "Epoch 4971 - lr: 0.05000 - Train loss: 17.90322 - Test loss: 53.85498\n",
      "Epoch 4972 - lr: 0.05000 - Train loss: 17.90319 - Test loss: 53.85469\n",
      "Epoch 4973 - lr: 0.05000 - Train loss: 17.90316 - Test loss: 53.85440\n",
      "Epoch 4974 - lr: 0.05000 - Train loss: 17.90313 - Test loss: 53.85412\n",
      "Epoch 4975 - lr: 0.05000 - Train loss: 17.90310 - Test loss: 53.85384\n",
      "Epoch 4976 - lr: 0.05000 - Train loss: 17.90307 - Test loss: 53.85356\n",
      "Epoch 4977 - lr: 0.05000 - Train loss: 17.90304 - Test loss: 53.85328\n",
      "Epoch 4978 - lr: 0.05000 - Train loss: 17.90301 - Test loss: 53.85301\n",
      "Epoch 4979 - lr: 0.05000 - Train loss: 17.90298 - Test loss: 53.85274\n",
      "Epoch 4980 - lr: 0.05000 - Train loss: 17.90295 - Test loss: 53.85248\n",
      "Epoch 4981 - lr: 0.05000 - Train loss: 17.90292 - Test loss: 53.85221\n",
      "Epoch 4982 - lr: 0.05000 - Train loss: 17.90289 - Test loss: 53.85195\n",
      "Epoch 4983 - lr: 0.05000 - Train loss: 17.90286 - Test loss: 53.85170\n",
      "Epoch 4984 - lr: 0.05000 - Train loss: 17.90282 - Test loss: 53.85145\n",
      "Epoch 4985 - lr: 0.05000 - Train loss: 17.90279 - Test loss: 53.85120\n",
      "Epoch 4986 - lr: 0.05000 - Train loss: 17.90275 - Test loss: 53.85096\n",
      "Epoch 4987 - lr: 0.05000 - Train loss: 17.90272 - Test loss: 53.85072\n",
      "Epoch 4988 - lr: 0.05000 - Train loss: 17.90268 - Test loss: 53.85049\n",
      "Epoch 4989 - lr: 0.05000 - Train loss: 17.90264 - Test loss: 53.85027\n",
      "Epoch 4990 - lr: 0.05000 - Train loss: 17.90260 - Test loss: 53.85005\n",
      "Epoch 4991 - lr: 0.05000 - Train loss: 17.90256 - Test loss: 53.84985\n",
      "Epoch 4992 - lr: 0.05000 - Train loss: 17.90251 - Test loss: 53.84965\n",
      "Epoch 4993 - lr: 0.05000 - Train loss: 17.90247 - Test loss: 53.84946\n",
      "Epoch 4994 - lr: 0.05000 - Train loss: 17.90242 - Test loss: 53.84929\n",
      "Epoch 4995 - lr: 0.05000 - Train loss: 17.90237 - Test loss: 53.84913\n",
      "Epoch 4996 - lr: 0.05000 - Train loss: 17.90231 - Test loss: 53.84898\n",
      "Epoch 4997 - lr: 0.05000 - Train loss: 17.90225 - Test loss: 53.84886\n",
      "Epoch 4998 - lr: 0.05000 - Train loss: 17.90219 - Test loss: 53.84875\n",
      "Epoch 4999 - lr: 0.05000 - Train loss: 17.90213 - Test loss: 53.84868\n",
      "Epoch 5000 - lr: 0.05000 - Train loss: 17.90205 - Test loss: 53.84863\n",
      "number of hidden neurons in first layer is: 120 number of hidden neurons in second layer is: 480\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of hidden neurons in first layer is: 240 number of hidden neurons in second layer is: 360\n",
      "Epoch 1 - lr: 0.05000 - Train loss: 14.90233 - Test loss: 186.42954\n",
      "Epoch 2 - lr: 0.05000 - Train loss: 9.49486 - Test loss: 118.93195\n",
      "Epoch 3 - lr: 0.05000 - Train loss: 10.00148 - Test loss: 113.29387\n",
      "Epoch 4 - lr: 0.05000 - Train loss: 9.41976 - Test loss: 103.59700\n",
      "Epoch 5 - lr: 0.05000 - Train loss: 9.24265 - Test loss: 82.70257\n",
      "Epoch 6 - lr: 0.05000 - Train loss: 7.83941 - Test loss: 44.23331\n",
      "Epoch 7 - lr: 0.05000 - Train loss: 5.61134 - Test loss: 20.88391\n",
      "Epoch 8 - lr: 0.05000 - Train loss: 4.84708 - Test loss: 13.20968\n",
      "Epoch 9 - lr: 0.05000 - Train loss: 4.44498 - Test loss: 9.67512\n",
      "Epoch 10 - lr: 0.05000 - Train loss: 4.05157 - Test loss: 7.55863\n",
      "Epoch 11 - lr: 0.05000 - Train loss: 3.69707 - Test loss: 6.18314\n",
      "Epoch 12 - lr: 0.05000 - Train loss: 3.39082 - Test loss: 5.22834\n",
      "Epoch 13 - lr: 0.05000 - Train loss: 3.13463 - Test loss: 4.53802\n",
      "Epoch 14 - lr: 0.05000 - Train loss: 2.92708 - Test loss: 4.02832\n",
      "Epoch 15 - lr: 0.05000 - Train loss: 2.76294 - Test loss: 3.64594\n",
      "Epoch 16 - lr: 0.05000 - Train loss: 2.63475 - Test loss: 3.35430\n",
      "Epoch 17 - lr: 0.05000 - Train loss: 2.53487 - Test loss: 3.12800\n",
      "Epoch 18 - lr: 0.05000 - Train loss: 2.45665 - Test loss: 2.94943\n",
      "Epoch 19 - lr: 0.05000 - Train loss: 2.39481 - Test loss: 2.80638\n",
      "Epoch 20 - lr: 0.05000 - Train loss: 2.34534 - Test loss: 2.69022\n",
      "Epoch 21 - lr: 0.05000 - Train loss: 2.30528 - Test loss: 2.59480\n",
      "Epoch 22 - lr: 0.05000 - Train loss: 2.27242 - Test loss: 2.51561\n",
      "Epoch 23 - lr: 0.05000 - Train loss: 2.24514 - Test loss: 2.44927\n",
      "Epoch 24 - lr: 0.05000 - Train loss: 2.22218 - Test loss: 2.39319\n",
      "Epoch 25 - lr: 0.05000 - Train loss: 2.20246 - Test loss: 2.34524\n",
      "Epoch 26 - lr: 0.05000 - Train loss: 2.18416 - Test loss: 2.30268\n",
      "Epoch 27 - lr: 0.05000 - Train loss: 2.15007 - Test loss: 2.27128\n",
      "Epoch 28 - lr: 0.05000 - Train loss: 2.11090 - Test loss: 2.25996\n",
      "Epoch 29 - lr: 0.05000 - Train loss: 2.08893 - Test loss: 2.22875\n",
      "Epoch 30 - lr: 0.05000 - Train loss: 2.07433 - Test loss: 2.20026\n",
      "Epoch 31 - lr: 0.05000 - Train loss: 2.06293 - Test loss: 2.17715\n",
      "Epoch 32 - lr: 0.05000 - Train loss: 2.05350 - Test loss: 2.15731\n",
      "Epoch 33 - lr: 0.05000 - Train loss: 2.04536 - Test loss: 2.13823\n",
      "Epoch 34 - lr: 0.05000 - Train loss: 2.03893 - Test loss: 2.12153\n",
      "Epoch 35 - lr: 0.05000 - Train loss: 2.03442 - Test loss: 2.10881\n",
      "Epoch 36 - lr: 0.05000 - Train loss: 2.03093 - Test loss: 2.10012\n",
      "Epoch 37 - lr: 0.05000 - Train loss: 2.02777 - Test loss: 2.09399\n",
      "Epoch 38 - lr: 0.05000 - Train loss: 2.02491 - Test loss: 2.08737\n",
      "Epoch 39 - lr: 0.05000 - Train loss: 2.02246 - Test loss: 2.07778\n",
      "Epoch 40 - lr: 0.05000 - Train loss: 2.02093 - Test loss: 2.06599\n",
      "Epoch 41 - lr: 0.05000 - Train loss: 2.02178 - Test loss: 2.05956\n",
      "Epoch 42 - lr: 0.05000 - Train loss: 2.02623 - Test loss: 2.09017\n",
      "Epoch 43 - lr: 0.05000 - Train loss: 2.01059 - Test loss: 2.04699\n",
      "Epoch 44 - lr: 0.05000 - Train loss: 2.00397 - Test loss: 2.03946\n",
      "Epoch 45 - lr: 0.05000 - Train loss: 2.01003 - Test loss: 2.06713\n",
      "Epoch 46 - lr: 0.05000 - Train loss: 2.00177 - Test loss: 2.03060\n",
      "Epoch 47 - lr: 0.05000 - Train loss: 2.02077 - Test loss: 2.07000\n",
      "Epoch 48 - lr: 0.05000 - Train loss: 2.01792 - Test loss: 2.06426\n",
      "Epoch 49 - lr: 0.05000 - Train loss: 2.01689 - Test loss: 2.06187\n",
      "Epoch 50 - lr: 0.05000 - Train loss: 2.02301 - Test loss: 2.09613\n",
      "Epoch 51 - lr: 0.05000 - Train loss: 2.00855 - Test loss: 2.05597\n",
      "Epoch 52 - lr: 0.05000 - Train loss: 1.99643 - Test loss: 2.01868\n",
      "Epoch 53 - lr: 0.05000 - Train loss: 2.00140 - Test loss: 2.02968\n",
      "Epoch 54 - lr: 0.05000 - Train loss: 2.00034 - Test loss: 2.01986\n",
      "Epoch 55 - lr: 0.05000 - Train loss: 2.00933 - Test loss: 2.04861\n",
      "Epoch 56 - lr: 0.05000 - Train loss: 2.01562 - Test loss: 2.06585\n",
      "Epoch 57 - lr: 0.05000 - Train loss: 2.02520 - Test loss: 2.06015\n",
      "Epoch 58 - lr: 0.05000 - Train loss: 2.02825 - Test loss: 2.06271\n",
      "Epoch 59 - lr: 0.05000 - Train loss: 2.01992 - Test loss: 2.06742\n",
      "Epoch 60 - lr: 0.05000 - Train loss: 2.01538 - Test loss: 2.04539\n",
      "Epoch 61 - lr: 0.05000 - Train loss: 2.01654 - Test loss: 2.07625\n",
      "Epoch 62 - lr: 0.05000 - Train loss: 2.01277 - Test loss: 2.06200\n",
      "Epoch 63 - lr: 0.05000 - Train loss: 2.01684 - Test loss: 2.08364\n",
      "Epoch 64 - lr: 0.05000 - Train loss: 2.01469 - Test loss: 2.10948\n",
      "Epoch 65 - lr: 0.05000 - Train loss: 2.02158 - Test loss: 2.11100\n",
      "Epoch 66 - lr: 0.05000 - Train loss: 2.02401 - Test loss: 2.08568\n",
      "Epoch 67 - lr: 0.05000 - Train loss: 2.01630 - Test loss: 2.07741\n",
      "Epoch 68 - lr: 0.05000 - Train loss: 2.01660 - Test loss: 2.09976\n",
      "Epoch 69 - lr: 0.05000 - Train loss: 2.01426 - Test loss: 2.07982\n",
      "Epoch 70 - lr: 0.05000 - Train loss: 2.02146 - Test loss: 2.09958\n",
      "Epoch 71 - lr: 0.05000 - Train loss: 2.01931 - Test loss: 2.06849\n",
      "Epoch 72 - lr: 0.05000 - Train loss: 2.01701 - Test loss: 2.06865\n",
      "Epoch 73 - lr: 0.05000 - Train loss: 2.01932 - Test loss: 2.08871\n",
      "Epoch 74 - lr: 0.05000 - Train loss: 2.01542 - Test loss: 2.08720\n",
      "Epoch 75 - lr: 0.05000 - Train loss: 2.01147 - Test loss: 2.07792\n",
      "Epoch 76 - lr: 0.05000 - Train loss: 2.01289 - Test loss: 2.07969\n",
      "Epoch 77 - lr: 0.05000 - Train loss: 2.01425 - Test loss: 2.08034\n",
      "Epoch 78 - lr: 0.05000 - Train loss: 2.01593 - Test loss: 2.08161\n",
      "Epoch 79 - lr: 0.05000 - Train loss: 2.01784 - Test loss: 2.08395\n",
      "Epoch 80 - lr: 0.05000 - Train loss: 2.02007 - Test loss: 2.08709\n",
      "Epoch 81 - lr: 0.05000 - Train loss: 2.02252 - Test loss: 2.09143\n",
      "Epoch 82 - lr: 0.05000 - Train loss: 2.02507 - Test loss: 2.09937\n",
      "Epoch 83 - lr: 0.05000 - Train loss: 2.02264 - Test loss: 2.09719\n",
      "Epoch 84 - lr: 0.05000 - Train loss: 2.03274 - Test loss: 2.11053\n",
      "Epoch 85 - lr: 0.05000 - Train loss: 2.01227 - Test loss: 2.06912\n",
      "Epoch 86 - lr: 0.05000 - Train loss: 2.06178 - Test loss: 2.12733\n",
      "Epoch 87 - lr: 0.05000 - Train loss: 2.02631 - Test loss: 2.07389\n",
      "Epoch 88 - lr: 0.05000 - Train loss: 2.04659 - Test loss: 2.11087\n",
      "Epoch 89 - lr: 0.05000 - Train loss: 2.04021 - Test loss: 2.12428\n",
      "Epoch 90 - lr: 0.05000 - Train loss: 2.02265 - Test loss: 2.11494\n",
      "Epoch 91 - lr: 0.05000 - Train loss: 2.04256 - Test loss: 2.28029\n",
      "Epoch 92 - lr: 0.05000 - Train loss: 2.02636 - Test loss: 2.21384\n",
      "Epoch 93 - lr: 0.05000 - Train loss: 2.02117 - Test loss: 2.35102\n",
      "Epoch 94 - lr: 0.05000 - Train loss: 2.01964 - Test loss: 2.28086\n",
      "Epoch 95 - lr: 0.05000 - Train loss: 2.10124 - Test loss: 2.47880\n",
      "Epoch 96 - lr: 0.05000 - Train loss: 2.01834 - Test loss: 2.44509\n",
      "Epoch 97 - lr: 0.05000 - Train loss: 1.94455 - Test loss: 2.37980\n",
      "Epoch 98 - lr: 0.05000 - Train loss: 1.89876 - Test loss: 2.17826\n",
      "Epoch 99 - lr: 0.05000 - Train loss: 1.85126 - Test loss: 2.55593\n",
      "Epoch 100 - lr: 0.05000 - Train loss: 1.87562 - Test loss: 2.47471\n",
      "Epoch 101 - lr: 0.05000 - Train loss: 1.97542 - Test loss: 2.52363\n",
      "Epoch 102 - lr: 0.05000 - Train loss: 2.03617 - Test loss: 2.42961\n",
      "Epoch 103 - lr: 0.05000 - Train loss: 2.23924 - Test loss: 2.31719\n",
      "Epoch 104 - lr: 0.05000 - Train loss: 2.10339 - Test loss: 2.46086\n",
      "Epoch 105 - lr: 0.05000 - Train loss: 1.96631 - Test loss: 2.46793\n",
      "Epoch 106 - lr: 0.05000 - Train loss: 1.91751 - Test loss: 2.51993\n",
      "Epoch 107 - lr: 0.05000 - Train loss: 1.85221 - Test loss: 2.46625\n",
      "Epoch 108 - lr: 0.05000 - Train loss: 1.89312 - Test loss: 2.91297\n",
      "Epoch 109 - lr: 0.05000 - Train loss: 1.92414 - Test loss: 2.33440\n",
      "Epoch 110 - lr: 0.05000 - Train loss: 1.71935 - Test loss: 2.20306\n",
      "Epoch 111 - lr: 0.05000 - Train loss: 1.65030 - Test loss: 2.28217\n",
      "Epoch 112 - lr: 0.05000 - Train loss: 1.64361 - Test loss: 2.24690\n",
      "Epoch 113 - lr: 0.05000 - Train loss: 2.14497 - Test loss: 2.38200\n",
      "Epoch 114 - lr: 0.05000 - Train loss: 2.08789 - Test loss: 2.18574\n",
      "Epoch 115 - lr: 0.05000 - Train loss: 2.08865 - Test loss: 2.15104\n",
      "Epoch 116 - lr: 0.05000 - Train loss: 1.88740 - Test loss: 2.49852\n",
      "Epoch 117 - lr: 0.05000 - Train loss: 1.75871 - Test loss: 2.41291\n",
      "Epoch 118 - lr: 0.05000 - Train loss: 1.89248 - Test loss: 2.15429\n",
      "Epoch 119 - lr: 0.05000 - Train loss: 1.84416 - Test loss: 2.18665\n",
      "Epoch 120 - lr: 0.05000 - Train loss: 1.62231 - Test loss: 2.21590\n",
      "Epoch 121 - lr: 0.05000 - Train loss: 1.53948 - Test loss: 1.90990\n",
      "Epoch 122 - lr: 0.05000 - Train loss: 1.49374 - Test loss: 2.03557\n",
      "Epoch 123 - lr: 0.05000 - Train loss: 1.51650 - Test loss: 2.15281\n",
      "Epoch 124 - lr: 0.05000 - Train loss: 1.53355 - Test loss: 2.16808\n",
      "Epoch 125 - lr: 0.05000 - Train loss: 1.50062 - Test loss: 2.12108\n",
      "Epoch 126 - lr: 0.05000 - Train loss: 1.65486 - Test loss: 2.64222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 127 - lr: 0.05000 - Train loss: 1.71197 - Test loss: 2.21265\n",
      "Epoch 128 - lr: 0.05000 - Train loss: 1.53787 - Test loss: 2.21376\n",
      "Epoch 129 - lr: 0.05000 - Train loss: 1.70126 - Test loss: 2.21003\n",
      "Epoch 130 - lr: 0.05000 - Train loss: 1.69294 - Test loss: 2.19676\n",
      "Epoch 131 - lr: 0.05000 - Train loss: 1.58939 - Test loss: 2.20096\n",
      "Epoch 132 - lr: 0.05000 - Train loss: 1.55242 - Test loss: 2.00729\n",
      "Epoch 133 - lr: 0.05000 - Train loss: 1.44753 - Test loss: 2.39788\n",
      "Epoch 134 - lr: 0.05000 - Train loss: 1.93094 - Test loss: 2.00384\n",
      "Epoch 135 - lr: 0.05000 - Train loss: 1.73909 - Test loss: 2.19647\n",
      "Epoch 136 - lr: 0.05000 - Train loss: 1.60056 - Test loss: 1.53663\n",
      "Epoch 137 - lr: 0.05000 - Train loss: 1.38662 - Test loss: 1.75123\n",
      "Epoch 138 - lr: 0.05000 - Train loss: 1.50944 - Test loss: 2.47235\n",
      "Epoch 139 - lr: 0.05000 - Train loss: 1.50913 - Test loss: 2.04206\n",
      "Epoch 140 - lr: 0.05000 - Train loss: 1.46841 - Test loss: 2.36048\n",
      "Epoch 141 - lr: 0.05000 - Train loss: 1.64512 - Test loss: 1.42882\n",
      "Epoch 142 - lr: 0.05000 - Train loss: 1.33447 - Test loss: 1.43095\n",
      "Epoch 143 - lr: 0.05000 - Train loss: 1.30093 - Test loss: 1.83969\n",
      "Epoch 144 - lr: 0.05000 - Train loss: 1.31033 - Test loss: 1.75052\n",
      "Epoch 145 - lr: 0.05000 - Train loss: 1.32678 - Test loss: 1.47858\n",
      "Epoch 146 - lr: 0.05000 - Train loss: 1.33896 - Test loss: 1.81332\n",
      "Epoch 147 - lr: 0.05000 - Train loss: 1.29426 - Test loss: 1.49687\n",
      "Epoch 148 - lr: 0.05000 - Train loss: 1.32967 - Test loss: 1.62740\n",
      "Epoch 149 - lr: 0.05000 - Train loss: 1.29219 - Test loss: 1.56158\n",
      "Epoch 150 - lr: 0.05000 - Train loss: 1.32722 - Test loss: 1.69756\n",
      "Epoch 151 - lr: 0.05000 - Train loss: 1.25912 - Test loss: 1.36804\n",
      "Epoch 152 - lr: 0.05000 - Train loss: 1.54849 - Test loss: 2.39459\n",
      "Epoch 153 - lr: 0.05000 - Train loss: 1.56272 - Test loss: 2.18536\n",
      "Epoch 154 - lr: 0.05000 - Train loss: 1.32907 - Test loss: 1.62575\n",
      "Epoch 155 - lr: 0.05000 - Train loss: 1.37514 - Test loss: 2.18827\n",
      "Epoch 156 - lr: 0.05000 - Train loss: 1.48730 - Test loss: 1.41548\n",
      "Epoch 157 - lr: 0.05000 - Train loss: 1.23439 - Test loss: 1.45840\n",
      "Epoch 158 - lr: 0.05000 - Train loss: 1.15416 - Test loss: 1.75635\n",
      "Epoch 159 - lr: 0.05000 - Train loss: 1.45677 - Test loss: 1.38657\n",
      "Epoch 160 - lr: 0.05000 - Train loss: 1.24360 - Test loss: 1.47611\n",
      "Epoch 161 - lr: 0.05000 - Train loss: 1.26905 - Test loss: 1.42958\n",
      "Epoch 162 - lr: 0.05000 - Train loss: 1.24223 - Test loss: 1.45054\n",
      "Epoch 163 - lr: 0.05000 - Train loss: 1.16421 - Test loss: 1.57149\n",
      "Epoch 164 - lr: 0.05000 - Train loss: 1.29796 - Test loss: 2.20378\n",
      "Epoch 165 - lr: 0.05000 - Train loss: 1.44924 - Test loss: 1.43820\n",
      "Epoch 166 - lr: 0.05000 - Train loss: 1.34017 - Test loss: 1.41062\n",
      "Epoch 167 - lr: 0.05000 - Train loss: 1.30053 - Test loss: 1.51104\n",
      "Epoch 168 - lr: 0.05000 - Train loss: 1.18558 - Test loss: 1.41235\n",
      "Epoch 169 - lr: 0.05000 - Train loss: 1.24426 - Test loss: 1.55520\n",
      "Epoch 170 - lr: 0.05000 - Train loss: 1.32701 - Test loss: 1.48521\n",
      "Epoch 171 - lr: 0.05000 - Train loss: 1.30827 - Test loss: 1.44172\n",
      "Epoch 172 - lr: 0.05000 - Train loss: 1.29732 - Test loss: 1.42067\n",
      "Epoch 173 - lr: 0.05000 - Train loss: 1.20013 - Test loss: 1.35968\n",
      "Epoch 174 - lr: 0.05000 - Train loss: 1.07958 - Test loss: 1.42012\n",
      "Epoch 175 - lr: 0.05000 - Train loss: 1.00431 - Test loss: 1.05975\n",
      "Epoch 176 - lr: 0.05000 - Train loss: 1.27441 - Test loss: 1.36913\n",
      "Epoch 177 - lr: 0.05000 - Train loss: 1.15032 - Test loss: 1.33353\n",
      "Epoch 178 - lr: 0.05000 - Train loss: 1.15546 - Test loss: 1.31153\n",
      "Epoch 179 - lr: 0.05000 - Train loss: 1.08006 - Test loss: 1.74450\n",
      "Epoch 180 - lr: 0.05000 - Train loss: 1.12892 - Test loss: 1.58077\n",
      "Epoch 181 - lr: 0.05000 - Train loss: 1.18076 - Test loss: 1.47300\n",
      "Epoch 182 - lr: 0.05000 - Train loss: 1.31415 - Test loss: 1.44508\n",
      "Epoch 183 - lr: 0.05000 - Train loss: 1.12122 - Test loss: 1.38128\n",
      "Epoch 184 - lr: 0.05000 - Train loss: 1.00209 - Test loss: 1.60396\n",
      "Epoch 185 - lr: 0.05000 - Train loss: 1.21723 - Test loss: 1.78634\n",
      "Epoch 186 - lr: 0.05000 - Train loss: 1.44894 - Test loss: 1.50531\n",
      "Epoch 187 - lr: 0.05000 - Train loss: 1.32727 - Test loss: 1.75481\n",
      "Epoch 188 - lr: 0.05000 - Train loss: 1.39780 - Test loss: 1.50981\n",
      "Epoch 189 - lr: 0.05000 - Train loss: 1.36412 - Test loss: 1.44250\n",
      "Epoch 190 - lr: 0.05000 - Train loss: 1.32913 - Test loss: 1.46173\n",
      "Epoch 191 - lr: 0.05000 - Train loss: 1.32693 - Test loss: 1.56707\n",
      "Epoch 192 - lr: 0.05000 - Train loss: 1.26667 - Test loss: 1.94122\n",
      "Epoch 193 - lr: 0.05000 - Train loss: 1.31941 - Test loss: 1.54846\n",
      "Epoch 194 - lr: 0.05000 - Train loss: 1.39915 - Test loss: 1.50809\n",
      "Epoch 195 - lr: 0.05000 - Train loss: 1.33121 - Test loss: 1.46800\n",
      "Epoch 196 - lr: 0.05000 - Train loss: 1.32362 - Test loss: 1.46351\n",
      "Epoch 197 - lr: 0.05000 - Train loss: 1.32406 - Test loss: 1.46412\n",
      "Epoch 198 - lr: 0.05000 - Train loss: 1.32312 - Test loss: 1.45270\n",
      "Epoch 199 - lr: 0.05000 - Train loss: 1.24045 - Test loss: 1.60970\n",
      "Epoch 200 - lr: 0.05000 - Train loss: 1.42711 - Test loss: 1.60557\n",
      "Epoch 201 - lr: 0.05000 - Train loss: 1.20085 - Test loss: 1.10042\n",
      "Epoch 202 - lr: 0.05000 - Train loss: 1.17614 - Test loss: 1.61902\n",
      "Epoch 203 - lr: 0.05000 - Train loss: 1.25098 - Test loss: 1.14131\n",
      "Epoch 204 - lr: 0.05000 - Train loss: 1.16515 - Test loss: 1.21551\n",
      "Epoch 205 - lr: 0.05000 - Train loss: 1.25055 - Test loss: 1.29343\n",
      "Epoch 206 - lr: 0.05000 - Train loss: 0.99090 - Test loss: 1.18616\n",
      "Epoch 207 - lr: 0.05000 - Train loss: 1.08575 - Test loss: 1.13721\n",
      "Epoch 208 - lr: 0.05000 - Train loss: 0.93919 - Test loss: 1.22618\n",
      "Epoch 209 - lr: 0.05000 - Train loss: 1.02608 - Test loss: 1.40349\n",
      "Epoch 210 - lr: 0.05000 - Train loss: 1.14771 - Test loss: 1.55667\n",
      "Epoch 211 - lr: 0.05000 - Train loss: 1.30589 - Test loss: 1.68622\n",
      "Epoch 212 - lr: 0.05000 - Train loss: 1.34772 - Test loss: 1.51097\n",
      "Epoch 213 - lr: 0.05000 - Train loss: 1.31579 - Test loss: 1.71026\n",
      "Epoch 214 - lr: 0.05000 - Train loss: 1.44281 - Test loss: 2.13055\n",
      "Epoch 215 - lr: 0.05000 - Train loss: 1.54807 - Test loss: 1.99691\n",
      "Epoch 216 - lr: 0.05000 - Train loss: 1.41727 - Test loss: 1.35552\n",
      "Epoch 217 - lr: 0.05000 - Train loss: 1.41420 - Test loss: 1.32937\n",
      "Epoch 218 - lr: 0.05000 - Train loss: 1.42270 - Test loss: 1.39642\n",
      "Epoch 219 - lr: 0.05000 - Train loss: 1.32063 - Test loss: 1.41046\n",
      "Epoch 220 - lr: 0.05000 - Train loss: 1.31335 - Test loss: 1.41216\n",
      "Epoch 221 - lr: 0.05000 - Train loss: 1.15962 - Test loss: 1.34027\n",
      "Epoch 222 - lr: 0.05000 - Train loss: 1.37646 - Test loss: 1.16410\n",
      "Epoch 223 - lr: 0.05000 - Train loss: 0.94897 - Test loss: 1.06452\n",
      "Epoch 224 - lr: 0.05000 - Train loss: 1.07438 - Test loss: 1.01803\n",
      "Epoch 225 - lr: 0.05000 - Train loss: 0.88035 - Test loss: 1.34364\n",
      "Epoch 226 - lr: 0.05000 - Train loss: 0.94772 - Test loss: 0.96228\n",
      "Epoch 227 - lr: 0.05000 - Train loss: 0.97848 - Test loss: 1.60900\n",
      "Epoch 228 - lr: 0.05000 - Train loss: 1.29367 - Test loss: 1.49820\n",
      "Epoch 229 - lr: 0.05000 - Train loss: 1.22514 - Test loss: 1.49313\n",
      "Epoch 230 - lr: 0.05000 - Train loss: 1.31489 - Test loss: 1.43482\n",
      "Epoch 231 - lr: 0.05000 - Train loss: 1.21516 - Test loss: 0.91748\n",
      "Epoch 232 - lr: 0.05000 - Train loss: 1.17722 - Test loss: 1.60865\n",
      "Epoch 233 - lr: 0.05000 - Train loss: 1.18546 - Test loss: 0.97473\n",
      "Epoch 234 - lr: 0.05000 - Train loss: 0.87055 - Test loss: 1.64938\n",
      "Epoch 235 - lr: 0.05000 - Train loss: 1.24069 - Test loss: 0.91234\n",
      "Epoch 236 - lr: 0.05000 - Train loss: 0.86062 - Test loss: 1.65317\n",
      "Epoch 237 - lr: 0.05000 - Train loss: 1.34373 - Test loss: 1.40403\n",
      "Epoch 238 - lr: 0.05000 - Train loss: 1.04913 - Test loss: 0.89973\n",
      "Epoch 239 - lr: 0.05000 - Train loss: 1.09589 - Test loss: 1.20171\n",
      "Epoch 240 - lr: 0.05000 - Train loss: 1.08418 - Test loss: 1.23810\n",
      "Epoch 241 - lr: 0.05000 - Train loss: 1.06772 - Test loss: 2.09657\n",
      "Epoch 242 - lr: 0.05000 - Train loss: 1.35360 - Test loss: 1.05990\n",
      "Epoch 243 - lr: 0.05000 - Train loss: 1.03717 - Test loss: 1.48252\n",
      "Epoch 244 - lr: 0.05000 - Train loss: 1.31725 - Test loss: 1.37008\n",
      "Epoch 245 - lr: 0.05000 - Train loss: 1.12898 - Test loss: 0.86866\n",
      "Epoch 246 - lr: 0.05000 - Train loss: 0.99808 - Test loss: 1.10376\n",
      "Epoch 247 - lr: 0.05000 - Train loss: 0.99693 - Test loss: 1.61655\n",
      "Epoch 248 - lr: 0.05000 - Train loss: 1.27259 - Test loss: 1.47735\n",
      "Epoch 249 - lr: 0.05000 - Train loss: 1.29444 - Test loss: 0.96484\n",
      "Epoch 250 - lr: 0.05000 - Train loss: 0.86872 - Test loss: 0.96157\n",
      "Epoch 251 - lr: 0.05000 - Train loss: 1.06206 - Test loss: 1.37239\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 252 - lr: 0.05000 - Train loss: 1.12522 - Test loss: 1.55787\n",
      "Epoch 253 - lr: 0.05000 - Train loss: 1.15014 - Test loss: 1.20227\n",
      "Epoch 254 - lr: 0.05000 - Train loss: 0.98142 - Test loss: 1.59278\n",
      "Epoch 255 - lr: 0.05000 - Train loss: 1.35798 - Test loss: 0.91049\n",
      "Epoch 256 - lr: 0.05000 - Train loss: 0.90175 - Test loss: 0.91388\n",
      "Epoch 257 - lr: 0.05000 - Train loss: 0.84789 - Test loss: 1.64345\n",
      "Epoch 258 - lr: 0.05000 - Train loss: 1.32622 - Test loss: 1.47591\n",
      "Epoch 259 - lr: 0.05000 - Train loss: 1.30458 - Test loss: 0.96927\n",
      "Epoch 260 - lr: 0.05000 - Train loss: 0.93853 - Test loss: 1.40088\n",
      "Epoch 261 - lr: 0.05000 - Train loss: 1.03150 - Test loss: 0.88318\n",
      "Epoch 262 - lr: 0.05000 - Train loss: 1.27772 - Test loss: 1.09732\n",
      "Epoch 263 - lr: 0.05000 - Train loss: 1.00778 - Test loss: 0.99095\n",
      "Epoch 264 - lr: 0.05000 - Train loss: 1.25965 - Test loss: 1.13454\n",
      "Epoch 265 - lr: 0.05000 - Train loss: 0.97712 - Test loss: 0.97430\n",
      "Epoch 266 - lr: 0.05000 - Train loss: 0.88371 - Test loss: 1.60699\n",
      "Epoch 267 - lr: 0.05000 - Train loss: 1.19280 - Test loss: 1.22257\n",
      "Epoch 268 - lr: 0.05000 - Train loss: 1.30666 - Test loss: 1.07687\n",
      "Epoch 269 - lr: 0.05000 - Train loss: 1.10703 - Test loss: 1.07389\n",
      "Epoch 270 - lr: 0.05000 - Train loss: 1.31194 - Test loss: 1.13701\n",
      "Epoch 271 - lr: 0.05000 - Train loss: 1.00631 - Test loss: 1.01495\n",
      "Epoch 272 - lr: 0.05000 - Train loss: 0.91267 - Test loss: 1.34205\n",
      "Epoch 273 - lr: 0.05000 - Train loss: 1.33237 - Test loss: 1.14551\n",
      "Epoch 274 - lr: 0.05000 - Train loss: 0.94530 - Test loss: 0.99463\n",
      "Epoch 275 - lr: 0.05000 - Train loss: 0.90699 - Test loss: 1.44367\n",
      "Epoch 276 - lr: 0.05000 - Train loss: 1.07799 - Test loss: 1.59974\n",
      "Epoch 277 - lr: 0.05000 - Train loss: 1.09017 - Test loss: 1.49609\n",
      "Epoch 278 - lr: 0.05000 - Train loss: 1.07280 - Test loss: 0.90386\n",
      "Epoch 279 - lr: 0.05000 - Train loss: 0.90058 - Test loss: 0.93218\n",
      "Epoch 280 - lr: 0.05000 - Train loss: 0.87543 - Test loss: 1.42846\n",
      "Epoch 281 - lr: 0.05000 - Train loss: 1.11176 - Test loss: 1.66597\n",
      "Epoch 282 - lr: 0.05000 - Train loss: 1.35060 - Test loss: 1.66015\n",
      "Epoch 283 - lr: 0.05000 - Train loss: 1.24621 - Test loss: 1.56685\n",
      "Epoch 284 - lr: 0.05000 - Train loss: 1.33038 - Test loss: 1.44150\n",
      "Epoch 285 - lr: 0.05000 - Train loss: 1.30439 - Test loss: 0.87928\n",
      "Epoch 286 - lr: 0.05000 - Train loss: 1.22027 - Test loss: 1.14613\n",
      "Epoch 287 - lr: 0.05000 - Train loss: 0.94955 - Test loss: 1.01529\n",
      "Epoch 288 - lr: 0.05000 - Train loss: 1.01033 - Test loss: 1.46737\n",
      "Epoch 289 - lr: 0.05000 - Train loss: 1.06456 - Test loss: 0.89810\n",
      "Epoch 290 - lr: 0.05000 - Train loss: 0.88408 - Test loss: 0.94841\n",
      "Epoch 291 - lr: 0.05000 - Train loss: 1.05890 - Test loss: 1.02070\n",
      "Epoch 292 - lr: 0.05000 - Train loss: 0.90571 - Test loss: 0.92729\n",
      "Epoch 293 - lr: 0.05000 - Train loss: 0.88419 - Test loss: 1.67949\n",
      "Epoch 294 - lr: 0.05000 - Train loss: 1.40247 - Test loss: 1.28348\n",
      "Epoch 295 - lr: 0.05000 - Train loss: 1.29309 - Test loss: 1.09947\n",
      "Epoch 296 - lr: 0.05000 - Train loss: 0.90842 - Test loss: 0.96614\n",
      "Epoch 297 - lr: 0.05000 - Train loss: 0.89588 - Test loss: 1.10565\n",
      "Epoch 298 - lr: 0.05000 - Train loss: 0.97735 - Test loss: 1.59177\n",
      "Epoch 299 - lr: 0.05000 - Train loss: 1.18385 - Test loss: 0.87288\n",
      "Epoch 300 - lr: 0.05000 - Train loss: 1.07298 - Test loss: 0.96813\n",
      "Epoch 301 - lr: 0.05000 - Train loss: 0.83541 - Test loss: 0.91779\n",
      "Epoch 302 - lr: 0.05000 - Train loss: 0.84960 - Test loss: 0.91918\n",
      "Epoch 303 - lr: 0.05000 - Train loss: 0.84733 - Test loss: 0.90242\n",
      "Epoch 304 - lr: 0.05000 - Train loss: 0.96964 - Test loss: 0.90701\n",
      "Epoch 305 - lr: 0.05000 - Train loss: 1.05482 - Test loss: 0.94245\n",
      "Epoch 306 - lr: 0.05000 - Train loss: 1.18300 - Test loss: 0.95450\n",
      "Epoch 307 - lr: 0.05000 - Train loss: 1.02056 - Test loss: 0.99107\n",
      "Epoch 308 - lr: 0.05000 - Train loss: 0.92174 - Test loss: 0.93941\n",
      "Epoch 309 - lr: 0.05000 - Train loss: 0.83818 - Test loss: 1.63435\n",
      "Epoch 310 - lr: 0.05000 - Train loss: 1.29751 - Test loss: 1.49625\n",
      "Epoch 311 - lr: 0.05000 - Train loss: 1.31921 - Test loss: 1.39732\n",
      "Epoch 312 - lr: 0.05000 - Train loss: 1.30783 - Test loss: 1.39126\n",
      "Epoch 313 - lr: 0.05000 - Train loss: 1.28690 - Test loss: 1.37566\n",
      "Epoch 314 - lr: 0.05000 - Train loss: 1.26848 - Test loss: 1.70033\n",
      "Epoch 315 - lr: 0.05000 - Train loss: 1.24998 - Test loss: 1.39653\n",
      "Epoch 316 - lr: 0.05000 - Train loss: 1.23528 - Test loss: 1.40613\n",
      "Epoch 317 - lr: 0.05000 - Train loss: 1.22289 - Test loss: 1.37177\n",
      "Epoch 318 - lr: 0.05000 - Train loss: 1.22328 - Test loss: 1.62671\n",
      "Epoch 319 - lr: 0.05000 - Train loss: 1.12416 - Test loss: 1.23191\n",
      "Epoch 320 - lr: 0.05000 - Train loss: 1.09645 - Test loss: 1.20840\n",
      "Epoch 321 - lr: 0.05000 - Train loss: 1.14890 - Test loss: 1.28321\n",
      "Epoch 322 - lr: 0.05000 - Train loss: 1.18605 - Test loss: 1.08598\n",
      "Epoch 323 - lr: 0.05000 - Train loss: 0.97116 - Test loss: 1.15152\n",
      "Epoch 324 - lr: 0.05000 - Train loss: 1.16526 - Test loss: 0.97831\n",
      "Epoch 325 - lr: 0.05000 - Train loss: 0.94599 - Test loss: 1.42236\n",
      "Epoch 326 - lr: 0.05000 - Train loss: 1.19756 - Test loss: 0.92091\n",
      "Epoch 327 - lr: 0.05000 - Train loss: 0.88243 - Test loss: 0.85179\n",
      "Epoch 328 - lr: 0.05000 - Train loss: 0.98740 - Test loss: 0.91066\n",
      "Epoch 329 - lr: 0.05000 - Train loss: 0.80780 - Test loss: 0.84992\n",
      "Epoch 330 - lr: 0.05000 - Train loss: 0.80482 - Test loss: 0.84360\n",
      "Epoch 331 - lr: 0.05000 - Train loss: 0.92404 - Test loss: 0.81662\n",
      "Epoch 332 - lr: 0.05000 - Train loss: 0.93460 - Test loss: 0.82032\n",
      "Epoch 333 - lr: 0.05000 - Train loss: 0.91562 - Test loss: 0.86485\n",
      "Epoch 334 - lr: 0.05000 - Train loss: 0.92272 - Test loss: 0.82138\n",
      "Epoch 335 - lr: 0.05000 - Train loss: 1.18491 - Test loss: 0.88303\n",
      "Epoch 336 - lr: 0.05000 - Train loss: 1.13859 - Test loss: 0.95746\n",
      "Epoch 337 - lr: 0.05000 - Train loss: 0.86297 - Test loss: 0.91281\n",
      "Epoch 338 - lr: 0.05000 - Train loss: 0.85821 - Test loss: 0.91711\n",
      "Epoch 339 - lr: 0.05000 - Train loss: 0.88824 - Test loss: 0.96179\n",
      "Epoch 340 - lr: 0.05000 - Train loss: 0.86795 - Test loss: 0.91558\n",
      "Epoch 341 - lr: 0.05000 - Train loss: 0.88699 - Test loss: 0.94499\n",
      "Epoch 342 - lr: 0.05000 - Train loss: 1.15291 - Test loss: 1.56555\n",
      "Epoch 343 - lr: 0.05000 - Train loss: 1.06159 - Test loss: 0.83027\n",
      "Epoch 344 - lr: 0.05000 - Train loss: 0.83974 - Test loss: 0.88028\n",
      "Epoch 345 - lr: 0.05000 - Train loss: 0.81556 - Test loss: 0.82275\n",
      "Epoch 346 - lr: 0.05000 - Train loss: 0.84873 - Test loss: 1.00971\n",
      "Epoch 347 - lr: 0.05000 - Train loss: 0.96582 - Test loss: 0.87393\n",
      "Epoch 348 - lr: 0.05000 - Train loss: 0.89154 - Test loss: 0.90442\n",
      "Epoch 349 - lr: 0.05000 - Train loss: 1.39451 - Test loss: 1.40468\n",
      "Epoch 350 - lr: 0.05000 - Train loss: 1.40607 - Test loss: 1.34069\n",
      "Epoch 351 - lr: 0.05000 - Train loss: 1.31206 - Test loss: 1.35179\n",
      "Epoch 352 - lr: 0.05000 - Train loss: 1.30227 - Test loss: 1.34476\n",
      "Epoch 353 - lr: 0.05000 - Train loss: 1.25526 - Test loss: 1.26602\n",
      "Epoch 354 - lr: 0.05000 - Train loss: 1.14125 - Test loss: 1.16221\n",
      "Epoch 355 - lr: 0.05000 - Train loss: 1.10042 - Test loss: 1.15954\n",
      "Epoch 356 - lr: 0.05000 - Train loss: 1.05505 - Test loss: 1.09330\n",
      "Epoch 357 - lr: 0.05000 - Train loss: 1.06455 - Test loss: 1.06509\n",
      "Epoch 358 - lr: 0.05000 - Train loss: 1.11228 - Test loss: 1.15730\n",
      "Epoch 359 - lr: 0.05000 - Train loss: 1.17212 - Test loss: 1.12541\n",
      "Epoch 360 - lr: 0.05000 - Train loss: 1.01107 - Test loss: 1.52368\n",
      "Epoch 361 - lr: 0.05000 - Train loss: 1.31636 - Test loss: 1.35254\n",
      "Epoch 362 - lr: 0.05000 - Train loss: 1.29042 - Test loss: 1.33674\n",
      "Epoch 363 - lr: 0.05000 - Train loss: 1.28759 - Test loss: 1.33253\n",
      "Epoch 364 - lr: 0.05000 - Train loss: 1.25679 - Test loss: 1.25843\n",
      "Epoch 365 - lr: 0.05000 - Train loss: 1.11016 - Test loss: 1.05809\n",
      "Epoch 366 - lr: 0.05000 - Train loss: 0.98236 - Test loss: 1.00243\n",
      "Epoch 367 - lr: 0.05000 - Train loss: 1.00606 - Test loss: 1.12245\n",
      "Epoch 368 - lr: 0.05000 - Train loss: 1.00540 - Test loss: 0.88427\n",
      "Epoch 369 - lr: 0.05000 - Train loss: 0.93483 - Test loss: 1.24230\n",
      "Epoch 370 - lr: 0.05000 - Train loss: 1.04729 - Test loss: 0.84369\n",
      "Epoch 371 - lr: 0.05000 - Train loss: 0.91147 - Test loss: 0.90387\n",
      "Epoch 372 - lr: 0.05000 - Train loss: 0.92508 - Test loss: 0.80169\n",
      "Epoch 373 - lr: 0.05000 - Train loss: 0.87359 - Test loss: 0.78090\n",
      "Epoch 374 - lr: 0.05000 - Train loss: 0.96983 - Test loss: 0.69132\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 375 - lr: 0.05000 - Train loss: 1.29759 - Test loss: 1.38130\n",
      "Epoch 376 - lr: 0.05000 - Train loss: 1.29715 - Test loss: 1.30980\n",
      "Epoch 377 - lr: 0.05000 - Train loss: 1.27092 - Test loss: 1.19354\n",
      "Epoch 378 - lr: 0.05000 - Train loss: 1.02346 - Test loss: 1.31432\n",
      "Epoch 379 - lr: 0.05000 - Train loss: 1.13929 - Test loss: 1.29936\n",
      "Epoch 380 - lr: 0.05000 - Train loss: 1.08014 - Test loss: 1.37537\n",
      "Epoch 381 - lr: 0.05000 - Train loss: 1.14269 - Test loss: 0.97342\n",
      "Epoch 382 - lr: 0.05000 - Train loss: 1.05519 - Test loss: 0.92166\n",
      "Epoch 383 - lr: 0.05000 - Train loss: 1.08693 - Test loss: 0.97147\n",
      "Epoch 384 - lr: 0.05000 - Train loss: 0.89211 - Test loss: 1.02399\n",
      "Epoch 385 - lr: 0.05000 - Train loss: 1.07991 - Test loss: 1.50157\n",
      "Epoch 386 - lr: 0.05000 - Train loss: 1.29908 - Test loss: 0.87144\n",
      "Epoch 387 - lr: 0.05000 - Train loss: 0.93130 - Test loss: 0.92387\n",
      "Epoch 388 - lr: 0.05000 - Train loss: 0.97180 - Test loss: 0.95804\n",
      "Epoch 389 - lr: 0.05000 - Train loss: 1.33135 - Test loss: 1.20375\n",
      "Epoch 390 - lr: 0.05000 - Train loss: 1.05282 - Test loss: 0.94302\n",
      "Epoch 391 - lr: 0.05000 - Train loss: 1.20018 - Test loss: 0.93859\n",
      "Epoch 392 - lr: 0.05000 - Train loss: 0.88750 - Test loss: 1.22857\n",
      "Epoch 393 - lr: 0.05000 - Train loss: 1.05492 - Test loss: 1.09366\n",
      "Epoch 394 - lr: 0.05000 - Train loss: 1.01922 - Test loss: 0.83601\n",
      "Epoch 395 - lr: 0.05000 - Train loss: 0.95204 - Test loss: 0.88329\n",
      "Epoch 396 - lr: 0.05000 - Train loss: 1.06290 - Test loss: 1.53637\n",
      "Epoch 397 - lr: 0.05000 - Train loss: 1.34330 - Test loss: 1.33482\n",
      "Epoch 398 - lr: 0.05000 - Train loss: 1.27734 - Test loss: 1.23276\n",
      "Epoch 399 - lr: 0.05000 - Train loss: 1.00964 - Test loss: 0.82473\n",
      "Epoch 400 - lr: 0.05000 - Train loss: 0.92427 - Test loss: 1.45547\n",
      "Epoch 401 - lr: 0.05000 - Train loss: 1.10529 - Test loss: 0.81854\n",
      "Epoch 402 - lr: 0.05000 - Train loss: 0.92289 - Test loss: 1.33351\n",
      "Epoch 403 - lr: 0.05000 - Train loss: 0.99439 - Test loss: 0.78111\n",
      "Epoch 404 - lr: 0.05000 - Train loss: 1.11880 - Test loss: 0.84249\n",
      "Epoch 405 - lr: 0.05000 - Train loss: 0.92751 - Test loss: 1.13342\n",
      "Epoch 406 - lr: 0.05000 - Train loss: 1.01086 - Test loss: 1.33590\n",
      "Epoch 407 - lr: 0.05000 - Train loss: 1.02769 - Test loss: 1.38303\n",
      "Epoch 408 - lr: 0.05000 - Train loss: 1.14238 - Test loss: 1.04965\n",
      "Epoch 409 - lr: 0.05000 - Train loss: 0.91569 - Test loss: 1.42792\n",
      "Epoch 410 - lr: 0.05000 - Train loss: 1.30165 - Test loss: 1.30460\n",
      "Epoch 411 - lr: 0.05000 - Train loss: 1.27044 - Test loss: 1.27619\n",
      "Epoch 412 - lr: 0.05000 - Train loss: 1.26692 - Test loss: 1.27179\n",
      "Epoch 413 - lr: 0.05000 - Train loss: 1.26460 - Test loss: 1.27028\n",
      "Epoch 414 - lr: 0.05000 - Train loss: 1.26589 - Test loss: 1.26940\n",
      "Epoch 415 - lr: 0.05000 - Train loss: 1.25541 - Test loss: 0.75299\n",
      "Epoch 416 - lr: 0.05000 - Train loss: 1.03240 - Test loss: 1.35677\n",
      "Epoch 417 - lr: 0.05000 - Train loss: 1.28830 - Test loss: 1.07269\n",
      "Epoch 418 - lr: 0.05000 - Train loss: 1.23557 - Test loss: 0.84043\n",
      "Epoch 419 - lr: 0.05000 - Train loss: 1.22772 - Test loss: 0.94285\n",
      "Epoch 420 - lr: 0.05000 - Train loss: 0.94377 - Test loss: 0.89588\n",
      "Epoch 421 - lr: 0.05000 - Train loss: 0.86118 - Test loss: 1.03120\n",
      "Epoch 422 - lr: 0.05000 - Train loss: 0.93364 - Test loss: 0.79363\n",
      "Epoch 423 - lr: 0.05000 - Train loss: 0.81158 - Test loss: 1.44656\n",
      "Epoch 424 - lr: 0.05000 - Train loss: 1.12174 - Test loss: 0.76792\n",
      "Epoch 425 - lr: 0.05000 - Train loss: 0.84234 - Test loss: 0.76355\n",
      "Epoch 426 - lr: 0.05000 - Train loss: 0.94902 - Test loss: 0.75250\n",
      "Epoch 427 - lr: 0.05000 - Train loss: 1.02879 - Test loss: 0.79425\n",
      "Epoch 428 - lr: 0.05000 - Train loss: 0.87623 - Test loss: 0.78613\n",
      "Epoch 429 - lr: 0.05000 - Train loss: 0.86885 - Test loss: 0.77947\n",
      "Epoch 430 - lr: 0.05000 - Train loss: 0.83840 - Test loss: 1.01802\n",
      "Epoch 431 - lr: 0.05000 - Train loss: 0.89840 - Test loss: 1.40966\n",
      "Epoch 432 - lr: 0.05000 - Train loss: 1.05125 - Test loss: 1.36107\n",
      "Epoch 433 - lr: 0.05000 - Train loss: 1.01652 - Test loss: 0.71235\n",
      "Epoch 434 - lr: 0.05000 - Train loss: 1.11174 - Test loss: 0.77818\n",
      "Epoch 435 - lr: 0.05000 - Train loss: 1.39732 - Test loss: 0.97014\n",
      "Epoch 436 - lr: 0.05000 - Train loss: 1.00472 - Test loss: 0.89993\n",
      "Epoch 437 - lr: 0.05000 - Train loss: 0.90206 - Test loss: 0.84293\n",
      "Epoch 438 - lr: 0.05000 - Train loss: 0.84155 - Test loss: 1.00450\n",
      "Epoch 439 - lr: 0.05000 - Train loss: 0.94310 - Test loss: 0.83009\n",
      "Epoch 440 - lr: 0.05000 - Train loss: 0.90956 - Test loss: 0.75493\n",
      "Epoch 441 - lr: 0.05000 - Train loss: 0.82603 - Test loss: 0.74799\n",
      "Epoch 442 - lr: 0.05000 - Train loss: 0.79427 - Test loss: 0.75155\n",
      "Epoch 443 - lr: 0.05000 - Train loss: 0.79195 - Test loss: 0.74989\n",
      "Epoch 444 - lr: 0.05000 - Train loss: 0.80687 - Test loss: 1.51355\n",
      "Epoch 445 - lr: 0.05000 - Train loss: 1.33737 - Test loss: 1.15127\n",
      "Epoch 446 - lr: 0.05000 - Train loss: 0.95734 - Test loss: 0.69754\n",
      "Epoch 447 - lr: 0.05000 - Train loss: 0.78298 - Test loss: 0.70181\n",
      "Epoch 448 - lr: 0.05000 - Train loss: 0.77923 - Test loss: 0.70050\n",
      "Epoch 449 - lr: 0.05000 - Train loss: 0.75091 - Test loss: 0.68998\n",
      "Epoch 450 - lr: 0.05000 - Train loss: 0.74415 - Test loss: 0.68051\n",
      "Epoch 451 - lr: 0.05000 - Train loss: 0.73396 - Test loss: 0.67355\n",
      "Epoch 452 - lr: 0.05000 - Train loss: 0.82877 - Test loss: 1.51653\n",
      "Epoch 453 - lr: 0.05000 - Train loss: 1.33685 - Test loss: 1.30231\n",
      "Epoch 454 - lr: 0.05000 - Train loss: 1.26734 - Test loss: 1.25594\n",
      "Epoch 455 - lr: 0.05000 - Train loss: 1.25657 - Test loss: 1.23873\n",
      "Epoch 456 - lr: 0.05000 - Train loss: 1.25998 - Test loss: 1.24318\n",
      "Epoch 457 - lr: 0.05000 - Train loss: 1.27041 - Test loss: 1.24170\n",
      "Epoch 458 - lr: 0.05000 - Train loss: 1.25541 - Test loss: 1.24216\n",
      "Epoch 459 - lr: 0.05000 - Train loss: 1.25438 - Test loss: 1.23821\n",
      "Epoch 460 - lr: 0.05000 - Train loss: 1.26052 - Test loss: 1.24840\n",
      "Epoch 461 - lr: 0.05000 - Train loss: 1.27192 - Test loss: 1.24335\n",
      "Epoch 462 - lr: 0.05000 - Train loss: 1.25425 - Test loss: 1.23794\n",
      "Epoch 463 - lr: 0.05000 - Train loss: 1.26657 - Test loss: 1.23513\n",
      "Epoch 464 - lr: 0.05000 - Train loss: 1.26908 - Test loss: 1.22299\n",
      "Epoch 465 - lr: 0.05000 - Train loss: 1.26448 - Test loss: 1.23526\n",
      "Epoch 466 - lr: 0.05000 - Train loss: 1.25424 - Test loss: 1.24322\n",
      "Epoch 467 - lr: 0.05000 - Train loss: 1.25381 - Test loss: 1.23959\n",
      "Epoch 468 - lr: 0.05000 - Train loss: 1.25954 - Test loss: 1.24938\n",
      "Epoch 469 - lr: 0.05000 - Train loss: 1.27014 - Test loss: 1.24193\n",
      "Epoch 470 - lr: 0.05000 - Train loss: 1.25349 - Test loss: 1.24065\n",
      "Epoch 471 - lr: 0.05000 - Train loss: 1.25375 - Test loss: 1.24409\n",
      "Epoch 472 - lr: 0.05000 - Train loss: 1.25362 - Test loss: 1.24177\n",
      "Epoch 473 - lr: 0.05000 - Train loss: 1.25352 - Test loss: 1.24439\n",
      "Epoch 474 - lr: 0.05000 - Train loss: 1.25078 - Test loss: 1.23615\n",
      "Epoch 475 - lr: 0.05000 - Train loss: 1.26776 - Test loss: 1.22112\n",
      "Epoch 476 - lr: 0.05000 - Train loss: 1.25952 - Test loss: 1.23863\n",
      "Epoch 477 - lr: 0.05000 - Train loss: 1.26303 - Test loss: 1.24085\n",
      "Epoch 478 - lr: 0.05000 - Train loss: 1.25123 - Test loss: 1.25171\n",
      "Epoch 479 - lr: 0.05000 - Train loss: 1.25683 - Test loss: 1.24244\n",
      "Epoch 480 - lr: 0.05000 - Train loss: 1.25137 - Test loss: 1.23342\n",
      "Epoch 481 - lr: 0.05000 - Train loss: 1.26557 - Test loss: 1.21811\n",
      "Epoch 482 - lr: 0.05000 - Train loss: 1.26371 - Test loss: 1.21368\n",
      "Epoch 483 - lr: 0.05000 - Train loss: 1.26336 - Test loss: 1.21159\n",
      "Epoch 484 - lr: 0.05000 - Train loss: 1.26144 - Test loss: 1.23359\n",
      "Epoch 485 - lr: 0.05000 - Train loss: 1.26279 - Test loss: 1.20849\n",
      "Epoch 486 - lr: 0.05000 - Train loss: 1.26042 - Test loss: 1.22034\n",
      "Epoch 487 - lr: 0.05000 - Train loss: 1.26441 - Test loss: 1.21858\n",
      "Epoch 488 - lr: 0.05000 - Train loss: 1.26266 - Test loss: 1.26104\n",
      "Epoch 489 - lr: 0.05000 - Train loss: 1.24894 - Test loss: 1.23871\n",
      "Epoch 490 - lr: 0.05000 - Train loss: 1.25278 - Test loss: 1.23218\n",
      "Epoch 491 - lr: 0.05000 - Train loss: 1.25033 - Test loss: 1.23952\n",
      "Epoch 492 - lr: 0.05000 - Train loss: 1.26693 - Test loss: 1.22784\n",
      "Epoch 493 - lr: 0.05000 - Train loss: 1.25827 - Test loss: 1.23507\n",
      "Epoch 494 - lr: 0.05000 - Train loss: 1.25899 - Test loss: 1.23635\n",
      "Epoch 495 - lr: 0.05000 - Train loss: 1.26372 - Test loss: 1.23042\n",
      "Epoch 496 - lr: 0.05000 - Train loss: 1.24793 - Test loss: 1.22474\n",
      "Epoch 497 - lr: 0.05000 - Train loss: 1.26306 - Test loss: 1.21046\n",
      "Epoch 498 - lr: 0.05000 - Train loss: 1.25848 - Test loss: 1.22076\n",
      "Epoch 499 - lr: 0.05000 - Train loss: 1.24792 - Test loss: 1.23307\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500 - lr: 0.05000 - Train loss: 1.26496 - Test loss: 1.22139\n",
      "Epoch 501 - lr: 0.05000 - Train loss: 1.25949 - Test loss: 1.21941\n",
      "Epoch 502 - lr: 0.05000 - Train loss: 1.26014 - Test loss: 1.25402\n",
      "Epoch 503 - lr: 0.05000 - Train loss: 1.24631 - Test loss: 1.23419\n",
      "Epoch 504 - lr: 0.05000 - Train loss: 1.25132 - Test loss: 1.22827\n",
      "Epoch 505 - lr: 0.05000 - Train loss: 1.24671 - Test loss: 1.22146\n",
      "Epoch 506 - lr: 0.05000 - Train loss: 1.25830 - Test loss: 1.22481\n",
      "Epoch 507 - lr: 0.05000 - Train loss: 1.24686 - Test loss: 1.22029\n",
      "Epoch 508 - lr: 0.05000 - Train loss: 1.25816 - Test loss: 1.22046\n",
      "Epoch 509 - lr: 0.05000 - Train loss: 1.24615 - Test loss: 1.22614\n",
      "Epoch 510 - lr: 0.05000 - Train loss: 1.24425 - Test loss: 1.21926\n",
      "Epoch 511 - lr: 0.05000 - Train loss: 1.26062 - Test loss: 1.20357\n",
      "Epoch 512 - lr: 0.05000 - Train loss: 1.25599 - Test loss: 1.21070\n",
      "Epoch 513 - lr: 0.05000 - Train loss: 1.25664 - Test loss: 1.21600\n",
      "Epoch 514 - lr: 0.05000 - Train loss: 1.24498 - Test loss: 1.22196\n",
      "Epoch 515 - lr: 0.05000 - Train loss: 1.24566 - Test loss: 1.22125\n",
      "Epoch 516 - lr: 0.05000 - Train loss: 1.24497 - Test loss: 1.22284\n",
      "Epoch 517 - lr: 0.05000 - Train loss: 1.24550 - Test loss: 1.22025\n",
      "Epoch 518 - lr: 0.05000 - Train loss: 1.24482 - Test loss: 1.22678\n",
      "Epoch 519 - lr: 0.05000 - Train loss: 1.26087 - Test loss: 1.21449\n",
      "Epoch 520 - lr: 0.05000 - Train loss: 1.25789 - Test loss: 1.20607\n",
      "Epoch 521 - lr: 0.05000 - Train loss: 1.26030 - Test loss: 1.21777\n",
      "Epoch 522 - lr: 0.05000 - Train loss: 1.25974 - Test loss: 1.22426\n",
      "Epoch 523 - lr: 0.05000 - Train loss: 1.25579 - Test loss: 1.25393\n",
      "Epoch 524 - lr: 0.05000 - Train loss: 1.24218 - Test loss: 1.21367\n",
      "Epoch 525 - lr: 0.05000 - Train loss: 1.24402 - Test loss: 1.21315\n",
      "Epoch 526 - lr: 0.05000 - Train loss: 1.24740 - Test loss: 1.22757\n",
      "Epoch 527 - lr: 0.05000 - Train loss: 1.24320 - Test loss: 1.21512\n",
      "Epoch 528 - lr: 0.05000 - Train loss: 1.25665 - Test loss: 1.22900\n",
      "Epoch 529 - lr: 0.05000 - Train loss: 1.25250 - Test loss: 1.20539\n",
      "Epoch 530 - lr: 0.05000 - Train loss: 1.25955 - Test loss: 1.21019\n",
      "Epoch 531 - lr: 0.05000 - Train loss: 1.24263 - Test loss: 1.21270\n",
      "Epoch 532 - lr: 0.05000 - Train loss: 1.24264 - Test loss: 1.21923\n",
      "Epoch 533 - lr: 0.05000 - Train loss: 1.24742 - Test loss: 1.22659\n",
      "Epoch 534 - lr: 0.05000 - Train loss: 1.24219 - Test loss: 1.21323\n",
      "Epoch 535 - lr: 0.05000 - Train loss: 1.25577 - Test loss: 1.22162\n",
      "Epoch 536 - lr: 0.05000 - Train loss: 1.25227 - Test loss: 1.21021\n",
      "Epoch 537 - lr: 0.05000 - Train loss: 1.25269 - Test loss: 1.24883\n",
      "Epoch 538 - lr: 0.05000 - Train loss: 1.23955 - Test loss: 1.20182\n",
      "Epoch 539 - lr: 0.05000 - Train loss: 1.24586 - Test loss: 1.22053\n",
      "Epoch 540 - lr: 0.05000 - Train loss: 1.24031 - Test loss: 1.20825\n",
      "Epoch 541 - lr: 0.05000 - Train loss: 1.25191 - Test loss: 1.21392\n",
      "Epoch 542 - lr: 0.05000 - Train loss: 1.23891 - Test loss: 1.20820\n",
      "Epoch 543 - lr: 0.05000 - Train loss: 1.25404 - Test loss: 1.24809\n",
      "Epoch 544 - lr: 0.05000 - Train loss: 1.23920 - Test loss: 1.20892\n",
      "Epoch 545 - lr: 0.05000 - Train loss: 1.24013 - Test loss: 1.20719\n",
      "Epoch 546 - lr: 0.05000 - Train loss: 1.24138 - Test loss: 1.22498\n",
      "Epoch 547 - lr: 0.05000 - Train loss: 1.24451 - Test loss: 1.20763\n",
      "Epoch 548 - lr: 0.05000 - Train loss: 1.25340 - Test loss: 1.20306\n",
      "Epoch 549 - lr: 0.05000 - Train loss: 1.25608 - Test loss: 1.20504\n",
      "Epoch 550 - lr: 0.05000 - Train loss: 1.23732 - Test loss: 1.20858\n",
      "Epoch 551 - lr: 0.05000 - Train loss: 1.24023 - Test loss: 1.20683\n",
      "Epoch 552 - lr: 0.05000 - Train loss: 1.23938 - Test loss: 1.21888\n",
      "Epoch 553 - lr: 0.05000 - Train loss: 1.23862 - Test loss: 1.21013\n",
      "Epoch 554 - lr: 0.05000 - Train loss: 1.23861 - Test loss: 1.20733\n",
      "Epoch 555 - lr: 0.05000 - Train loss: 1.25573 - Test loss: 1.20133\n",
      "Epoch 556 - lr: 0.05000 - Train loss: 1.23738 - Test loss: 1.20358\n",
      "Epoch 557 - lr: 0.05000 - Train loss: 1.25571 - Test loss: 1.20202\n",
      "Epoch 558 - lr: 0.05000 - Train loss: 1.23869 - Test loss: 1.20405\n",
      "Epoch 559 - lr: 0.05000 - Train loss: 1.23843 - Test loss: 1.20856\n",
      "Epoch 560 - lr: 0.05000 - Train loss: 1.23606 - Test loss: 1.20241\n",
      "Epoch 561 - lr: 0.05000 - Train loss: 1.25225 - Test loss: 1.21526\n",
      "Epoch 562 - lr: 0.05000 - Train loss: 1.24953 - Test loss: 1.18845\n",
      "Epoch 563 - lr: 0.05000 - Train loss: 1.25324 - Test loss: 1.21236\n",
      "Epoch 564 - lr: 0.05000 - Train loss: 1.24154 - Test loss: 1.20207\n",
      "Epoch 565 - lr: 0.05000 - Train loss: 1.25440 - Test loss: 1.19702\n",
      "Epoch 566 - lr: 0.05000 - Train loss: 1.23702 - Test loss: 1.20264\n",
      "Epoch 567 - lr: 0.05000 - Train loss: 1.23713 - Test loss: 1.19968\n",
      "Epoch 568 - lr: 0.05000 - Train loss: 1.25097 - Test loss: 1.23197\n",
      "Epoch 569 - lr: 0.05000 - Train loss: 1.23675 - Test loss: 1.21515\n",
      "Epoch 570 - lr: 0.05000 - Train loss: 1.24113 - Test loss: 1.20211\n",
      "Epoch 571 - lr: 0.05000 - Train loss: 1.25347 - Test loss: 1.19360\n",
      "Epoch 572 - lr: 0.05000 - Train loss: 1.23810 - Test loss: 1.21622\n",
      "Epoch 573 - lr: 0.05000 - Train loss: 1.24110 - Test loss: 1.19945\n",
      "Epoch 574 - lr: 0.05000 - Train loss: 1.25078 - Test loss: 1.18650\n",
      "Epoch 575 - lr: 0.05000 - Train loss: 1.25247 - Test loss: 1.19467\n",
      "Epoch 576 - lr: 0.05000 - Train loss: 1.23562 - Test loss: 1.19621\n",
      "Epoch 577 - lr: 0.05000 - Train loss: 1.25195 - Test loss: 1.18875\n",
      "Epoch 578 - lr: 0.05000 - Train loss: 1.24780 - Test loss: 1.19278\n",
      "Epoch 579 - lr: 0.05000 - Train loss: 1.23716 - Test loss: 1.21417\n",
      "Epoch 580 - lr: 0.05000 - Train loss: 1.24038 - Test loss: 1.19789\n",
      "Epoch 581 - lr: 0.05000 - Train loss: 1.24824 - Test loss: 1.19676\n",
      "Epoch 582 - lr: 0.05000 - Train loss: 1.23586 - Test loss: 1.20381\n",
      "Epoch 583 - lr: 0.05000 - Train loss: 1.23378 - Test loss: 1.21659\n",
      "Epoch 584 - lr: 0.05000 - Train loss: 1.23985 - Test loss: 1.19952\n",
      "Epoch 585 - lr: 0.05000 - Train loss: 1.24971 - Test loss: 1.22601\n",
      "Epoch 586 - lr: 0.05000 - Train loss: 1.23872 - Test loss: 1.20645\n",
      "Epoch 587 - lr: 0.05000 - Train loss: 1.23407 - Test loss: 1.19662\n",
      "Epoch 588 - lr: 0.05000 - Train loss: 1.25169 - Test loss: 1.18955\n",
      "Epoch 589 - lr: 0.05000 - Train loss: 1.23521 - Test loss: 1.21262\n",
      "Epoch 590 - lr: 0.05000 - Train loss: 1.23893 - Test loss: 1.19711\n",
      "Epoch 591 - lr: 0.05000 - Train loss: 1.24974 - Test loss: 1.19755\n",
      "Epoch 592 - lr: 0.05000 - Train loss: 1.25104 - Test loss: 1.19144\n",
      "Epoch 593 - lr: 0.05000 - Train loss: 1.23456 - Test loss: 1.19068\n",
      "Epoch 594 - lr: 0.05000 - Train loss: 1.24603 - Test loss: 1.19559\n",
      "Epoch 595 - lr: 0.05000 - Train loss: 1.23447 - Test loss: 1.19153\n",
      "Epoch 596 - lr: 0.05000 - Train loss: 1.24603 - Test loss: 1.19406\n",
      "Epoch 597 - lr: 0.05000 - Train loss: 1.23296 - Test loss: 1.19288\n",
      "Epoch 598 - lr: 0.05000 - Train loss: 1.25013 - Test loss: 1.18659\n",
      "Epoch 599 - lr: 0.05000 - Train loss: 1.23391 - Test loss: 1.20934\n",
      "Epoch 600 - lr: 0.05000 - Train loss: 1.23791 - Test loss: 1.19241\n",
      "Epoch 601 - lr: 0.05000 - Train loss: 1.24722 - Test loss: 1.18997\n",
      "Epoch 602 - lr: 0.05000 - Train loss: 1.24997 - Test loss: 1.19098\n",
      "Epoch 603 - lr: 0.05000 - Train loss: 1.23124 - Test loss: 1.18916\n",
      "Epoch 604 - lr: 0.05000 - Train loss: 1.24748 - Test loss: 1.21267\n",
      "Epoch 605 - lr: 0.05000 - Train loss: 1.24353 - Test loss: 1.18753\n",
      "Epoch 606 - lr: 0.05000 - Train loss: 1.23264 - Test loss: 1.19236\n",
      "Epoch 607 - lr: 0.05000 - Train loss: 1.23222 - Test loss: 1.19305\n",
      "Epoch 608 - lr: 0.05000 - Train loss: 1.25045 - Test loss: 1.19077\n",
      "Epoch 609 - lr: 0.05000 - Train loss: 1.23293 - Test loss: 1.19224\n",
      "Epoch 610 - lr: 0.05000 - Train loss: 1.23296 - Test loss: 1.19646\n",
      "Epoch 611 - lr: 0.05000 - Train loss: 1.23141 - Test loss: 1.18862\n",
      "Epoch 612 - lr: 0.05000 - Train loss: 1.24470 - Test loss: 1.23007\n",
      "Epoch 613 - lr: 0.05000 - Train loss: 1.23129 - Test loss: 1.19363\n",
      "Epoch 614 - lr: 0.05000 - Train loss: 1.23078 - Test loss: 1.21377\n",
      "Epoch 615 - lr: 0.05000 - Train loss: 1.23762 - Test loss: 1.19054\n",
      "Epoch 616 - lr: 0.05000 - Train loss: 1.23945 - Test loss: 1.20104\n",
      "Epoch 617 - lr: 0.05000 - Train loss: 1.24692 - Test loss: 1.19003\n",
      "Epoch 618 - lr: 0.05000 - Train loss: 1.23220 - Test loss: 1.19626\n",
      "Epoch 619 - lr: 0.05000 - Train loss: 1.23219 - Test loss: 1.21323\n",
      "Epoch 620 - lr: 0.05000 - Train loss: 1.23730 - Test loss: 1.19099\n",
      "Epoch 621 - lr: 0.05000 - Train loss: 1.23261 - Test loss: 1.20876\n",
      "Epoch 622 - lr: 0.05000 - Train loss: 1.23583 - Test loss: 1.18878\n",
      "Epoch 623 - lr: 0.05000 - Train loss: 1.24586 - Test loss: 1.18827\n",
      "Epoch 624 - lr: 0.05000 - Train loss: 1.24503 - Test loss: 1.17655\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 625 - lr: 0.05000 - Train loss: 1.24800 - Test loss: 1.18273\n",
      "Epoch 626 - lr: 0.05000 - Train loss: 1.22998 - Test loss: 1.18872\n",
      "Epoch 627 - lr: 0.05000 - Train loss: 1.24900 - Test loss: 1.18816\n",
      "Epoch 628 - lr: 0.05000 - Train loss: 1.22891 - Test loss: 1.19899\n",
      "Epoch 629 - lr: 0.05000 - Train loss: 1.23799 - Test loss: 1.20239\n",
      "Epoch 630 - lr: 0.05000 - Train loss: 1.22986 - Test loss: 1.18619\n",
      "Epoch 631 - lr: 0.05000 - Train loss: 1.24380 - Test loss: 1.18115\n",
      "Epoch 632 - lr: 0.05000 - Train loss: 1.24550 - Test loss: 1.16903\n",
      "Epoch 633 - lr: 0.05000 - Train loss: 1.24163 - Test loss: 1.17681\n",
      "Epoch 634 - lr: 0.05000 - Train loss: 1.24422 - Test loss: 1.16882\n",
      "Epoch 635 - lr: 0.05000 - Train loss: 1.24351 - Test loss: 1.20154\n",
      "Epoch 636 - lr: 0.05000 - Train loss: 1.24078 - Test loss: 1.17805\n",
      "Epoch 637 - lr: 0.05000 - Train loss: 1.24829 - Test loss: 1.18659\n",
      "Epoch 638 - lr: 0.05000 - Train loss: 1.24124 - Test loss: 1.19069\n",
      "Epoch 639 - lr: 0.05000 - Train loss: 1.22846 - Test loss: 1.20559\n",
      "Epoch 640 - lr: 0.05000 - Train loss: 1.23439 - Test loss: 1.18905\n",
      "Epoch 641 - lr: 0.05000 - Train loss: 1.24737 - Test loss: 1.18125\n",
      "Epoch 642 - lr: 0.05000 - Train loss: 1.23011 - Test loss: 1.19312\n",
      "Epoch 643 - lr: 0.05000 - Train loss: 1.24615 - Test loss: 1.20165\n",
      "Epoch 644 - lr: 0.05000 - Train loss: 1.24333 - Test loss: 1.16638\n",
      "Epoch 645 - lr: 0.05000 - Train loss: 1.24173 - Test loss: 1.18033\n",
      "Epoch 646 - lr: 0.05000 - Train loss: 1.24593 - Test loss: 1.18159\n",
      "Epoch 647 - lr: 0.05000 - Train loss: 1.22944 - Test loss: 1.18389\n",
      "Epoch 648 - lr: 0.05000 - Train loss: 1.22994 - Test loss: 1.19424\n",
      "Epoch 649 - lr: 0.05000 - Train loss: 1.24646 - Test loss: 1.20453\n",
      "Epoch 650 - lr: 0.05000 - Train loss: 1.24037 - Test loss: 1.22382\n",
      "Epoch 651 - lr: 0.05000 - Train loss: 1.22817 - Test loss: 1.18437\n",
      "Epoch 652 - lr: 0.05000 - Train loss: 1.22989 - Test loss: 1.18543\n",
      "Epoch 653 - lr: 0.05000 - Train loss: 1.22912 - Test loss: 1.18626\n",
      "Epoch 654 - lr: 0.05000 - Train loss: 1.22864 - Test loss: 1.18378\n",
      "Epoch 655 - lr: 0.05000 - Train loss: 1.24597 - Test loss: 1.17892\n",
      "Epoch 656 - lr: 0.05000 - Train loss: 1.22827 - Test loss: 1.18375\n",
      "Epoch 657 - lr: 0.05000 - Train loss: 1.22797 - Test loss: 1.18453\n",
      "Epoch 658 - lr: 0.05000 - Train loss: 1.24661 - Test loss: 1.18283\n",
      "Epoch 659 - lr: 0.05000 - Train loss: 1.22686 - Test loss: 1.17841\n",
      "Epoch 660 - lr: 0.05000 - Train loss: 1.24306 - Test loss: 1.16529\n",
      "Epoch 661 - lr: 0.05000 - Train loss: 1.24257 - Test loss: 1.16287\n",
      "Epoch 662 - lr: 0.05000 - Train loss: 1.23687 - Test loss: 1.18462\n",
      "Epoch 663 - lr: 0.05000 - Train loss: 1.23420 - Test loss: 1.19493\n",
      "Epoch 664 - lr: 0.05000 - Train loss: 1.22698 - Test loss: 1.18073\n",
      "Epoch 665 - lr: 0.05000 - Train loss: 1.24238 - Test loss: 1.20104\n",
      "Epoch 666 - lr: 0.05000 - Train loss: 1.23724 - Test loss: 1.18229\n",
      "Epoch 667 - lr: 0.05000 - Train loss: 1.22608 - Test loss: 1.17850\n",
      "Epoch 668 - lr: 0.05000 - Train loss: 1.24227 - Test loss: 1.21015\n",
      "Epoch 669 - lr: 0.05000 - Train loss: 1.22968 - Test loss: 1.19610\n",
      "Epoch 670 - lr: 0.05000 - Train loss: 1.23131 - Test loss: 1.18514\n",
      "Epoch 671 - lr: 0.05000 - Train loss: 1.24621 - Test loss: 1.18229\n",
      "Epoch 672 - lr: 0.05000 - Train loss: 1.22482 - Test loss: 1.17661\n",
      "Epoch 673 - lr: 0.05000 - Train loss: 1.23987 - Test loss: 1.18149\n",
      "Epoch 674 - lr: 0.05000 - Train loss: 1.22737 - Test loss: 1.17810\n",
      "Epoch 675 - lr: 0.05000 - Train loss: 1.24126 - Test loss: 1.21583\n",
      "Epoch 676 - lr: 0.05000 - Train loss: 1.22624 - Test loss: 1.18375\n",
      "Epoch 677 - lr: 0.05000 - Train loss: 1.23687 - Test loss: 1.18798\n",
      "Epoch 678 - lr: 0.05000 - Train loss: 1.24346 - Test loss: 1.21593\n",
      "Epoch 679 - lr: 0.05000 - Train loss: 1.22664 - Test loss: 1.19849\n",
      "Epoch 680 - lr: 0.05000 - Train loss: 1.23225 - Test loss: 1.18192\n",
      "Epoch 681 - lr: 0.05000 - Train loss: 1.22771 - Test loss: 1.18595\n",
      "Epoch 682 - lr: 0.05000 - Train loss: 1.22445 - Test loss: 1.17854\n",
      "Epoch 683 - lr: 0.05000 - Train loss: 1.24306 - Test loss: 1.17007\n",
      "Epoch 684 - lr: 0.05000 - Train loss: 1.24139 - Test loss: 1.16061\n",
      "Epoch 685 - lr: 0.05000 - Train loss: 1.24021 - Test loss: 1.19305\n",
      "Epoch 686 - lr: 0.05000 - Train loss: 1.23716 - Test loss: 1.19986\n",
      "Epoch 687 - lr: 0.05000 - Train loss: 1.23662 - Test loss: 1.17851\n",
      "Epoch 688 - lr: 0.05000 - Train loss: 1.22691 - Test loss: 1.17939\n",
      "Epoch 689 - lr: 0.05000 - Train loss: 1.22703 - Test loss: 1.18484\n",
      "Epoch 690 - lr: 0.05000 - Train loss: 1.22438 - Test loss: 1.18024\n",
      "Epoch 691 - lr: 0.05000 - Train loss: 1.22621 - Test loss: 1.17779\n",
      "Epoch 692 - lr: 0.05000 - Train loss: 1.22650 - Test loss: 1.19399\n",
      "Epoch 693 - lr: 0.05000 - Train loss: 1.22960 - Test loss: 1.18347\n",
      "Epoch 694 - lr: 0.05000 - Train loss: 1.22585 - Test loss: 1.17583\n",
      "Epoch 695 - lr: 0.05000 - Train loss: 1.24056 - Test loss: 1.17606\n",
      "Epoch 696 - lr: 0.05000 - Train loss: 1.24150 - Test loss: 1.17041\n",
      "Epoch 697 - lr: 0.05000 - Train loss: 1.22570 - Test loss: 1.18057\n",
      "Epoch 698 - lr: 0.05000 - Train loss: 1.22519 - Test loss: 1.17493\n",
      "Epoch 699 - lr: 0.05000 - Train loss: 1.23764 - Test loss: 1.18059\n",
      "Epoch 700 - lr: 0.05000 - Train loss: 1.22585 - Test loss: 1.17821\n",
      "Epoch 701 - lr: 0.05000 - Train loss: 1.22615 - Test loss: 1.18790\n",
      "Epoch 702 - lr: 0.05000 - Train loss: 1.24182 - Test loss: 1.21178\n",
      "Epoch 703 - lr: 0.05000 - Train loss: 1.22571 - Test loss: 1.19691\n",
      "Epoch 704 - lr: 0.05000 - Train loss: 1.23154 - Test loss: 1.18389\n",
      "Epoch 705 - lr: 0.05000 - Train loss: 1.22521 - Test loss: 1.17576\n",
      "Epoch 706 - lr: 0.05000 - Train loss: 1.23601 - Test loss: 1.18303\n",
      "Epoch 707 - lr: 0.05000 - Train loss: 1.23374 - Test loss: 1.18732\n",
      "Epoch 708 - lr: 0.05000 - Train loss: 1.24134 - Test loss: 1.20543\n",
      "Epoch 709 - lr: 0.05000 - Train loss: 1.23330 - Test loss: 1.18204\n",
      "Epoch 710 - lr: 0.05000 - Train loss: 1.24087 - Test loss: 1.21793\n",
      "Epoch 711 - lr: 0.05000 - Train loss: 1.22429 - Test loss: 1.17714\n",
      "Epoch 712 - lr: 0.05000 - Train loss: 1.22580 - Test loss: 1.17674\n",
      "Epoch 713 - lr: 0.05000 - Train loss: 1.22557 - Test loss: 1.18403\n",
      "Epoch 714 - lr: 0.05000 - Train loss: 1.24007 - Test loss: 1.17292\n",
      "Epoch 715 - lr: 0.05000 - Train loss: 1.24151 - Test loss: 1.16761\n",
      "Epoch 716 - lr: 0.05000 - Train loss: 1.23186 - Test loss: 1.18520\n",
      "Epoch 717 - lr: 0.05000 - Train loss: 1.23886 - Test loss: 1.17753\n",
      "Epoch 718 - lr: 0.05000 - Train loss: 1.22445 - Test loss: 1.17688\n",
      "Epoch 719 - lr: 0.05000 - Train loss: 1.24299 - Test loss: 1.17615\n",
      "Epoch 720 - lr: 0.05000 - Train loss: 1.22253 - Test loss: 1.17362\n",
      "Epoch 721 - lr: 0.05000 - Train loss: 1.24209 - Test loss: 1.17164\n",
      "Epoch 722 - lr: 0.05000 - Train loss: 1.22348 - Test loss: 1.16988\n",
      "Epoch 723 - lr: 0.05000 - Train loss: 1.23971 - Test loss: 1.15678\n",
      "Epoch 724 - lr: 0.05000 - Train loss: 1.23727 - Test loss: 1.15931\n",
      "Epoch 725 - lr: 0.05000 - Train loss: 1.24077 - Test loss: 1.19079\n",
      "Epoch 726 - lr: 0.05000 - Train loss: 1.22894 - Test loss: 1.17331\n",
      "Epoch 727 - lr: 0.05000 - Train loss: 1.22927 - Test loss: 1.19184\n",
      "Epoch 728 - lr: 0.05000 - Train loss: 1.22830 - Test loss: 1.17415\n",
      "Epoch 729 - lr: 0.05000 - Train loss: 1.23730 - Test loss: 1.21439\n",
      "Epoch 730 - lr: 0.05000 - Train loss: 1.22363 - Test loss: 1.17985\n",
      "Epoch 731 - lr: 0.05000 - Train loss: 1.23828 - Test loss: 1.17340\n",
      "Epoch 732 - lr: 0.05000 - Train loss: 1.22456 - Test loss: 1.18058\n",
      "Epoch 733 - lr: 0.05000 - Train loss: 1.22252 - Test loss: 1.18491\n",
      "Epoch 734 - lr: 0.05000 - Train loss: 1.24126 - Test loss: 1.21514\n",
      "Epoch 735 - lr: 0.05000 - Train loss: 1.22334 - Test loss: 1.17791\n",
      "Epoch 736 - lr: 0.05000 - Train loss: 1.22190 - Test loss: 1.17782\n",
      "Epoch 737 - lr: 0.05000 - Train loss: 1.22488 - Test loss: 1.17564\n",
      "Epoch 738 - lr: 0.05000 - Train loss: 1.22445 - Test loss: 1.18086\n",
      "Epoch 739 - lr: 0.05000 - Train loss: 1.22395 - Test loss: 1.20112\n",
      "Epoch 740 - lr: 0.05000 - Train loss: 1.23013 - Test loss: 1.18056\n",
      "Epoch 741 - lr: 0.05000 - Train loss: 1.22495 - Test loss: 1.17662\n",
      "Epoch 742 - lr: 0.05000 - Train loss: 1.22378 - Test loss: 1.17542\n",
      "Epoch 743 - lr: 0.05000 - Train loss: 1.22276 - Test loss: 1.17385\n",
      "Epoch 744 - lr: 0.05000 - Train loss: 1.24193 - Test loss: 1.17418\n",
      "Epoch 745 - lr: 0.05000 - Train loss: 1.22061 - Test loss: 1.17104\n",
      "Epoch 746 - lr: 0.05000 - Train loss: 1.22350 - Test loss: 1.18915\n",
      "Epoch 747 - lr: 0.05000 - Train loss: 1.22711 - Test loss: 1.17317\n",
      "Epoch 748 - lr: 0.05000 - Train loss: 1.23133 - Test loss: 1.18405\n",
      "Epoch 749 - lr: 0.05000 - Train loss: 1.23819 - Test loss: 1.17046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 750 - lr: 0.05000 - Train loss: 1.22623 - Test loss: 1.19231\n",
      "Epoch 751 - lr: 0.05000 - Train loss: 1.22804 - Test loss: 1.17462\n",
      "Epoch 752 - lr: 0.05000 - Train loss: 1.22387 - Test loss: 1.17975\n",
      "Epoch 753 - lr: 0.05000 - Train loss: 1.22531 - Test loss: 1.19653\n",
      "Epoch 754 - lr: 0.05000 - Train loss: 1.22996 - Test loss: 1.18393\n",
      "Epoch 755 - lr: 0.05000 - Train loss: 1.23966 - Test loss: 1.21085\n",
      "Epoch 756 - lr: 0.05000 - Train loss: 1.22204 - Test loss: 1.18778\n",
      "Epoch 757 - lr: 0.05000 - Train loss: 1.22633 - Test loss: 1.17329\n",
      "Epoch 758 - lr: 0.05000 - Train loss: 1.24098 - Test loss: 1.16972\n",
      "Epoch 759 - lr: 0.05000 - Train loss: 1.22181 - Test loss: 1.17517\n",
      "Epoch 760 - lr: 0.05000 - Train loss: 1.24134 - Test loss: 1.17399\n",
      "Epoch 761 - lr: 0.05000 - Train loss: 1.23020 - Test loss: 1.18270\n",
      "Epoch 762 - lr: 0.05000 - Train loss: 1.23766 - Test loss: 1.17107\n",
      "Epoch 763 - lr: 0.05000 - Train loss: 1.22321 - Test loss: 1.18385\n",
      "Epoch 764 - lr: 0.05000 - Train loss: 1.23596 - Test loss: 1.17747\n",
      "Epoch 765 - lr: 0.05000 - Train loss: 1.22196 - Test loss: 1.16993\n",
      "Epoch 766 - lr: 0.05000 - Train loss: 1.23470 - Test loss: 1.17518\n",
      "Epoch 767 - lr: 0.05000 - Train loss: 1.22267 - Test loss: 1.17322\n",
      "Epoch 768 - lr: 0.05000 - Train loss: 1.22305 - Test loss: 1.17731\n",
      "Epoch 769 - lr: 0.05000 - Train loss: 1.21986 - Test loss: 1.17084\n",
      "Epoch 770 - lr: 0.05000 - Train loss: 1.24018 - Test loss: 1.16836\n",
      "Epoch 771 - lr: 0.05000 - Train loss: 1.22112 - Test loss: 1.17185\n",
      "Epoch 772 - lr: 0.05000 - Train loss: 1.24102 - Test loss: 1.17438\n",
      "Epoch 773 - lr: 0.05000 - Train loss: 1.23747 - Test loss: 1.20693\n",
      "Epoch 774 - lr: 0.05000 - Train loss: 1.22146 - Test loss: 1.18823\n",
      "Epoch 775 - lr: 0.05000 - Train loss: 1.22640 - Test loss: 1.16890\n",
      "Epoch 776 - lr: 0.05000 - Train loss: 1.23456 - Test loss: 1.17417\n",
      "Epoch 777 - lr: 0.05000 - Train loss: 1.22274 - Test loss: 1.17375\n",
      "Epoch 778 - lr: 0.05000 - Train loss: 1.22178 - Test loss: 1.16746\n",
      "Epoch 779 - lr: 0.05000 - Train loss: 1.23463 - Test loss: 1.17138\n",
      "Epoch 780 - lr: 0.05000 - Train loss: 1.22144 - Test loss: 1.17172\n",
      "Epoch 781 - lr: 0.05000 - Train loss: 1.24049 - Test loss: 1.17180\n",
      "Epoch 782 - lr: 0.05000 - Train loss: 1.21884 - Test loss: 1.16589\n",
      "Epoch 783 - lr: 0.05000 - Train loss: 1.23708 - Test loss: 1.15332\n",
      "Epoch 784 - lr: 0.05000 - Train loss: 1.23654 - Test loss: 1.16173\n",
      "Epoch 785 - lr: 0.05000 - Train loss: 1.23995 - Test loss: 1.17890\n",
      "Epoch 786 - lr: 0.05000 - Train loss: 1.22081 - Test loss: 1.17018\n",
      "Epoch 787 - lr: 0.05000 - Train loss: 1.24050 - Test loss: 1.17088\n",
      "Epoch 788 - lr: 0.05000 - Train loss: 1.22164 - Test loss: 1.17078\n",
      "Epoch 789 - lr: 0.05000 - Train loss: 1.22247 - Test loss: 1.18067\n",
      "Epoch 790 - lr: 0.05000 - Train loss: 1.23904 - Test loss: 1.21233\n",
      "Epoch 791 - lr: 0.05000 - Train loss: 1.22129 - Test loss: 1.17534\n",
      "Epoch 792 - lr: 0.05000 - Train loss: 1.23338 - Test loss: 1.17571\n",
      "Epoch 793 - lr: 0.05000 - Train loss: 1.22001 - Test loss: 1.17747\n",
      "Epoch 794 - lr: 0.05000 - Train loss: 1.22529 - Test loss: 1.19116\n",
      "Epoch 795 - lr: 0.05000 - Train loss: 1.22740 - Test loss: 1.17677\n",
      "Epoch 796 - lr: 0.05000 - Train loss: 1.22267 - Test loss: 1.17418\n",
      "Epoch 797 - lr: 0.05000 - Train loss: 1.22105 - Test loss: 1.17027\n",
      "Epoch 798 - lr: 0.05000 - Train loss: 1.23972 - Test loss: 1.16905\n",
      "Epoch 799 - lr: 0.05000 - Train loss: 1.22186 - Test loss: 1.17085\n",
      "Epoch 800 - lr: 0.05000 - Train loss: 1.22131 - Test loss: 1.17114\n",
      "Epoch 801 - lr: 0.05000 - Train loss: 1.22050 - Test loss: 1.16923\n",
      "Epoch 802 - lr: 0.05000 - Train loss: 1.23967 - Test loss: 1.16958\n",
      "Epoch 803 - lr: 0.05000 - Train loss: 1.22068 - Test loss: 1.16870\n",
      "Epoch 804 - lr: 0.05000 - Train loss: 1.22175 - Test loss: 1.18614\n",
      "Epoch 805 - lr: 0.05000 - Train loss: 1.22499 - Test loss: 1.16930\n",
      "Epoch 806 - lr: 0.05000 - Train loss: 1.23083 - Test loss: 1.17754\n",
      "Epoch 807 - lr: 0.05000 - Train loss: 1.23777 - Test loss: 1.20902\n",
      "Epoch 808 - lr: 0.05000 - Train loss: 1.22041 - Test loss: 1.17843\n",
      "Epoch 809 - lr: 0.05000 - Train loss: 1.23641 - Test loss: 1.16753\n",
      "Epoch 810 - lr: 0.05000 - Train loss: 1.23982 - Test loss: 1.17081\n",
      "Epoch 811 - lr: 0.05000 - Train loss: 1.21899 - Test loss: 1.19135\n",
      "Epoch 812 - lr: 0.05000 - Train loss: 1.22762 - Test loss: 1.17859\n",
      "Epoch 813 - lr: 0.05000 - Train loss: 1.23210 - Test loss: 1.17714\n",
      "Epoch 814 - lr: 0.05000 - Train loss: 1.23587 - Test loss: 1.16680\n",
      "Epoch 815 - lr: 0.05000 - Train loss: 1.22291 - Test loss: 1.18912\n",
      "Epoch 816 - lr: 0.05000 - Train loss: 1.22591 - Test loss: 1.17173\n",
      "Epoch 817 - lr: 0.05000 - Train loss: 1.22106 - Test loss: 1.17073\n",
      "Epoch 818 - lr: 0.05000 - Train loss: 1.22011 - Test loss: 1.16597\n",
      "Epoch 819 - lr: 0.05000 - Train loss: 1.23568 - Test loss: 1.19342\n",
      "Epoch 820 - lr: 0.05000 - Train loss: 1.23012 - Test loss: 1.17091\n",
      "Epoch 821 - lr: 0.05000 - Train loss: 1.21854 - Test loss: 1.17280\n",
      "Epoch 822 - lr: 0.05000 - Train loss: 1.22063 - Test loss: 1.16970\n",
      "Epoch 823 - lr: 0.05000 - Train loss: 1.22142 - Test loss: 1.17777\n",
      "Epoch 824 - lr: 0.05000 - Train loss: 1.23794 - Test loss: 1.20904\n",
      "Epoch 825 - lr: 0.05000 - Train loss: 1.22000 - Test loss: 1.17729\n",
      "Epoch 826 - lr: 0.05000 - Train loss: 1.23667 - Test loss: 1.20835\n",
      "Epoch 827 - lr: 0.05000 - Train loss: 1.21988 - Test loss: 1.17810\n",
      "Epoch 828 - lr: 0.05000 - Train loss: 1.23436 - Test loss: 1.17118\n",
      "Epoch 829 - lr: 0.05000 - Train loss: 1.22130 - Test loss: 1.16796\n",
      "Epoch 830 - lr: 0.05000 - Train loss: 1.22210 - Test loss: 1.18874\n",
      "Epoch 831 - lr: 0.05000 - Train loss: 1.22566 - Test loss: 1.17257\n",
      "Epoch 832 - lr: 0.05000 - Train loss: 1.22032 - Test loss: 1.16826\n",
      "Epoch 833 - lr: 0.05000 - Train loss: 1.23834 - Test loss: 1.16586\n",
      "Epoch 834 - lr: 0.05000 - Train loss: 1.21939 - Test loss: 1.16815\n",
      "Epoch 835 - lr: 0.05000 - Train loss: 1.23912 - Test loss: 1.17000\n",
      "Epoch 836 - lr: 0.05000 - Train loss: 1.22444 - Test loss: 1.18466\n",
      "Epoch 837 - lr: 0.05000 - Train loss: 1.22393 - Test loss: 1.16625\n",
      "Epoch 838 - lr: 0.05000 - Train loss: 1.23408 - Test loss: 1.18182\n",
      "Epoch 839 - lr: 0.05000 - Train loss: 1.23083 - Test loss: 1.20960\n",
      "Epoch 840 - lr: 0.05000 - Train loss: 1.21872 - Test loss: 1.16250\n",
      "Epoch 841 - lr: 0.05000 - Train loss: 1.22112 - Test loss: 1.18731\n",
      "Epoch 842 - lr: 0.05000 - Train loss: 1.22527 - Test loss: 1.17194\n",
      "Epoch 843 - lr: 0.05000 - Train loss: 1.22007 - Test loss: 1.16776\n",
      "Epoch 844 - lr: 0.05000 - Train loss: 1.23808 - Test loss: 1.16545\n",
      "Epoch 845 - lr: 0.05000 - Train loss: 1.21898 - Test loss: 1.16896\n",
      "Epoch 846 - lr: 0.05000 - Train loss: 1.23893 - Test loss: 1.17039\n",
      "Epoch 847 - lr: 0.05000 - Train loss: 1.23427 - Test loss: 1.16309\n",
      "Epoch 848 - lr: 0.05000 - Train loss: 1.22637 - Test loss: 1.17962\n",
      "Epoch 849 - lr: 0.05000 - Train loss: 1.21905 - Test loss: 1.18914\n",
      "Epoch 850 - lr: 0.05000 - Train loss: 1.22528 - Test loss: 1.17176\n",
      "Epoch 851 - lr: 0.05000 - Train loss: 1.21954 - Test loss: 1.16686\n",
      "Epoch 852 - lr: 0.05000 - Train loss: 1.23770 - Test loss: 1.16394\n",
      "Epoch 853 - lr: 0.05000 - Train loss: 1.21903 - Test loss: 1.16494\n",
      "Epoch 854 - lr: 0.05000 - Train loss: 1.22062 - Test loss: 1.18489\n",
      "Epoch 855 - lr: 0.05000 - Train loss: 1.22332 - Test loss: 1.16772\n",
      "Epoch 856 - lr: 0.05000 - Train loss: 1.23608 - Test loss: 1.16093\n",
      "Epoch 857 - lr: 0.05000 - Train loss: 1.23824 - Test loss: 1.16767\n",
      "Epoch 858 - lr: 0.05000 - Train loss: 1.21638 - Test loss: 1.16228\n",
      "Epoch 859 - lr: 0.05000 - Train loss: 1.23437 - Test loss: 1.16463\n",
      "Epoch 860 - lr: 0.05000 - Train loss: 1.23679 - Test loss: 1.16576\n",
      "Epoch 861 - lr: 0.05000 - Train loss: 1.21796 - Test loss: 1.16231\n",
      "Epoch 862 - lr: 0.05000 - Train loss: 1.23269 - Test loss: 1.16182\n",
      "Epoch 863 - lr: 0.05000 - Train loss: 1.23578 - Test loss: 1.15701\n",
      "Epoch 864 - lr: 0.05000 - Train loss: 1.23237 - Test loss: 1.18368\n",
      "Epoch 865 - lr: 0.05000 - Train loss: 1.23038 - Test loss: 1.15729\n",
      "Epoch 866 - lr: 0.05000 - Train loss: 1.23185 - Test loss: 1.18143\n",
      "Epoch 867 - lr: 0.05000 - Train loss: 1.23324 - Test loss: 1.14770\n",
      "Epoch 868 - lr: 0.05000 - Train loss: 1.23086 - Test loss: 1.17428\n",
      "Epoch 869 - lr: 0.05000 - Train loss: 1.23242 - Test loss: 1.20882\n",
      "Epoch 870 - lr: 0.05000 - Train loss: 1.21795 - Test loss: 1.15982\n",
      "Epoch 871 - lr: 0.05000 - Train loss: 1.22922 - Test loss: 1.17280\n",
      "Epoch 872 - lr: 0.05000 - Train loss: 1.23444 - Test loss: 1.16174\n",
      "Epoch 873 - lr: 0.05000 - Train loss: 1.23410 - Test loss: 1.15688\n",
      "Epoch 874 - lr: 0.05000 - Train loss: 1.23615 - Test loss: 1.19132\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 875 - lr: 0.05000 - Train loss: 1.22376 - Test loss: 1.17229\n",
      "Epoch 876 - lr: 0.05000 - Train loss: 1.21930 - Test loss: 1.16837\n",
      "Epoch 877 - lr: 0.05000 - Train loss: 1.23807 - Test loss: 1.16755\n",
      "Epoch 878 - lr: 0.05000 - Train loss: 1.21878 - Test loss: 1.16700\n",
      "Epoch 879 - lr: 0.05000 - Train loss: 1.22004 - Test loss: 1.17723\n",
      "Epoch 880 - lr: 0.05000 - Train loss: 1.23676 - Test loss: 1.20353\n",
      "Epoch 881 - lr: 0.05000 - Train loss: 1.22059 - Test loss: 1.18500\n",
      "Epoch 882 - lr: 0.05000 - Train loss: 1.22410 - Test loss: 1.17015\n",
      "Epoch 883 - lr: 0.05000 - Train loss: 1.21881 - Test loss: 1.16792\n",
      "Epoch 884 - lr: 0.05000 - Train loss: 1.23810 - Test loss: 1.16780\n",
      "Epoch 885 - lr: 0.05000 - Train loss: 1.21583 - Test loss: 1.16113\n",
      "Epoch 886 - lr: 0.05000 - Train loss: 1.23202 - Test loss: 1.17381\n",
      "Epoch 887 - lr: 0.05000 - Train loss: 1.23421 - Test loss: 1.15604\n",
      "Epoch 888 - lr: 0.05000 - Train loss: 1.23287 - Test loss: 1.20660\n",
      "Epoch 889 - lr: 0.05000 - Train loss: 1.21793 - Test loss: 1.16640\n",
      "Epoch 890 - lr: 0.05000 - Train loss: 1.21967 - Test loss: 1.16719\n",
      "Epoch 891 - lr: 0.05000 - Train loss: 1.21930 - Test loss: 1.17070\n",
      "Epoch 892 - lr: 0.05000 - Train loss: 1.21925 - Test loss: 1.16933\n",
      "Epoch 893 - lr: 0.05000 - Train loss: 1.21831 - Test loss: 1.16225\n",
      "Epoch 894 - lr: 0.05000 - Train loss: 1.23485 - Test loss: 1.14950\n",
      "Epoch 895 - lr: 0.05000 - Train loss: 1.22509 - Test loss: 1.17394\n",
      "Epoch 896 - lr: 0.05000 - Train loss: 1.23488 - Test loss: 1.20503\n",
      "Epoch 897 - lr: 0.05000 - Train loss: 1.21814 - Test loss: 1.17966\n",
      "Epoch 898 - lr: 0.05000 - Train loss: 1.22194 - Test loss: 1.17050\n",
      "Epoch 899 - lr: 0.05000 - Train loss: 1.21842 - Test loss: 1.16736\n",
      "Epoch 900 - lr: 0.05000 - Train loss: 1.23754 - Test loss: 1.16692\n",
      "Epoch 901 - lr: 0.05000 - Train loss: 1.21565 - Test loss: 1.16455\n",
      "Epoch 902 - lr: 0.05000 - Train loss: 1.23738 - Test loss: 1.16643\n",
      "Epoch 903 - lr: 0.05000 - Train loss: 1.21640 - Test loss: 1.16133\n",
      "Epoch 904 - lr: 0.05000 - Train loss: 1.23461 - Test loss: 1.14839\n",
      "Epoch 905 - lr: 0.05000 - Train loss: 1.22737 - Test loss: 1.16942\n",
      "Epoch 906 - lr: 0.05000 - Train loss: 1.23096 - Test loss: 1.16908\n",
      "Epoch 907 - lr: 0.05000 - Train loss: 1.21854 - Test loss: 1.16793\n",
      "Epoch 908 - lr: 0.05000 - Train loss: 1.21825 - Test loss: 1.16478\n",
      "Epoch 909 - lr: 0.05000 - Train loss: 1.21910 - Test loss: 1.18017\n",
      "Epoch 910 - lr: 0.05000 - Train loss: 1.21972 - Test loss: 1.16773\n",
      "Epoch 911 - lr: 0.05000 - Train loss: 1.21940 - Test loss: 1.17654\n",
      "Epoch 912 - lr: 0.05000 - Train loss: 1.23556 - Test loss: 1.20623\n",
      "Epoch 913 - lr: 0.05000 - Train loss: 1.21784 - Test loss: 1.17641\n",
      "Epoch 914 - lr: 0.05000 - Train loss: 1.21699 - Test loss: 1.18402\n",
      "Epoch 915 - lr: 0.05000 - Train loss: 1.22180 - Test loss: 1.16766\n",
      "Epoch 916 - lr: 0.05000 - Train loss: 1.23736 - Test loss: 1.16573\n",
      "Epoch 917 - lr: 0.05000 - Train loss: 1.21818 - Test loss: 1.16737\n",
      "Epoch 918 - lr: 0.05000 - Train loss: 1.21757 - Test loss: 1.16075\n",
      "Epoch 919 - lr: 0.05000 - Train loss: 1.23312 - Test loss: 1.14819\n",
      "Epoch 920 - lr: 0.05000 - Train loss: 1.23203 - Test loss: 1.14657\n",
      "Epoch 921 - lr: 0.05000 - Train loss: 1.23539 - Test loss: 1.16328\n",
      "Epoch 922 - lr: 0.05000 - Train loss: 1.21757 - Test loss: 1.16534\n",
      "Epoch 923 - lr: 0.05000 - Train loss: 1.21845 - Test loss: 1.16993\n",
      "Epoch 924 - lr: 0.05000 - Train loss: 1.21714 - Test loss: 1.16306\n",
      "Epoch 925 - lr: 0.05000 - Train loss: 1.22866 - Test loss: 1.17121\n",
      "Epoch 926 - lr: 0.05000 - Train loss: 1.22944 - Test loss: 1.17094\n",
      "Epoch 927 - lr: 0.05000 - Train loss: 1.21654 - Test loss: 1.18684\n",
      "Epoch 928 - lr: 0.05000 - Train loss: 1.22447 - Test loss: 1.17419\n",
      "Epoch 929 - lr: 0.05000 - Train loss: 1.22831 - Test loss: 1.17374\n",
      "Epoch 930 - lr: 0.05000 - Train loss: 1.23510 - Test loss: 1.20503\n",
      "Epoch 931 - lr: 0.05000 - Train loss: 1.21751 - Test loss: 1.17874\n",
      "Epoch 932 - lr: 0.05000 - Train loss: 1.22108 - Test loss: 1.16996\n",
      "Epoch 933 - lr: 0.05000 - Train loss: 1.21807 - Test loss: 1.16356\n",
      "Epoch 934 - lr: 0.05000 - Train loss: 1.23360 - Test loss: 1.16909\n",
      "Epoch 935 - lr: 0.05000 - Train loss: 1.23493 - Test loss: 1.16206\n",
      "Epoch 936 - lr: 0.05000 - Train loss: 1.21758 - Test loss: 1.15973\n",
      "Epoch 937 - lr: 0.05000 - Train loss: 1.23056 - Test loss: 1.20574\n",
      "Epoch 938 - lr: 0.05000 - Train loss: 1.21726 - Test loss: 1.16951\n",
      "Epoch 939 - lr: 0.05000 - Train loss: 1.23221 - Test loss: 1.16090\n",
      "Epoch 940 - lr: 0.05000 - Train loss: 1.22264 - Test loss: 1.18022\n",
      "Epoch 941 - lr: 0.05000 - Train loss: 1.22125 - Test loss: 1.16947\n",
      "Epoch 942 - lr: 0.05000 - Train loss: 1.21736 - Test loss: 1.16335\n",
      "Epoch 943 - lr: 0.05000 - Train loss: 1.23417 - Test loss: 1.15556\n",
      "Epoch 944 - lr: 0.05000 - Train loss: 1.23320 - Test loss: 1.14655\n",
      "Epoch 945 - lr: 0.05000 - Train loss: 1.22695 - Test loss: 1.16752\n",
      "Epoch 946 - lr: 0.05000 - Train loss: 1.21620 - Test loss: 1.18764\n",
      "Epoch 947 - lr: 0.05000 - Train loss: 1.22450 - Test loss: 1.17585\n",
      "Epoch 948 - lr: 0.05000 - Train loss: 1.23571 - Test loss: 1.20457\n",
      "Epoch 949 - lr: 0.05000 - Train loss: 1.21752 - Test loss: 1.18156\n",
      "Epoch 950 - lr: 0.05000 - Train loss: 1.22100 - Test loss: 1.16207\n",
      "Epoch 951 - lr: 0.05000 - Train loss: 1.23370 - Test loss: 1.14640\n",
      "Epoch 952 - lr: 0.05000 - Train loss: 1.22900 - Test loss: 1.20179\n",
      "Epoch 953 - lr: 0.05000 - Train loss: 1.21675 - Test loss: 1.17429\n",
      "Epoch 954 - lr: 0.05000 - Train loss: 1.22692 - Test loss: 1.17410\n",
      "Epoch 955 - lr: 0.05000 - Train loss: 1.23583 - Test loss: 1.20410\n",
      "Epoch 956 - lr: 0.05000 - Train loss: 1.21748 - Test loss: 1.18172\n",
      "Epoch 957 - lr: 0.05000 - Train loss: 1.22123 - Test loss: 1.16169\n",
      "Epoch 958 - lr: 0.05000 - Train loss: 1.23089 - Test loss: 1.16130\n",
      "Epoch 959 - lr: 0.05000 - Train loss: 1.21898 - Test loss: 1.18337\n",
      "Epoch 960 - lr: 0.05000 - Train loss: 1.22139 - Test loss: 1.16224\n",
      "Epoch 961 - lr: 0.05000 - Train loss: 1.23065 - Test loss: 1.16380\n",
      "Epoch 962 - lr: 0.05000 - Train loss: 1.21708 - Test loss: 1.16588\n",
      "Epoch 963 - lr: 0.05000 - Train loss: 1.21656 - Test loss: 1.16434\n",
      "Epoch 964 - lr: 0.05000 - Train loss: 1.23608 - Test loss: 1.16500\n",
      "Epoch 965 - lr: 0.05000 - Train loss: 1.21626 - Test loss: 1.16288\n",
      "Epoch 966 - lr: 0.05000 - Train loss: 1.22019 - Test loss: 1.18339\n",
      "Epoch 967 - lr: 0.05000 - Train loss: 1.22123 - Test loss: 1.16198\n",
      "Epoch 968 - lr: 0.05000 - Train loss: 1.23073 - Test loss: 1.15963\n",
      "Epoch 969 - lr: 0.05000 - Train loss: 1.22597 - Test loss: 1.17211\n",
      "Epoch 970 - lr: 0.05000 - Train loss: 1.23498 - Test loss: 1.19828\n",
      "Epoch 971 - lr: 0.05000 - Train loss: 1.22263 - Test loss: 1.17490\n",
      "Epoch 972 - lr: 0.05000 - Train loss: 1.21770 - Test loss: 1.19020\n",
      "Epoch 973 - lr: 0.05000 - Train loss: 1.22474 - Test loss: 1.17846\n",
      "Epoch 974 - lr: 0.05000 - Train loss: 1.23435 - Test loss: 1.18394\n",
      "Epoch 975 - lr: 0.05000 - Train loss: 1.23101 - Test loss: 1.14661\n",
      "Epoch 976 - lr: 0.05000 - Train loss: 1.23073 - Test loss: 1.19458\n",
      "Epoch 977 - lr: 0.05000 - Train loss: 1.21959 - Test loss: 1.17883\n",
      "Epoch 978 - lr: 0.05000 - Train loss: 1.22020 - Test loss: 1.16374\n",
      "Epoch 979 - lr: 0.05000 - Train loss: 1.23471 - Test loss: 1.15827\n",
      "Epoch 980 - lr: 0.05000 - Train loss: 1.21901 - Test loss: 1.18253\n",
      "Epoch 981 - lr: 0.05000 - Train loss: 1.22093 - Test loss: 1.16145\n",
      "Epoch 982 - lr: 0.05000 - Train loss: 1.23057 - Test loss: 1.16214\n",
      "Epoch 983 - lr: 0.05000 - Train loss: 1.23602 - Test loss: 1.16592\n",
      "Epoch 984 - lr: 0.05000 - Train loss: 1.21606 - Test loss: 1.18647\n",
      "Epoch 985 - lr: 0.05000 - Train loss: 1.22377 - Test loss: 1.17315\n",
      "Epoch 986 - lr: 0.05000 - Train loss: 1.22853 - Test loss: 1.17116\n",
      "Epoch 987 - lr: 0.05000 - Train loss: 1.22790 - Test loss: 1.17118\n",
      "Epoch 988 - lr: 0.05000 - Train loss: 1.23154 - Test loss: 1.16512\n",
      "Epoch 989 - lr: 0.05000 - Train loss: 1.21619 - Test loss: 1.15937\n",
      "Epoch 990 - lr: 0.05000 - Train loss: 1.23194 - Test loss: 1.14717\n",
      "Epoch 991 - lr: 0.05000 - Train loss: 1.23123 - Test loss: 1.14366\n",
      "Epoch 992 - lr: 0.05000 - Train loss: 1.23103 - Test loss: 1.14454\n",
      "Epoch 993 - lr: 0.05000 - Train loss: 1.22712 - Test loss: 1.16486\n",
      "Epoch 994 - lr: 0.05000 - Train loss: 1.21640 - Test loss: 1.16538\n",
      "Epoch 995 - lr: 0.05000 - Train loss: 1.21688 - Test loss: 1.16632\n",
      "Epoch 996 - lr: 0.05000 - Train loss: 1.21657 - Test loss: 1.16190\n",
      "Epoch 997 - lr: 0.05000 - Train loss: 1.23220 - Test loss: 1.17697\n",
      "Epoch 998 - lr: 0.05000 - Train loss: 1.23175 - Test loss: 1.14714\n",
      "Epoch 999 - lr: 0.05000 - Train loss: 1.21588 - Test loss: 1.17505\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000 - lr: 0.05000 - Train loss: 1.21514 - Test loss: 1.16280\n",
      "Epoch 1001 - lr: 0.05000 - Train loss: 1.23421 - Test loss: 1.15675\n",
      "Epoch 1002 - lr: 0.05000 - Train loss: 1.22348 - Test loss: 1.17426\n",
      "Epoch 1003 - lr: 0.05000 - Train loss: 1.23192 - Test loss: 1.16132\n",
      "Epoch 1004 - lr: 0.05000 - Train loss: 1.22043 - Test loss: 1.18117\n",
      "Epoch 1005 - lr: 0.05000 - Train loss: 1.22024 - Test loss: 1.16233\n",
      "Epoch 1006 - lr: 0.05000 - Train loss: 1.23332 - Test loss: 1.14736\n",
      "Epoch 1007 - lr: 0.05000 - Train loss: 1.22406 - Test loss: 1.16988\n",
      "Epoch 1008 - lr: 0.05000 - Train loss: 1.23464 - Test loss: 1.19454\n",
      "Epoch 1009 - lr: 0.05000 - Train loss: 1.22631 - Test loss: 1.16699\n",
      "Epoch 1010 - lr: 0.05000 - Train loss: 1.21475 - Test loss: 1.17085\n",
      "Epoch 1011 - lr: 0.05000 - Train loss: 1.23238 - Test loss: 1.16236\n",
      "Epoch 1012 - lr: 0.05000 - Train loss: 1.23550 - Test loss: 1.16471\n",
      "Epoch 1013 - lr: 0.05000 - Train loss: 1.21342 - Test loss: 1.16128\n",
      "Epoch 1014 - lr: 0.05000 - Train loss: 1.23435 - Test loss: 1.16020\n",
      "Epoch 1015 - lr: 0.05000 - Train loss: 1.21573 - Test loss: 1.16267\n",
      "Epoch 1016 - lr: 0.05000 - Train loss: 1.21693 - Test loss: 1.16891\n",
      "Epoch 1017 - lr: 0.05000 - Train loss: 1.21397 - Test loss: 1.16445\n",
      "Epoch 1018 - lr: 0.05000 - Train loss: 1.21649 - Test loss: 1.16549\n",
      "Epoch 1019 - lr: 0.05000 - Train loss: 1.21588 - Test loss: 1.16357\n",
      "Epoch 1020 - lr: 0.05000 - Train loss: 1.23525 - Test loss: 1.16383\n",
      "Epoch 1021 - lr: 0.05000 - Train loss: 1.21623 - Test loss: 1.16601\n",
      "Epoch 1022 - lr: 0.05000 - Train loss: 1.21573 - Test loss: 1.16190\n",
      "Epoch 1023 - lr: 0.05000 - Train loss: 1.23350 - Test loss: 1.15621\n",
      "Epoch 1024 - lr: 0.05000 - Train loss: 1.22388 - Test loss: 1.17216\n",
      "Epoch 1025 - lr: 0.05000 - Train loss: 1.23429 - Test loss: 1.19404\n",
      "Epoch 1026 - lr: 0.05000 - Train loss: 1.22654 - Test loss: 1.16604\n",
      "Epoch 1027 - lr: 0.05000 - Train loss: 1.21375 - Test loss: 1.16019\n",
      "Epoch 1028 - lr: 0.05000 - Train loss: 1.23089 - Test loss: 1.20670\n",
      "Epoch 1029 - lr: 0.05000 - Train loss: 1.21529 - Test loss: 1.16353\n",
      "Epoch 1030 - lr: 0.05000 - Train loss: 1.21670 - Test loss: 1.16117\n",
      "Epoch 1031 - lr: 0.05000 - Train loss: 1.22334 - Test loss: 1.17514\n",
      "Epoch 1032 - lr: 0.05000 - Train loss: 1.22895 - Test loss: 1.16905\n",
      "Epoch 1033 - lr: 0.05000 - Train loss: 1.21354 - Test loss: 1.16145\n",
      "Epoch 1034 - lr: 0.05000 - Train loss: 1.23247 - Test loss: 1.15709\n",
      "Epoch 1035 - lr: 0.05000 - Train loss: 1.23505 - Test loss: 1.16483\n",
      "Epoch 1036 - lr: 0.05000 - Train loss: 1.21790 - Test loss: 1.18420\n",
      "Epoch 1037 - lr: 0.05000 - Train loss: 1.22255 - Test loss: 1.17166\n",
      "Epoch 1038 - lr: 0.05000 - Train loss: 1.21789 - Test loss: 1.18723\n",
      "Epoch 1039 - lr: 0.05000 - Train loss: 1.22382 - Test loss: 1.17729\n",
      "Epoch 1040 - lr: 0.05000 - Train loss: 1.23400 - Test loss: 1.18366\n",
      "Epoch 1041 - lr: 0.05000 - Train loss: 1.23029 - Test loss: 1.14487\n",
      "Epoch 1042 - lr: 0.05000 - Train loss: 1.23150 - Test loss: 1.14681\n",
      "Epoch 1043 - lr: 0.05000 - Train loss: 1.21515 - Test loss: 1.16984\n",
      "Epoch 1044 - lr: 0.05000 - Train loss: 1.23475 - Test loss: 1.17949\n",
      "Epoch 1045 - lr: 0.05000 - Train loss: 1.23167 - Test loss: 1.14735\n",
      "Epoch 1046 - lr: 0.05000 - Train loss: 1.21535 - Test loss: 1.17414\n",
      "Epoch 1047 - lr: 0.05000 - Train loss: 1.21476 - Test loss: 1.16876\n",
      "Epoch 1048 - lr: 0.05000 - Train loss: 1.21618 - Test loss: 1.16594\n",
      "Epoch 1049 - lr: 0.05000 - Train loss: 1.21618 - Test loss: 1.16462\n",
      "Epoch 1050 - lr: 0.05000 - Train loss: 1.21564 - Test loss: 1.16131\n",
      "Epoch 1051 - lr: 0.05000 - Train loss: 1.21924 - Test loss: 1.18107\n",
      "Epoch 1052 - lr: 0.05000 - Train loss: 1.21937 - Test loss: 1.16369\n",
      "Epoch 1053 - lr: 0.05000 - Train loss: 1.23319 - Test loss: 1.15475\n",
      "Epoch 1054 - lr: 0.05000 - Train loss: 1.22851 - Test loss: 1.16113\n",
      "Epoch 1055 - lr: 0.05000 - Train loss: 1.21595 - Test loss: 1.16738\n",
      "Epoch 1056 - lr: 0.05000 - Train loss: 1.21517 - Test loss: 1.16315\n",
      "Epoch 1057 - lr: 0.05000 - Train loss: 1.21768 - Test loss: 1.18059\n",
      "Epoch 1058 - lr: 0.05000 - Train loss: 1.21966 - Test loss: 1.16159\n",
      "Epoch 1059 - lr: 0.05000 - Train loss: 1.22958 - Test loss: 1.15760\n",
      "Epoch 1060 - lr: 0.05000 - Train loss: 1.22855 - Test loss: 1.16246\n",
      "Epoch 1061 - lr: 0.05000 - Train loss: 1.21511 - Test loss: 1.16166\n",
      "Epoch 1062 - lr: 0.05000 - Train loss: 1.21686 - Test loss: 1.17791\n",
      "Epoch 1063 - lr: 0.05000 - Train loss: 1.21774 - Test loss: 1.16814\n",
      "Epoch 1064 - lr: 0.05000 - Train loss: 1.21567 - Test loss: 1.16290\n",
      "Epoch 1065 - lr: 0.05000 - Train loss: 1.23348 - Test loss: 1.15878\n",
      "Epoch 1066 - lr: 0.05000 - Train loss: 1.21604 - Test loss: 1.17036\n",
      "Epoch 1067 - lr: 0.05000 - Train loss: 1.23329 - Test loss: 1.20173\n",
      "Epoch 1068 - lr: 0.05000 - Train loss: 1.21709 - Test loss: 1.18037\n",
      "Epoch 1069 - lr: 0.05000 - Train loss: 1.21962 - Test loss: 1.16168\n",
      "Epoch 1070 - lr: 0.05000 - Train loss: 1.22358 - Test loss: 1.17384\n",
      "Epoch 1071 - lr: 0.05000 - Train loss: 1.23288 - Test loss: 1.19148\n",
      "Epoch 1072 - lr: 0.05000 - Train loss: 1.22746 - Test loss: 1.16038\n",
      "Epoch 1073 - lr: 0.05000 - Train loss: 1.21574 - Test loss: 1.16760\n",
      "Epoch 1074 - lr: 0.05000 - Train loss: 1.21301 - Test loss: 1.16079\n",
      "Epoch 1075 - lr: 0.05000 - Train loss: 1.23154 - Test loss: 1.16464\n",
      "Epoch 1076 - lr: 0.05000 - Train loss: 1.23444 - Test loss: 1.16650\n",
      "Epoch 1077 - lr: 0.05000 - Train loss: 1.23324 - Test loss: 1.20653\n",
      "Epoch 1078 - lr: 0.05000 - Train loss: 1.21540 - Test loss: 1.16837\n",
      "Epoch 1079 - lr: 0.05000 - Train loss: 1.23179 - Test loss: 1.19766\n",
      "Epoch 1080 - lr: 0.05000 - Train loss: 1.22197 - Test loss: 1.17166\n",
      "Epoch 1081 - lr: 0.05000 - Train loss: 1.23300 - Test loss: 1.20427\n",
      "Epoch 1082 - lr: 0.05000 - Train loss: 1.21583 - Test loss: 1.17814\n",
      "Epoch 1083 - lr: 0.05000 - Train loss: 1.21925 - Test loss: 1.16251\n",
      "Epoch 1084 - lr: 0.05000 - Train loss: 1.22461 - Test loss: 1.17212\n",
      "Epoch 1085 - lr: 0.05000 - Train loss: 1.23476 - Test loss: 1.16058\n",
      "Epoch 1086 - lr: 0.05000 - Train loss: 1.23166 - Test loss: 1.15569\n",
      "Epoch 1087 - lr: 0.05000 - Train loss: 1.23242 - Test loss: 1.15706\n",
      "Epoch 1088 - lr: 0.05000 - Train loss: 1.21627 - Test loss: 1.17756\n",
      "Epoch 1089 - lr: 0.05000 - Train loss: 1.21840 - Test loss: 1.16930\n",
      "Epoch 1090 - lr: 0.05000 - Train loss: 1.21690 - Test loss: 1.16492\n",
      "Epoch 1091 - lr: 0.05000 - Train loss: 1.21673 - Test loss: 1.17172\n",
      "Epoch 1092 - lr: 0.05000 - Train loss: 1.23384 - Test loss: 1.17586\n",
      "Epoch 1093 - lr: 0.05000 - Train loss: 1.22901 - Test loss: 1.19915\n",
      "Epoch 1094 - lr: 0.05000 - Train loss: 1.21473 - Test loss: 1.17299\n",
      "Epoch 1095 - lr: 0.05000 - Train loss: 1.21520 - Test loss: 1.18641\n",
      "Epoch 1096 - lr: 0.05000 - Train loss: 1.22330 - Test loss: 1.17834\n",
      "Epoch 1097 - lr: 0.05000 - Train loss: 1.22207 - Test loss: 1.17978\n",
      "Epoch 1098 - lr: 0.05000 - Train loss: 1.21835 - Test loss: 1.16993\n",
      "Epoch 1099 - lr: 0.05000 - Train loss: 1.21710 - Test loss: 1.16555\n",
      "Epoch 1100 - lr: 0.05000 - Train loss: 1.21632 - Test loss: 1.16858\n",
      "Epoch 1101 - lr: 0.05000 - Train loss: 1.21278 - Test loss: 1.16007\n",
      "Epoch 1102 - lr: 0.05000 - Train loss: 1.23029 - Test loss: 1.17358\n",
      "Epoch 1103 - lr: 0.05000 - Train loss: 1.22915 - Test loss: 1.14407\n",
      "Epoch 1104 - lr: 0.05000 - Train loss: 1.22235 - Test loss: 1.16843\n",
      "Epoch 1105 - lr: 0.05000 - Train loss: 1.23407 - Test loss: 1.16644\n",
      "Epoch 1106 - lr: 0.05000 - Train loss: 1.23428 - Test loss: 1.16444\n",
      "Epoch 1107 - lr: 0.05000 - Train loss: 1.21337 - Test loss: 1.17123\n",
      "Epoch 1108 - lr: 0.05000 - Train loss: 1.23470 - Test loss: 1.15886\n",
      "Epoch 1109 - lr: 0.05000 - Train loss: 1.23019 - Test loss: 1.16050\n",
      "Epoch 1110 - lr: 0.05000 - Train loss: 1.23318 - Test loss: 1.16331\n",
      "Epoch 1111 - lr: 0.05000 - Train loss: 1.21864 - Test loss: 1.17899\n",
      "Epoch 1112 - lr: 0.05000 - Train loss: 1.21924 - Test loss: 1.16401\n",
      "Epoch 1113 - lr: 0.05000 - Train loss: 1.21898 - Test loss: 1.18018\n",
      "Epoch 1114 - lr: 0.05000 - Train loss: 1.21946 - Test loss: 1.16615\n",
      "Epoch 1115 - lr: 0.05000 - Train loss: 1.21664 - Test loss: 1.17028\n",
      "Epoch 1116 - lr: 0.05000 - Train loss: 1.22977 - Test loss: 1.16506\n",
      "Epoch 1117 - lr: 0.05000 - Train loss: 1.21498 - Test loss: 1.15963\n",
      "Epoch 1118 - lr: 0.05000 - Train loss: 1.23161 - Test loss: 1.14589\n",
      "Epoch 1119 - lr: 0.05000 - Train loss: 1.22474 - Test loss: 1.16548\n",
      "Epoch 1120 - lr: 0.05000 - Train loss: 1.21267 - Test loss: 1.16665\n",
      "Epoch 1121 - lr: 0.05000 - Train loss: 1.21596 - Test loss: 1.16789\n",
      "Epoch 1122 - lr: 0.05000 - Train loss: 1.21635 - Test loss: 1.16451\n",
      "Epoch 1123 - lr: 0.05000 - Train loss: 1.21638 - Test loss: 1.17123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1124 - lr: 0.05000 - Train loss: 1.23359 - Test loss: 1.16490\n",
      "Epoch 1125 - lr: 0.05000 - Train loss: 1.23403 - Test loss: 1.16377\n",
      "Epoch 1126 - lr: 0.05000 - Train loss: 1.21301 - Test loss: 1.15868\n",
      "Epoch 1127 - lr: 0.05000 - Train loss: 1.22831 - Test loss: 1.16108\n",
      "Epoch 1128 - lr: 0.05000 - Train loss: 1.21581 - Test loss: 1.17103\n",
      "Epoch 1129 - lr: 0.05000 - Train loss: 1.23416 - Test loss: 1.15816\n",
      "Epoch 1130 - lr: 0.05000 - Train loss: 1.23176 - Test loss: 1.14841\n",
      "Epoch 1131 - lr: 0.05000 - Train loss: 1.21447 - Test loss: 1.17199\n",
      "Epoch 1132 - lr: 0.05000 - Train loss: 1.23154 - Test loss: 1.16706\n",
      "Epoch 1133 - lr: 0.05000 - Train loss: 1.23405 - Test loss: 1.16412\n",
      "Epoch 1134 - lr: 0.05000 - Train loss: 1.21179 - Test loss: 1.16233\n",
      "Epoch 1135 - lr: 0.05000 - Train loss: 1.21564 - Test loss: 1.16836\n",
      "Epoch 1136 - lr: 0.05000 - Train loss: 1.21350 - Test loss: 1.16977\n",
      "Epoch 1137 - lr: 0.05000 - Train loss: 1.22880 - Test loss: 1.16622\n",
      "Epoch 1138 - lr: 0.05000 - Train loss: 1.21610 - Test loss: 1.16727\n",
      "Epoch 1139 - lr: 0.05000 - Train loss: 1.21544 - Test loss: 1.15958\n",
      "Epoch 1140 - lr: 0.05000 - Train loss: 1.22838 - Test loss: 1.17251\n",
      "Epoch 1141 - lr: 0.05000 - Train loss: 1.22959 - Test loss: 1.16445\n",
      "Epoch 1142 - lr: 0.05000 - Train loss: 1.23358 - Test loss: 1.16472\n",
      "Epoch 1143 - lr: 0.05000 - Train loss: 1.23000 - Test loss: 1.15680\n",
      "Epoch 1144 - lr: 0.05000 - Train loss: 1.22913 - Test loss: 1.18785\n",
      "Epoch 1145 - lr: 0.05000 - Train loss: 1.22433 - Test loss: 1.16419\n",
      "Epoch 1146 - lr: 0.05000 - Train loss: 1.21275 - Test loss: 1.15832\n",
      "Epoch 1147 - lr: 0.05000 - Train loss: 1.22828 - Test loss: 1.16080\n",
      "Epoch 1148 - lr: 0.05000 - Train loss: 1.23370 - Test loss: 1.16370\n",
      "Epoch 1149 - lr: 0.05000 - Train loss: 1.21239 - Test loss: 1.15797\n",
      "Epoch 1150 - lr: 0.05000 - Train loss: 1.22836 - Test loss: 1.19214\n",
      "Epoch 1151 - lr: 0.05000 - Train loss: 1.22296 - Test loss: 1.16677\n",
      "Epoch 1152 - lr: 0.05000 - Train loss: 1.23011 - Test loss: 1.15641\n",
      "Epoch 1153 - lr: 0.05000 - Train loss: 1.23034 - Test loss: 1.14558\n",
      "Epoch 1154 - lr: 0.05000 - Train loss: 1.22617 - Test loss: 1.16136\n",
      "Epoch 1155 - lr: 0.05000 - Train loss: 1.21348 - Test loss: 1.15887\n",
      "Epoch 1156 - lr: 0.05000 - Train loss: 1.22913 - Test loss: 1.20497\n",
      "Epoch 1157 - lr: 0.05000 - Train loss: 1.21410 - Test loss: 1.16564\n",
      "Epoch 1158 - lr: 0.05000 - Train loss: 1.21257 - Test loss: 1.16757\n",
      "Epoch 1159 - lr: 0.05000 - Train loss: 1.21231 - Test loss: 1.15993\n",
      "Epoch 1160 - lr: 0.05000 - Train loss: 1.22954 - Test loss: 1.20095\n",
      "Epoch 1161 - lr: 0.05000 - Train loss: 1.21422 - Test loss: 1.17228\n",
      "Epoch 1162 - lr: 0.05000 - Train loss: 1.22818 - Test loss: 1.16643\n",
      "Epoch 1163 - lr: 0.05000 - Train loss: 1.21565 - Test loss: 1.16773\n",
      "Epoch 1164 - lr: 0.05000 - Train loss: 1.21587 - Test loss: 1.16389\n",
      "Epoch 1165 - lr: 0.05000 - Train loss: 1.21621 - Test loss: 1.17546\n",
      "Epoch 1166 - lr: 0.05000 - Train loss: 1.22675 - Test loss: 1.16940\n",
      "Epoch 1167 - lr: 0.05000 - Train loss: 1.21534 - Test loss: 1.18486\n",
      "Epoch 1168 - lr: 0.05000 - Train loss: 1.22177 - Test loss: 1.17218\n",
      "Epoch 1169 - lr: 0.05000 - Train loss: 1.23126 - Test loss: 1.15787\n",
      "Epoch 1170 - lr: 0.05000 - Train loss: 1.23154 - Test loss: 1.14844\n",
      "Epoch 1171 - lr: 0.05000 - Train loss: 1.21389 - Test loss: 1.16932\n",
      "Epoch 1172 - lr: 0.05000 - Train loss: 1.23400 - Test loss: 1.15740\n",
      "Epoch 1173 - lr: 0.05000 - Train loss: 1.22760 - Test loss: 1.20617\n",
      "Epoch 1174 - lr: 0.05000 - Train loss: 1.21444 - Test loss: 1.16959\n",
      "Epoch 1175 - lr: 0.05000 - Train loss: 1.23403 - Test loss: 1.15740\n",
      "Epoch 1176 - lr: 0.05000 - Train loss: 1.23078 - Test loss: 1.14720\n",
      "Epoch 1177 - lr: 0.05000 - Train loss: 1.21815 - Test loss: 1.17457\n",
      "Epoch 1178 - lr: 0.05000 - Train loss: 1.21407 - Test loss: 1.16050\n",
      "Epoch 1179 - lr: 0.05000 - Train loss: 1.22785 - Test loss: 1.16387\n",
      "Epoch 1180 - lr: 0.05000 - Train loss: 1.21366 - Test loss: 1.16082\n",
      "Epoch 1181 - lr: 0.05000 - Train loss: 1.23076 - Test loss: 1.15470\n",
      "Epoch 1182 - lr: 0.05000 - Train loss: 1.22878 - Test loss: 1.19807\n",
      "Epoch 1183 - lr: 0.05000 - Train loss: 1.21696 - Test loss: 1.17652\n",
      "Epoch 1184 - lr: 0.05000 - Train loss: 1.21805 - Test loss: 1.16684\n",
      "Epoch 1185 - lr: 0.05000 - Train loss: 1.21415 - Test loss: 1.15863\n",
      "Epoch 1186 - lr: 0.05000 - Train loss: 1.22828 - Test loss: 1.18406\n",
      "Epoch 1187 - lr: 0.05000 - Train loss: 1.22567 - Test loss: 1.15342\n",
      "Epoch 1188 - lr: 0.05000 - Train loss: 1.22741 - Test loss: 1.18228\n",
      "Epoch 1189 - lr: 0.05000 - Train loss: 1.22541 - Test loss: 1.15387\n",
      "Epoch 1190 - lr: 0.05000 - Train loss: 1.22816 - Test loss: 1.20609\n",
      "Epoch 1191 - lr: 0.05000 - Train loss: 1.21319 - Test loss: 1.16272\n",
      "Epoch 1192 - lr: 0.05000 - Train loss: 1.21469 - Test loss: 1.16045\n",
      "Epoch 1193 - lr: 0.05000 - Train loss: 1.22286 - Test loss: 1.17170\n",
      "Epoch 1194 - lr: 0.05000 - Train loss: 1.23384 - Test loss: 1.15809\n",
      "Epoch 1195 - lr: 0.05000 - Train loss: 1.22921 - Test loss: 1.14716\n",
      "Epoch 1196 - lr: 0.05000 - Train loss: 1.22758 - Test loss: 1.20571\n",
      "Epoch 1197 - lr: 0.05000 - Train loss: 1.21329 - Test loss: 1.16384\n",
      "Epoch 1198 - lr: 0.05000 - Train loss: 1.21484 - Test loss: 1.16682\n",
      "Epoch 1199 - lr: 0.05000 - Train loss: 1.21509 - Test loss: 1.16126\n",
      "Epoch 1200 - lr: 0.05000 - Train loss: 1.22395 - Test loss: 1.17024\n",
      "Epoch 1201 - lr: 0.05000 - Train loss: 1.23182 - Test loss: 1.19051\n",
      "Epoch 1202 - lr: 0.05000 - Train loss: 1.22613 - Test loss: 1.15625\n",
      "Epoch 1203 - lr: 0.05000 - Train loss: 1.22485 - Test loss: 1.16696\n",
      "Epoch 1204 - lr: 0.05000 - Train loss: 1.21204 - Test loss: 1.16761\n",
      "Epoch 1205 - lr: 0.05000 - Train loss: 1.21300 - Test loss: 1.16052\n",
      "Epoch 1206 - lr: 0.05000 - Train loss: 1.22576 - Test loss: 1.16759\n",
      "Epoch 1207 - lr: 0.05000 - Train loss: 1.21145 - Test loss: 1.16344\n",
      "Epoch 1208 - lr: 0.05000 - Train loss: 1.21438 - Test loss: 1.16633\n",
      "Epoch 1209 - lr: 0.05000 - Train loss: 1.21524 - Test loss: 1.16642\n",
      "Epoch 1210 - lr: 0.05000 - Train loss: 1.21374 - Test loss: 1.16198\n",
      "Epoch 1211 - lr: 0.05000 - Train loss: 1.23152 - Test loss: 1.15698\n",
      "Epoch 1212 - lr: 0.05000 - Train loss: 1.21692 - Test loss: 1.17943\n",
      "Epoch 1213 - lr: 0.05000 - Train loss: 1.21726 - Test loss: 1.16233\n",
      "Epoch 1214 - lr: 0.05000 - Train loss: 1.22985 - Test loss: 1.17824\n",
      "Epoch 1215 - lr: 0.05000 - Train loss: 1.22904 - Test loss: 1.14651\n",
      "Epoch 1216 - lr: 0.05000 - Train loss: 1.21328 - Test loss: 1.17405\n",
      "Epoch 1217 - lr: 0.05000 - Train loss: 1.21268 - Test loss: 1.16024\n",
      "Epoch 1218 - lr: 0.05000 - Train loss: 1.22970 - Test loss: 1.15058\n",
      "Epoch 1219 - lr: 0.05000 - Train loss: 1.23189 - Test loss: 1.18130\n",
      "Epoch 1220 - lr: 0.05000 - Train loss: 1.21989 - Test loss: 1.17019\n",
      "Epoch 1221 - lr: 0.05000 - Train loss: 1.21653 - Test loss: 1.18424\n",
      "Epoch 1222 - lr: 0.05000 - Train loss: 1.22101 - Test loss: 1.17233\n",
      "Epoch 1223 - lr: 0.05000 - Train loss: 1.23086 - Test loss: 1.16383\n",
      "Epoch 1224 - lr: 0.05000 - Train loss: 1.23335 - Test loss: 1.16438\n",
      "Epoch 1225 - lr: 0.05000 - Train loss: 1.21066 - Test loss: 1.16085\n",
      "Epoch 1226 - lr: 0.05000 - Train loss: 1.21643 - Test loss: 1.17975\n",
      "Epoch 1227 - lr: 0.05000 - Train loss: 1.21792 - Test loss: 1.16227\n",
      "Epoch 1228 - lr: 0.05000 - Train loss: 1.22563 - Test loss: 1.16820\n",
      "Epoch 1229 - lr: 0.05000 - Train loss: 1.21237 - Test loss: 1.16942\n",
      "Epoch 1230 - lr: 0.05000 - Train loss: 1.22323 - Test loss: 1.17146\n",
      "Epoch 1231 - lr: 0.05000 - Train loss: 1.23361 - Test loss: 1.15805\n",
      "Epoch 1232 - lr: 0.05000 - Train loss: 1.22715 - Test loss: 1.20694\n",
      "Epoch 1233 - lr: 0.05000 - Train loss: 1.21373 - Test loss: 1.16804\n",
      "Epoch 1234 - lr: 0.05000 - Train loss: 1.23092 - Test loss: 1.20012\n",
      "Epoch 1235 - lr: 0.05000 - Train loss: 1.21973 - Test loss: 1.17206\n",
      "Epoch 1236 - lr: 0.05000 - Train loss: 1.23097 - Test loss: 1.19054\n",
      "Epoch 1237 - lr: 0.05000 - Train loss: 1.22596 - Test loss: 1.15498\n",
      "Epoch 1238 - lr: 0.05000 - Train loss: 1.22947 - Test loss: 1.14412\n",
      "Epoch 1239 - lr: 0.05000 - Train loss: 1.22558 - Test loss: 1.15364\n",
      "Epoch 1240 - lr: 0.05000 - Train loss: 1.22651 - Test loss: 1.20072\n",
      "Epoch 1241 - lr: 0.05000 - Train loss: 1.21385 - Test loss: 1.17625\n",
      "Epoch 1242 - lr: 0.05000 - Train loss: 1.21716 - Test loss: 1.16853\n",
      "Epoch 1243 - lr: 0.05000 - Train loss: 1.21500 - Test loss: 1.16227\n",
      "Epoch 1244 - lr: 0.05000 - Train loss: 1.22163 - Test loss: 1.17347\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1245 - lr: 0.05000 - Train loss: 1.23264 - Test loss: 1.16497\n",
      "Epoch 1246 - lr: 0.05000 - Train loss: 1.23280 - Test loss: 1.16396\n",
      "Epoch 1247 - lr: 0.05000 - Train loss: 1.21229 - Test loss: 1.16156\n",
      "Epoch 1248 - lr: 0.05000 - Train loss: 1.21836 - Test loss: 1.17815\n",
      "Epoch 1249 - lr: 0.05000 - Train loss: 1.21663 - Test loss: 1.16972\n",
      "Epoch 1250 - lr: 0.05000 - Train loss: 1.21547 - Test loss: 1.16729\n",
      "Epoch 1251 - lr: 0.05000 - Train loss: 1.21347 - Test loss: 1.16024\n",
      "Epoch 1252 - lr: 0.05000 - Train loss: 1.22938 - Test loss: 1.14687\n",
      "Epoch 1253 - lr: 0.05000 - Train loss: 1.22879 - Test loss: 1.15172\n",
      "Epoch 1254 - lr: 0.05000 - Train loss: 1.22985 - Test loss: 1.14726\n",
      "Epoch 1255 - lr: 0.05000 - Train loss: 1.21287 - Test loss: 1.17002\n",
      "Epoch 1256 - lr: 0.05000 - Train loss: 1.23316 - Test loss: 1.15819\n",
      "Epoch 1257 - lr: 0.05000 - Train loss: 1.23010 - Test loss: 1.14635\n",
      "Epoch 1258 - lr: 0.05000 - Train loss: 1.22073 - Test loss: 1.16881\n",
      "Epoch 1259 - lr: 0.05000 - Train loss: 1.23284 - Test loss: 1.15876\n",
      "Epoch 1260 - lr: 0.05000 - Train loss: 1.22808 - Test loss: 1.19001\n",
      "Epoch 1261 - lr: 0.05000 - Train loss: 1.22219 - Test loss: 1.16567\n",
      "Epoch 1262 - lr: 0.05000 - Train loss: 1.21779 - Test loss: 1.17820\n",
      "Epoch 1263 - lr: 0.05000 - Train loss: 1.21723 - Test loss: 1.16934\n",
      "Epoch 1264 - lr: 0.05000 - Train loss: 1.21515 - Test loss: 1.16463\n",
      "Epoch 1265 - lr: 0.05000 - Train loss: 1.21528 - Test loss: 1.17444\n",
      "Epoch 1266 - lr: 0.05000 - Train loss: 1.23182 - Test loss: 1.17756\n",
      "Epoch 1267 - lr: 0.05000 - Train loss: 1.22749 - Test loss: 1.17727\n",
      "Epoch 1268 - lr: 0.05000 - Train loss: 1.22452 - Test loss: 1.19645\n",
      "Epoch 1269 - lr: 0.05000 - Train loss: 1.21416 - Test loss: 1.17788\n",
      "Epoch 1270 - lr: 0.05000 - Train loss: 1.21646 - Test loss: 1.16273\n",
      "Epoch 1271 - lr: 0.05000 - Train loss: 1.23159 - Test loss: 1.15901\n",
      "Epoch 1272 - lr: 0.05000 - Train loss: 1.21406 - Test loss: 1.17043\n",
      "Epoch 1273 - lr: 0.05000 - Train loss: 1.23256 - Test loss: 1.16859\n",
      "Epoch 1274 - lr: 0.05000 - Train loss: 1.23229 - Test loss: 1.16375\n",
      "Epoch 1275 - lr: 0.05000 - Train loss: 1.21239 - Test loss: 1.16352\n",
      "Epoch 1276 - lr: 0.05000 - Train loss: 1.21472 - Test loss: 1.17121\n",
      "Epoch 1277 - lr: 0.05000 - Train loss: 1.23250 - Test loss: 1.16431\n",
      "Epoch 1278 - lr: 0.05000 - Train loss: 1.23245 - Test loss: 1.16385\n",
      "Epoch 1279 - lr: 0.05000 - Train loss: 1.21281 - Test loss: 1.16547\n",
      "Epoch 1280 - lr: 0.05000 - Train loss: 1.21297 - Test loss: 1.15902\n",
      "Epoch 1281 - lr: 0.05000 - Train loss: 1.22690 - Test loss: 1.15987\n",
      "Epoch 1282 - lr: 0.05000 - Train loss: 1.21751 - Test loss: 1.17774\n",
      "Epoch 1283 - lr: 0.05000 - Train loss: 1.21589 - Test loss: 1.16953\n",
      "Epoch 1284 - lr: 0.05000 - Train loss: 1.21504 - Test loss: 1.16624\n",
      "Epoch 1285 - lr: 0.05000 - Train loss: 1.21384 - Test loss: 1.16654\n",
      "Epoch 1286 - lr: 0.05000 - Train loss: 1.21406 - Test loss: 1.16120\n",
      "Epoch 1287 - lr: 0.05000 - Train loss: 1.22436 - Test loss: 1.16859\n",
      "Epoch 1288 - lr: 0.05000 - Train loss: 1.21511 - Test loss: 1.18312\n",
      "Epoch 1289 - lr: 0.05000 - Train loss: 1.21869 - Test loss: 1.16805\n",
      "Epoch 1290 - lr: 0.05000 - Train loss: 1.21431 - Test loss: 1.16117\n",
      "Epoch 1291 - lr: 0.05000 - Train loss: 1.22539 - Test loss: 1.16742\n",
      "Epoch 1292 - lr: 0.05000 - Train loss: 1.21205 - Test loss: 1.16187\n",
      "Epoch 1293 - lr: 0.05000 - Train loss: 1.22100 - Test loss: 1.17343\n",
      "Epoch 1294 - lr: 0.05000 - Train loss: 1.23244 - Test loss: 1.16025\n",
      "Epoch 1295 - lr: 0.05000 - Train loss: 1.22782 - Test loss: 1.20879\n",
      "Epoch 1296 - lr: 0.05000 - Train loss: 1.21240 - Test loss: 1.16427\n",
      "Epoch 1297 - lr: 0.05000 - Train loss: 1.21404 - Test loss: 1.16721\n",
      "Epoch 1298 - lr: 0.05000 - Train loss: 1.21398 - Test loss: 1.16097\n",
      "Epoch 1299 - lr: 0.05000 - Train loss: 1.22552 - Test loss: 1.16701\n",
      "Epoch 1300 - lr: 0.05000 - Train loss: 1.21333 - Test loss: 1.16748\n",
      "Epoch 1301 - lr: 0.05000 - Train loss: 1.21347 - Test loss: 1.15994\n",
      "Epoch 1302 - lr: 0.05000 - Train loss: 1.22840 - Test loss: 1.14782\n",
      "Epoch 1303 - lr: 0.05000 - Train loss: 1.22628 - Test loss: 1.20509\n",
      "Epoch 1304 - lr: 0.05000 - Train loss: 1.21202 - Test loss: 1.16426\n",
      "Epoch 1305 - lr: 0.05000 - Train loss: 1.21350 - Test loss: 1.16741\n",
      "Epoch 1306 - lr: 0.05000 - Train loss: 1.21419 - Test loss: 1.16344\n",
      "Epoch 1307 - lr: 0.05000 - Train loss: 1.21612 - Test loss: 1.18042\n",
      "Epoch 1308 - lr: 0.05000 - Train loss: 1.21701 - Test loss: 1.16263\n",
      "Epoch 1309 - lr: 0.05000 - Train loss: 1.22523 - Test loss: 1.16800\n",
      "Epoch 1310 - lr: 0.05000 - Train loss: 1.21067 - Test loss: 1.15982\n",
      "Epoch 1311 - lr: 0.05000 - Train loss: 1.23022 - Test loss: 1.14883\n",
      "Epoch 1312 - lr: 0.05000 - Train loss: 1.21250 - Test loss: 1.17289\n",
      "Epoch 1313 - lr: 0.05000 - Train loss: 1.22790 - Test loss: 1.16352\n",
      "Epoch 1314 - lr: 0.05000 - Train loss: 1.21327 - Test loss: 1.16715\n",
      "Epoch 1315 - lr: 0.05000 - Train loss: 1.21408 - Test loss: 1.16827\n",
      "Epoch 1316 - lr: 0.05000 - Train loss: 1.21416 - Test loss: 1.16368\n",
      "Epoch 1317 - lr: 0.05000 - Train loss: 1.21619 - Test loss: 1.18054\n",
      "Epoch 1318 - lr: 0.05000 - Train loss: 1.21684 - Test loss: 1.16242\n",
      "Epoch 1319 - lr: 0.05000 - Train loss: 1.22590 - Test loss: 1.16673\n",
      "Epoch 1320 - lr: 0.05000 - Train loss: 1.21383 - Test loss: 1.16807\n",
      "Epoch 1321 - lr: 0.05000 - Train loss: 1.21379 - Test loss: 1.16207\n",
      "Epoch 1322 - lr: 0.05000 - Train loss: 1.22291 - Test loss: 1.17042\n",
      "Epoch 1323 - lr: 0.05000 - Train loss: 1.22967 - Test loss: 1.18680\n",
      "Epoch 1324 - lr: 0.05000 - Train loss: 1.22469 - Test loss: 1.20893\n",
      "Epoch 1325 - lr: 0.05000 - Train loss: 1.21188 - Test loss: 1.16381\n",
      "Epoch 1326 - lr: 0.05000 - Train loss: 1.21333 - Test loss: 1.16227\n",
      "Epoch 1327 - lr: 0.05000 - Train loss: 1.21677 - Test loss: 1.18031\n",
      "Epoch 1328 - lr: 0.05000 - Train loss: 1.21658 - Test loss: 1.16180\n",
      "Epoch 1329 - lr: 0.05000 - Train loss: 1.22679 - Test loss: 1.15880\n",
      "Epoch 1330 - lr: 0.05000 - Train loss: 1.22421 - Test loss: 1.16744\n",
      "Epoch 1331 - lr: 0.05000 - Train loss: 1.21052 - Test loss: 1.15957\n",
      "Epoch 1332 - lr: 0.05000 - Train loss: 1.22891 - Test loss: 1.14703\n",
      "Epoch 1333 - lr: 0.05000 - Train loss: 1.22322 - Test loss: 1.16497\n",
      "Epoch 1334 - lr: 0.05000 - Train loss: 1.21247 - Test loss: 1.16653\n",
      "Epoch 1335 - lr: 0.05000 - Train loss: 1.21229 - Test loss: 1.16238\n",
      "Epoch 1336 - lr: 0.05000 - Train loss: 1.22998 - Test loss: 1.15637\n",
      "Epoch 1337 - lr: 0.05000 - Train loss: 1.22235 - Test loss: 1.16885\n",
      "Epoch 1338 - lr: 0.05000 - Train loss: 1.22769 - Test loss: 1.15824\n",
      "Epoch 1339 - lr: 0.05000 - Train loss: 1.22557 - Test loss: 1.16326\n",
      "Epoch 1340 - lr: 0.05000 - Train loss: 1.21193 - Test loss: 1.16154\n",
      "Epoch 1341 - lr: 0.05000 - Train loss: 1.21582 - Test loss: 1.18100\n",
      "Epoch 1342 - lr: 0.05000 - Train loss: 1.21596 - Test loss: 1.16417\n",
      "Epoch 1343 - lr: 0.05000 - Train loss: 1.23030 - Test loss: 1.15689\n",
      "Epoch 1344 - lr: 0.05000 - Train loss: 1.22127 - Test loss: 1.17059\n",
      "Epoch 1345 - lr: 0.05000 - Train loss: 1.23129 - Test loss: 1.19619\n",
      "Epoch 1346 - lr: 0.05000 - Train loss: 1.22293 - Test loss: 1.16634\n",
      "Epoch 1347 - lr: 0.05000 - Train loss: 1.20979 - Test loss: 1.16076\n",
      "Epoch 1348 - lr: 0.05000 - Train loss: 1.22840 - Test loss: 1.17332\n",
      "Epoch 1349 - lr: 0.05000 - Train loss: 1.22607 - Test loss: 1.20456\n",
      "Epoch 1350 - lr: 0.05000 - Train loss: 1.21202 - Test loss: 1.16822\n",
      "Epoch 1351 - lr: 0.05000 - Train loss: 1.22965 - Test loss: 1.20116\n",
      "Epoch 1352 - lr: 0.05000 - Train loss: 1.21296 - Test loss: 1.17954\n",
      "Epoch 1353 - lr: 0.05000 - Train loss: 1.21625 - Test loss: 1.16197\n",
      "Epoch 1354 - lr: 0.05000 - Train loss: 1.22168 - Test loss: 1.17243\n",
      "Epoch 1355 - lr: 0.05000 - Train loss: 1.23238 - Test loss: 1.16363\n",
      "Epoch 1356 - lr: 0.05000 - Train loss: 1.23154 - Test loss: 1.16442\n",
      "Epoch 1357 - lr: 0.05000 - Train loss: 1.21202 - Test loss: 1.16600\n",
      "Epoch 1358 - lr: 0.05000 - Train loss: 1.21209 - Test loss: 1.15929\n",
      "Epoch 1359 - lr: 0.05000 - Train loss: 1.22598 - Test loss: 1.17545\n",
      "Epoch 1360 - lr: 0.05000 - Train loss: 1.22596 - Test loss: 1.21123\n",
      "Epoch 1361 - lr: 0.05000 - Train loss: 1.21091 - Test loss: 1.15520\n",
      "Epoch 1362 - lr: 0.05000 - Train loss: 1.22720 - Test loss: 1.14651\n",
      "Epoch 1363 - lr: 0.05000 - Train loss: 1.22602 - Test loss: 1.20157\n",
      "Epoch 1364 - lr: 0.05000 - Train loss: 1.21195 - Test loss: 1.17498\n",
      "Epoch 1365 - lr: 0.05000 - Train loss: 1.21204 - Test loss: 1.16075\n",
      "Epoch 1366 - lr: 0.05000 - Train loss: 1.22649 - Test loss: 1.15799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1367 - lr: 0.05000 - Train loss: 1.22539 - Test loss: 1.16303\n",
      "Epoch 1368 - lr: 0.05000 - Train loss: 1.21198 - Test loss: 1.16495\n",
      "Epoch 1369 - lr: 0.05000 - Train loss: 1.21167 - Test loss: 1.15921\n",
      "Epoch 1370 - lr: 0.05000 - Train loss: 1.22917 - Test loss: 1.14889\n",
      "Epoch 1371 - lr: 0.05000 - Train loss: 1.21236 - Test loss: 1.17833\n",
      "Epoch 1372 - lr: 0.05000 - Train loss: 1.21543 - Test loss: 1.16273\n",
      "Epoch 1373 - lr: 0.05000 - Train loss: 1.22784 - Test loss: 1.20187\n",
      "Epoch 1374 - lr: 0.05000 - Train loss: 1.21308 - Test loss: 1.18029\n",
      "Epoch 1375 - lr: 0.05000 - Train loss: 1.21680 - Test loss: 1.16734\n",
      "Epoch 1376 - lr: 0.05000 - Train loss: 1.21250 - Test loss: 1.16251\n",
      "Epoch 1377 - lr: 0.05000 - Train loss: 1.22943 - Test loss: 1.15485\n",
      "Epoch 1378 - lr: 0.05000 - Train loss: 1.22494 - Test loss: 1.20287\n",
      "Epoch 1379 - lr: 0.05000 - Train loss: 1.21302 - Test loss: 1.17996\n",
      "Epoch 1380 - lr: 0.05000 - Train loss: 1.21581 - Test loss: 1.16137\n",
      "Epoch 1381 - lr: 0.05000 - Train loss: 1.22476 - Test loss: 1.16761\n",
      "Epoch 1382 - lr: 0.05000 - Train loss: 1.21214 - Test loss: 1.16657\n",
      "Epoch 1383 - lr: 0.05000 - Train loss: 1.21219 - Test loss: 1.16234\n",
      "Epoch 1384 - lr: 0.05000 - Train loss: 1.21669 - Test loss: 1.18023\n",
      "Epoch 1385 - lr: 0.05000 - Train loss: 1.21611 - Test loss: 1.16452\n",
      "Epoch 1386 - lr: 0.05000 - Train loss: 1.21870 - Test loss: 1.17793\n",
      "Epoch 1387 - lr: 0.05000 - Train loss: 1.21107 - Test loss: 1.16246\n",
      "Epoch 1388 - lr: 0.05000 - Train loss: 1.22754 - Test loss: 1.18470\n",
      "Epoch 1389 - lr: 0.05000 - Train loss: 1.22337 - Test loss: 1.15686\n",
      "Epoch 1390 - lr: 0.05000 - Train loss: 1.21989 - Test loss: 1.17219\n",
      "Epoch 1391 - lr: 0.05000 - Train loss: 1.23151 - Test loss: 1.17962\n",
      "Epoch 1392 - lr: 0.05000 - Train loss: 1.22711 - Test loss: 1.14478\n",
      "Epoch 1393 - lr: 0.05000 - Train loss: 1.22436 - Test loss: 1.17519\n",
      "Epoch 1394 - lr: 0.05000 - Train loss: 1.22454 - Test loss: 1.14514\n",
      "Epoch 1395 - lr: 0.05000 - Train loss: 1.22860 - Test loss: 1.15945\n",
      "Epoch 1396 - lr: 0.05000 - Train loss: 1.21045 - Test loss: 1.15916\n",
      "Epoch 1397 - lr: 0.05000 - Train loss: 1.22758 - Test loss: 1.17818\n",
      "Epoch 1398 - lr: 0.05000 - Train loss: 1.22761 - Test loss: 1.14832\n",
      "Epoch 1399 - lr: 0.05000 - Train loss: 1.21055 - Test loss: 1.16575\n",
      "Epoch 1400 - lr: 0.05000 - Train loss: 1.21091 - Test loss: 1.16199\n",
      "Epoch 1401 - lr: 0.05000 - Train loss: 1.22015 - Test loss: 1.17433\n",
      "Epoch 1402 - lr: 0.05000 - Train loss: 1.23029 - Test loss: 1.21133\n",
      "Epoch 1403 - lr: 0.05000 - Train loss: 1.21201 - Test loss: 1.16722\n",
      "Epoch 1404 - lr: 0.05000 - Train loss: 1.20914 - Test loss: 1.16233\n",
      "Epoch 1405 - lr: 0.05000 - Train loss: 1.23066 - Test loss: 1.16275\n",
      "Epoch 1406 - lr: 0.05000 - Train loss: 1.21163 - Test loss: 1.15952\n",
      "Epoch 1407 - lr: 0.05000 - Train loss: 1.22907 - Test loss: 1.14948\n",
      "Epoch 1408 - lr: 0.05000 - Train loss: 1.21131 - Test loss: 1.17296\n",
      "Epoch 1409 - lr: 0.05000 - Train loss: 1.22821 - Test loss: 1.19947\n",
      "Epoch 1410 - lr: 0.05000 - Train loss: 1.21987 - Test loss: 1.17055\n",
      "Epoch 1411 - lr: 0.05000 - Train loss: 1.23099 - Test loss: 1.20489\n",
      "Epoch 1412 - lr: 0.05000 - Train loss: 1.21334 - Test loss: 1.18099\n",
      "Epoch 1413 - lr: 0.05000 - Train loss: 1.21639 - Test loss: 1.16735\n",
      "Epoch 1414 - lr: 0.05000 - Train loss: 1.21169 - Test loss: 1.16290\n",
      "Epoch 1415 - lr: 0.05000 - Train loss: 1.22986 - Test loss: 1.15992\n",
      "Epoch 1416 - lr: 0.05000 - Train loss: 1.21247 - Test loss: 1.17324\n",
      "Epoch 1417 - lr: 0.05000 - Train loss: 1.23142 - Test loss: 1.18376\n",
      "Epoch 1418 - lr: 0.05000 - Train loss: 1.22737 - Test loss: 1.14804\n",
      "Epoch 1419 - lr: 0.05000 - Train loss: 1.21369 - Test loss: 1.17902\n",
      "Epoch 1420 - lr: 0.05000 - Train loss: 1.21483 - Test loss: 1.16405\n",
      "Epoch 1421 - lr: 0.05000 - Train loss: 1.22985 - Test loss: 1.15926\n",
      "Epoch 1422 - lr: 0.05000 - Train loss: 1.21339 - Test loss: 1.18051\n",
      "Epoch 1423 - lr: 0.05000 - Train loss: 1.21528 - Test loss: 1.16290\n",
      "Epoch 1424 - lr: 0.05000 - Train loss: 1.22791 - Test loss: 1.15043\n",
      "Epoch 1425 - lr: 0.05000 - Train loss: 1.23052 - Test loss: 1.16673\n",
      "Epoch 1426 - lr: 0.05000 - Train loss: 1.22990 - Test loss: 1.21166\n",
      "Epoch 1427 - lr: 0.05000 - Train loss: 1.21138 - Test loss: 1.16589\n",
      "Epoch 1428 - lr: 0.05000 - Train loss: 1.21274 - Test loss: 1.16862\n",
      "Epoch 1429 - lr: 0.05000 - Train loss: 1.21298 - Test loss: 1.16362\n",
      "Epoch 1430 - lr: 0.05000 - Train loss: 1.21895 - Test loss: 1.17628\n",
      "Epoch 1431 - lr: 0.05000 - Train loss: 1.22684 - Test loss: 1.16611\n",
      "Epoch 1432 - lr: 0.05000 - Train loss: 1.21127 - Test loss: 1.15977\n",
      "Epoch 1433 - lr: 0.05000 - Train loss: 1.22521 - Test loss: 1.21143\n",
      "Epoch 1434 - lr: 0.05000 - Train loss: 1.21120 - Test loss: 1.16598\n",
      "Epoch 1435 - lr: 0.05000 - Train loss: 1.21254 - Test loss: 1.16866\n",
      "Epoch 1436 - lr: 0.05000 - Train loss: 1.21284 - Test loss: 1.16355\n",
      "Epoch 1437 - lr: 0.05000 - Train loss: 1.21948 - Test loss: 1.17530\n",
      "Epoch 1438 - lr: 0.05000 - Train loss: 1.22955 - Test loss: 1.20540\n",
      "Epoch 1439 - lr: 0.05000 - Train loss: 1.21437 - Test loss: 1.18009\n",
      "Epoch 1440 - lr: 0.05000 - Train loss: 1.21478 - Test loss: 1.16513\n",
      "Epoch 1441 - lr: 0.05000 - Train loss: 1.23011 - Test loss: 1.16187\n",
      "Epoch 1442 - lr: 0.05000 - Train loss: 1.21104 - Test loss: 1.16534\n",
      "Epoch 1443 - lr: 0.05000 - Train loss: 1.21097 - Test loss: 1.15963\n",
      "Epoch 1444 - lr: 0.05000 - Train loss: 1.22652 - Test loss: 1.15369\n",
      "Epoch 1445 - lr: 0.05000 - Train loss: 1.23012 - Test loss: 1.17445\n",
      "Epoch 1446 - lr: 0.05000 - Train loss: 1.21427 - Test loss: 1.16989\n",
      "Epoch 1447 - lr: 0.05000 - Train loss: 1.21322 - Test loss: 1.16690\n",
      "Epoch 1448 - lr: 0.05000 - Train loss: 1.21281 - Test loss: 1.17045\n",
      "Epoch 1449 - lr: 0.05000 - Train loss: 1.20964 - Test loss: 1.16842\n",
      "Epoch 1450 - lr: 0.05000 - Train loss: 1.21310 - Test loss: 1.16810\n",
      "Epoch 1451 - lr: 0.05000 - Train loss: 1.21140 - Test loss: 1.16053\n",
      "Epoch 1452 - lr: 0.05000 - Train loss: 1.22663 - Test loss: 1.15249\n",
      "Epoch 1453 - lr: 0.05000 - Train loss: 1.23023 - Test loss: 1.16701\n",
      "Epoch 1454 - lr: 0.05000 - Train loss: 1.22919 - Test loss: 1.21138\n",
      "Epoch 1455 - lr: 0.05000 - Train loss: 1.21115 - Test loss: 1.16688\n",
      "Epoch 1456 - lr: 0.05000 - Train loss: 1.21137 - Test loss: 1.16750\n",
      "Epoch 1457 - lr: 0.05000 - Train loss: 1.21125 - Test loss: 1.16040\n",
      "Epoch 1458 - lr: 0.05000 - Train loss: 1.22718 - Test loss: 1.14758\n",
      "Epoch 1459 - lr: 0.05000 - Train loss: 1.22613 - Test loss: 1.14644\n",
      "Epoch 1460 - lr: 0.05000 - Train loss: 1.22093 - Test loss: 1.16681\n",
      "Epoch 1461 - lr: 0.05000 - Train loss: 1.20886 - Test loss: 1.16801\n",
      "Epoch 1462 - lr: 0.05000 - Train loss: 1.21274 - Test loss: 1.16940\n",
      "Epoch 1463 - lr: 0.05000 - Train loss: 1.21256 - Test loss: 1.16313\n",
      "Epoch 1464 - lr: 0.05000 - Train loss: 1.22289 - Test loss: 1.17008\n",
      "Epoch 1465 - lr: 0.05000 - Train loss: 1.21047 - Test loss: 1.18124\n",
      "Epoch 1466 - lr: 0.05000 - Train loss: 1.21558 - Test loss: 1.16691\n",
      "Epoch 1467 - lr: 0.05000 - Train loss: 1.21389 - Test loss: 1.18067\n",
      "Epoch 1468 - lr: 0.05000 - Train loss: 1.21498 - Test loss: 1.17166\n",
      "Epoch 1469 - lr: 0.05000 - Train loss: 1.21331 - Test loss: 1.16838\n",
      "Epoch 1470 - lr: 0.05000 - Train loss: 1.21173 - Test loss: 1.16638\n",
      "Epoch 1471 - lr: 0.05000 - Train loss: 1.21158 - Test loss: 1.16767\n",
      "Epoch 1472 - lr: 0.05000 - Train loss: 1.21157 - Test loss: 1.16141\n",
      "Epoch 1473 - lr: 0.05000 - Train loss: 1.22806 - Test loss: 1.14974\n",
      "Epoch 1474 - lr: 0.05000 - Train loss: 1.21529 - Test loss: 1.17639\n",
      "Epoch 1475 - lr: 0.05000 - Train loss: 1.21135 - Test loss: 1.16439\n",
      "Epoch 1476 - lr: 0.05000 - Train loss: 1.21965 - Test loss: 1.17534\n",
      "Epoch 1477 - lr: 0.05000 - Train loss: 1.23029 - Test loss: 1.19853\n",
      "Epoch 1478 - lr: 0.05000 - Train loss: 1.22215 - Test loss: 1.16734\n",
      "Epoch 1479 - lr: 0.05000 - Train loss: 1.21082 - Test loss: 1.16733\n",
      "Epoch 1480 - lr: 0.05000 - Train loss: 1.21123 - Test loss: 1.16469\n",
      "Epoch 1481 - lr: 0.05000 - Train loss: 1.21273 - Test loss: 1.17762\n",
      "Epoch 1482 - lr: 0.05000 - Train loss: 1.21788 - Test loss: 1.18010\n",
      "Epoch 1483 - lr: 0.05000 - Train loss: 1.21197 - Test loss: 1.16539\n",
      "Epoch 1484 - lr: 0.05000 - Train loss: 1.21957 - Test loss: 1.17607\n",
      "Epoch 1485 - lr: 0.05000 - Train loss: 1.22952 - Test loss: 1.21011\n",
      "Epoch 1486 - lr: 0.05000 - Train loss: 1.21172 - Test loss: 1.17653\n",
      "Epoch 1487 - lr: 0.05000 - Train loss: 1.21051 - Test loss: 1.17474\n",
      "Epoch 1488 - lr: 0.05000 - Train loss: 1.23177 - Test loss: 1.16852\n",
      "Epoch 1489 - lr: 0.05000 - Train loss: 1.23102 - Test loss: 1.16784\n",
      "Epoch 1490 - lr: 0.05000 - Train loss: 1.22539 - Test loss: 1.16341\n",
      "Epoch 1491 - lr: 0.05000 - Train loss: 1.21222 - Test loss: 1.17437\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1492 - lr: 0.05000 - Train loss: 1.23153 - Test loss: 1.16112\n",
      "Epoch 1493 - lr: 0.05000 - Train loss: 1.22464 - Test loss: 1.16303\n",
      "Epoch 1494 - lr: 0.05000 - Train loss: 1.21275 - Test loss: 1.17976\n",
      "Epoch 1495 - lr: 0.05000 - Train loss: 1.21371 - Test loss: 1.17172\n",
      "Epoch 1496 - lr: 0.05000 - Train loss: 1.21296 - Test loss: 1.16835\n",
      "Epoch 1497 - lr: 0.05000 - Train loss: 1.21189 - Test loss: 1.16950\n",
      "Epoch 1498 - lr: 0.05000 - Train loss: 1.21249 - Test loss: 1.16990\n",
      "Epoch 1499 - lr: 0.05000 - Train loss: 1.21167 - Test loss: 1.16217\n",
      "Epoch 1500 - lr: 0.05000 - Train loss: 1.22580 - Test loss: 1.16428\n",
      "Epoch 1501 - lr: 0.05000 - Train loss: 1.22925 - Test loss: 1.16644\n",
      "Epoch 1502 - lr: 0.05000 - Train loss: 1.22489 - Test loss: 1.16344\n",
      "Epoch 1503 - lr: 0.05000 - Train loss: 1.21186 - Test loss: 1.17242\n",
      "Epoch 1504 - lr: 0.05000 - Train loss: 1.22912 - Test loss: 1.20991\n",
      "Epoch 1505 - lr: 0.05000 - Train loss: 1.21160 - Test loss: 1.17696\n",
      "Epoch 1506 - lr: 0.05000 - Train loss: 1.20917 - Test loss: 1.16572\n",
      "Epoch 1507 - lr: 0.05000 - Train loss: 1.21285 - Test loss: 1.17676\n",
      "Epoch 1508 - lr: 0.05000 - Train loss: 1.22905 - Test loss: 1.20991\n",
      "Epoch 1509 - lr: 0.05000 - Train loss: 1.21207 - Test loss: 1.17975\n",
      "Epoch 1510 - lr: 0.05000 - Train loss: 1.21490 - Test loss: 1.16932\n",
      "Epoch 1511 - lr: 0.05000 - Train loss: 1.21149 - Test loss: 1.16678\n",
      "Epoch 1512 - lr: 0.05000 - Train loss: 1.21159 - Test loss: 1.16968\n",
      "Epoch 1513 - lr: 0.05000 - Train loss: 1.21214 - Test loss: 1.17088\n",
      "Epoch 1514 - lr: 0.05000 - Train loss: 1.21256 - Test loss: 1.16864\n",
      "Epoch 1515 - lr: 0.05000 - Train loss: 1.21143 - Test loss: 1.16871\n",
      "Epoch 1516 - lr: 0.05000 - Train loss: 1.21122 - Test loss: 1.16208\n",
      "Epoch 1517 - lr: 0.05000 - Train loss: 1.22653 - Test loss: 1.14905\n",
      "Epoch 1518 - lr: 0.05000 - Train loss: 1.22675 - Test loss: 1.15046\n",
      "Epoch 1519 - lr: 0.05000 - Train loss: 1.20899 - Test loss: 1.16648\n",
      "Epoch 1520 - lr: 0.05000 - Train loss: 1.21167 - Test loss: 1.16988\n",
      "Epoch 1521 - lr: 0.05000 - Train loss: 1.21199 - Test loss: 1.16496\n",
      "Epoch 1522 - lr: 0.05000 - Train loss: 1.21821 - Test loss: 1.17709\n",
      "Epoch 1523 - lr: 0.05000 - Train loss: 1.22757 - Test loss: 1.17469\n",
      "Epoch 1524 - lr: 0.05000 - Train loss: 1.22850 - Test loss: 1.16347\n",
      "Epoch 1525 - lr: 0.05000 - Train loss: 1.20942 - Test loss: 1.16288\n",
      "Epoch 1526 - lr: 0.05000 - Train loss: 1.22818 - Test loss: 1.15957\n",
      "Epoch 1527 - lr: 0.05000 - Train loss: 1.21545 - Test loss: 1.17897\n",
      "Epoch 1528 - lr: 0.05000 - Train loss: 1.21235 - Test loss: 1.17037\n",
      "Epoch 1529 - lr: 0.05000 - Train loss: 1.21086 - Test loss: 1.16393\n",
      "Epoch 1530 - lr: 0.05000 - Train loss: 1.22607 - Test loss: 1.20902\n",
      "Epoch 1531 - lr: 0.05000 - Train loss: 1.21062 - Test loss: 1.17232\n",
      "Epoch 1532 - lr: 0.05000 - Train loss: 1.23089 - Test loss: 1.16464\n",
      "Epoch 1533 - lr: 0.05000 - Train loss: 1.22977 - Test loss: 1.16617\n",
      "Epoch 1534 - lr: 0.05000 - Train loss: 1.21078 - Test loss: 1.16977\n",
      "Epoch 1535 - lr: 0.05000 - Train loss: 1.21178 - Test loss: 1.16479\n",
      "Epoch 1536 - lr: 0.05000 - Train loss: 1.21947 - Test loss: 1.17483\n",
      "Epoch 1537 - lr: 0.05000 - Train loss: 1.23101 - Test loss: 1.16284\n",
      "Epoch 1538 - lr: 0.05000 - Train loss: 1.22185 - Test loss: 1.17078\n",
      "Epoch 1539 - lr: 0.05000 - Train loss: 1.20949 - Test loss: 1.17662\n",
      "Epoch 1540 - lr: 0.05000 - Train loss: 1.22981 - Test loss: 1.19795\n",
      "Epoch 1541 - lr: 0.05000 - Train loss: 1.22271 - Test loss: 1.16524\n",
      "Epoch 1542 - lr: 0.05000 - Train loss: 1.20955 - Test loss: 1.16200\n",
      "Epoch 1543 - lr: 0.05000 - Train loss: 1.22538 - Test loss: 1.20518\n",
      "Epoch 1544 - lr: 0.05000 - Train loss: 1.21023 - Test loss: 1.17429\n",
      "Epoch 1545 - lr: 0.05000 - Train loss: 1.22791 - Test loss: 1.21504\n",
      "Epoch 1546 - lr: 0.05000 - Train loss: 1.21026 - Test loss: 1.16689\n",
      "Epoch 1547 - lr: 0.05000 - Train loss: 1.21154 - Test loss: 1.16636\n",
      "Epoch 1548 - lr: 0.05000 - Train loss: 1.21238 - Test loss: 1.17784\n",
      "Epoch 1549 - lr: 0.05000 - Train loss: 1.22721 - Test loss: 1.16318\n",
      "Epoch 1550 - lr: 0.05000 - Train loss: 1.22637 - Test loss: 1.17948\n",
      "Epoch 1551 - lr: 0.05000 - Train loss: 1.22534 - Test loss: 1.14655\n",
      "Epoch 1552 - lr: 0.05000 - Train loss: 1.22177 - Test loss: 1.16422\n",
      "Epoch 1553 - lr: 0.05000 - Train loss: 1.20939 - Test loss: 1.16508\n",
      "Epoch 1554 - lr: 0.05000 - Train loss: 1.22948 - Test loss: 1.16615\n",
      "Epoch 1555 - lr: 0.05000 - Train loss: 1.21097 - Test loss: 1.17023\n",
      "Epoch 1556 - lr: 0.05000 - Train loss: 1.21185 - Test loss: 1.16741\n",
      "Epoch 1557 - lr: 0.05000 - Train loss: 1.21232 - Test loss: 1.17695\n",
      "Epoch 1558 - lr: 0.05000 - Train loss: 1.22978 - Test loss: 1.19024\n",
      "Epoch 1559 - lr: 0.05000 - Train loss: 1.22316 - Test loss: 1.19481\n",
      "Epoch 1560 - lr: 0.05000 - Train loss: 1.21883 - Test loss: 1.16914\n",
      "Epoch 1561 - lr: 0.05000 - Train loss: 1.22443 - Test loss: 1.16580\n",
      "Epoch 1562 - lr: 0.05000 - Train loss: 1.21014 - Test loss: 1.16664\n",
      "Epoch 1563 - lr: 0.05000 - Train loss: 1.21075 - Test loss: 1.16947\n",
      "Epoch 1564 - lr: 0.05000 - Train loss: 1.21178 - Test loss: 1.16903\n",
      "Epoch 1565 - lr: 0.05000 - Train loss: 1.21062 - Test loss: 1.16655\n",
      "Epoch 1566 - lr: 0.05000 - Train loss: 1.21173 - Test loss: 1.17403\n",
      "Epoch 1567 - lr: 0.05000 - Train loss: 1.22926 - Test loss: 1.20830\n",
      "Epoch 1568 - lr: 0.05000 - Train loss: 1.21250 - Test loss: 1.18195\n",
      "Epoch 1569 - lr: 0.05000 - Train loss: 1.21353 - Test loss: 1.16514\n",
      "Epoch 1570 - lr: 0.05000 - Train loss: 1.22609 - Test loss: 1.20382\n",
      "Epoch 1571 - lr: 0.05000 - Train loss: 1.21362 - Test loss: 1.17955\n",
      "Epoch 1572 - lr: 0.05000 - Train loss: 1.21397 - Test loss: 1.17103\n",
      "Epoch 1573 - lr: 0.05000 - Train loss: 1.21053 - Test loss: 1.16576\n",
      "Epoch 1574 - lr: 0.05000 - Train loss: 1.22767 - Test loss: 1.15803\n",
      "Epoch 1575 - lr: 0.05000 - Train loss: 1.22326 - Test loss: 1.16134\n",
      "Epoch 1576 - lr: 0.05000 - Train loss: 1.21879 - Test loss: 1.17371\n",
      "Epoch 1577 - lr: 0.05000 - Train loss: 1.23002 - Test loss: 1.16545\n",
      "Epoch 1578 - lr: 0.05000 - Train loss: 1.22840 - Test loss: 1.16476\n",
      "Epoch 1579 - lr: 0.05000 - Train loss: 1.20908 - Test loss: 1.16515\n",
      "Epoch 1580 - lr: 0.05000 - Train loss: 1.22876 - Test loss: 1.16551\n",
      "Epoch 1581 - lr: 0.05000 - Train loss: 1.21037 - Test loss: 1.16360\n",
      "Epoch 1582 - lr: 0.05000 - Train loss: 1.22117 - Test loss: 1.17181\n",
      "Epoch 1583 - lr: 0.05000 - Train loss: 1.21884 - Test loss: 1.17505\n",
      "Epoch 1584 - lr: 0.05000 - Train loss: 1.23076 - Test loss: 1.16298\n",
      "Epoch 1585 - lr: 0.05000 - Train loss: 1.22270 - Test loss: 1.16965\n",
      "Epoch 1586 - lr: 0.05000 - Train loss: 1.21097 - Test loss: 1.17144\n",
      "Epoch 1587 - lr: 0.05000 - Train loss: 1.21167 - Test loss: 1.16855\n",
      "Epoch 1588 - lr: 0.05000 - Train loss: 1.21174 - Test loss: 1.17402\n",
      "Epoch 1589 - lr: 0.05000 - Train loss: 1.22750 - Test loss: 1.20503\n",
      "Epoch 1590 - lr: 0.05000 - Train loss: 1.21663 - Test loss: 1.17513\n",
      "Epoch 1591 - lr: 0.05000 - Train loss: 1.22885 - Test loss: 1.21237\n",
      "Epoch 1592 - lr: 0.05000 - Train loss: 1.21070 - Test loss: 1.17613\n",
      "Epoch 1593 - lr: 0.05000 - Train loss: 1.22728 - Test loss: 1.21510\n",
      "Epoch 1594 - lr: 0.05000 - Train loss: 1.21043 - Test loss: 1.17057\n",
      "Epoch 1595 - lr: 0.05000 - Train loss: 1.21385 - Test loss: 1.18395\n",
      "Epoch 1596 - lr: 0.05000 - Train loss: 1.21399 - Test loss: 1.16392\n",
      "Epoch 1597 - lr: 0.05000 - Train loss: 1.22543 - Test loss: 1.16359\n",
      "Epoch 1598 - lr: 0.05000 - Train loss: 1.22884 - Test loss: 1.17288\n",
      "Epoch 1599 - lr: 0.05000 - Train loss: 1.22577 - Test loss: 1.17093\n",
      "Epoch 1600 - lr: 0.05000 - Train loss: 1.22975 - Test loss: 1.16891\n",
      "Epoch 1601 - lr: 0.05000 - Train loss: 1.22492 - Test loss: 1.16126\n",
      "Epoch 1602 - lr: 0.05000 - Train loss: 1.22222 - Test loss: 1.16967\n",
      "Epoch 1603 - lr: 0.05000 - Train loss: 1.21070 - Test loss: 1.17181\n",
      "Epoch 1604 - lr: 0.05000 - Train loss: 1.21172 - Test loss: 1.17019\n",
      "Epoch 1605 - lr: 0.05000 - Train loss: 1.21037 - Test loss: 1.16788\n",
      "Epoch 1606 - lr: 0.05000 - Train loss: 1.21104 - Test loss: 1.17220\n",
      "Epoch 1607 - lr: 0.05000 - Train loss: 1.20749 - Test loss: 1.16567\n",
      "Epoch 1608 - lr: 0.05000 - Train loss: 1.21429 - Test loss: 1.18256\n",
      "Epoch 1609 - lr: 0.05000 - Train loss: 1.21421 - Test loss: 1.17270\n",
      "Epoch 1610 - lr: 0.05000 - Train loss: 1.21075 - Test loss: 1.16470\n",
      "Epoch 1611 - lr: 0.05000 - Train loss: 1.22757 - Test loss: 1.15236\n",
      "Epoch 1612 - lr: 0.05000 - Train loss: 1.21020 - Test loss: 1.17913\n",
      "Epoch 1613 - lr: 0.05000 - Train loss: 1.21259 - Test loss: 1.17346\n",
      "Epoch 1614 - lr: 0.05000 - Train loss: 1.21205 - Test loss: 1.17212\n",
      "Epoch 1615 - lr: 0.05000 - Train loss: 1.21065 - Test loss: 1.16416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1616 - lr: 0.05000 - Train loss: 1.22643 - Test loss: 1.15078\n",
      "Epoch 1617 - lr: 0.05000 - Train loss: 1.22111 - Test loss: 1.16708\n",
      "Epoch 1618 - lr: 0.05000 - Train loss: 1.21029 - Test loss: 1.16665\n",
      "Epoch 1619 - lr: 0.05000 - Train loss: 1.21301 - Test loss: 1.18296\n",
      "Epoch 1620 - lr: 0.05000 - Train loss: 1.21402 - Test loss: 1.17268\n",
      "Epoch 1621 - lr: 0.05000 - Train loss: 1.21025 - Test loss: 1.16597\n",
      "Epoch 1622 - lr: 0.05000 - Train loss: 1.22504 - Test loss: 1.21645\n",
      "Epoch 1623 - lr: 0.05000 - Train loss: 1.20882 - Test loss: 1.16491\n",
      "Epoch 1624 - lr: 0.05000 - Train loss: 1.21030 - Test loss: 1.17204\n",
      "Epoch 1625 - lr: 0.05000 - Train loss: 1.20832 - Test loss: 1.17294\n",
      "Epoch 1626 - lr: 0.05000 - Train loss: 1.20914 - Test loss: 1.17930\n",
      "Epoch 1627 - lr: 0.05000 - Train loss: 1.22718 - Test loss: 1.18104\n",
      "Epoch 1628 - lr: 0.05000 - Train loss: 1.22452 - Test loss: 1.20335\n",
      "Epoch 1629 - lr: 0.05000 - Train loss: 1.21504 - Test loss: 1.17612\n",
      "Epoch 1630 - lr: 0.05000 - Train loss: 1.22633 - Test loss: 1.19920\n",
      "Epoch 1631 - lr: 0.05000 - Train loss: 1.22189 - Test loss: 1.16574\n",
      "Epoch 1632 - lr: 0.05000 - Train loss: 1.20918 - Test loss: 1.16805\n",
      "Epoch 1633 - lr: 0.05000 - Train loss: 1.20951 - Test loss: 1.16698\n",
      "Epoch 1634 - lr: 0.05000 - Train loss: 1.21134 - Test loss: 1.17758\n",
      "Epoch 1635 - lr: 0.05000 - Train loss: 1.23005 - Test loss: 1.16373\n",
      "Epoch 1636 - lr: 0.05000 - Train loss: 1.22661 - Test loss: 1.15097\n",
      "Epoch 1637 - lr: 0.05000 - Train loss: 1.21720 - Test loss: 1.17210\n",
      "Epoch 1638 - lr: 0.05000 - Train loss: 1.22860 - Test loss: 1.19073\n",
      "Epoch 1639 - lr: 0.05000 - Train loss: 1.22247 - Test loss: 1.18279\n",
      "Epoch 1640 - lr: 0.05000 - Train loss: 1.22136 - Test loss: 1.17685\n",
      "Epoch 1641 - lr: 0.05000 - Train loss: 1.22497 - Test loss: 1.15175\n",
      "Epoch 1642 - lr: 0.05000 - Train loss: 1.20662 - Test loss: 1.16141\n",
      "Epoch 1643 - lr: 0.05000 - Train loss: 1.22735 - Test loss: 1.16346\n",
      "Epoch 1644 - lr: 0.05000 - Train loss: 1.20953 - Test loss: 1.17101\n",
      "Epoch 1645 - lr: 0.05000 - Train loss: 1.20938 - Test loss: 1.16955\n",
      "Epoch 1646 - lr: 0.05000 - Train loss: 1.21063 - Test loss: 1.17213\n",
      "Epoch 1647 - lr: 0.05000 - Train loss: 1.21044 - Test loss: 1.17257\n",
      "Epoch 1648 - lr: 0.05000 - Train loss: 1.21100 - Test loss: 1.16720\n",
      "Epoch 1649 - lr: 0.05000 - Train loss: 1.21732 - Test loss: 1.17833\n",
      "Epoch 1650 - lr: 0.05000 - Train loss: 1.22842 - Test loss: 1.19743\n",
      "Epoch 1651 - lr: 0.05000 - Train loss: 1.22201 - Test loss: 1.15966\n",
      "Epoch 1652 - lr: 0.05000 - Train loss: 1.22359 - Test loss: 1.16238\n",
      "Epoch 1653 - lr: 0.05000 - Train loss: 1.22776 - Test loss: 1.17835\n",
      "Epoch 1654 - lr: 0.05000 - Train loss: 1.21254 - Test loss: 1.16537\n",
      "Epoch 1655 - lr: 0.05000 - Train loss: 1.22274 - Test loss: 1.17049\n",
      "Epoch 1656 - lr: 0.05000 - Train loss: 1.21071 - Test loss: 1.17187\n",
      "Epoch 1657 - lr: 0.05000 - Train loss: 1.20989 - Test loss: 1.16506\n",
      "Epoch 1658 - lr: 0.05000 - Train loss: 1.22720 - Test loss: 1.15390\n",
      "Epoch 1659 - lr: 0.05000 - Train loss: 1.20867 - Test loss: 1.17219\n",
      "Epoch 1660 - lr: 0.05000 - Train loss: 1.22688 - Test loss: 1.21637\n",
      "Epoch 1661 - lr: 0.05000 - Train loss: 1.20958 - Test loss: 1.17096\n",
      "Epoch 1662 - lr: 0.05000 - Train loss: 1.20681 - Test loss: 1.16925\n",
      "Epoch 1663 - lr: 0.05000 - Train loss: 1.20919 - Test loss: 1.16308\n",
      "Epoch 1664 - lr: 0.05000 - Train loss: 1.22346 - Test loss: 1.20770\n",
      "Epoch 1665 - lr: 0.05000 - Train loss: 1.20988 - Test loss: 1.18110\n",
      "Epoch 1666 - lr: 0.05000 - Train loss: 1.21319 - Test loss: 1.17174\n",
      "Epoch 1667 - lr: 0.05000 - Train loss: 1.20997 - Test loss: 1.16915\n",
      "Epoch 1668 - lr: 0.05000 - Train loss: 1.21031 - Test loss: 1.17286\n",
      "Epoch 1669 - lr: 0.05000 - Train loss: 1.20875 - Test loss: 1.16802\n",
      "Epoch 1670 - lr: 0.05000 - Train loss: 1.21392 - Test loss: 1.18385\n",
      "Epoch 1671 - lr: 0.05000 - Train loss: 1.21350 - Test loss: 1.17388\n",
      "Epoch 1672 - lr: 0.05000 - Train loss: 1.20997 - Test loss: 1.16656\n",
      "Epoch 1673 - lr: 0.05000 - Train loss: 1.22484 - Test loss: 1.17408\n",
      "Epoch 1674 - lr: 0.05000 - Train loss: 1.22181 - Test loss: 1.21742\n",
      "Epoch 1675 - lr: 0.05000 - Train loss: 1.20770 - Test loss: 1.16707\n",
      "Epoch 1676 - lr: 0.05000 - Train loss: 1.22879 - Test loss: 1.18058\n",
      "Epoch 1677 - lr: 0.05000 - Train loss: 1.21203 - Test loss: 1.16766\n",
      "Epoch 1678 - lr: 0.05000 - Train loss: 1.22661 - Test loss: 1.15905\n",
      "Epoch 1679 - lr: 0.05000 - Train loss: 1.22341 - Test loss: 1.16140\n",
      "Epoch 1680 - lr: 0.05000 - Train loss: 1.22751 - Test loss: 1.18283\n",
      "Epoch 1681 - lr: 0.05000 - Train loss: 1.21506 - Test loss: 1.17521\n",
      "Epoch 1682 - lr: 0.05000 - Train loss: 1.22148 - Test loss: 1.17342\n",
      "Epoch 1683 - lr: 0.05000 - Train loss: 1.20777 - Test loss: 1.17335\n",
      "Epoch 1684 - lr: 0.05000 - Train loss: 1.20810 - Test loss: 1.16534\n",
      "Epoch 1685 - lr: 0.05000 - Train loss: 1.22348 - Test loss: 1.16622\n",
      "Epoch 1686 - lr: 0.05000 - Train loss: 1.21175 - Test loss: 1.18260\n",
      "Epoch 1687 - lr: 0.05000 - Train loss: 1.21181 - Test loss: 1.17511\n",
      "Epoch 1688 - lr: 0.05000 - Train loss: 1.21140 - Test loss: 1.17295\n",
      "Epoch 1689 - lr: 0.05000 - Train loss: 1.20929 - Test loss: 1.16502\n",
      "Epoch 1690 - lr: 0.05000 - Train loss: 1.22644 - Test loss: 1.15410\n",
      "Epoch 1691 - lr: 0.05000 - Train loss: 1.20995 - Test loss: 1.18071\n",
      "Epoch 1692 - lr: 0.05000 - Train loss: 1.21217 - Test loss: 1.17520\n",
      "Epoch 1693 - lr: 0.05000 - Train loss: 1.21128 - Test loss: 1.17441\n",
      "Epoch 1694 - lr: 0.05000 - Train loss: 1.21093 - Test loss: 1.17118\n",
      "Epoch 1695 - lr: 0.05000 - Train loss: 1.21069 - Test loss: 1.17503\n",
      "Epoch 1696 - lr: 0.05000 - Train loss: 1.21975 - Test loss: 1.17504\n",
      "Epoch 1697 - lr: 0.05000 - Train loss: 1.22552 - Test loss: 1.16330\n",
      "Epoch 1698 - lr: 0.05000 - Train loss: 1.22599 - Test loss: 1.15399\n",
      "Epoch 1699 - lr: 0.05000 - Train loss: 1.20973 - Test loss: 1.18066\n",
      "Epoch 1700 - lr: 0.05000 - Train loss: 1.21187 - Test loss: 1.17531\n",
      "Epoch 1701 - lr: 0.05000 - Train loss: 1.21119 - Test loss: 1.17455\n",
      "Epoch 1702 - lr: 0.05000 - Train loss: 1.21084 - Test loss: 1.17129\n",
      "Epoch 1703 - lr: 0.05000 - Train loss: 1.21065 - Test loss: 1.17539\n",
      "Epoch 1704 - lr: 0.05000 - Train loss: 1.22318 - Test loss: 1.17175\n",
      "Epoch 1705 - lr: 0.05000 - Train loss: 1.21023 - Test loss: 1.17127\n",
      "Epoch 1706 - lr: 0.05000 - Train loss: 1.20982 - Test loss: 1.17327\n",
      "Epoch 1707 - lr: 0.05000 - Train loss: 1.21016 - Test loss: 1.17458\n",
      "Epoch 1708 - lr: 0.05000 - Train loss: 1.21081 - Test loss: 1.17387\n",
      "Epoch 1709 - lr: 0.05000 - Train loss: 1.20955 - Test loss: 1.16595\n",
      "Epoch 1710 - lr: 0.05000 - Train loss: 1.22487 - Test loss: 1.15215\n",
      "Epoch 1711 - lr: 0.05000 - Train loss: 1.22256 - Test loss: 1.15080\n",
      "Epoch 1712 - lr: 0.05000 - Train loss: 1.22186 - Test loss: 1.19988\n",
      "Epoch 1713 - lr: 0.05000 - Train loss: 1.21276 - Test loss: 1.17751\n",
      "Epoch 1714 - lr: 0.05000 - Train loss: 1.21633 - Test loss: 1.17962\n",
      "Epoch 1715 - lr: 0.05000 - Train loss: 1.22853 - Test loss: 1.19865\n",
      "Epoch 1716 - lr: 0.05000 - Train loss: 1.22148 - Test loss: 1.16865\n",
      "Epoch 1717 - lr: 0.05000 - Train loss: 1.22851 - Test loss: 1.17251\n",
      "Epoch 1718 - lr: 0.05000 - Train loss: 1.22890 - Test loss: 1.16593\n",
      "Epoch 1719 - lr: 0.05000 - Train loss: 1.21535 - Test loss: 1.17944\n",
      "Epoch 1720 - lr: 0.05000 - Train loss: 1.22681 - Test loss: 1.18998\n",
      "Epoch 1721 - lr: 0.05000 - Train loss: 1.22521 - Test loss: 1.15429\n",
      "Epoch 1722 - lr: 0.05000 - Train loss: 1.20717 - Test loss: 1.17032\n",
      "Epoch 1723 - lr: 0.05000 - Train loss: 1.20745 - Test loss: 1.16812\n",
      "Epoch 1724 - lr: 0.05000 - Train loss: 1.21205 - Test loss: 1.18258\n",
      "Epoch 1725 - lr: 0.05000 - Train loss: 1.21038 - Test loss: 1.15675\n",
      "Epoch 1726 - lr: 0.05000 - Train loss: 1.15438 - Test loss: 1.06363\n",
      "Epoch 1727 - lr: 0.05000 - Train loss: 1.10177 - Test loss: 1.12783\n",
      "Epoch 1728 - lr: 0.05000 - Train loss: 1.05484 - Test loss: 1.25924\n",
      "Epoch 1729 - lr: 0.05000 - Train loss: 1.02589 - Test loss: 1.24513\n",
      "Epoch 1730 - lr: 0.05000 - Train loss: 1.11406 - Test loss: 1.26293\n",
      "Epoch 1731 - lr: 0.05000 - Train loss: 1.15748 - Test loss: 1.20341\n",
      "Epoch 1732 - lr: 0.05000 - Train loss: 1.23060 - Test loss: 0.99887\n",
      "Epoch 1733 - lr: 0.05000 - Train loss: 1.13091 - Test loss: 1.04573\n",
      "Epoch 1734 - lr: 0.05000 - Train loss: 0.91606 - Test loss: 1.08252\n",
      "Epoch 1735 - lr: 0.05000 - Train loss: 0.91139 - Test loss: 1.14642\n",
      "Epoch 1736 - lr: 0.05000 - Train loss: 0.98312 - Test loss: 1.29153\n",
      "Epoch 1737 - lr: 0.05000 - Train loss: 1.29151 - Test loss: 0.93621\n",
      "Epoch 1738 - lr: 0.05000 - Train loss: 1.21895 - Test loss: 1.33413\n",
      "Epoch 1739 - lr: 0.05000 - Train loss: 1.25577 - Test loss: 1.25474\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1740 - lr: 0.05000 - Train loss: 1.23245 - Test loss: 1.22704\n",
      "Epoch 1741 - lr: 0.05000 - Train loss: 1.22527 - Test loss: 1.22330\n",
      "Epoch 1742 - lr: 0.05000 - Train loss: 1.17361 - Test loss: 1.08690\n",
      "Epoch 1743 - lr: 0.05000 - Train loss: 0.93599 - Test loss: 0.87537\n",
      "Epoch 1744 - lr: 0.05000 - Train loss: 0.94703 - Test loss: 1.25368\n",
      "Epoch 1745 - lr: 0.05000 - Train loss: 1.00176 - Test loss: 1.26461\n",
      "Epoch 1746 - lr: 0.05000 - Train loss: 1.26428 - Test loss: 1.10096\n",
      "Epoch 1747 - lr: 0.05000 - Train loss: 1.05022 - Test loss: 1.18562\n",
      "Epoch 1748 - lr: 0.05000 - Train loss: 1.28883 - Test loss: 1.19041\n",
      "Epoch 1749 - lr: 0.05000 - Train loss: 0.97453 - Test loss: 0.88340\n",
      "Epoch 1750 - lr: 0.05000 - Train loss: 0.95486 - Test loss: 0.95935\n",
      "Epoch 1751 - lr: 0.05000 - Train loss: 0.91264 - Test loss: 1.12473\n",
      "Epoch 1752 - lr: 0.05000 - Train loss: 0.85233 - Test loss: 0.70202\n",
      "Epoch 1753 - lr: 0.05000 - Train loss: 0.85808 - Test loss: 1.28467\n",
      "Epoch 1754 - lr: 0.05000 - Train loss: 1.24214 - Test loss: 1.20508\n",
      "Epoch 1755 - lr: 0.05000 - Train loss: 1.21560 - Test loss: 1.18448\n",
      "Epoch 1756 - lr: 0.05000 - Train loss: 1.20924 - Test loss: 1.17170\n",
      "Epoch 1757 - lr: 0.05000 - Train loss: 1.22141 - Test loss: 1.17225\n",
      "Epoch 1758 - lr: 0.05000 - Train loss: 1.20842 - Test loss: 1.17631\n",
      "Epoch 1759 - lr: 0.05000 - Train loss: 1.20579 - Test loss: 1.16743\n",
      "Epoch 1760 - lr: 0.05000 - Train loss: 1.22243 - Test loss: 1.20033\n",
      "Epoch 1761 - lr: 0.05000 - Train loss: 1.21850 - Test loss: 1.17184\n",
      "Epoch 1762 - lr: 0.05000 - Train loss: 1.20846 - Test loss: 1.17569\n",
      "Epoch 1763 - lr: 0.05000 - Train loss: 1.20977 - Test loss: 1.17502\n",
      "Epoch 1764 - lr: 0.05000 - Train loss: 1.20811 - Test loss: 1.16731\n",
      "Epoch 1765 - lr: 0.05000 - Train loss: 1.22534 - Test loss: 1.15618\n",
      "Epoch 1766 - lr: 0.05000 - Train loss: 1.21004 - Test loss: 1.18355\n",
      "Epoch 1767 - lr: 0.05000 - Train loss: 1.21176 - Test loss: 1.17646\n",
      "Epoch 1768 - lr: 0.05000 - Train loss: 1.20941 - Test loss: 1.16844\n",
      "Epoch 1769 - lr: 0.05000 - Train loss: 1.22213 - Test loss: 1.17042\n",
      "Epoch 1770 - lr: 0.05000 - Train loss: 1.20855 - Test loss: 1.17529\n",
      "Epoch 1771 - lr: 0.05000 - Train loss: 1.20687 - Test loss: 1.16866\n",
      "Epoch 1772 - lr: 0.05000 - Train loss: 1.22007 - Test loss: 1.17534\n",
      "Epoch 1773 - lr: 0.05000 - Train loss: 1.20543 - Test loss: 1.17110\n",
      "Epoch 1774 - lr: 0.05000 - Train loss: 1.20941 - Test loss: 1.17670\n",
      "Epoch 1775 - lr: 0.05000 - Train loss: 1.21776 - Test loss: 1.17834\n",
      "Epoch 1776 - lr: 0.05000 - Train loss: 1.22802 - Test loss: 1.17103\n",
      "Epoch 1777 - lr: 0.05000 - Train loss: 1.22661 - Test loss: 1.16935\n",
      "Epoch 1778 - lr: 0.05000 - Train loss: 1.20759 - Test loss: 1.16579\n",
      "Epoch 1779 - lr: 0.05000 - Train loss: 1.22296 - Test loss: 1.16650\n",
      "Epoch 1780 - lr: 0.05000 - Train loss: 1.22687 - Test loss: 1.17349\n",
      "Epoch 1781 - lr: 0.05000 - Train loss: 1.22802 - Test loss: 1.16714\n",
      "Epoch 1782 - lr: 0.05000 - Train loss: 1.21541 - Test loss: 1.18017\n",
      "Epoch 1783 - lr: 0.05000 - Train loss: 1.22802 - Test loss: 1.16592\n",
      "Epoch 1784 - lr: 0.05000 - Train loss: 1.22449 - Test loss: 1.15380\n",
      "Epoch 1785 - lr: 0.05000 - Train loss: 1.21893 - Test loss: 1.17086\n",
      "Epoch 1786 - lr: 0.05000 - Train loss: 1.20878 - Test loss: 1.17471\n",
      "Epoch 1787 - lr: 0.05000 - Train loss: 1.20940 - Test loss: 1.17091\n",
      "Epoch 1788 - lr: 0.05000 - Train loss: 1.21208 - Test loss: 1.18577\n",
      "Epoch 1789 - lr: 0.05000 - Train loss: 1.21163 - Test loss: 1.17763\n",
      "Epoch 1790 - lr: 0.05000 - Train loss: 1.21028 - Test loss: 1.17582\n",
      "Epoch 1791 - lr: 0.05000 - Train loss: 1.20884 - Test loss: 1.16762\n",
      "Epoch 1792 - lr: 0.05000 - Train loss: 1.22326 - Test loss: 1.16394\n",
      "Epoch 1793 - lr: 0.05000 - Train loss: 1.22691 - Test loss: 1.17414\n",
      "Epoch 1794 - lr: 0.05000 - Train loss: 1.22796 - Test loss: 1.16863\n",
      "Epoch 1795 - lr: 0.05000 - Train loss: 1.21069 - Test loss: 1.18500\n",
      "Epoch 1796 - lr: 0.05000 - Train loss: 1.21094 - Test loss: 1.17752\n",
      "Epoch 1797 - lr: 0.05000 - Train loss: 1.21027 - Test loss: 1.17574\n",
      "Epoch 1798 - lr: 0.05000 - Train loss: 1.20861 - Test loss: 1.16807\n",
      "Epoch 1799 - lr: 0.05000 - Train loss: 1.22606 - Test loss: 1.15740\n",
      "Epoch 1800 - lr: 0.05000 - Train loss: 1.20636 - Test loss: 1.17193\n",
      "Epoch 1801 - lr: 0.05000 - Train loss: 1.20884 - Test loss: 1.17559\n",
      "Epoch 1802 - lr: 0.05000 - Train loss: 1.20972 - Test loss: 1.17435\n",
      "Epoch 1803 - lr: 0.05000 - Train loss: 1.20818 - Test loss: 1.16970\n",
      "Epoch 1804 - lr: 0.05000 - Train loss: 1.21312 - Test loss: 1.18473\n",
      "Epoch 1805 - lr: 0.05000 - Train loss: 1.20897 - Test loss: 1.17373\n",
      "Epoch 1806 - lr: 0.05000 - Train loss: 1.21008 - Test loss: 1.17908\n",
      "Epoch 1807 - lr: 0.05000 - Train loss: 1.22758 - Test loss: 1.17835\n",
      "Epoch 1808 - lr: 0.05000 - Train loss: 1.22606 - Test loss: 1.16934\n",
      "Epoch 1809 - lr: 0.05000 - Train loss: 1.20730 - Test loss: 1.16608\n",
      "Epoch 1810 - lr: 0.05000 - Train loss: 1.22375 - Test loss: 1.15338\n",
      "Epoch 1811 - lr: 0.05000 - Train loss: 1.22052 - Test loss: 1.17857\n",
      "Epoch 1812 - lr: 0.05000 - Train loss: 1.22264 - Test loss: 1.15292\n",
      "Epoch 1813 - lr: 0.05000 - Train loss: 1.20749 - Test loss: 1.18153\n",
      "Epoch 1814 - lr: 0.05000 - Train loss: 1.20963 - Test loss: 1.17612\n",
      "Epoch 1815 - lr: 0.05000 - Train loss: 1.20911 - Test loss: 1.16792\n",
      "Epoch 1816 - lr: 0.05000 - Train loss: 1.22199 - Test loss: 1.19418\n",
      "Epoch 1817 - lr: 0.05000 - Train loss: 1.22031 - Test loss: 1.19401\n",
      "Epoch 1818 - lr: 0.05000 - Train loss: 1.21890 - Test loss: 1.16486\n",
      "Epoch 1819 - lr: 0.05000 - Train loss: 1.21108 - Test loss: 1.18516\n",
      "Epoch 1820 - lr: 0.05000 - Train loss: 1.21163 - Test loss: 1.17682\n",
      "Epoch 1821 - lr: 0.05000 - Train loss: 1.20907 - Test loss: 1.16835\n",
      "Epoch 1822 - lr: 0.05000 - Train loss: 1.22200 - Test loss: 1.16740\n",
      "Epoch 1823 - lr: 0.05000 - Train loss: 1.22325 - Test loss: 1.17913\n",
      "Epoch 1824 - lr: 0.05000 - Train loss: 1.22294 - Test loss: 1.16008\n",
      "Epoch 1825 - lr: 0.05000 - Train loss: 1.22430 - Test loss: 1.15596\n",
      "Epoch 1826 - lr: 0.05000 - Train loss: 1.20610 - Test loss: 1.17236\n",
      "Epoch 1827 - lr: 0.05000 - Train loss: 1.20731 - Test loss: 1.17303\n",
      "Epoch 1828 - lr: 0.05000 - Train loss: 1.20847 - Test loss: 1.17446\n",
      "Epoch 1829 - lr: 0.05000 - Train loss: 1.20900 - Test loss: 1.17174\n",
      "Epoch 1830 - lr: 0.05000 - Train loss: 1.21056 - Test loss: 1.18639\n",
      "Epoch 1831 - lr: 0.05000 - Train loss: 1.21123 - Test loss: 1.17825\n",
      "Epoch 1832 - lr: 0.05000 - Train loss: 1.20988 - Test loss: 1.17588\n",
      "Epoch 1833 - lr: 0.05000 - Train loss: 1.20779 - Test loss: 1.16853\n",
      "Epoch 1834 - lr: 0.05000 - Train loss: 1.22537 - Test loss: 1.15694\n",
      "Epoch 1835 - lr: 0.05000 - Train loss: 1.20690 - Test loss: 1.17741\n",
      "Epoch 1836 - lr: 0.05000 - Train loss: 1.22810 - Test loss: 1.16723\n",
      "Epoch 1837 - lr: 0.05000 - Train loss: 1.22025 - Test loss: 1.17398\n",
      "Epoch 1838 - lr: 0.05000 - Train loss: 1.20853 - Test loss: 1.17601\n",
      "Epoch 1839 - lr: 0.05000 - Train loss: 1.20880 - Test loss: 1.17075\n",
      "Epoch 1840 - lr: 0.05000 - Train loss: 1.21626 - Test loss: 1.18043\n",
      "Epoch 1841 - lr: 0.05000 - Train loss: 1.22805 - Test loss: 1.16973\n",
      "Epoch 1842 - lr: 0.05000 - Train loss: 1.21383 - Test loss: 1.18350\n",
      "Epoch 1843 - lr: 0.05000 - Train loss: 1.21504 - Test loss: 1.18398\n",
      "Epoch 1844 - lr: 0.05000 - Train loss: 1.22442 - Test loss: 1.16682\n",
      "Epoch 1845 - lr: 0.05000 - Train loss: 1.22202 - Test loss: 1.18471\n",
      "Epoch 1846 - lr: 0.05000 - Train loss: 1.22232 - Test loss: 1.15462\n",
      "Epoch 1847 - lr: 0.05000 - Train loss: 1.20675 - Test loss: 1.18020\n",
      "Epoch 1848 - lr: 0.05000 - Train loss: 1.21984 - Test loss: 1.17643\n",
      "Epoch 1849 - lr: 0.05000 - Train loss: 1.20501 - Test loss: 1.16845\n",
      "Epoch 1850 - lr: 0.05000 - Train loss: 1.21975 - Test loss: 1.17564\n",
      "Epoch 1851 - lr: 0.05000 - Train loss: 1.20499 - Test loss: 1.16852\n",
      "Epoch 1852 - lr: 0.05000 - Train loss: 1.22456 - Test loss: 1.15404\n",
      "Epoch 1853 - lr: 0.05000 - Train loss: 1.21800 - Test loss: 1.17209\n",
      "Epoch 1854 - lr: 0.05000 - Train loss: 1.20775 - Test loss: 1.17600\n",
      "Epoch 1855 - lr: 0.05000 - Train loss: 1.20887 - Test loss: 1.17291\n",
      "Epoch 1856 - lr: 0.05000 - Train loss: 1.20972 - Test loss: 1.18502\n",
      "Epoch 1857 - lr: 0.05000 - Train loss: 1.20710 - Test loss: 1.17927\n",
      "Epoch 1858 - lr: 0.05000 - Train loss: 1.21838 - Test loss: 1.17879\n",
      "Epoch 1859 - lr: 0.05000 - Train loss: 1.22458 - Test loss: 1.20423\n",
      "Epoch 1860 - lr: 0.05000 - Train loss: 1.21935 - Test loss: 1.17107\n",
      "Epoch 1861 - lr: 0.05000 - Train loss: 1.20623 - Test loss: 1.16883\n",
      "Epoch 1862 - lr: 0.05000 - Train loss: 1.22245 - Test loss: 1.21941\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1863 - lr: 0.05000 - Train loss: 1.20661 - Test loss: 1.17300\n",
      "Epoch 1864 - lr: 0.05000 - Train loss: 1.20826 - Test loss: 1.17633\n",
      "Epoch 1865 - lr: 0.05000 - Train loss: 1.20869 - Test loss: 1.17167\n",
      "Epoch 1866 - lr: 0.05000 - Train loss: 1.21435 - Test loss: 1.18397\n",
      "Epoch 1867 - lr: 0.05000 - Train loss: 1.22155 - Test loss: 1.17591\n",
      "Epoch 1868 - lr: 0.05000 - Train loss: 1.20875 - Test loss: 1.17631\n",
      "Epoch 1869 - lr: 0.05000 - Train loss: 1.20743 - Test loss: 1.17093\n",
      "Epoch 1870 - lr: 0.05000 - Train loss: 1.22240 - Test loss: 1.22249\n",
      "Epoch 1871 - lr: 0.05000 - Train loss: 1.20594 - Test loss: 1.16523\n",
      "Epoch 1872 - lr: 0.05000 - Train loss: 1.21982 - Test loss: 1.17352\n",
      "Epoch 1873 - lr: 0.05000 - Train loss: 1.20796 - Test loss: 1.17313\n",
      "Epoch 1874 - lr: 0.05000 - Train loss: 1.20891 - Test loss: 1.18158\n",
      "Epoch 1875 - lr: 0.05000 - Train loss: 1.22797 - Test loss: 1.16854\n",
      "Epoch 1876 - lr: 0.05000 - Train loss: 1.22051 - Test loss: 1.17286\n",
      "Epoch 1877 - lr: 0.05000 - Train loss: 1.20624 - Test loss: 1.16800\n",
      "Epoch 1878 - lr: 0.05000 - Train loss: 1.22470 - Test loss: 1.15817\n",
      "Epoch 1879 - lr: 0.05000 - Train loss: 1.20584 - Test loss: 1.17493\n",
      "Epoch 1880 - lr: 0.05000 - Train loss: 1.20599 - Test loss: 1.18741\n",
      "Epoch 1881 - lr: 0.05000 - Train loss: 1.21139 - Test loss: 1.17788\n",
      "Epoch 1882 - lr: 0.05000 - Train loss: 1.20763 - Test loss: 1.17170\n",
      "Epoch 1883 - lr: 0.05000 - Train loss: 1.22301 - Test loss: 1.19910\n",
      "Epoch 1884 - lr: 0.05000 - Train loss: 1.21880 - Test loss: 1.16446\n",
      "Epoch 1885 - lr: 0.05000 - Train loss: 1.21938 - Test loss: 1.17295\n",
      "Epoch 1886 - lr: 0.05000 - Train loss: 1.20689 - Test loss: 1.16829\n",
      "Epoch 1887 - lr: 0.05000 - Train loss: 1.22127 - Test loss: 1.19758\n",
      "Epoch 1888 - lr: 0.05000 - Train loss: 1.21798 - Test loss: 1.17021\n",
      "Epoch 1889 - lr: 0.05000 - Train loss: 1.20566 - Test loss: 1.16709\n",
      "Epoch 1890 - lr: 0.05000 - Train loss: 1.22403 - Test loss: 1.15715\n",
      "Epoch 1891 - lr: 0.05000 - Train loss: 1.20725 - Test loss: 1.18620\n",
      "Epoch 1892 - lr: 0.05000 - Train loss: 1.20998 - Test loss: 1.17298\n",
      "Epoch 1893 - lr: 0.05000 - Train loss: 1.22465 - Test loss: 1.16389\n",
      "Epoch 1894 - lr: 0.05000 - Train loss: 1.22001 - Test loss: 1.16488\n",
      "Epoch 1895 - lr: 0.05000 - Train loss: 1.22194 - Test loss: 1.15467\n",
      "Epoch 1896 - lr: 0.05000 - Train loss: 1.22270 - Test loss: 1.15644\n",
      "Epoch 1897 - lr: 0.05000 - Train loss: 1.20479 - Test loss: 1.17259\n",
      "Epoch 1898 - lr: 0.05000 - Train loss: 1.20789 - Test loss: 1.17571\n",
      "Epoch 1899 - lr: 0.05000 - Train loss: 1.20687 - Test loss: 1.17155\n",
      "Epoch 1900 - lr: 0.05000 - Train loss: 1.22344 - Test loss: 1.16965\n",
      "Epoch 1901 - lr: 0.05000 - Train loss: 1.22618 - Test loss: 1.17653\n",
      "Epoch 1902 - lr: 0.05000 - Train loss: 1.22640 - Test loss: 1.18797\n",
      "Epoch 1903 - lr: 0.05000 - Train loss: 1.22189 - Test loss: 1.15321\n",
      "Epoch 1904 - lr: 0.05000 - Train loss: 1.22268 - Test loss: 1.15612\n",
      "Epoch 1905 - lr: 0.05000 - Train loss: 1.20452 - Test loss: 1.17168\n",
      "Epoch 1906 - lr: 0.05000 - Train loss: 1.20681 - Test loss: 1.16864\n",
      "Epoch 1907 - lr: 0.05000 - Train loss: 1.22457 - Test loss: 1.15833\n",
      "Epoch 1908 - lr: 0.05000 - Train loss: 1.20584 - Test loss: 1.17691\n",
      "Epoch 1909 - lr: 0.05000 - Train loss: 1.22417 - Test loss: 1.21813\n",
      "Epoch 1910 - lr: 0.05000 - Train loss: 1.20677 - Test loss: 1.17635\n",
      "Epoch 1911 - lr: 0.05000 - Train loss: 1.21549 - Test loss: 1.18089\n",
      "Epoch 1912 - lr: 0.05000 - Train loss: 1.22743 - Test loss: 1.17782\n",
      "Epoch 1913 - lr: 0.05000 - Train loss: 1.22612 - Test loss: 1.17369\n",
      "Epoch 1914 - lr: 0.05000 - Train loss: 1.20333 - Test loss: 1.16995\n",
      "Epoch 1915 - lr: 0.05000 - Train loss: 1.22241 - Test loss: 1.20754\n",
      "Epoch 1916 - lr: 0.05000 - Train loss: 1.21385 - Test loss: 1.17865\n",
      "Epoch 1917 - lr: 0.05000 - Train loss: 1.22676 - Test loss: 1.18408\n",
      "Epoch 1918 - lr: 0.05000 - Train loss: 1.22266 - Test loss: 1.16381\n",
      "Epoch 1919 - lr: 0.05000 - Train loss: 1.22087 - Test loss: 1.19429\n",
      "Epoch 1920 - lr: 0.05000 - Train loss: 1.21757 - Test loss: 1.16744\n",
      "Epoch 1921 - lr: 0.05000 - Train loss: 1.20754 - Test loss: 1.18557\n",
      "Epoch 1922 - lr: 0.05000 - Train loss: 1.20674 - Test loss: 1.17155\n",
      "Epoch 1923 - lr: 0.05000 - Train loss: 1.22084 - Test loss: 1.17423\n",
      "Epoch 1924 - lr: 0.05000 - Train loss: 1.20597 - Test loss: 1.17118\n",
      "Epoch 1925 - lr: 0.05000 - Train loss: 1.22238 - Test loss: 1.19679\n",
      "Epoch 1926 - lr: 0.05000 - Train loss: 1.21817 - Test loss: 1.19542\n",
      "Epoch 1927 - lr: 0.05000 - Train loss: 1.21802 - Test loss: 1.22266\n",
      "Epoch 1928 - lr: 0.05000 - Train loss: 1.20525 - Test loss: 1.16659\n",
      "Epoch 1929 - lr: 0.05000 - Train loss: 1.21829 - Test loss: 1.17633\n",
      "Epoch 1930 - lr: 0.05000 - Train loss: 1.20487 - Test loss: 1.16960\n",
      "Epoch 1931 - lr: 0.05000 - Train loss: 1.22064 - Test loss: 1.18949\n",
      "Epoch 1932 - lr: 0.05000 - Train loss: 1.22242 - Test loss: 1.15667\n",
      "Epoch 1933 - lr: 0.05000 - Train loss: 1.20612 - Test loss: 1.18608\n",
      "Epoch 1934 - lr: 0.05000 - Train loss: 1.20974 - Test loss: 1.17122\n",
      "Epoch 1935 - lr: 0.05000 - Train loss: 1.22389 - Test loss: 1.15745\n",
      "Epoch 1936 - lr: 0.05000 - Train loss: 1.21343 - Test loss: 1.17905\n",
      "Epoch 1937 - lr: 0.05000 - Train loss: 1.22644 - Test loss: 1.18359\n",
      "Epoch 1938 - lr: 0.05000 - Train loss: 1.22302 - Test loss: 1.16455\n",
      "Epoch 1939 - lr: 0.05000 - Train loss: 1.21867 - Test loss: 1.17367\n",
      "Epoch 1940 - lr: 0.05000 - Train loss: 1.20615 - Test loss: 1.16981\n",
      "Epoch 1941 - lr: 0.05000 - Train loss: 1.22412 - Test loss: 1.15913\n",
      "Epoch 1942 - lr: 0.05000 - Train loss: 1.20533 - Test loss: 1.17728\n",
      "Epoch 1943 - lr: 0.05000 - Train loss: 1.22233 - Test loss: 1.20621\n",
      "Epoch 1944 - lr: 0.05000 - Train loss: 1.21749 - Test loss: 1.17492\n",
      "Epoch 1945 - lr: 0.05000 - Train loss: 1.20668 - Test loss: 1.17775\n",
      "Epoch 1946 - lr: 0.05000 - Train loss: 1.20735 - Test loss: 1.17089\n",
      "Epoch 1947 - lr: 0.05000 - Train loss: 1.22045 - Test loss: 1.17513\n",
      "Epoch 1948 - lr: 0.05000 - Train loss: 1.22552 - Test loss: 1.17527\n",
      "Epoch 1949 - lr: 0.05000 - Train loss: 1.21583 - Test loss: 1.17918\n",
      "Epoch 1950 - lr: 0.05000 - Train loss: 1.22254 - Test loss: 1.20385\n",
      "Epoch 1951 - lr: 0.05000 - Train loss: 1.21851 - Test loss: 1.17021\n",
      "Epoch 1952 - lr: 0.05000 - Train loss: 1.20714 - Test loss: 1.18458\n",
      "Epoch 1953 - lr: 0.05000 - Train loss: 1.22261 - Test loss: 1.17423\n",
      "Epoch 1954 - lr: 0.05000 - Train loss: 1.22555 - Test loss: 1.17406\n",
      "Epoch 1955 - lr: 0.05000 - Train loss: 1.20579 - Test loss: 1.17662\n",
      "Epoch 1956 - lr: 0.05000 - Train loss: 1.20617 - Test loss: 1.17034\n",
      "Epoch 1957 - lr: 0.05000 - Train loss: 1.22399 - Test loss: 1.15963\n",
      "Epoch 1958 - lr: 0.05000 - Train loss: 1.20521 - Test loss: 1.17815\n",
      "Epoch 1959 - lr: 0.05000 - Train loss: 1.22371 - Test loss: 1.21701\n",
      "Epoch 1960 - lr: 0.05000 - Train loss: 1.20633 - Test loss: 1.18141\n",
      "Epoch 1961 - lr: 0.05000 - Train loss: 1.22613 - Test loss: 1.21724\n",
      "Epoch 1962 - lr: 0.05000 - Train loss: 1.20829 - Test loss: 1.19000\n",
      "Epoch 1963 - lr: 0.05000 - Train loss: 1.21072 - Test loss: 1.17813\n",
      "Epoch 1964 - lr: 0.05000 - Train loss: 1.20654 - Test loss: 1.17471\n",
      "Epoch 1965 - lr: 0.05000 - Train loss: 1.22432 - Test loss: 1.16968\n",
      "Epoch 1966 - lr: 0.05000 - Train loss: 1.20705 - Test loss: 1.18639\n",
      "Epoch 1967 - lr: 0.05000 - Train loss: 1.20499 - Test loss: 1.17905\n",
      "Epoch 1968 - lr: 0.05000 - Train loss: 1.20811 - Test loss: 1.17743\n",
      "Epoch 1969 - lr: 0.05000 - Train loss: 1.20716 - Test loss: 1.17873\n",
      "Epoch 1970 - lr: 0.05000 - Train loss: 1.20762 - Test loss: 1.17941\n",
      "Epoch 1971 - lr: 0.05000 - Train loss: 1.20695 - Test loss: 1.17182\n",
      "Epoch 1972 - lr: 0.05000 - Train loss: 1.22075 - Test loss: 1.19068\n",
      "Epoch 1973 - lr: 0.05000 - Train loss: 1.21804 - Test loss: 1.17952\n",
      "Epoch 1974 - lr: 0.05000 - Train loss: 1.21841 - Test loss: 1.19106\n",
      "Epoch 1975 - lr: 0.05000 - Train loss: 1.21674 - Test loss: 1.16596\n",
      "Epoch 1976 - lr: 0.05000 - Train loss: 1.22393 - Test loss: 1.17253\n",
      "Epoch 1977 - lr: 0.05000 - Train loss: 1.20631 - Test loss: 1.17530\n",
      "Epoch 1978 - lr: 0.05000 - Train loss: 1.20731 - Test loss: 1.18078\n",
      "Epoch 1979 - lr: 0.05000 - Train loss: 1.21720 - Test loss: 1.18056\n",
      "Epoch 1980 - lr: 0.05000 - Train loss: 1.21869 - Test loss: 1.17894\n",
      "Epoch 1981 - lr: 0.05000 - Train loss: 1.20526 - Test loss: 1.17420\n",
      "Epoch 1982 - lr: 0.05000 - Train loss: 1.21315 - Test loss: 1.18687\n",
      "Epoch 1983 - lr: 0.05000 - Train loss: 1.21562 - Test loss: 1.18417\n",
      "Epoch 1984 - lr: 0.05000 - Train loss: 1.22680 - Test loss: 1.17513\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1985 - lr: 0.05000 - Train loss: 1.22412 - Test loss: 1.17218\n",
      "Epoch 1986 - lr: 0.05000 - Train loss: 1.20471 - Test loss: 1.17116\n",
      "Epoch 1987 - lr: 0.05000 - Train loss: 1.21572 - Test loss: 1.18120\n",
      "Epoch 1988 - lr: 0.05000 - Train loss: 1.22401 - Test loss: 1.22455\n",
      "Epoch 1989 - lr: 0.05000 - Train loss: 1.20552 - Test loss: 1.17588\n",
      "Epoch 1990 - lr: 0.05000 - Train loss: 1.20679 - Test loss: 1.17597\n",
      "Epoch 1991 - lr: 0.05000 - Train loss: 1.20770 - Test loss: 1.18488\n",
      "Epoch 1992 - lr: 0.05000 - Train loss: 1.22655 - Test loss: 1.17546\n",
      "Epoch 1993 - lr: 0.05000 - Train loss: 1.22408 - Test loss: 1.17261\n",
      "Epoch 1994 - lr: 0.05000 - Train loss: 1.20447 - Test loss: 1.16962\n",
      "Epoch 1995 - lr: 0.05000 - Train loss: 1.21941 - Test loss: 1.20964\n",
      "Epoch 1996 - lr: 0.05000 - Train loss: 1.21507 - Test loss: 1.17814\n",
      "Epoch 1997 - lr: 0.05000 - Train loss: 1.21306 - Test loss: 1.18452\n",
      "Epoch 1998 - lr: 0.05000 - Train loss: 1.22567 - Test loss: 1.21370\n",
      "Epoch 1999 - lr: 0.05000 - Train loss: 1.21428 - Test loss: 1.18062\n",
      "Epoch 2000 - lr: 0.05000 - Train loss: 1.22469 - Test loss: 1.22683\n",
      "Epoch 2001 - lr: 0.05000 - Train loss: 1.20523 - Test loss: 1.17360\n",
      "Epoch 2002 - lr: 0.05000 - Train loss: 1.20613 - Test loss: 1.17932\n",
      "Epoch 2003 - lr: 0.05000 - Train loss: 1.20592 - Test loss: 1.17854\n",
      "Epoch 2004 - lr: 0.05000 - Train loss: 1.20618 - Test loss: 1.17576\n",
      "Epoch 2005 - lr: 0.05000 - Train loss: 1.20739 - Test loss: 1.18535\n",
      "Epoch 2006 - lr: 0.05000 - Train loss: 1.22622 - Test loss: 1.18144\n",
      "Epoch 2007 - lr: 0.05000 - Train loss: 1.22469 - Test loss: 1.17551\n",
      "Epoch 2008 - lr: 0.05000 - Train loss: 1.20533 - Test loss: 1.17911\n",
      "Epoch 2009 - lr: 0.05000 - Train loss: 1.20612 - Test loss: 1.17326\n",
      "Epoch 2010 - lr: 0.05000 - Train loss: 1.22327 - Test loss: 1.16007\n",
      "Epoch 2011 - lr: 0.05000 - Train loss: 1.20638 - Test loss: 1.18919\n",
      "Epoch 2012 - lr: 0.05000 - Train loss: 1.20866 - Test loss: 1.17468\n",
      "Epoch 2013 - lr: 0.05000 - Train loss: 1.22122 - Test loss: 1.21048\n",
      "Epoch 2014 - lr: 0.05000 - Train loss: 1.20799 - Test loss: 1.18843\n",
      "Epoch 2015 - lr: 0.05000 - Train loss: 1.20877 - Test loss: 1.17338\n",
      "Epoch 2016 - lr: 0.05000 - Train loss: 1.22138 - Test loss: 1.16719\n",
      "Epoch 2017 - lr: 0.05000 - Train loss: 1.22419 - Test loss: 1.18317\n",
      "Epoch 2018 - lr: 0.05000 - Train loss: 1.20449 - Test loss: 1.17289\n",
      "Epoch 2019 - lr: 0.05000 - Train loss: 1.22003 - Test loss: 1.17224\n",
      "Epoch 2020 - lr: 0.05000 - Train loss: 1.21220 - Test loss: 1.18634\n",
      "Epoch 2021 - lr: 0.05000 - Train loss: 1.22248 - Test loss: 1.20004\n",
      "Epoch 2022 - lr: 0.05000 - Train loss: 1.21884 - Test loss: 1.18565\n",
      "Epoch 2023 - lr: 0.05000 - Train loss: 1.22109 - Test loss: 1.15941\n",
      "Epoch 2024 - lr: 0.05000 - Train loss: 1.20239 - Test loss: 1.16745\n",
      "Epoch 2025 - lr: 0.05000 - Train loss: 1.21910 - Test loss: 1.16832\n",
      "Epoch 2026 - lr: 0.05000 - Train loss: 1.21868 - Test loss: 1.22645\n",
      "Epoch 2027 - lr: 0.05000 - Train loss: 1.20431 - Test loss: 1.17029\n",
      "Epoch 2028 - lr: 0.05000 - Train loss: 1.21453 - Test loss: 1.18247\n",
      "Epoch 2029 - lr: 0.05000 - Train loss: 1.22511 - Test loss: 1.21074\n",
      "Epoch 2030 - lr: 0.05000 - Train loss: 1.21659 - Test loss: 1.17748\n",
      "Epoch 2031 - lr: 0.05000 - Train loss: 1.20564 - Test loss: 1.18042\n",
      "Epoch 2032 - lr: 0.05000 - Train loss: 1.20663 - Test loss: 1.17451\n",
      "Epoch 2033 - lr: 0.05000 - Train loss: 1.21794 - Test loss: 1.17994\n",
      "Epoch 2034 - lr: 0.05000 - Train loss: 1.20486 - Test loss: 1.17810\n",
      "Epoch 2035 - lr: 0.05000 - Train loss: 1.20693 - Test loss: 1.18282\n",
      "Epoch 2036 - lr: 0.05000 - Train loss: 1.21988 - Test loss: 1.17794\n",
      "Epoch 2037 - lr: 0.05000 - Train loss: 1.20495 - Test loss: 1.17564\n",
      "Epoch 2038 - lr: 0.05000 - Train loss: 1.22209 - Test loss: 1.16843\n",
      "Epoch 2039 - lr: 0.05000 - Train loss: 1.21935 - Test loss: 1.22824\n",
      "Epoch 2040 - lr: 0.05000 - Train loss: 1.20404 - Test loss: 1.17841\n",
      "Epoch 2041 - lr: 0.05000 - Train loss: 1.22506 - Test loss: 1.18129\n",
      "Epoch 2042 - lr: 0.05000 - Train loss: 1.22475 - Test loss: 1.21465\n",
      "Epoch 2043 - lr: 0.05000 - Train loss: 1.21348 - Test loss: 1.18180\n",
      "Epoch 2044 - lr: 0.05000 - Train loss: 1.22476 - Test loss: 1.21754\n",
      "Epoch 2045 - lr: 0.05000 - Train loss: 1.21029 - Test loss: 1.18723\n",
      "Epoch 2046 - lr: 0.05000 - Train loss: 1.20384 - Test loss: 1.18112\n",
      "Epoch 2047 - lr: 0.05000 - Train loss: 1.20732 - Test loss: 1.18069\n",
      "Epoch 2048 - lr: 0.05000 - Train loss: 1.20545 - Test loss: 1.17292\n",
      "Epoch 2049 - lr: 0.05000 - Train loss: 1.21932 - Test loss: 1.22330\n",
      "Epoch 2050 - lr: 0.05000 - Train loss: 1.20471 - Test loss: 1.17898\n",
      "Epoch 2051 - lr: 0.05000 - Train loss: 1.20222 - Test loss: 1.17455\n",
      "Epoch 2052 - lr: 0.05000 - Train loss: 1.22094 - Test loss: 1.21958\n",
      "Epoch 2053 - lr: 0.05000 - Train loss: 1.20525 - Test loss: 1.18770\n",
      "Epoch 2054 - lr: 0.05000 - Train loss: 1.20413 - Test loss: 1.17467\n",
      "Epoch 2055 - lr: 0.05000 - Train loss: 1.22389 - Test loss: 1.16297\n",
      "Epoch 2056 - lr: 0.05000 - Train loss: 1.20329 - Test loss: 1.17746\n",
      "Epoch 2057 - lr: 0.05000 - Train loss: 1.20623 - Test loss: 1.18012\n",
      "Epoch 2058 - lr: 0.05000 - Train loss: 1.20513 - Test loss: 1.17321\n",
      "Epoch 2059 - lr: 0.05000 - Train loss: 1.21912 - Test loss: 1.17719\n",
      "Epoch 2060 - lr: 0.05000 - Train loss: 1.22403 - Test loss: 1.17731\n",
      "Epoch 2061 - lr: 0.05000 - Train loss: 1.20135 - Test loss: 1.17323\n",
      "Epoch 2062 - lr: 0.05000 - Train loss: 1.22103 - Test loss: 1.16536\n",
      "Epoch 2063 - lr: 0.05000 - Train loss: 1.22332 - Test loss: 1.18675\n",
      "Epoch 2064 - lr: 0.05000 - Train loss: 1.20747 - Test loss: 1.17527\n",
      "Epoch 2065 - lr: 0.05000 - Train loss: 1.22164 - Test loss: 1.16084\n",
      "Epoch 2066 - lr: 0.05000 - Train loss: 1.22034 - Test loss: 1.16503\n",
      "Epoch 2067 - lr: 0.05000 - Train loss: 1.21753 - Test loss: 1.22838\n",
      "Epoch 2068 - lr: 0.05000 - Train loss: 1.20386 - Test loss: 1.17121\n",
      "Epoch 2069 - lr: 0.05000 - Train loss: 1.21531 - Test loss: 1.18193\n",
      "Epoch 2070 - lr: 0.05000 - Train loss: 1.22016 - Test loss: 1.17364\n",
      "Epoch 2071 - lr: 0.05000 - Train loss: 1.21303 - Test loss: 1.18569\n",
      "Epoch 2072 - lr: 0.05000 - Train loss: 1.22576 - Test loss: 1.17492\n",
      "Epoch 2073 - lr: 0.05000 - Train loss: 1.21407 - Test loss: 1.18458\n",
      "Epoch 2074 - lr: 0.05000 - Train loss: 1.22523 - Test loss: 1.17288\n",
      "Epoch 2075 - lr: 0.05000 - Train loss: 1.22206 - Test loss: 1.16203\n",
      "Epoch 2076 - lr: 0.05000 - Train loss: 1.20544 - Test loss: 1.18952\n",
      "Epoch 2077 - lr: 0.05000 - Train loss: 1.20870 - Test loss: 1.18069\n",
      "Epoch 2078 - lr: 0.05000 - Train loss: 1.20679 - Test loss: 1.18362\n",
      "Epoch 2079 - lr: 0.05000 - Train loss: 1.20643 - Test loss: 1.19757\n",
      "Epoch 2080 - lr: 0.05000 - Train loss: 1.21364 - Test loss: 1.19325\n",
      "Epoch 2081 - lr: 0.05000 - Train loss: 1.20468 - Test loss: 1.18373\n",
      "Epoch 2082 - lr: 0.05000 - Train loss: 1.20704 - Test loss: 1.18381\n",
      "Epoch 2083 - lr: 0.05000 - Train loss: 1.20687 - Test loss: 1.18109\n",
      "Epoch 2084 - lr: 0.05000 - Train loss: 1.20603 - Test loss: 1.18268\n",
      "Epoch 2085 - lr: 0.05000 - Train loss: 1.20556 - Test loss: 1.18315\n",
      "Epoch 2086 - lr: 0.05000 - Train loss: 1.20611 - Test loss: 1.17668\n",
      "Epoch 2087 - lr: 0.05000 - Train loss: 1.21716 - Test loss: 1.18192\n",
      "Epoch 2088 - lr: 0.05000 - Train loss: 1.20352 - Test loss: 1.17721\n",
      "Epoch 2089 - lr: 0.05000 - Train loss: 1.21316 - Test loss: 1.18717\n",
      "Epoch 2090 - lr: 0.05000 - Train loss: 1.22552 - Test loss: 1.17721\n",
      "Epoch 2091 - lr: 0.05000 - Train loss: 1.20847 - Test loss: 1.19285\n",
      "Epoch 2092 - lr: 0.05000 - Train loss: 1.20854 - Test loss: 1.18499\n",
      "Epoch 2093 - lr: 0.05000 - Train loss: 1.20667 - Test loss: 1.17983\n",
      "Epoch 2094 - lr: 0.05000 - Train loss: 1.20786 - Test loss: 1.19297\n",
      "Epoch 2095 - lr: 0.05000 - Train loss: 1.20768 - Test loss: 1.18563\n",
      "Epoch 2096 - lr: 0.05000 - Train loss: 1.20697 - Test loss: 1.18325\n",
      "Epoch 2097 - lr: 0.05000 - Train loss: 1.20483 - Test loss: 1.17581\n",
      "Epoch 2098 - lr: 0.05000 - Train loss: 1.22283 - Test loss: 1.16437\n",
      "Epoch 2099 - lr: 0.05000 - Train loss: 1.20254 - Test loss: 1.17873\n",
      "Epoch 2100 - lr: 0.05000 - Train loss: 1.20563 - Test loss: 1.18147\n",
      "Epoch 2101 - lr: 0.05000 - Train loss: 1.20464 - Test loss: 1.17519\n",
      "Epoch 2102 - lr: 0.05000 - Train loss: 1.21801 - Test loss: 1.17972\n",
      "Epoch 2103 - lr: 0.05000 - Train loss: 1.20428 - Test loss: 1.17522\n",
      "Epoch 2104 - lr: 0.05000 - Train loss: 1.22256 - Test loss: 1.16438\n",
      "Epoch 2105 - lr: 0.05000 - Train loss: 1.20265 - Test loss: 1.17944\n",
      "Epoch 2106 - lr: 0.05000 - Train loss: 1.20522 - Test loss: 1.18342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2107 - lr: 0.05000 - Train loss: 1.20620 - Test loss: 1.18083\n",
      "Epoch 2108 - lr: 0.05000 - Train loss: 1.20641 - Test loss: 1.18695\n",
      "Epoch 2109 - lr: 0.05000 - Train loss: 1.22482 - Test loss: 1.18393\n",
      "Epoch 2110 - lr: 0.05000 - Train loss: 1.22318 - Test loss: 1.17789\n",
      "Epoch 2111 - lr: 0.05000 - Train loss: 1.20478 - Test loss: 1.18268\n",
      "Epoch 2112 - lr: 0.05000 - Train loss: 1.20554 - Test loss: 1.17717\n",
      "Epoch 2113 - lr: 0.05000 - Train loss: 1.21676 - Test loss: 1.18265\n",
      "Epoch 2114 - lr: 0.05000 - Train loss: 1.20280 - Test loss: 1.17617\n",
      "Epoch 2115 - lr: 0.05000 - Train loss: 1.21816 - Test loss: 1.17952\n",
      "Epoch 2116 - lr: 0.05000 - Train loss: 1.20350 - Test loss: 1.17536\n",
      "Epoch 2117 - lr: 0.05000 - Train loss: 1.22166 - Test loss: 1.16223\n",
      "Epoch 2118 - lr: 0.05000 - Train loss: 1.20784 - Test loss: 1.18872\n",
      "Epoch 2119 - lr: 0.05000 - Train loss: 1.20398 - Test loss: 1.17731\n",
      "Epoch 2120 - lr: 0.05000 - Train loss: 1.21833 - Test loss: 1.18071\n",
      "Epoch 2121 - lr: 0.05000 - Train loss: 1.20465 - Test loss: 1.17578\n",
      "Epoch 2122 - lr: 0.05000 - Train loss: 1.21838 - Test loss: 1.17374\n",
      "Epoch 2123 - lr: 0.05000 - Train loss: 1.21619 - Test loss: 1.18142\n",
      "Epoch 2124 - lr: 0.05000 - Train loss: 1.20454 - Test loss: 1.18358\n",
      "Epoch 2125 - lr: 0.05000 - Train loss: 1.20539 - Test loss: 1.17721\n",
      "Epoch 2126 - lr: 0.05000 - Train loss: 1.21757 - Test loss: 1.18125\n",
      "Epoch 2127 - lr: 0.05000 - Train loss: 1.20501 - Test loss: 1.18067\n",
      "Epoch 2128 - lr: 0.05000 - Train loss: 1.20582 - Test loss: 1.18671\n",
      "Epoch 2129 - lr: 0.05000 - Train loss: 1.22308 - Test loss: 1.23065\n",
      "Epoch 2130 - lr: 0.05000 - Train loss: 1.20365 - Test loss: 1.17810\n",
      "Epoch 2131 - lr: 0.05000 - Train loss: 1.20387 - Test loss: 1.17989\n",
      "Epoch 2132 - lr: 0.05000 - Train loss: 1.20443 - Test loss: 1.18167\n",
      "Epoch 2133 - lr: 0.05000 - Train loss: 1.20415 - Test loss: 1.17927\n",
      "Epoch 2134 - lr: 0.05000 - Train loss: 1.22149 - Test loss: 1.17027\n",
      "Epoch 2135 - lr: 0.05000 - Train loss: 1.21702 - Test loss: 1.17662\n",
      "Epoch 2136 - lr: 0.05000 - Train loss: 1.22290 - Test loss: 1.17919\n",
      "Epoch 2137 - lr: 0.05000 - Train loss: 1.20038 - Test loss: 1.17630\n",
      "Epoch 2138 - lr: 0.05000 - Train loss: 1.21954 - Test loss: 1.22957\n",
      "Epoch 2139 - lr: 0.05000 - Train loss: 1.20276 - Test loss: 1.17277\n",
      "Epoch 2140 - lr: 0.05000 - Train loss: 1.21634 - Test loss: 1.18165\n",
      "Epoch 2141 - lr: 0.05000 - Train loss: 1.20451 - Test loss: 1.18409\n",
      "Epoch 2142 - lr: 0.05000 - Train loss: 1.20520 - Test loss: 1.17736\n",
      "Epoch 2143 - lr: 0.05000 - Train loss: 1.21809 - Test loss: 1.17757\n",
      "Epoch 2144 - lr: 0.05000 - Train loss: 1.20533 - Test loss: 1.19253\n",
      "Epoch 2145 - lr: 0.05000 - Train loss: 1.20396 - Test loss: 1.17844\n",
      "Epoch 2146 - lr: 0.05000 - Train loss: 1.21853 - Test loss: 1.17713\n",
      "Epoch 2147 - lr: 0.05000 - Train loss: 1.20683 - Test loss: 1.19582\n",
      "Epoch 2148 - lr: 0.05000 - Train loss: 1.20744 - Test loss: 1.17762\n",
      "Epoch 2149 - lr: 0.05000 - Train loss: 1.21946 - Test loss: 1.17750\n",
      "Epoch 2150 - lr: 0.05000 - Train loss: 1.22218 - Test loss: 1.18041\n",
      "Epoch 2151 - lr: 0.05000 - Train loss: 1.22219 - Test loss: 1.18754\n",
      "Epoch 2152 - lr: 0.05000 - Train loss: 1.21762 - Test loss: 1.16438\n",
      "Epoch 2153 - lr: 0.05000 - Train loss: 1.22138 - Test loss: 1.20063\n",
      "Epoch 2154 - lr: 0.05000 - Train loss: 1.21042 - Test loss: 1.18972\n",
      "Epoch 2155 - lr: 0.05000 - Train loss: 1.22439 - Test loss: 1.23068\n",
      "Epoch 2156 - lr: 0.05000 - Train loss: 1.20456 - Test loss: 1.18395\n",
      "Epoch 2157 - lr: 0.05000 - Train loss: 1.21701 - Test loss: 1.18288\n",
      "Epoch 2158 - lr: 0.05000 - Train loss: 1.20507 - Test loss: 1.18531\n",
      "Epoch 2159 - lr: 0.05000 - Train loss: 1.20571 - Test loss: 1.18259\n",
      "Epoch 2160 - lr: 0.05000 - Train loss: 1.20570 - Test loss: 1.18721\n",
      "Epoch 2161 - lr: 0.05000 - Train loss: 1.22111 - Test loss: 1.22362\n",
      "Epoch 2162 - lr: 0.05000 - Train loss: 1.20677 - Test loss: 1.19374\n",
      "Epoch 2163 - lr: 0.05000 - Train loss: 1.20743 - Test loss: 1.17956\n",
      "Epoch 2164 - lr: 0.05000 - Train loss: 1.21720 - Test loss: 1.18349\n",
      "Epoch 2165 - lr: 0.05000 - Train loss: 1.20459 - Test loss: 1.18556\n",
      "Epoch 2166 - lr: 0.05000 - Train loss: 1.20549 - Test loss: 1.18280\n",
      "Epoch 2167 - lr: 0.05000 - Train loss: 1.20564 - Test loss: 1.18796\n",
      "Epoch 2168 - lr: 0.05000 - Train loss: 1.22258 - Test loss: 1.23155\n",
      "Epoch 2169 - lr: 0.05000 - Train loss: 1.20392 - Test loss: 1.18310\n",
      "Epoch 2170 - lr: 0.05000 - Train loss: 1.20197 - Test loss: 1.17708\n",
      "Epoch 2171 - lr: 0.05000 - Train loss: 1.21812 - Test loss: 1.18452\n",
      "Epoch 2172 - lr: 0.05000 - Train loss: 1.22258 - Test loss: 1.18028\n",
      "Epoch 2173 - lr: 0.05000 - Train loss: 1.20030 - Test loss: 1.17742\n",
      "Epoch 2174 - lr: 0.05000 - Train loss: 1.21933 - Test loss: 1.19300\n",
      "Epoch 2175 - lr: 0.05000 - Train loss: 1.21789 - Test loss: 1.16119\n",
      "Epoch 2176 - lr: 0.05000 - Train loss: 1.20770 - Test loss: 1.18863\n",
      "Epoch 2177 - lr: 0.05000 - Train loss: 1.20806 - Test loss: 1.19774\n",
      "Epoch 2178 - lr: 0.05000 - Train loss: 1.20785 - Test loss: 1.17886\n",
      "Epoch 2179 - lr: 0.05000 - Train loss: 1.21969 - Test loss: 1.17210\n",
      "Epoch 2180 - lr: 0.05000 - Train loss: 1.22247 - Test loss: 1.18388\n",
      "Epoch 2181 - lr: 0.05000 - Train loss: 1.22343 - Test loss: 1.18019\n",
      "Epoch 2182 - lr: 0.05000 - Train loss: 1.22201 - Test loss: 1.17824\n",
      "Epoch 2183 - lr: 0.05000 - Train loss: 1.20249 - Test loss: 1.17876\n",
      "Epoch 2184 - lr: 0.05000 - Train loss: 1.21947 - Test loss: 1.20508\n",
      "Epoch 2185 - lr: 0.05000 - Train loss: 1.21521 - Test loss: 1.21637\n",
      "Epoch 2186 - lr: 0.05000 - Train loss: 1.21137 - Test loss: 1.18446\n",
      "Epoch 2187 - lr: 0.05000 - Train loss: 1.22215 - Test loss: 1.23389\n",
      "Epoch 2188 - lr: 0.05000 - Train loss: 1.20304 - Test loss: 1.17810\n",
      "Epoch 2189 - lr: 0.05000 - Train loss: 1.20511 - Test loss: 1.19098\n",
      "Epoch 2190 - lr: 0.05000 - Train loss: 1.22322 - Test loss: 1.22780\n",
      "Epoch 2191 - lr: 0.05000 - Train loss: 1.20565 - Test loss: 1.19444\n",
      "Epoch 2192 - lr: 0.05000 - Train loss: 1.20756 - Test loss: 1.18544\n",
      "Epoch 2193 - lr: 0.05000 - Train loss: 1.20487 - Test loss: 1.18539\n",
      "Epoch 2194 - lr: 0.05000 - Train loss: 1.20522 - Test loss: 1.18558\n",
      "Epoch 2195 - lr: 0.05000 - Train loss: 1.20365 - Test loss: 1.17864\n",
      "Epoch 2196 - lr: 0.05000 - Train loss: 1.22195 - Test loss: 1.16721\n",
      "Epoch 2197 - lr: 0.05000 - Train loss: 1.20090 - Test loss: 1.17895\n",
      "Epoch 2198 - lr: 0.05000 - Train loss: 1.20380 - Test loss: 1.18505\n",
      "Epoch 2199 - lr: 0.05000 - Train loss: 1.20413 - Test loss: 1.18655\n",
      "Epoch 2200 - lr: 0.05000 - Train loss: 1.20478 - Test loss: 1.18003\n",
      "Epoch 2201 - lr: 0.05000 - Train loss: 1.21644 - Test loss: 1.18449\n",
      "Epoch 2202 - lr: 0.05000 - Train loss: 1.20416 - Test loss: 1.18690\n",
      "Epoch 2203 - lr: 0.05000 - Train loss: 1.20511 - Test loss: 1.18498\n",
      "Epoch 2204 - lr: 0.05000 - Train loss: 1.20429 - Test loss: 1.18582\n",
      "Epoch 2205 - lr: 0.05000 - Train loss: 1.20483 - Test loss: 1.18674\n",
      "Epoch 2206 - lr: 0.05000 - Train loss: 1.20419 - Test loss: 1.17912\n",
      "Epoch 2207 - lr: 0.05000 - Train loss: 1.21738 - Test loss: 1.23425\n",
      "Epoch 2208 - lr: 0.05000 - Train loss: 1.20231 - Test loss: 1.17881\n",
      "Epoch 2209 - lr: 0.05000 - Train loss: 1.20467 - Test loss: 1.19306\n",
      "Epoch 2210 - lr: 0.05000 - Train loss: 1.21806 - Test loss: 1.18486\n",
      "Epoch 2211 - lr: 0.05000 - Train loss: 1.20408 - Test loss: 1.17920\n",
      "Epoch 2212 - lr: 0.05000 - Train loss: 1.21749 - Test loss: 1.17770\n",
      "Epoch 2213 - lr: 0.05000 - Train loss: 1.21210 - Test loss: 1.18812\n",
      "Epoch 2214 - lr: 0.05000 - Train loss: 1.22268 - Test loss: 1.20735\n",
      "Epoch 2215 - lr: 0.05000 - Train loss: 1.21573 - Test loss: 1.22877\n",
      "Epoch 2216 - lr: 0.05000 - Train loss: 1.20207 - Test loss: 1.18202\n",
      "Epoch 2217 - lr: 0.05000 - Train loss: 1.20409 - Test loss: 1.18385\n",
      "Epoch 2218 - lr: 0.05000 - Train loss: 1.20451 - Test loss: 1.18732\n",
      "Epoch 2219 - lr: 0.05000 - Train loss: 1.20059 - Test loss: 1.17839\n",
      "Epoch 2220 - lr: 0.05000 - Train loss: 1.21736 - Test loss: 1.22143\n",
      "Epoch 2221 - lr: 0.05000 - Train loss: 1.20914 - Test loss: 1.18973\n",
      "Epoch 2222 - lr: 0.05000 - Train loss: 1.22226 - Test loss: 1.23675\n",
      "Epoch 2223 - lr: 0.05000 - Train loss: 1.20260 - Test loss: 1.17772\n",
      "Epoch 2224 - lr: 0.05000 - Train loss: 1.21072 - Test loss: 1.19121\n",
      "Epoch 2225 - lr: 0.05000 - Train loss: 1.22321 - Test loss: 1.20821\n",
      "Epoch 2226 - lr: 0.05000 - Train loss: 1.21589 - Test loss: 1.22552\n",
      "Epoch 2227 - lr: 0.05000 - Train loss: 1.20250 - Test loss: 1.18573\n",
      "Epoch 2228 - lr: 0.05000 - Train loss: 1.22032 - Test loss: 1.23069\n",
      "Epoch 2229 - lr: 0.05000 - Train loss: 1.20274 - Test loss: 1.18396\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2230 - lr: 0.05000 - Train loss: 1.20345 - Test loss: 1.18709\n",
      "Epoch 2231 - lr: 0.05000 - Train loss: 1.20424 - Test loss: 1.18013\n",
      "Epoch 2232 - lr: 0.05000 - Train loss: 1.21737 - Test loss: 1.17629\n",
      "Epoch 2233 - lr: 0.05000 - Train loss: 1.21631 - Test loss: 1.18251\n",
      "Epoch 2234 - lr: 0.05000 - Train loss: 1.22156 - Test loss: 1.18170\n",
      "Epoch 2235 - lr: 0.05000 - Train loss: 1.20152 - Test loss: 1.18336\n",
      "Epoch 2236 - lr: 0.05000 - Train loss: 1.20475 - Test loss: 1.19023\n",
      "Epoch 2237 - lr: 0.05000 - Train loss: 1.22284 - Test loss: 1.22109\n",
      "Epoch 2238 - lr: 0.05000 - Train loss: 1.21244 - Test loss: 1.18593\n",
      "Epoch 2239 - lr: 0.05000 - Train loss: 1.21653 - Test loss: 1.18394\n",
      "Epoch 2240 - lr: 0.05000 - Train loss: 1.20304 - Test loss: 1.17935\n",
      "Epoch 2241 - lr: 0.05000 - Train loss: 1.21960 - Test loss: 1.16490\n",
      "Epoch 2242 - lr: 0.05000 - Train loss: 1.21446 - Test loss: 1.17780\n",
      "Epoch 2243 - lr: 0.05000 - Train loss: 1.20370 - Test loss: 1.19228\n",
      "Epoch 2244 - lr: 0.05000 - Train loss: 1.22161 - Test loss: 1.23704\n",
      "Epoch 2245 - lr: 0.05000 - Train loss: 1.20237 - Test loss: 1.17867\n",
      "Epoch 2246 - lr: 0.05000 - Train loss: 1.20869 - Test loss: 1.19484\n",
      "Epoch 2247 - lr: 0.05000 - Train loss: 1.20120 - Test loss: 1.18303\n",
      "Epoch 2248 - lr: 0.05000 - Train loss: 1.20905 - Test loss: 1.19682\n",
      "Epoch 2249 - lr: 0.05000 - Train loss: 1.20486 - Test loss: 1.18923\n",
      "Epoch 2250 - lr: 0.05000 - Train loss: 1.20361 - Test loss: 1.18298\n",
      "Epoch 2251 - lr: 0.05000 - Train loss: 1.21814 - Test loss: 1.23544\n",
      "Epoch 2252 - lr: 0.05000 - Train loss: 1.20129 - Test loss: 1.17862\n",
      "Epoch 2253 - lr: 0.05000 - Train loss: 1.22049 - Test loss: 1.17818\n",
      "Epoch 2254 - lr: 0.05000 - Train loss: 1.20291 - Test loss: 1.18814\n",
      "Epoch 2255 - lr: 0.05000 - Train loss: 1.21782 - Test loss: 1.18168\n",
      "Epoch 2256 - lr: 0.05000 - Train loss: 1.20452 - Test loss: 1.19488\n",
      "Epoch 2257 - lr: 0.05000 - Train loss: 1.21504 - Test loss: 1.18924\n",
      "Epoch 2258 - lr: 0.05000 - Train loss: 1.20242 - Test loss: 1.19793\n",
      "Epoch 2259 - lr: 0.05000 - Train loss: 1.20539 - Test loss: 1.19043\n",
      "Epoch 2260 - lr: 0.05000 - Train loss: 1.20438 - Test loss: 1.18175\n",
      "Epoch 2261 - lr: 0.05000 - Train loss: 1.21702 - Test loss: 1.17991\n",
      "Epoch 2262 - lr: 0.05000 - Train loss: 1.20951 - Test loss: 1.19268\n",
      "Epoch 2263 - lr: 0.05000 - Train loss: 1.22218 - Test loss: 1.21232\n",
      "Epoch 2264 - lr: 0.05000 - Train loss: 1.21514 - Test loss: 1.21128\n",
      "Epoch 2265 - lr: 0.05000 - Train loss: 1.21445 - Test loss: 1.17411\n",
      "Epoch 2266 - lr: 0.05000 - Train loss: 1.21552 - Test loss: 1.23577\n",
      "Epoch 2267 - lr: 0.05000 - Train loss: 1.20123 - Test loss: 1.17596\n",
      "Epoch 2268 - lr: 0.05000 - Train loss: 1.22054 - Test loss: 1.16838\n",
      "Epoch 2269 - lr: 0.05000 - Train loss: 1.19979 - Test loss: 1.17879\n",
      "Epoch 2270 - lr: 0.05000 - Train loss: 1.20637 - Test loss: 1.19823\n",
      "Epoch 2271 - lr: 0.05000 - Train loss: 1.20651 - Test loss: 1.18386\n",
      "Epoch 2272 - lr: 0.05000 - Train loss: 1.21520 - Test loss: 1.18795\n",
      "Epoch 2273 - lr: 0.05000 - Train loss: 1.19972 - Test loss: 1.18110\n",
      "Epoch 2274 - lr: 0.05000 - Train loss: 1.22085 - Test loss: 1.16857\n",
      "Epoch 2275 - lr: 0.05000 - Train loss: 1.20105 - Test loss: 1.18621\n",
      "Epoch 2276 - lr: 0.05000 - Train loss: 1.21065 - Test loss: 1.19196\n",
      "Epoch 2277 - lr: 0.05000 - Train loss: 1.22393 - Test loss: 1.18278\n",
      "Epoch 2278 - lr: 0.05000 - Train loss: 1.20697 - Test loss: 1.19819\n",
      "Epoch 2279 - lr: 0.05000 - Train loss: 1.20611 - Test loss: 1.19125\n",
      "Epoch 2280 - lr: 0.05000 - Train loss: 1.20463 - Test loss: 1.18737\n",
      "Epoch 2281 - lr: 0.05000 - Train loss: 1.20407 - Test loss: 1.19007\n",
      "Epoch 2282 - lr: 0.05000 - Train loss: 1.20281 - Test loss: 1.20305\n",
      "Epoch 2283 - lr: 0.05000 - Train loss: 1.21008 - Test loss: 1.19519\n",
      "Epoch 2284 - lr: 0.05000 - Train loss: 1.22425 - Test loss: 1.18460\n",
      "Epoch 2285 - lr: 0.05000 - Train loss: 1.20475 - Test loss: 1.19676\n",
      "Epoch 2286 - lr: 0.05000 - Train loss: 1.20269 - Test loss: 1.19923\n",
      "Epoch 2287 - lr: 0.05000 - Train loss: 1.20149 - Test loss: 1.18270\n",
      "Epoch 2288 - lr: 0.05000 - Train loss: 1.21744 - Test loss: 1.22801\n",
      "Epoch 2289 - lr: 0.05000 - Train loss: 1.20246 - Test loss: 1.19303\n",
      "Epoch 2290 - lr: 0.05000 - Train loss: 1.21849 - Test loss: 1.17967\n",
      "Epoch 2291 - lr: 0.05000 - Train loss: 1.22090 - Test loss: 1.16969\n",
      "Epoch 2292 - lr: 0.05000 - Train loss: 1.19930 - Test loss: 1.17706\n",
      "Epoch 2293 - lr: 0.05000 - Train loss: 1.21567 - Test loss: 1.18363\n",
      "Epoch 2294 - lr: 0.05000 - Train loss: 1.20158 - Test loss: 1.18292\n",
      "Epoch 2295 - lr: 0.05000 - Train loss: 1.20635 - Test loss: 1.19879\n",
      "Epoch 2296 - lr: 0.05000 - Train loss: 1.20603 - Test loss: 1.19200\n",
      "Epoch 2297 - lr: 0.05000 - Train loss: 1.20454 - Test loss: 1.18867\n",
      "Epoch 2298 - lr: 0.05000 - Train loss: 1.20319 - Test loss: 1.18868\n",
      "Epoch 2299 - lr: 0.05000 - Train loss: 1.20364 - Test loss: 1.18781\n",
      "Epoch 2300 - lr: 0.05000 - Train loss: 1.20331 - Test loss: 1.18953\n",
      "Epoch 2301 - lr: 0.05000 - Train loss: 1.20290 - Test loss: 1.19040\n",
      "Epoch 2302 - lr: 0.05000 - Train loss: 1.20356 - Test loss: 1.18438\n",
      "Epoch 2303 - lr: 0.05000 - Train loss: 1.21327 - Test loss: 1.19015\n",
      "Epoch 2304 - lr: 0.05000 - Train loss: 1.21485 - Test loss: 1.18842\n",
      "Epoch 2305 - lr: 0.05000 - Train loss: 1.20271 - Test loss: 1.19038\n",
      "Epoch 2306 - lr: 0.05000 - Train loss: 1.20365 - Test loss: 1.18597\n",
      "Epoch 2307 - lr: 0.05000 - Train loss: 1.20727 - Test loss: 1.19901\n",
      "Epoch 2308 - lr: 0.05000 - Train loss: 1.20426 - Test loss: 1.19203\n",
      "Epoch 2309 - lr: 0.05000 - Train loss: 1.20362 - Test loss: 1.18368\n",
      "Epoch 2310 - lr: 0.05000 - Train loss: 1.21617 - Test loss: 1.18427\n",
      "Epoch 2311 - lr: 0.05000 - Train loss: 1.20267 - Test loss: 1.19058\n",
      "Epoch 2312 - lr: 0.05000 - Train loss: 1.21277 - Test loss: 1.19086\n",
      "Epoch 2313 - lr: 0.05000 - Train loss: 1.21660 - Test loss: 1.18653\n",
      "Epoch 2314 - lr: 0.05000 - Train loss: 1.20147 - Test loss: 1.18221\n",
      "Epoch 2315 - lr: 0.05000 - Train loss: 1.22026 - Test loss: 1.17024\n",
      "Epoch 2316 - lr: 0.05000 - Train loss: 1.19995 - Test loss: 1.18614\n",
      "Epoch 2317 - lr: 0.05000 - Train loss: 1.20247 - Test loss: 1.19038\n",
      "Epoch 2318 - lr: 0.05000 - Train loss: 1.20360 - Test loss: 1.18668\n",
      "Epoch 2319 - lr: 0.05000 - Train loss: 1.20584 - Test loss: 1.20067\n",
      "Epoch 2320 - lr: 0.05000 - Train loss: 1.20604 - Test loss: 1.19160\n",
      "Epoch 2321 - lr: 0.05000 - Train loss: 1.20239 - Test loss: 1.18293\n",
      "Epoch 2322 - lr: 0.05000 - Train loss: 1.21892 - Test loss: 1.16835\n",
      "Epoch 2323 - lr: 0.05000 - Train loss: 1.21130 - Test loss: 1.18637\n",
      "Epoch 2324 - lr: 0.05000 - Train loss: 1.19836 - Test loss: 1.18105\n",
      "Epoch 2325 - lr: 0.05000 - Train loss: 1.21601 - Test loss: 1.21589\n",
      "Epoch 2326 - lr: 0.05000 - Train loss: 1.21385 - Test loss: 1.17966\n",
      "Epoch 2327 - lr: 0.05000 - Train loss: 1.20761 - Test loss: 1.19584\n",
      "Epoch 2328 - lr: 0.05000 - Train loss: 1.21622 - Test loss: 1.18859\n",
      "Epoch 2329 - lr: 0.05000 - Train loss: 1.20280 - Test loss: 1.18441\n",
      "Epoch 2330 - lr: 0.05000 - Train loss: 1.21345 - Test loss: 1.19022\n",
      "Epoch 2331 - lr: 0.05000 - Train loss: 1.20151 - Test loss: 1.20370\n",
      "Epoch 2332 - lr: 0.05000 - Train loss: 1.20847 - Test loss: 1.19388\n",
      "Epoch 2333 - lr: 0.05000 - Train loss: 1.20476 - Test loss: 1.20543\n",
      "Epoch 2334 - lr: 0.05000 - Train loss: 1.21019 - Test loss: 1.20016\n",
      "Epoch 2335 - lr: 0.05000 - Train loss: 1.21965 - Test loss: 1.20162\n",
      "Epoch 2336 - lr: 0.05000 - Train loss: 1.21620 - Test loss: 1.23356\n",
      "Epoch 2337 - lr: 0.05000 - Train loss: 1.20025 - Test loss: 1.18398\n",
      "Epoch 2338 - lr: 0.05000 - Train loss: 1.20128 - Test loss: 1.18590\n",
      "Epoch 2339 - lr: 0.05000 - Train loss: 1.20327 - Test loss: 1.19280\n",
      "Epoch 2340 - lr: 0.05000 - Train loss: 1.21993 - Test loss: 1.24173\n",
      "Epoch 2341 - lr: 0.05000 - Train loss: 1.20116 - Test loss: 1.18184\n",
      "Epoch 2342 - lr: 0.05000 - Train loss: 1.20918 - Test loss: 1.19500\n",
      "Epoch 2343 - lr: 0.05000 - Train loss: 1.22253 - Test loss: 1.18185\n",
      "Epoch 2344 - lr: 0.05000 - Train loss: 1.21649 - Test loss: 1.19531\n",
      "Epoch 2345 - lr: 0.05000 - Train loss: 1.21655 - Test loss: 1.16635\n",
      "Epoch 2346 - lr: 0.05000 - Train loss: 1.20426 - Test loss: 1.19523\n",
      "Epoch 2347 - lr: 0.05000 - Train loss: 1.20211 - Test loss: 1.18883\n",
      "Epoch 2348 - lr: 0.05000 - Train loss: 1.20463 - Test loss: 1.19862\n",
      "Epoch 2349 - lr: 0.05000 - Train loss: 1.21757 - Test loss: 1.18713\n",
      "Epoch 2350 - lr: 0.05000 - Train loss: 1.20302 - Test loss: 1.19228\n",
      "Epoch 2351 - lr: 0.05000 - Train loss: 1.21457 - Test loss: 1.19026\n",
      "Epoch 2352 - lr: 0.05000 - Train loss: 1.20192 - Test loss: 1.19132\n",
      "Epoch 2353 - lr: 0.05000 - Train loss: 1.20212 - Test loss: 1.18514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2354 - lr: 0.05000 - Train loss: 1.21983 - Test loss: 1.17108\n",
      "Epoch 2355 - lr: 0.05000 - Train loss: 1.20039 - Test loss: 1.19057\n",
      "Epoch 2356 - lr: 0.05000 - Train loss: 1.22135 - Test loss: 1.21198\n",
      "Epoch 2357 - lr: 0.05000 - Train loss: 1.21458 - Test loss: 1.21480\n",
      "Epoch 2358 - lr: 0.05000 - Train loss: 1.21130 - Test loss: 1.18601\n",
      "Epoch 2359 - lr: 0.05000 - Train loss: 1.20153 - Test loss: 1.19147\n",
      "Epoch 2360 - lr: 0.05000 - Train loss: 1.20324 - Test loss: 1.18926\n",
      "Epoch 2361 - lr: 0.05000 - Train loss: 1.20362 - Test loss: 1.19525\n",
      "Epoch 2362 - lr: 0.05000 - Train loss: 1.22263 - Test loss: 1.18560\n",
      "Epoch 2363 - lr: 0.05000 - Train loss: 1.20725 - Test loss: 1.19871\n",
      "Epoch 2364 - lr: 0.05000 - Train loss: 1.20278 - Test loss: 1.20690\n",
      "Epoch 2365 - lr: 0.05000 - Train loss: 1.20969 - Test loss: 1.19863\n",
      "Epoch 2366 - lr: 0.05000 - Train loss: 1.22319 - Test loss: 1.19158\n",
      "Epoch 2367 - lr: 0.05000 - Train loss: 1.20293 - Test loss: 1.18957\n",
      "Epoch 2368 - lr: 0.05000 - Train loss: 1.20343 - Test loss: 1.19462\n",
      "Epoch 2369 - lr: 0.05000 - Train loss: 1.22123 - Test loss: 1.20416\n",
      "Epoch 2370 - lr: 0.05000 - Train loss: 1.21727 - Test loss: 1.16687\n",
      "Epoch 2371 - lr: 0.05000 - Train loss: 1.21279 - Test loss: 1.17985\n",
      "Epoch 2372 - lr: 0.05000 - Train loss: 1.20750 - Test loss: 1.19581\n",
      "Epoch 2373 - lr: 0.05000 - Train loss: 1.22080 - Test loss: 1.22106\n",
      "Epoch 2374 - lr: 0.05000 - Train loss: 1.21401 - Test loss: 1.18224\n",
      "Epoch 2375 - lr: 0.05000 - Train loss: 1.20532 - Test loss: 1.19955\n",
      "Epoch 2376 - lr: 0.05000 - Train loss: 1.20261 - Test loss: 1.19226\n",
      "Epoch 2377 - lr: 0.05000 - Train loss: 1.20247 - Test loss: 1.19062\n",
      "Epoch 2378 - lr: 0.05000 - Train loss: 1.20130 - Test loss: 1.18609\n",
      "Epoch 2379 - lr: 0.05000 - Train loss: 1.21786 - Test loss: 1.16891\n",
      "Epoch 2380 - lr: 0.05000 - Train loss: 1.21723 - Test loss: 1.16925\n",
      "Epoch 2381 - lr: 0.05000 - Train loss: 1.19916 - Test loss: 1.18827\n",
      "Epoch 2382 - lr: 0.05000 - Train loss: 1.19865 - Test loss: 1.18465\n",
      "Epoch 2383 - lr: 0.05000 - Train loss: 1.22037 - Test loss: 1.17277\n",
      "Epoch 2384 - lr: 0.05000 - Train loss: 1.19818 - Test loss: 1.18036\n",
      "Epoch 2385 - lr: 0.05000 - Train loss: 1.21460 - Test loss: 1.18695\n",
      "Epoch 2386 - lr: 0.05000 - Train loss: 1.20033 - Test loss: 1.18402\n",
      "Epoch 2387 - lr: 0.05000 - Train loss: 1.21391 - Test loss: 1.19002\n",
      "Epoch 2388 - lr: 0.05000 - Train loss: 1.20188 - Test loss: 1.19326\n",
      "Epoch 2389 - lr: 0.05000 - Train loss: 1.20298 - Test loss: 1.19178\n",
      "Epoch 2390 - lr: 0.05000 - Train loss: 1.20173 - Test loss: 1.19047\n",
      "Epoch 2391 - lr: 0.05000 - Train loss: 1.20084 - Test loss: 1.18420\n",
      "Epoch 2392 - lr: 0.05000 - Train loss: 1.21520 - Test loss: 1.19005\n",
      "Epoch 2393 - lr: 0.05000 - Train loss: 1.21935 - Test loss: 1.18615\n",
      "Epoch 2394 - lr: 0.05000 - Train loss: 1.20121 - Test loss: 1.19238\n",
      "Epoch 2395 - lr: 0.05000 - Train loss: 1.20253 - Test loss: 1.18935\n",
      "Epoch 2396 - lr: 0.05000 - Train loss: 1.20479 - Test loss: 1.20276\n",
      "Epoch 2397 - lr: 0.05000 - Train loss: 1.20472 - Test loss: 1.19592\n",
      "Epoch 2398 - lr: 0.05000 - Train loss: 1.20330 - Test loss: 1.19277\n",
      "Epoch 2399 - lr: 0.05000 - Train loss: 1.20156 - Test loss: 1.19013\n",
      "Epoch 2400 - lr: 0.05000 - Train loss: 1.20163 - Test loss: 1.19242\n",
      "Epoch 2401 - lr: 0.05000 - Train loss: 1.20230 - Test loss: 1.19424\n",
      "Epoch 2402 - lr: 0.05000 - Train loss: 1.20266 - Test loss: 1.19109\n",
      "Epoch 2403 - lr: 0.05000 - Train loss: 1.20305 - Test loss: 1.19748\n",
      "Epoch 2404 - lr: 0.05000 - Train loss: 1.22214 - Test loss: 1.19032\n",
      "Epoch 2405 - lr: 0.05000 - Train loss: 1.20122 - Test loss: 1.19161\n",
      "Epoch 2406 - lr: 0.05000 - Train loss: 1.20151 - Test loss: 1.18609\n",
      "Epoch 2407 - lr: 0.05000 - Train loss: 1.21514 - Test loss: 1.18283\n",
      "Epoch 2408 - lr: 0.05000 - Train loss: 1.21362 - Test loss: 1.18851\n",
      "Epoch 2409 - lr: 0.05000 - Train loss: 1.19985 - Test loss: 1.18705\n",
      "Epoch 2410 - lr: 0.05000 - Train loss: 1.21593 - Test loss: 1.23482\n",
      "Epoch 2411 - lr: 0.05000 - Train loss: 1.19948 - Test loss: 1.18863\n",
      "Epoch 2412 - lr: 0.05000 - Train loss: 1.20172 - Test loss: 1.19096\n",
      "Epoch 2413 - lr: 0.05000 - Train loss: 1.20217 - Test loss: 1.19414\n",
      "Epoch 2414 - lr: 0.05000 - Train loss: 1.19817 - Test loss: 1.18557\n",
      "Epoch 2415 - lr: 0.05000 - Train loss: 1.21506 - Test loss: 1.22792\n",
      "Epoch 2416 - lr: 0.05000 - Train loss: 1.20765 - Test loss: 1.19456\n",
      "Epoch 2417 - lr: 0.05000 - Train loss: 1.22162 - Test loss: 1.18490\n",
      "Epoch 2418 - lr: 0.05000 - Train loss: 1.21378 - Test loss: 1.19025\n",
      "Epoch 2419 - lr: 0.05000 - Train loss: 1.20137 - Test loss: 1.19043\n",
      "Epoch 2420 - lr: 0.05000 - Train loss: 1.20269 - Test loss: 1.19836\n",
      "Epoch 2421 - lr: 0.05000 - Train loss: 1.22199 - Test loss: 1.18716\n",
      "Epoch 2422 - lr: 0.05000 - Train loss: 1.20955 - Test loss: 1.19633\n",
      "Epoch 2423 - lr: 0.05000 - Train loss: 1.22151 - Test loss: 1.18588\n",
      "Epoch 2424 - lr: 0.05000 - Train loss: 1.21242 - Test loss: 1.19229\n",
      "Epoch 2425 - lr: 0.05000 - Train loss: 1.19777 - Test loss: 1.18687\n",
      "Epoch 2426 - lr: 0.05000 - Train loss: 1.21878 - Test loss: 1.17270\n",
      "Epoch 2427 - lr: 0.05000 - Train loss: 1.19983 - Test loss: 1.19796\n",
      "Epoch 2428 - lr: 0.05000 - Train loss: 1.20015 - Test loss: 1.19947\n",
      "Epoch 2429 - lr: 0.05000 - Train loss: 1.22289 - Test loss: 1.18730\n",
      "Epoch 2430 - lr: 0.05000 - Train loss: 1.21096 - Test loss: 1.19481\n",
      "Epoch 2431 - lr: 0.05000 - Train loss: 1.21784 - Test loss: 1.24471\n",
      "Epoch 2432 - lr: 0.05000 - Train loss: 1.19984 - Test loss: 1.18280\n",
      "Epoch 2433 - lr: 0.05000 - Train loss: 1.21427 - Test loss: 1.18409\n",
      "Epoch 2434 - lr: 0.05000 - Train loss: 1.20960 - Test loss: 1.19492\n",
      "Epoch 2435 - lr: 0.05000 - Train loss: 1.21966 - Test loss: 1.24417\n",
      "Epoch 2436 - lr: 0.05000 - Train loss: 1.19999 - Test loss: 1.18647\n",
      "Epoch 2437 - lr: 0.05000 - Train loss: 1.20276 - Test loss: 1.20396\n",
      "Epoch 2438 - lr: 0.05000 - Train loss: 1.20465 - Test loss: 1.19415\n",
      "Epoch 2439 - lr: 0.05000 - Train loss: 1.20249 - Test loss: 1.19582\n",
      "Epoch 2440 - lr: 0.05000 - Train loss: 1.20022 - Test loss: 1.20584\n",
      "Epoch 2441 - lr: 0.05000 - Train loss: 1.20492 - Test loss: 1.19140\n",
      "Epoch 2442 - lr: 0.05000 - Train loss: 1.21104 - Test loss: 1.19665\n",
      "Epoch 2443 - lr: 0.05000 - Train loss: 1.21963 - Test loss: 1.24097\n",
      "Epoch 2444 - lr: 0.05000 - Train loss: 1.20090 - Test loss: 1.19376\n",
      "Epoch 2445 - lr: 0.05000 - Train loss: 1.21562 - Test loss: 1.18538\n",
      "Epoch 2446 - lr: 0.05000 - Train loss: 1.21174 - Test loss: 1.19334\n",
      "Epoch 2447 - lr: 0.05000 - Train loss: 1.19857 - Test loss: 1.19543\n",
      "Epoch 2448 - lr: 0.05000 - Train loss: 1.19947 - Test loss: 1.19677\n",
      "Epoch 2449 - lr: 0.05000 - Train loss: 1.21451 - Test loss: 1.19320\n",
      "Epoch 2450 - lr: 0.05000 - Train loss: 1.20181 - Test loss: 1.19498\n",
      "Epoch 2451 - lr: 0.05000 - Train loss: 1.20087 - Test loss: 1.18894\n",
      "Epoch 2452 - lr: 0.05000 - Train loss: 1.21855 - Test loss: 1.17409\n",
      "Epoch 2453 - lr: 0.05000 - Train loss: 1.19926 - Test loss: 1.19600\n",
      "Epoch 2454 - lr: 0.05000 - Train loss: 1.22141 - Test loss: 1.18748\n",
      "Epoch 2455 - lr: 0.05000 - Train loss: 1.21030 - Test loss: 1.19576\n",
      "Epoch 2456 - lr: 0.05000 - Train loss: 1.21831 - Test loss: 1.24580\n",
      "Epoch 2457 - lr: 0.05000 - Train loss: 1.19963 - Test loss: 1.18523\n",
      "Epoch 2458 - lr: 0.05000 - Train loss: 1.21066 - Test loss: 1.19473\n",
      "Epoch 2459 - lr: 0.05000 - Train loss: 1.21548 - Test loss: 1.18617\n",
      "Epoch 2460 - lr: 0.05000 - Train loss: 1.21092 - Test loss: 1.19450\n",
      "Epoch 2461 - lr: 0.05000 - Train loss: 1.21170 - Test loss: 1.19460\n",
      "Epoch 2462 - lr: 0.05000 - Train loss: 1.19806 - Test loss: 1.19415\n",
      "Epoch 2463 - lr: 0.05000 - Train loss: 1.20165 - Test loss: 1.19106\n",
      "Epoch 2464 - lr: 0.05000 - Train loss: 1.20652 - Test loss: 1.20285\n",
      "Epoch 2465 - lr: 0.05000 - Train loss: 1.20022 - Test loss: 1.20197\n",
      "Epoch 2466 - lr: 0.05000 - Train loss: 1.22232 - Test loss: 1.19265\n",
      "Epoch 2467 - lr: 0.05000 - Train loss: 1.20111 - Test loss: 1.19519\n",
      "Epoch 2468 - lr: 0.05000 - Train loss: 1.20078 - Test loss: 1.19625\n",
      "Epoch 2469 - lr: 0.05000 - Train loss: 1.20127 - Test loss: 1.18876\n",
      "Epoch 2470 - lr: 0.05000 - Train loss: 1.21450 - Test loss: 1.18661\n",
      "Epoch 2471 - lr: 0.05000 - Train loss: 1.21458 - Test loss: 1.22460\n",
      "Epoch 2472 - lr: 0.05000 - Train loss: 1.20368 - Test loss: 1.19876\n",
      "Epoch 2473 - lr: 0.05000 - Train loss: 1.20224 - Test loss: 1.21122\n",
      "Epoch 2474 - lr: 0.05000 - Train loss: 1.21286 - Test loss: 1.20791\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2475 - lr: 0.05000 - Train loss: 1.20379 - Test loss: 1.19967\n",
      "Epoch 2476 - lr: 0.05000 - Train loss: 1.20266 - Test loss: 1.19710\n",
      "Epoch 2477 - lr: 0.05000 - Train loss: 1.20076 - Test loss: 1.18964\n",
      "Epoch 2478 - lr: 0.05000 - Train loss: 1.21869 - Test loss: 1.17622\n",
      "Epoch 2479 - lr: 0.05000 - Train loss: 1.19736 - Test loss: 1.19011\n",
      "Epoch 2480 - lr: 0.05000 - Train loss: 1.19933 - Test loss: 1.18943\n",
      "Epoch 2481 - lr: 0.05000 - Train loss: 1.21561 - Test loss: 1.20614\n",
      "Epoch 2482 - lr: 0.05000 - Train loss: 1.21194 - Test loss: 1.18929\n",
      "Epoch 2483 - lr: 0.05000 - Train loss: 1.21597 - Test loss: 1.18558\n",
      "Epoch 2484 - lr: 0.05000 - Train loss: 1.19871 - Test loss: 1.18831\n",
      "Epoch 2485 - lr: 0.05000 - Train loss: 1.21603 - Test loss: 1.18361\n",
      "Epoch 2486 - lr: 0.05000 - Train loss: 1.21840 - Test loss: 1.19287\n",
      "Epoch 2487 - lr: 0.05000 - Train loss: 1.22037 - Test loss: 1.18807\n",
      "Epoch 2488 - lr: 0.05000 - Train loss: 1.20855 - Test loss: 1.19851\n",
      "Epoch 2489 - lr: 0.05000 - Train loss: 1.22091 - Test loss: 1.19438\n",
      "Epoch 2490 - lr: 0.05000 - Train loss: 1.20023 - Test loss: 1.18883\n",
      "Epoch 2491 - lr: 0.05000 - Train loss: 1.21652 - Test loss: 1.17282\n",
      "Epoch 2492 - lr: 0.05000 - Train loss: 1.21147 - Test loss: 1.23444\n",
      "Epoch 2493 - lr: 0.05000 - Train loss: 1.20043 - Test loss: 1.20181\n",
      "Epoch 2494 - lr: 0.05000 - Train loss: 1.20084 - Test loss: 1.19621\n",
      "Epoch 2495 - lr: 0.05000 - Train loss: 1.20154 - Test loss: 1.19664\n",
      "Epoch 2496 - lr: 0.05000 - Train loss: 1.20109 - Test loss: 1.19809\n",
      "Epoch 2497 - lr: 0.05000 - Train loss: 1.20186 - Test loss: 1.19618\n",
      "Epoch 2498 - lr: 0.05000 - Train loss: 1.20038 - Test loss: 1.19401\n",
      "Epoch 2499 - lr: 0.05000 - Train loss: 1.20013 - Test loss: 1.19481\n",
      "Epoch 2500 - lr: 0.05000 - Train loss: 1.19983 - Test loss: 1.19056\n",
      "Epoch 2501 - lr: 0.05000 - Train loss: 1.21783 - Test loss: 1.17514\n",
      "Epoch 2502 - lr: 0.05000 - Train loss: 1.19962 - Test loss: 1.20227\n",
      "Epoch 2503 - lr: 0.05000 - Train loss: 1.20248 - Test loss: 1.19961\n",
      "Epoch 2504 - lr: 0.05000 - Train loss: 1.20216 - Test loss: 1.19797\n",
      "Epoch 2505 - lr: 0.05000 - Train loss: 1.20029 - Test loss: 1.19122\n",
      "Epoch 2506 - lr: 0.05000 - Train loss: 1.21825 - Test loss: 1.17660\n",
      "Epoch 2507 - lr: 0.05000 - Train loss: 1.19761 - Test loss: 1.19329\n",
      "Epoch 2508 - lr: 0.05000 - Train loss: 1.19904 - Test loss: 1.19481\n",
      "Epoch 2509 - lr: 0.05000 - Train loss: 1.20135 - Test loss: 1.19820\n",
      "Epoch 2510 - lr: 0.05000 - Train loss: 1.20071 - Test loss: 1.21145\n",
      "Epoch 2511 - lr: 0.05000 - Train loss: 1.20834 - Test loss: 1.20714\n",
      "Epoch 2512 - lr: 0.05000 - Train loss: 1.21520 - Test loss: 1.19631\n",
      "Epoch 2513 - lr: 0.05000 - Train loss: 1.20056 - Test loss: 1.19066\n",
      "Epoch 2514 - lr: 0.05000 - Train loss: 1.21359 - Test loss: 1.19266\n",
      "Epoch 2515 - lr: 0.05000 - Train loss: 1.19873 - Test loss: 1.18911\n",
      "Epoch 2516 - lr: 0.05000 - Train loss: 1.21252 - Test loss: 1.19437\n",
      "Epoch 2517 - lr: 0.05000 - Train loss: 1.20048 - Test loss: 1.19752\n",
      "Epoch 2518 - lr: 0.05000 - Train loss: 1.20051 - Test loss: 1.19056\n",
      "Epoch 2519 - lr: 0.05000 - Train loss: 1.21391 - Test loss: 1.18629\n",
      "Epoch 2520 - lr: 0.05000 - Train loss: 1.21506 - Test loss: 1.17334\n",
      "Epoch 2521 - lr: 0.05000 - Train loss: 1.21126 - Test loss: 1.23449\n",
      "Epoch 2522 - lr: 0.05000 - Train loss: 1.19877 - Test loss: 1.19580\n",
      "Epoch 2523 - lr: 0.05000 - Train loss: 1.21823 - Test loss: 1.23510\n",
      "Epoch 2524 - lr: 0.05000 - Train loss: 1.20060 - Test loss: 1.20480\n",
      "Epoch 2525 - lr: 0.05000 - Train loss: 1.20337 - Test loss: 1.19690\n",
      "Epoch 2526 - lr: 0.05000 - Train loss: 1.20195 - Test loss: 1.20082\n",
      "Epoch 2527 - lr: 0.05000 - Train loss: 1.21938 - Test loss: 1.23626\n",
      "Epoch 2528 - lr: 0.05000 - Train loss: 1.20639 - Test loss: 1.19934\n",
      "Epoch 2529 - lr: 0.05000 - Train loss: 1.22040 - Test loss: 1.19531\n",
      "Epoch 2530 - lr: 0.05000 - Train loss: 1.19915 - Test loss: 1.18999\n",
      "Epoch 2531 - lr: 0.05000 - Train loss: 1.21727 - Test loss: 1.17668\n",
      "Epoch 2532 - lr: 0.05000 - Train loss: 1.19916 - Test loss: 1.20251\n",
      "Epoch 2533 - lr: 0.05000 - Train loss: 1.19875 - Test loss: 1.19187\n",
      "Epoch 2534 - lr: 0.05000 - Train loss: 1.21437 - Test loss: 1.19183\n",
      "Epoch 2535 - lr: 0.05000 - Train loss: 1.21618 - Test loss: 1.18398\n",
      "Epoch 2536 - lr: 0.05000 - Train loss: 1.21040 - Test loss: 1.19477\n",
      "Epoch 2537 - lr: 0.05000 - Train loss: 1.19851 - Test loss: 1.19581\n",
      "Epoch 2538 - lr: 0.05000 - Train loss: 1.20093 - Test loss: 1.19877\n",
      "Epoch 2539 - lr: 0.05000 - Train loss: 1.19763 - Test loss: 1.19840\n",
      "Epoch 2540 - lr: 0.05000 - Train loss: 1.20069 - Test loss: 1.19988\n",
      "Epoch 2541 - lr: 0.05000 - Train loss: 1.20130 - Test loss: 1.19762\n",
      "Epoch 2542 - lr: 0.05000 - Train loss: 1.20021 - Test loss: 1.19768\n",
      "Epoch 2543 - lr: 0.05000 - Train loss: 1.20072 - Test loss: 1.19781\n",
      "Epoch 2544 - lr: 0.05000 - Train loss: 1.19952 - Test loss: 1.19347\n",
      "Epoch 2545 - lr: 0.05000 - Train loss: 1.20343 - Test loss: 1.20766\n",
      "Epoch 2546 - lr: 0.05000 - Train loss: 1.20255 - Test loss: 1.20190\n",
      "Epoch 2547 - lr: 0.05000 - Train loss: 1.20156 - Test loss: 1.19960\n",
      "Epoch 2548 - lr: 0.05000 - Train loss: 1.19953 - Test loss: 1.19312\n",
      "Epoch 2549 - lr: 0.05000 - Train loss: 1.21707 - Test loss: 1.17654\n",
      "Epoch 2550 - lr: 0.05000 - Train loss: 1.20047 - Test loss: 1.20451\n",
      "Epoch 2551 - lr: 0.05000 - Train loss: 1.20276 - Test loss: 1.20031\n",
      "Epoch 2552 - lr: 0.05000 - Train loss: 1.20004 - Test loss: 1.19321\n",
      "Epoch 2553 - lr: 0.05000 - Train loss: 1.21789 - Test loss: 1.17838\n",
      "Epoch 2554 - lr: 0.05000 - Train loss: 1.19659 - Test loss: 1.19400\n",
      "Epoch 2555 - lr: 0.05000 - Train loss: 1.20011 - Test loss: 1.19672\n",
      "Epoch 2556 - lr: 0.05000 - Train loss: 1.20028 - Test loss: 1.19898\n",
      "Epoch 2557 - lr: 0.05000 - Train loss: 1.19921 - Test loss: 1.19871\n",
      "Epoch 2558 - lr: 0.05000 - Train loss: 1.19938 - Test loss: 1.19275\n",
      "Epoch 2559 - lr: 0.05000 - Train loss: 1.20938 - Test loss: 1.19966\n",
      "Epoch 2560 - lr: 0.05000 - Train loss: 1.21608 - Test loss: 1.24748\n",
      "Epoch 2561 - lr: 0.05000 - Train loss: 1.19890 - Test loss: 1.19613\n",
      "Epoch 2562 - lr: 0.05000 - Train loss: 1.19984 - Test loss: 1.20023\n",
      "Epoch 2563 - lr: 0.05000 - Train loss: 1.20088 - Test loss: 1.19828\n",
      "Epoch 2564 - lr: 0.05000 - Train loss: 1.20013 - Test loss: 1.19927\n",
      "Epoch 2565 - lr: 0.05000 - Train loss: 1.19985 - Test loss: 1.20068\n",
      "Epoch 2566 - lr: 0.05000 - Train loss: 1.20062 - Test loss: 1.19685\n",
      "Epoch 2567 - lr: 0.05000 - Train loss: 1.20175 - Test loss: 1.20792\n",
      "Epoch 2568 - lr: 0.05000 - Train loss: 1.19795 - Test loss: 1.19449\n",
      "Epoch 2569 - lr: 0.05000 - Train loss: 1.21735 - Test loss: 1.17854\n",
      "Epoch 2570 - lr: 0.05000 - Train loss: 1.19865 - Test loss: 1.20519\n",
      "Epoch 2571 - lr: 0.05000 - Train loss: 1.20186 - Test loss: 1.20236\n",
      "Epoch 2572 - lr: 0.05000 - Train loss: 1.20123 - Test loss: 1.20022\n",
      "Epoch 2573 - lr: 0.05000 - Train loss: 1.19906 - Test loss: 1.19235\n",
      "Epoch 2574 - lr: 0.05000 - Train loss: 1.21551 - Test loss: 1.17598\n",
      "Epoch 2575 - lr: 0.05000 - Train loss: 1.21044 - Test loss: 1.18727\n",
      "Epoch 2576 - lr: 0.05000 - Train loss: 1.21315 - Test loss: 1.24289\n",
      "Epoch 2577 - lr: 0.05000 - Train loss: 1.19768 - Test loss: 1.19543\n",
      "Epoch 2578 - lr: 0.05000 - Train loss: 1.19993 - Test loss: 1.19940\n",
      "Epoch 2579 - lr: 0.05000 - Train loss: 1.19903 - Test loss: 1.19530\n",
      "Epoch 2580 - lr: 0.05000 - Train loss: 1.21411 - Test loss: 1.24910\n",
      "Epoch 2581 - lr: 0.05000 - Train loss: 1.19748 - Test loss: 1.19690\n",
      "Epoch 2582 - lr: 0.05000 - Train loss: 1.21830 - Test loss: 1.19427\n",
      "Epoch 2583 - lr: 0.05000 - Train loss: 1.19552 - Test loss: 1.19850\n",
      "Epoch 2584 - lr: 0.05000 - Train loss: 1.20050 - Test loss: 1.19974\n",
      "Epoch 2585 - lr: 0.05000 - Train loss: 1.19894 - Test loss: 1.19245\n",
      "Epoch 2586 - lr: 0.05000 - Train loss: 1.21282 - Test loss: 1.19191\n",
      "Epoch 2587 - lr: 0.05000 - Train loss: 1.20240 - Test loss: 1.20867\n",
      "Epoch 2588 - lr: 0.05000 - Train loss: 1.20228 - Test loss: 1.20288\n",
      "Epoch 2589 - lr: 0.05000 - Train loss: 1.20060 - Test loss: 1.19698\n",
      "Epoch 2590 - lr: 0.05000 - Train loss: 1.20425 - Test loss: 1.20873\n",
      "Epoch 2591 - lr: 0.05000 - Train loss: 1.19978 - Test loss: 1.20041\n",
      "Epoch 2592 - lr: 0.05000 - Train loss: 1.20054 - Test loss: 1.20144\n",
      "Epoch 2593 - lr: 0.05000 - Train loss: 1.19660 - Test loss: 1.19981\n",
      "Epoch 2594 - lr: 0.05000 - Train loss: 1.19998 - Test loss: 1.19609\n",
      "Epoch 2595 - lr: 0.05000 - Train loss: 1.20653 - Test loss: 1.20527\n",
      "Epoch 2596 - lr: 0.05000 - Train loss: 1.21880 - Test loss: 1.21654\n",
      "Epoch 2597 - lr: 0.05000 - Train loss: 1.21609 - Test loss: 1.17916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2598 - lr: 0.05000 - Train loss: 1.19517 - Test loss: 1.18951\n",
      "Epoch 2599 - lr: 0.05000 - Train loss: 1.20879 - Test loss: 1.19996\n",
      "Epoch 2600 - lr: 0.05000 - Train loss: 1.21450 - Test loss: 1.20926\n",
      "Epoch 2601 - lr: 0.05000 - Train loss: 1.21262 - Test loss: 1.24413\n",
      "Epoch 2602 - lr: 0.05000 - Train loss: 1.19656 - Test loss: 1.18849\n",
      "Epoch 2603 - lr: 0.05000 - Train loss: 1.21174 - Test loss: 1.19561\n",
      "Epoch 2604 - lr: 0.05000 - Train loss: 1.19780 - Test loss: 1.19582\n",
      "Epoch 2605 - lr: 0.05000 - Train loss: 1.21408 - Test loss: 1.24439\n",
      "Epoch 2606 - lr: 0.05000 - Train loss: 1.19773 - Test loss: 1.19774\n",
      "Epoch 2607 - lr: 0.05000 - Train loss: 1.19742 - Test loss: 1.19649\n",
      "Epoch 2608 - lr: 0.05000 - Train loss: 1.20501 - Test loss: 1.20854\n",
      "Epoch 2609 - lr: 0.05000 - Train loss: 1.19713 - Test loss: 1.20093\n",
      "Epoch 2610 - lr: 0.05000 - Train loss: 1.19882 - Test loss: 1.19682\n",
      "Epoch 2611 - lr: 0.05000 - Train loss: 1.21387 - Test loss: 1.25513\n",
      "Epoch 2612 - lr: 0.05000 - Train loss: 1.19858 - Test loss: 1.19018\n",
      "Epoch 2613 - lr: 0.05000 - Train loss: 1.21495 - Test loss: 1.17667\n",
      "Epoch 2614 - lr: 0.05000 - Train loss: 1.20979 - Test loss: 1.19102\n",
      "Epoch 2615 - lr: 0.05000 - Train loss: 1.19903 - Test loss: 1.20360\n",
      "Epoch 2616 - lr: 0.05000 - Train loss: 1.21983 - Test loss: 1.19481\n",
      "Epoch 2617 - lr: 0.05000 - Train loss: 1.20471 - Test loss: 1.20732\n",
      "Epoch 2618 - lr: 0.05000 - Train loss: 1.21050 - Test loss: 1.20210\n",
      "Epoch 2619 - lr: 0.05000 - Train loss: 1.19756 - Test loss: 1.20479\n",
      "Epoch 2620 - lr: 0.05000 - Train loss: 1.21901 - Test loss: 1.23557\n",
      "Epoch 2621 - lr: 0.05000 - Train loss: 1.20894 - Test loss: 1.19792\n",
      "Epoch 2622 - lr: 0.05000 - Train loss: 1.19523 - Test loss: 1.19471\n",
      "Epoch 2623 - lr: 0.05000 - Train loss: 1.21683 - Test loss: 1.17954\n",
      "Epoch 2624 - lr: 0.05000 - Train loss: 1.19700 - Test loss: 1.20129\n",
      "Epoch 2625 - lr: 0.05000 - Train loss: 1.21961 - Test loss: 1.19500\n",
      "Epoch 2626 - lr: 0.05000 - Train loss: 1.20318 - Test loss: 1.20956\n",
      "Epoch 2627 - lr: 0.05000 - Train loss: 1.20043 - Test loss: 1.20407\n",
      "Epoch 2628 - lr: 0.05000 - Train loss: 1.19989 - Test loss: 1.19596\n",
      "Epoch 2629 - lr: 0.05000 - Train loss: 1.21187 - Test loss: 1.19806\n",
      "Epoch 2630 - lr: 0.05000 - Train loss: 1.19838 - Test loss: 1.19504\n",
      "Epoch 2631 - lr: 0.05000 - Train loss: 1.21088 - Test loss: 1.19926\n",
      "Epoch 2632 - lr: 0.05000 - Train loss: 1.19838 - Test loss: 1.20233\n",
      "Epoch 2633 - lr: 0.05000 - Train loss: 1.19958 - Test loss: 1.19787\n",
      "Epoch 2634 - lr: 0.05000 - Train loss: 1.20446 - Test loss: 1.20864\n",
      "Epoch 2635 - lr: 0.05000 - Train loss: 1.20055 - Test loss: 1.21728\n",
      "Epoch 2636 - lr: 0.05000 - Train loss: 1.20905 - Test loss: 1.21472\n",
      "Epoch 2637 - lr: 0.05000 - Train loss: 1.20280 - Test loss: 1.20230\n",
      "Epoch 2638 - lr: 0.05000 - Train loss: 1.20012 - Test loss: 1.20336\n",
      "Epoch 2639 - lr: 0.05000 - Train loss: 1.21337 - Test loss: 1.19443\n",
      "Epoch 2640 - lr: 0.05000 - Train loss: 1.20112 - Test loss: 1.20467\n",
      "Epoch 2641 - lr: 0.05000 - Train loss: 1.19479 - Test loss: 1.23352\n",
      "Epoch 2642 - lr: 0.05000 - Train loss: 1.20382 - Test loss: 1.20311\n",
      "Epoch 2643 - lr: 0.05000 - Train loss: 1.20135 - Test loss: 1.21168\n",
      "Epoch 2644 - lr: 0.05000 - Train loss: 1.20016 - Test loss: 1.20494\n",
      "Epoch 2645 - lr: 0.05000 - Train loss: 1.19933 - Test loss: 1.19616\n",
      "Epoch 2646 - lr: 0.05000 - Train loss: 1.21229 - Test loss: 1.19611\n",
      "Epoch 2647 - lr: 0.05000 - Train loss: 1.21443 - Test loss: 1.18771\n",
      "Epoch 2648 - lr: 0.05000 - Train loss: 1.20983 - Test loss: 1.19739\n",
      "Epoch 2649 - lr: 0.05000 - Train loss: 1.19825 - Test loss: 1.19751\n",
      "Epoch 2650 - lr: 0.05000 - Train loss: 1.20382 - Test loss: 1.21049\n",
      "Epoch 2651 - lr: 0.05000 - Train loss: 1.19875 - Test loss: 1.20139\n",
      "Epoch 2652 - lr: 0.05000 - Train loss: 1.20094 - Test loss: 1.21182\n",
      "Epoch 2653 - lr: 0.05000 - Train loss: 1.19945 - Test loss: 1.20348\n",
      "Epoch 2654 - lr: 0.05000 - Train loss: 1.19923 - Test loss: 1.20227\n",
      "Epoch 2655 - lr: 0.05000 - Train loss: 1.19929 - Test loss: 1.20122\n",
      "Epoch 2656 - lr: 0.05000 - Train loss: 1.19926 - Test loss: 1.20351\n",
      "Epoch 2657 - lr: 0.05000 - Train loss: 1.19538 - Test loss: 1.19922\n",
      "Epoch 2658 - lr: 0.05000 - Train loss: 1.19986 - Test loss: 1.20820\n",
      "Epoch 2659 - lr: 0.05000 - Train loss: 1.21853 - Test loss: 1.23207\n",
      "Epoch 2660 - lr: 0.05000 - Train loss: 1.21037 - Test loss: 1.19379\n",
      "Epoch 2661 - lr: 0.05000 - Train loss: 1.19900 - Test loss: 1.21062\n",
      "Epoch 2662 - lr: 0.05000 - Train loss: 1.19945 - Test loss: 1.20436\n",
      "Epoch 2663 - lr: 0.05000 - Train loss: 1.19839 - Test loss: 1.19582\n",
      "Epoch 2664 - lr: 0.05000 - Train loss: 1.21185 - Test loss: 1.19548\n",
      "Epoch 2665 - lr: 0.05000 - Train loss: 1.19935 - Test loss: 1.21140\n",
      "Epoch 2666 - lr: 0.05000 - Train loss: 1.20065 - Test loss: 1.20622\n",
      "Epoch 2667 - lr: 0.05000 - Train loss: 1.19989 - Test loss: 1.20199\n",
      "Epoch 2668 - lr: 0.05000 - Train loss: 1.19955 - Test loss: 1.20536\n",
      "Epoch 2669 - lr: 0.05000 - Train loss: 1.21448 - Test loss: 1.25082\n",
      "Epoch 2670 - lr: 0.05000 - Train loss: 1.19762 - Test loss: 1.20078\n",
      "Epoch 2671 - lr: 0.05000 - Train loss: 1.19762 - Test loss: 1.20291\n",
      "Epoch 2672 - lr: 0.05000 - Train loss: 1.19789 - Test loss: 1.19571\n",
      "Epoch 2673 - lr: 0.05000 - Train loss: 1.21187 - Test loss: 1.19182\n",
      "Epoch 2674 - lr: 0.05000 - Train loss: 1.21088 - Test loss: 1.21198\n",
      "Epoch 2675 - lr: 0.05000 - Train loss: 1.21222 - Test loss: 1.17896\n",
      "Epoch 2676 - lr: 0.05000 - Train loss: 1.21106 - Test loss: 1.21468\n",
      "Epoch 2677 - lr: 0.05000 - Train loss: 1.21049 - Test loss: 1.18438\n",
      "Epoch 2678 - lr: 0.05000 - Train loss: 1.21548 - Test loss: 1.20475\n",
      "Epoch 2679 - lr: 0.05000 - Train loss: 1.20032 - Test loss: 1.20145\n",
      "Epoch 2680 - lr: 0.05000 - Train loss: 1.20017 - Test loss: 1.20994\n",
      "Epoch 2681 - lr: 0.05000 - Train loss: 1.21729 - Test loss: 1.24769\n",
      "Epoch 2682 - lr: 0.05000 - Train loss: 1.19763 - Test loss: 1.20212\n",
      "Epoch 2683 - lr: 0.05000 - Train loss: 1.20781 - Test loss: 1.20455\n",
      "Epoch 2684 - lr: 0.05000 - Train loss: 1.21401 - Test loss: 1.23385\n",
      "Epoch 2685 - lr: 0.05000 - Train loss: 1.20941 - Test loss: 1.19800\n",
      "Epoch 2686 - lr: 0.05000 - Train loss: 1.19743 - Test loss: 1.19605\n",
      "Epoch 2687 - lr: 0.05000 - Train loss: 1.21186 - Test loss: 1.19222\n",
      "Epoch 2688 - lr: 0.05000 - Train loss: 1.21085 - Test loss: 1.20217\n",
      "Epoch 2689 - lr: 0.05000 - Train loss: 1.21510 - Test loss: 1.19532\n",
      "Epoch 2690 - lr: 0.05000 - Train loss: 1.19679 - Test loss: 1.19534\n",
      "Epoch 2691 - lr: 0.05000 - Train loss: 1.21164 - Test loss: 1.22388\n",
      "Epoch 2692 - lr: 0.05000 - Train loss: 1.20941 - Test loss: 1.25778\n",
      "Epoch 2693 - lr: 0.05000 - Train loss: 1.19786 - Test loss: 1.19322\n",
      "Epoch 2694 - lr: 0.05000 - Train loss: 1.21125 - Test loss: 1.25677\n",
      "Epoch 2695 - lr: 0.05000 - Train loss: 1.19671 - Test loss: 1.20272\n",
      "Epoch 2696 - lr: 0.05000 - Train loss: 1.21679 - Test loss: 1.19803\n",
      "Epoch 2697 - lr: 0.05000 - Train loss: 1.19477 - Test loss: 1.19628\n",
      "Epoch 2698 - lr: 0.05000 - Train loss: 1.21175 - Test loss: 1.25485\n",
      "Epoch 2699 - lr: 0.05000 - Train loss: 1.19621 - Test loss: 1.19311\n",
      "Epoch 2700 - lr: 0.05000 - Train loss: 1.21367 - Test loss: 1.17935\n",
      "Epoch 2701 - lr: 0.05000 - Train loss: 1.20886 - Test loss: 1.23007\n",
      "Epoch 2702 - lr: 0.05000 - Train loss: 1.20790 - Test loss: 1.19893\n",
      "Epoch 2703 - lr: 0.05000 - Train loss: 1.19776 - Test loss: 1.20470\n",
      "Epoch 2704 - lr: 0.05000 - Train loss: 1.19864 - Test loss: 1.19899\n",
      "Epoch 2705 - lr: 0.05000 - Train loss: 1.21013 - Test loss: 1.20248\n",
      "Epoch 2706 - lr: 0.05000 - Train loss: 1.19720 - Test loss: 1.20445\n",
      "Epoch 2707 - lr: 0.05000 - Train loss: 1.19742 - Test loss: 1.20011\n",
      "Epoch 2708 - lr: 0.05000 - Train loss: 1.21260 - Test loss: 1.24080\n",
      "Epoch 2709 - lr: 0.05000 - Train loss: 1.19626 - Test loss: 1.20426\n",
      "Epoch 2710 - lr: 0.05000 - Train loss: 1.21752 - Test loss: 1.26042\n",
      "Epoch 2711 - lr: 0.05000 - Train loss: 1.19763 - Test loss: 1.19764\n",
      "Epoch 2712 - lr: 0.05000 - Train loss: 1.21300 - Test loss: 1.22968\n",
      "Epoch 2713 - lr: 0.05000 - Train loss: 1.20830 - Test loss: 1.19806\n",
      "Epoch 2714 - lr: 0.05000 - Train loss: 1.19645 - Test loss: 1.19956\n",
      "Epoch 2715 - lr: 0.05000 - Train loss: 1.21254 - Test loss: 1.26060\n",
      "Epoch 2716 - lr: 0.05000 - Train loss: 1.19828 - Test loss: 1.19733\n",
      "Epoch 2717 - lr: 0.05000 - Train loss: 1.20375 - Test loss: 1.21164\n",
      "Epoch 2718 - lr: 0.05000 - Train loss: 1.19712 - Test loss: 1.21907\n",
      "Epoch 2719 - lr: 0.05000 - Train loss: 1.20305 - Test loss: 1.20730\n",
      "Epoch 2720 - lr: 0.05000 - Train loss: 1.19915 - Test loss: 1.20676\n",
      "Epoch 2721 - lr: 0.05000 - Train loss: 1.19828 - Test loss: 1.19889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2722 - lr: 0.05000 - Train loss: 1.21137 - Test loss: 1.22327\n",
      "Epoch 2723 - lr: 0.05000 - Train loss: 1.20983 - Test loss: 1.21831\n",
      "Epoch 2724 - lr: 0.05000 - Train loss: 1.20777 - Test loss: 1.22254\n",
      "Epoch 2725 - lr: 0.05000 - Train loss: 1.20853 - Test loss: 1.24999\n",
      "Epoch 2726 - lr: 0.05000 - Train loss: 1.19580 - Test loss: 1.19948\n",
      "Epoch 2727 - lr: 0.05000 - Train loss: 1.19674 - Test loss: 1.20047\n",
      "Epoch 2728 - lr: 0.05000 - Train loss: 1.19899 - Test loss: 1.21335\n",
      "Epoch 2729 - lr: 0.05000 - Train loss: 1.19614 - Test loss: 1.20723\n",
      "Epoch 2730 - lr: 0.05000 - Train loss: 1.19909 - Test loss: 1.20706\n",
      "Epoch 2731 - lr: 0.05000 - Train loss: 1.19800 - Test loss: 1.19950\n",
      "Epoch 2732 - lr: 0.05000 - Train loss: 1.21297 - Test loss: 1.18680\n",
      "Epoch 2733 - lr: 0.05000 - Train loss: 1.21294 - Test loss: 1.19380\n",
      "Epoch 2734 - lr: 0.05000 - Train loss: 1.19585 - Test loss: 1.20275\n",
      "Epoch 2735 - lr: 0.05000 - Train loss: 1.19790 - Test loss: 1.20114\n",
      "Epoch 2736 - lr: 0.05000 - Train loss: 1.20451 - Test loss: 1.21156\n",
      "Epoch 2737 - lr: 0.05000 - Train loss: 1.21365 - Test loss: 1.25306\n",
      "Epoch 2738 - lr: 0.05000 - Train loss: 1.19722 - Test loss: 1.20459\n",
      "Epoch 2739 - lr: 0.05000 - Train loss: 1.19579 - Test loss: 1.21653\n",
      "Epoch 2740 - lr: 0.05000 - Train loss: 1.20134 - Test loss: 1.20699\n",
      "Epoch 2741 - lr: 0.05000 - Train loss: 1.19844 - Test loss: 1.20631\n",
      "Epoch 2742 - lr: 0.05000 - Train loss: 1.19810 - Test loss: 1.20748\n",
      "Epoch 2743 - lr: 0.05000 - Train loss: 1.19826 - Test loss: 1.20127\n",
      "Epoch 2744 - lr: 0.05000 - Train loss: 1.20833 - Test loss: 1.20571\n",
      "Epoch 2745 - lr: 0.05000 - Train loss: 1.20280 - Test loss: 1.21308\n",
      "Epoch 2746 - lr: 0.05000 - Train loss: 1.20634 - Test loss: 1.21104\n",
      "Epoch 2747 - lr: 0.05000 - Train loss: 1.21857 - Test loss: 1.19792\n",
      "Epoch 2748 - lr: 0.05000 - Train loss: 1.21244 - Test loss: 1.18746\n",
      "Epoch 2749 - lr: 0.05000 - Train loss: 1.21304 - Test loss: 1.19529\n",
      "Epoch 2750 - lr: 0.05000 - Train loss: 1.19494 - Test loss: 1.20048\n",
      "Epoch 2751 - lr: 0.05000 - Train loss: 1.21299 - Test loss: 1.21089\n",
      "Epoch 2752 - lr: 0.05000 - Train loss: 1.21011 - Test loss: 1.26180\n",
      "Epoch 2753 - lr: 0.05000 - Train loss: 1.19814 - Test loss: 1.20049\n",
      "Epoch 2754 - lr: 0.05000 - Train loss: 1.19845 - Test loss: 1.21208\n",
      "Epoch 2755 - lr: 0.05000 - Train loss: 1.21704 - Test loss: 1.24346\n",
      "Epoch 2756 - lr: 0.05000 - Train loss: 1.19831 - Test loss: 1.21540\n",
      "Epoch 2757 - lr: 0.05000 - Train loss: 1.20181 - Test loss: 1.20829\n",
      "Epoch 2758 - lr: 0.05000 - Train loss: 1.19803 - Test loss: 1.20824\n",
      "Epoch 2759 - lr: 0.05000 - Train loss: 1.19790 - Test loss: 1.20036\n",
      "Epoch 2760 - lr: 0.05000 - Train loss: 1.21093 - Test loss: 1.25123\n",
      "Epoch 2761 - lr: 0.05000 - Train loss: 1.19656 - Test loss: 1.20590\n",
      "Epoch 2762 - lr: 0.05000 - Train loss: 1.21271 - Test loss: 1.25366\n",
      "Epoch 2763 - lr: 0.05000 - Train loss: 1.19680 - Test loss: 1.20526\n",
      "Epoch 2764 - lr: 0.05000 - Train loss: 1.19476 - Test loss: 1.20778\n",
      "Epoch 2765 - lr: 0.05000 - Train loss: 1.19564 - Test loss: 1.20083\n",
      "Epoch 2766 - lr: 0.05000 - Train loss: 1.21129 - Test loss: 1.20965\n",
      "Epoch 2767 - lr: 0.05000 - Train loss: 1.21329 - Test loss: 1.19460\n",
      "Epoch 2768 - lr: 0.05000 - Train loss: 1.19863 - Test loss: 1.21640\n",
      "Epoch 2769 - lr: 0.05000 - Train loss: 1.19999 - Test loss: 1.20146\n",
      "Epoch 2770 - lr: 0.05000 - Train loss: 1.21338 - Test loss: 1.18374\n",
      "Epoch 2771 - lr: 0.05000 - Train loss: 1.21260 - Test loss: 1.18307\n",
      "Epoch 2772 - lr: 0.05000 - Train loss: 1.19388 - Test loss: 1.20234\n",
      "Epoch 2773 - lr: 0.05000 - Train loss: 1.19757 - Test loss: 1.20778\n",
      "Epoch 2774 - lr: 0.05000 - Train loss: 1.19793 - Test loss: 1.20207\n",
      "Epoch 2775 - lr: 0.05000 - Train loss: 1.20894 - Test loss: 1.20574\n",
      "Epoch 2776 - lr: 0.05000 - Train loss: 1.19428 - Test loss: 1.20048\n",
      "Epoch 2777 - lr: 0.05000 - Train loss: 1.21152 - Test loss: 1.22804\n",
      "Epoch 2778 - lr: 0.05000 - Train loss: 1.20722 - Test loss: 1.19876\n",
      "Epoch 2779 - lr: 0.05000 - Train loss: 1.19527 - Test loss: 1.20023\n",
      "Epoch 2780 - lr: 0.05000 - Train loss: 1.20404 - Test loss: 1.21216\n",
      "Epoch 2781 - lr: 0.05000 - Train loss: 1.21596 - Test loss: 1.24418\n",
      "Epoch 2782 - lr: 0.05000 - Train loss: 1.19796 - Test loss: 1.21618\n",
      "Epoch 2783 - lr: 0.05000 - Train loss: 1.20120 - Test loss: 1.20874\n",
      "Epoch 2784 - lr: 0.05000 - Train loss: 1.19843 - Test loss: 1.20867\n",
      "Epoch 2785 - lr: 0.05000 - Train loss: 1.19692 - Test loss: 1.20407\n",
      "Epoch 2786 - lr: 0.05000 - Train loss: 1.21186 - Test loss: 1.24376\n",
      "Epoch 2787 - lr: 0.05000 - Train loss: 1.19552 - Test loss: 1.20876\n",
      "Epoch 2788 - lr: 0.05000 - Train loss: 1.21769 - Test loss: 1.20338\n",
      "Epoch 2789 - lr: 0.05000 - Train loss: 1.21300 - Test loss: 1.19560\n",
      "Epoch 2790 - lr: 0.05000 - Train loss: 1.21122 - Test loss: 1.22207\n",
      "Epoch 2791 - lr: 0.05000 - Train loss: 1.20904 - Test loss: 1.20426\n",
      "Epoch 2792 - lr: 0.05000 - Train loss: 1.20931 - Test loss: 1.23205\n",
      "Epoch 2793 - lr: 0.05000 - Train loss: 1.20633 - Test loss: 1.20259\n",
      "Epoch 2794 - lr: 0.05000 - Train loss: 1.19613 - Test loss: 1.20782\n",
      "Epoch 2795 - lr: 0.05000 - Train loss: 1.19666 - Test loss: 1.20501\n",
      "Epoch 2796 - lr: 0.05000 - Train loss: 1.21207 - Test loss: 1.24901\n",
      "Epoch 2797 - lr: 0.05000 - Train loss: 1.19597 - Test loss: 1.20901\n",
      "Epoch 2798 - lr: 0.05000 - Train loss: 1.21757 - Test loss: 1.20817\n",
      "Epoch 2799 - lr: 0.05000 - Train loss: 1.21449 - Test loss: 1.19942\n",
      "Epoch 2800 - lr: 0.05000 - Train loss: 1.19467 - Test loss: 1.20054\n",
      "Epoch 2801 - lr: 0.05000 - Train loss: 1.21419 - Test loss: 1.18455\n",
      "Epoch 2802 - lr: 0.05000 - Train loss: 1.19714 - Test loss: 1.21519\n",
      "Epoch 2803 - lr: 0.05000 - Train loss: 1.19960 - Test loss: 1.20215\n",
      "Epoch 2804 - lr: 0.05000 - Train loss: 1.21074 - Test loss: 1.20120\n",
      "Epoch 2805 - lr: 0.05000 - Train loss: 1.19770 - Test loss: 1.21507\n",
      "Epoch 2806 - lr: 0.05000 - Train loss: 1.20150 - Test loss: 1.22056\n",
      "Epoch 2807 - lr: 0.05000 - Train loss: 1.20047 - Test loss: 1.20421\n",
      "Epoch 2808 - lr: 0.05000 - Train loss: 1.21179 - Test loss: 1.22390\n",
      "Epoch 2809 - lr: 0.05000 - Train loss: 1.20718 - Test loss: 1.24737\n",
      "Epoch 2810 - lr: 0.05000 - Train loss: 1.19609 - Test loss: 1.21290\n",
      "Epoch 2811 - lr: 0.05000 - Train loss: 1.20540 - Test loss: 1.21315\n",
      "Epoch 2812 - lr: 0.05000 - Train loss: 1.21835 - Test loss: 1.20123\n",
      "Epoch 2813 - lr: 0.05000 - Train loss: 1.20870 - Test loss: 1.20616\n",
      "Epoch 2814 - lr: 0.05000 - Train loss: 1.19619 - Test loss: 1.20908\n",
      "Epoch 2815 - lr: 0.05000 - Train loss: 1.19675 - Test loss: 1.20338\n",
      "Epoch 2816 - lr: 0.05000 - Train loss: 1.21499 - Test loss: 1.18728\n",
      "Epoch 2817 - lr: 0.05000 - Train loss: 1.19314 - Test loss: 1.20279\n",
      "Epoch 2818 - lr: 0.05000 - Train loss: 1.19539 - Test loss: 1.20091\n",
      "Epoch 2819 - lr: 0.05000 - Train loss: 1.21415 - Test loss: 1.18661\n",
      "Epoch 2820 - lr: 0.05000 - Train loss: 1.19496 - Test loss: 1.21158\n",
      "Epoch 2821 - lr: 0.05000 - Train loss: 1.21368 - Test loss: 1.26324\n",
      "Epoch 2822 - lr: 0.05000 - Train loss: 1.19568 - Test loss: 1.20723\n",
      "Epoch 2823 - lr: 0.05000 - Train loss: 1.21497 - Test loss: 1.20189\n",
      "Epoch 2824 - lr: 0.05000 - Train loss: 1.19637 - Test loss: 1.20883\n",
      "Epoch 2825 - lr: 0.05000 - Train loss: 1.19646 - Test loss: 1.20474\n",
      "Epoch 2826 - lr: 0.05000 - Train loss: 1.21302 - Test loss: 1.18373\n",
      "Epoch 2827 - lr: 0.05000 - Train loss: 1.20836 - Test loss: 1.20941\n",
      "Epoch 2828 - lr: 0.05000 - Train loss: 1.21088 - Test loss: 1.18269\n",
      "Epoch 2829 - lr: 0.05000 - Train loss: 1.19483 - Test loss: 1.21427\n",
      "Epoch 2830 - lr: 0.05000 - Train loss: 1.19940 - Test loss: 1.21213\n",
      "Epoch 2831 - lr: 0.05000 - Train loss: 1.19797 - Test loss: 1.20635\n",
      "Epoch 2832 - lr: 0.05000 - Train loss: 1.20258 - Test loss: 1.21664\n",
      "Epoch 2833 - lr: 0.05000 - Train loss: 1.19763 - Test loss: 1.22612\n",
      "Epoch 2834 - lr: 0.05000 - Train loss: 1.21222 - Test loss: 1.21710\n",
      "Epoch 2835 - lr: 0.05000 - Train loss: 1.21774 - Test loss: 1.20240\n",
      "Epoch 2836 - lr: 0.05000 - Train loss: 1.20906 - Test loss: 1.20609\n",
      "Epoch 2837 - lr: 0.05000 - Train loss: 1.19674 - Test loss: 1.20854\n",
      "Epoch 2838 - lr: 0.05000 - Train loss: 1.19628 - Test loss: 1.20629\n",
      "Epoch 2839 - lr: 0.05000 - Train loss: 1.19726 - Test loss: 1.21086\n",
      "Epoch 2840 - lr: 0.05000 - Train loss: 1.20843 - Test loss: 1.20892\n",
      "Epoch 2841 - lr: 0.05000 - Train loss: 1.19543 - Test loss: 1.20801\n",
      "Epoch 2842 - lr: 0.05000 - Train loss: 1.19810 - Test loss: 1.21364\n",
      "Epoch 2843 - lr: 0.05000 - Train loss: 1.21737 - Test loss: 1.20421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2844 - lr: 0.05000 - Train loss: 1.20111 - Test loss: 1.21739\n",
      "Epoch 2845 - lr: 0.05000 - Train loss: 1.19501 - Test loss: 1.20518\n",
      "Epoch 2846 - lr: 0.05000 - Train loss: 1.21177 - Test loss: 1.20804\n",
      "Epoch 2847 - lr: 0.05000 - Train loss: 1.20929 - Test loss: 1.21969\n",
      "Epoch 2848 - lr: 0.05000 - Train loss: 1.21207 - Test loss: 1.18618\n",
      "Epoch 2849 - lr: 0.05000 - Train loss: 1.19225 - Test loss: 1.19910\n",
      "Epoch 2850 - lr: 0.05000 - Train loss: 1.20671 - Test loss: 1.20888\n",
      "Epoch 2851 - lr: 0.05000 - Train loss: 1.20544 - Test loss: 1.21142\n",
      "Epoch 2852 - lr: 0.05000 - Train loss: 1.21391 - Test loss: 1.26395\n",
      "Epoch 2853 - lr: 0.05000 - Train loss: 1.19507 - Test loss: 1.20492\n",
      "Epoch 2854 - lr: 0.05000 - Train loss: 1.21286 - Test loss: 1.19533\n",
      "Epoch 2855 - lr: 0.05000 - Train loss: 1.20856 - Test loss: 1.20203\n",
      "Epoch 2856 - lr: 0.05000 - Train loss: 1.19689 - Test loss: 1.21441\n",
      "Epoch 2857 - lr: 0.05000 - Train loss: 1.21732 - Test loss: 1.20480\n",
      "Epoch 2858 - lr: 0.05000 - Train loss: 1.20131 - Test loss: 1.21762\n",
      "Epoch 2859 - lr: 0.05000 - Train loss: 1.19429 - Test loss: 1.20978\n",
      "Epoch 2860 - lr: 0.05000 - Train loss: 1.19676 - Test loss: 1.20985\n",
      "Epoch 2861 - lr: 0.05000 - Train loss: 1.19692 - Test loss: 1.20697\n",
      "Epoch 2862 - lr: 0.05000 - Train loss: 1.20210 - Test loss: 1.21755\n",
      "Epoch 2863 - lr: 0.05000 - Train loss: 1.20320 - Test loss: 1.21801\n",
      "Epoch 2864 - lr: 0.05000 - Train loss: 1.21553 - Test loss: 1.26874\n",
      "Epoch 2865 - lr: 0.05000 - Train loss: 1.19558 - Test loss: 1.20660\n",
      "Epoch 2866 - lr: 0.05000 - Train loss: 1.21275 - Test loss: 1.19575\n",
      "Epoch 2867 - lr: 0.05000 - Train loss: 1.20860 - Test loss: 1.20021\n",
      "Epoch 2868 - lr: 0.05000 - Train loss: 1.20379 - Test loss: 1.21213\n",
      "Epoch 2869 - lr: 0.05000 - Train loss: 1.21677 - Test loss: 1.20580\n",
      "Epoch 2870 - lr: 0.05000 - Train loss: 1.19797 - Test loss: 1.21771\n",
      "Epoch 2871 - lr: 0.05000 - Train loss: 1.20729 - Test loss: 1.21268\n",
      "Epoch 2872 - lr: 0.05000 - Train loss: 1.20784 - Test loss: 1.21069\n",
      "Epoch 2873 - lr: 0.05000 - Train loss: 1.19289 - Test loss: 1.20378\n",
      "Epoch 2874 - lr: 0.05000 - Train loss: 1.20997 - Test loss: 1.26563\n",
      "Epoch 2875 - lr: 0.05000 - Train loss: 1.19472 - Test loss: 1.20544\n",
      "Epoch 2876 - lr: 0.05000 - Train loss: 1.21196 - Test loss: 1.20017\n",
      "Epoch 2877 - lr: 0.05000 - Train loss: 1.21150 - Test loss: 1.19472\n",
      "Epoch 2878 - lr: 0.05000 - Train loss: 1.20840 - Test loss: 1.26680\n",
      "Epoch 2879 - lr: 0.05000 - Train loss: 1.19594 - Test loss: 1.20094\n",
      "Epoch 2880 - lr: 0.05000 - Train loss: 1.20965 - Test loss: 1.26300\n",
      "Epoch 2881 - lr: 0.05000 - Train loss: 1.19432 - Test loss: 1.20692\n",
      "Epoch 2882 - lr: 0.05000 - Train loss: 1.21339 - Test loss: 1.20113\n",
      "Epoch 2883 - lr: 0.05000 - Train loss: 1.19568 - Test loss: 1.21172\n",
      "Epoch 2884 - lr: 0.05000 - Train loss: 1.21002 - Test loss: 1.20789\n",
      "Epoch 2885 - lr: 0.05000 - Train loss: 1.19498 - Test loss: 1.20339\n",
      "Epoch 2886 - lr: 0.05000 - Train loss: 1.20964 - Test loss: 1.22156\n",
      "Epoch 2887 - lr: 0.05000 - Train loss: 1.21084 - Test loss: 1.18401\n",
      "Epoch 2888 - lr: 0.05000 - Train loss: 1.20660 - Test loss: 1.23036\n",
      "Epoch 2889 - lr: 0.05000 - Train loss: 1.20704 - Test loss: 1.26163\n",
      "Epoch 2890 - lr: 0.05000 - Train loss: 1.19402 - Test loss: 1.20045\n",
      "Epoch 2891 - lr: 0.05000 - Train loss: 1.21031 - Test loss: 1.21688\n",
      "Epoch 2892 - lr: 0.05000 - Train loss: 1.21194 - Test loss: 1.18737\n",
      "Epoch 2893 - lr: 0.05000 - Train loss: 1.19191 - Test loss: 1.20395\n",
      "Epoch 2894 - lr: 0.05000 - Train loss: 1.21101 - Test loss: 1.24112\n",
      "Epoch 2895 - lr: 0.05000 - Train loss: 1.20518 - Test loss: 1.20748\n",
      "Epoch 2896 - lr: 0.05000 - Train loss: 1.19356 - Test loss: 1.20631\n",
      "Epoch 2897 - lr: 0.05000 - Train loss: 1.20787 - Test loss: 1.21044\n",
      "Epoch 2898 - lr: 0.05000 - Train loss: 1.19324 - Test loss: 1.20537\n",
      "Epoch 2899 - lr: 0.05000 - Train loss: 1.20970 - Test loss: 1.24291\n",
      "Epoch 2900 - lr: 0.05000 - Train loss: 1.20626 - Test loss: 1.20690\n",
      "Epoch 2901 - lr: 0.05000 - Train loss: 1.19592 - Test loss: 1.21208\n",
      "Epoch 2902 - lr: 0.05000 - Train loss: 1.19554 - Test loss: 1.20782\n",
      "Epoch 2903 - lr: 0.05000 - Train loss: 1.21265 - Test loss: 1.18668\n",
      "Epoch 2904 - lr: 0.05000 - Train loss: 1.20593 - Test loss: 1.20428\n",
      "Epoch 2905 - lr: 0.05000 - Train loss: 1.19400 - Test loss: 1.20699\n",
      "Epoch 2906 - lr: 0.05000 - Train loss: 1.21053 - Test loss: 1.24508\n",
      "Epoch 2907 - lr: 0.05000 - Train loss: 1.19550 - Test loss: 1.21964\n",
      "Epoch 2908 - lr: 0.05000 - Train loss: 1.19848 - Test loss: 1.20630\n",
      "Epoch 2909 - lr: 0.05000 - Train loss: 1.20990 - Test loss: 1.21511\n",
      "Epoch 2910 - lr: 0.05000 - Train loss: 1.21165 - Test loss: 1.19849\n",
      "Epoch 2911 - lr: 0.05000 - Train loss: 1.20418 - Test loss: 1.21158\n",
      "Epoch 2912 - lr: 0.05000 - Train loss: 1.21246 - Test loss: 1.26760\n",
      "Epoch 2913 - lr: 0.05000 - Train loss: 1.19486 - Test loss: 1.20870\n",
      "Epoch 2914 - lr: 0.05000 - Train loss: 1.21268 - Test loss: 1.20029\n",
      "Epoch 2915 - lr: 0.05000 - Train loss: 1.19970 - Test loss: 1.21907\n",
      "Epoch 2916 - lr: 0.05000 - Train loss: 1.19367 - Test loss: 1.20852\n",
      "Epoch 2917 - lr: 0.05000 - Train loss: 1.21488 - Test loss: 1.19135\n",
      "Epoch 2918 - lr: 0.05000 - Train loss: 1.19162 - Test loss: 1.20087\n",
      "Epoch 2919 - lr: 0.05000 - Train loss: 1.20899 - Test loss: 1.21294\n",
      "Epoch 2920 - lr: 0.05000 - Train loss: 1.21197 - Test loss: 1.20141\n",
      "Epoch 2921 - lr: 0.05000 - Train loss: 1.19559 - Test loss: 1.21585\n",
      "Epoch 2922 - lr: 0.05000 - Train loss: 1.21672 - Test loss: 1.21059\n",
      "Epoch 2923 - lr: 0.05000 - Train loss: 1.19491 - Test loss: 1.20634\n",
      "Epoch 2924 - lr: 0.05000 - Train loss: 1.20641 - Test loss: 1.21196\n",
      "Epoch 2925 - lr: 0.05000 - Train loss: 1.19548 - Test loss: 1.22649\n",
      "Epoch 2926 - lr: 0.05000 - Train loss: 1.20420 - Test loss: 1.22434\n",
      "Epoch 2927 - lr: 0.05000 - Train loss: 1.19624 - Test loss: 1.21201\n",
      "Epoch 2928 - lr: 0.05000 - Train loss: 1.20212 - Test loss: 1.22021\n",
      "Epoch 2929 - lr: 0.05000 - Train loss: 1.21034 - Test loss: 1.21053\n",
      "Epoch 2930 - lr: 0.05000 - Train loss: 1.19490 - Test loss: 1.20775\n",
      "Epoch 2931 - lr: 0.05000 - Train loss: 1.20143 - Test loss: 1.21920\n",
      "Epoch 2932 - lr: 0.05000 - Train loss: 1.21179 - Test loss: 1.21119\n",
      "Epoch 2933 - lr: 0.05000 - Train loss: 1.21122 - Test loss: 1.20625\n",
      "Epoch 2934 - lr: 0.05000 - Train loss: 1.21216 - Test loss: 1.20358\n",
      "Epoch 2935 - lr: 0.05000 - Train loss: 1.19335 - Test loss: 1.20416\n",
      "Epoch 2936 - lr: 0.05000 - Train loss: 1.20928 - Test loss: 1.26289\n",
      "Epoch 2937 - lr: 0.05000 - Train loss: 1.19345 - Test loss: 1.20221\n",
      "Epoch 2938 - lr: 0.05000 - Train loss: 1.20893 - Test loss: 1.20174\n",
      "Epoch 2939 - lr: 0.05000 - Train loss: 1.21163 - Test loss: 1.18900\n",
      "Epoch 2940 - lr: 0.05000 - Train loss: 1.20151 - Test loss: 1.21141\n",
      "Epoch 2941 - lr: 0.05000 - Train loss: 1.21487 - Test loss: 1.23489\n",
      "Epoch 2942 - lr: 0.05000 - Train loss: 1.20834 - Test loss: 1.22669\n",
      "Epoch 2943 - lr: 0.05000 - Train loss: 1.20564 - Test loss: 1.27047\n",
      "Epoch 2944 - lr: 0.05000 - Train loss: 1.19612 - Test loss: 1.20658\n",
      "Epoch 2945 - lr: 0.05000 - Train loss: 1.20193 - Test loss: 1.21894\n",
      "Epoch 2946 - lr: 0.05000 - Train loss: 1.21431 - Test loss: 1.27329\n",
      "Epoch 2947 - lr: 0.05000 - Train loss: 1.19519 - Test loss: 1.20838\n",
      "Epoch 2948 - lr: 0.05000 - Train loss: 1.21000 - Test loss: 1.27120\n",
      "Epoch 2949 - lr: 0.05000 - Train loss: 1.19561 - Test loss: 1.20476\n",
      "Epoch 2950 - lr: 0.05000 - Train loss: 1.20860 - Test loss: 1.20861\n",
      "Epoch 2951 - lr: 0.05000 - Train loss: 1.19419 - Test loss: 1.20985\n",
      "Epoch 2952 - lr: 0.05000 - Train loss: 1.19665 - Test loss: 1.21768\n",
      "Epoch 2953 - lr: 0.05000 - Train loss: 1.21628 - Test loss: 1.21258\n",
      "Epoch 2954 - lr: 0.05000 - Train loss: 1.19470 - Test loss: 1.20879\n",
      "Epoch 2955 - lr: 0.05000 - Train loss: 1.21393 - Test loss: 1.19224\n",
      "Epoch 2956 - lr: 0.05000 - Train loss: 1.19117 - Test loss: 1.20320\n",
      "Epoch 2957 - lr: 0.05000 - Train loss: 1.20715 - Test loss: 1.21099\n",
      "Epoch 2958 - lr: 0.05000 - Train loss: 1.19533 - Test loss: 1.21570\n",
      "Epoch 2959 - lr: 0.05000 - Train loss: 1.19651 - Test loss: 1.21371\n",
      "Epoch 2960 - lr: 0.05000 - Train loss: 1.19604 - Test loss: 1.21498\n",
      "Epoch 2961 - lr: 0.05000 - Train loss: 1.19334 - Test loss: 1.20909\n",
      "Epoch 2962 - lr: 0.05000 - Train loss: 1.20849 - Test loss: 1.21071\n",
      "Epoch 2963 - lr: 0.05000 - Train loss: 1.19436 - Test loss: 1.20761\n",
      "Epoch 2964 - lr: 0.05000 - Train loss: 1.21007 - Test loss: 1.20827\n",
      "Epoch 2965 - lr: 0.05000 - Train loss: 1.20997 - Test loss: 1.20130\n",
      "Epoch 2966 - lr: 0.05000 - Train loss: 1.19523 - Test loss: 1.21945\n",
      "Epoch 2967 - lr: 0.05000 - Train loss: 1.21295 - Test loss: 1.27324\n",
      "Epoch 2968 - lr: 0.05000 - Train loss: 1.19498 - Test loss: 1.20832\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2969 - lr: 0.05000 - Train loss: 1.20989 - Test loss: 1.24620\n",
      "Epoch 2970 - lr: 0.05000 - Train loss: 1.19707 - Test loss: 1.21975\n",
      "Epoch 2971 - lr: 0.05000 - Train loss: 1.19675 - Test loss: 1.21799\n",
      "Epoch 2972 - lr: 0.05000 - Train loss: 1.19660 - Test loss: 1.21219\n",
      "Epoch 2973 - lr: 0.05000 - Train loss: 1.20118 - Test loss: 1.22159\n",
      "Epoch 2974 - lr: 0.05000 - Train loss: 1.21049 - Test loss: 1.20912\n",
      "Epoch 2975 - lr: 0.05000 - Train loss: 1.19996 - Test loss: 1.22200\n",
      "Epoch 2976 - lr: 0.05000 - Train loss: 1.19453 - Test loss: 1.21990\n",
      "Epoch 2977 - lr: 0.05000 - Train loss: 1.21526 - Test loss: 1.25858\n",
      "Epoch 2978 - lr: 0.05000 - Train loss: 1.19887 - Test loss: 1.22070\n",
      "Epoch 2979 - lr: 0.05000 - Train loss: 1.19332 - Test loss: 1.21678\n",
      "Epoch 2980 - lr: 0.05000 - Train loss: 1.19672 - Test loss: 1.21734\n",
      "Epoch 2981 - lr: 0.05000 - Train loss: 1.19614 - Test loss: 1.21190\n",
      "Epoch 2982 - lr: 0.05000 - Train loss: 1.20254 - Test loss: 1.21921\n",
      "Epoch 2983 - lr: 0.05000 - Train loss: 1.21531 - Test loss: 1.21508\n",
      "Epoch 2984 - lr: 0.05000 - Train loss: 1.19602 - Test loss: 1.21646\n",
      "Epoch 2985 - lr: 0.05000 - Train loss: 1.19534 - Test loss: 1.20911\n",
      "Epoch 2986 - lr: 0.05000 - Train loss: 1.20871 - Test loss: 1.22343\n",
      "Epoch 2987 - lr: 0.05000 - Train loss: 1.20840 - Test loss: 1.23147\n",
      "Epoch 2988 - lr: 0.05000 - Train loss: 1.20510 - Test loss: 1.20018\n",
      "Epoch 2989 - lr: 0.05000 - Train loss: 1.21146 - Test loss: 1.19160\n",
      "Epoch 2990 - lr: 0.05000 - Train loss: 1.19336 - Test loss: 1.21760\n",
      "Epoch 2991 - lr: 0.05000 - Train loss: 1.21416 - Test loss: 1.27409\n",
      "Epoch 2992 - lr: 0.05000 - Train loss: 1.19431 - Test loss: 1.21135\n",
      "Epoch 2993 - lr: 0.05000 - Train loss: 1.21053 - Test loss: 1.21694\n",
      "Epoch 2994 - lr: 0.05000 - Train loss: 1.21012 - Test loss: 1.19983\n",
      "Epoch 2995 - lr: 0.05000 - Train loss: 1.20687 - Test loss: 1.20944\n",
      "Epoch 2996 - lr: 0.05000 - Train loss: 1.19344 - Test loss: 1.20835\n",
      "Epoch 2997 - lr: 0.05000 - Train loss: 1.20656 - Test loss: 1.21372\n",
      "Epoch 2998 - lr: 0.05000 - Train loss: 1.19338 - Test loss: 1.21379\n",
      "Epoch 2999 - lr: 0.05000 - Train loss: 1.19687 - Test loss: 1.22124\n",
      "Epoch 3000 - lr: 0.05000 - Train loss: 1.21583 - Test loss: 1.21252\n",
      "Epoch 3001 - lr: 0.05000 - Train loss: 1.19591 - Test loss: 1.21773\n",
      "Epoch 3002 - lr: 0.05000 - Train loss: 1.21230 - Test loss: 1.27243\n",
      "Epoch 3003 - lr: 0.05000 - Train loss: 1.19371 - Test loss: 1.20649\n",
      "Epoch 3004 - lr: 0.05000 - Train loss: 1.21245 - Test loss: 1.19326\n",
      "Epoch 3005 - lr: 0.05000 - Train loss: 1.19260 - Test loss: 1.21410\n",
      "Epoch 3006 - lr: 0.05000 - Train loss: 1.21082 - Test loss: 1.25868\n",
      "Epoch 3007 - lr: 0.05000 - Train loss: 1.19777 - Test loss: 1.22234\n",
      "Epoch 3008 - lr: 0.05000 - Train loss: 1.19537 - Test loss: 1.21732\n",
      "Epoch 3009 - lr: 0.05000 - Train loss: 1.19562 - Test loss: 1.21641\n",
      "Epoch 3010 - lr: 0.05000 - Train loss: 1.19584 - Test loss: 1.21778\n",
      "Epoch 3011 - lr: 0.05000 - Train loss: 1.19528 - Test loss: 1.21064\n",
      "Epoch 3012 - lr: 0.05000 - Train loss: 1.20819 - Test loss: 1.21045\n",
      "Epoch 3013 - lr: 0.05000 - Train loss: 1.19444 - Test loss: 1.21600\n",
      "Epoch 3014 - lr: 0.05000 - Train loss: 1.19157 - Test loss: 1.21021\n",
      "Epoch 3015 - lr: 0.05000 - Train loss: 1.21086 - Test loss: 1.19173\n",
      "Epoch 3016 - lr: 0.05000 - Train loss: 1.20615 - Test loss: 1.23817\n",
      "Epoch 3017 - lr: 0.05000 - Train loss: 1.20375 - Test loss: 1.20931\n",
      "Epoch 3018 - lr: 0.05000 - Train loss: 1.19444 - Test loss: 1.21561\n",
      "Epoch 3019 - lr: 0.05000 - Train loss: 1.19376 - Test loss: 1.20938\n",
      "Epoch 3020 - lr: 0.05000 - Train loss: 1.21102 - Test loss: 1.18384\n",
      "Epoch 3021 - lr: 0.05000 - Train loss: 1.15292 - Test loss: 1.26527\n",
      "Epoch 3022 - lr: 0.05000 - Train loss: 1.22025 - Test loss: 1.22238\n",
      "Epoch 3023 - lr: 0.05000 - Train loss: 1.19563 - Test loss: 1.21692\n",
      "Epoch 3024 - lr: 0.05000 - Train loss: 1.19410 - Test loss: 1.21371\n",
      "Epoch 3025 - lr: 0.05000 - Train loss: 1.19448 - Test loss: 1.21631\n",
      "Epoch 3026 - lr: 0.05000 - Train loss: 1.19318 - Test loss: 1.21498\n",
      "Epoch 3027 - lr: 0.05000 - Train loss: 1.19494 - Test loss: 1.21526\n",
      "Epoch 3028 - lr: 0.05000 - Train loss: 1.20381 - Test loss: 0.74148\n",
      "Epoch 3029 - lr: 0.05000 - Train loss: 1.19974 - Test loss: 1.07370\n",
      "Epoch 3030 - lr: 0.05000 - Train loss: 0.87563 - Test loss: 0.82343\n",
      "Epoch 3031 - lr: 0.05000 - Train loss: 0.87459 - Test loss: 1.32307\n",
      "Epoch 3032 - lr: 0.05000 - Train loss: 1.22678 - Test loss: 1.23585\n",
      "Epoch 3033 - lr: 0.05000 - Train loss: 1.19901 - Test loss: 1.21735\n",
      "Epoch 3034 - lr: 0.05000 - Train loss: 1.19407 - Test loss: 1.21385\n",
      "Epoch 3035 - lr: 0.05000 - Train loss: 1.19422 - Test loss: 1.21658\n",
      "Epoch 3036 - lr: 0.05000 - Train loss: 1.19409 - Test loss: 1.21857\n",
      "Epoch 3037 - lr: 0.05000 - Train loss: 1.19465 - Test loss: 1.21156\n",
      "Epoch 3038 - lr: 0.05000 - Train loss: 1.20799 - Test loss: 1.20664\n",
      "Epoch 3039 - lr: 0.05000 - Train loss: 1.20817 - Test loss: 1.21379\n",
      "Epoch 3040 - lr: 0.05000 - Train loss: 1.20682 - Test loss: 1.23519\n",
      "Epoch 3041 - lr: 0.05000 - Train loss: 1.20500 - Test loss: 1.25007\n",
      "Epoch 3042 - lr: 0.05000 - Train loss: 1.20157 - Test loss: 1.21625\n",
      "Epoch 3043 - lr: 0.05000 - Train loss: 1.21316 - Test loss: 1.23705\n",
      "Epoch 3044 - lr: 0.05000 - Train loss: 1.20528 - Test loss: 1.20849\n",
      "Epoch 3045 - lr: 0.05000 - Train loss: 1.21016 - Test loss: 1.20288\n",
      "Epoch 3046 - lr: 0.05000 - Train loss: 1.20467 - Test loss: 1.21445\n",
      "Epoch 3047 - lr: 0.05000 - Train loss: 1.19225 - Test loss: 1.21314\n",
      "Epoch 3048 - lr: 0.05000 - Train loss: 1.20312 - Test loss: 1.21963\n",
      "Epoch 3049 - lr: 0.05000 - Train loss: 1.21465 - Test loss: 1.20873\n",
      "Epoch 3050 - lr: 0.05000 - Train loss: 1.20744 - Test loss: 1.20817\n",
      "Epoch 3051 - lr: 0.05000 - Train loss: 1.20972 - Test loss: 1.19001\n",
      "Epoch 3052 - lr: 0.05000 - Train loss: 1.20442 - Test loss: 1.20268\n",
      "Epoch 3053 - lr: 0.05000 - Train loss: 1.21001 - Test loss: 1.18639\n",
      "Epoch 3054 - lr: 0.05000 - Train loss: 1.05639 - Test loss: 0.75306\n",
      "Epoch 3055 - lr: 0.05000 - Train loss: 0.90975 - Test loss: 1.41051\n",
      "Epoch 3056 - lr: 0.05000 - Train loss: 1.17236 - Test loss: 0.98553\n",
      "Epoch 3057 - lr: 0.05000 - Train loss: 0.92584 - Test loss: 0.73853\n",
      "Epoch 3058 - lr: 0.05000 - Train loss: 0.85499 - Test loss: 0.79365\n",
      "Epoch 3059 - lr: 0.05000 - Train loss: 0.87530 - Test loss: 1.32810\n",
      "Epoch 3060 - lr: 0.05000 - Train loss: 1.16480 - Test loss: 1.09249\n",
      "Epoch 3061 - lr: 0.05000 - Train loss: 1.00745 - Test loss: 0.78311\n",
      "Epoch 3062 - lr: 0.05000 - Train loss: 0.75921 - Test loss: 1.42107\n",
      "Epoch 3063 - lr: 0.05000 - Train loss: 1.24394 - Test loss: 0.78221\n",
      "Epoch 3064 - lr: 0.05000 - Train loss: 0.83969 - Test loss: 1.35770\n",
      "Epoch 3065 - lr: 0.05000 - Train loss: 1.22630 - Test loss: 1.23805\n",
      "Epoch 3066 - lr: 0.05000 - Train loss: 1.19889 - Test loss: 1.22325\n",
      "Epoch 3067 - lr: 0.05000 - Train loss: 1.21005 - Test loss: 1.22124\n",
      "Epoch 3068 - lr: 0.05000 - Train loss: 1.20716 - Test loss: 1.27121\n",
      "Epoch 3069 - lr: 0.05000 - Train loss: 1.19336 - Test loss: 1.21012\n",
      "Epoch 3070 - lr: 0.05000 - Train loss: 1.20556 - Test loss: 1.21604\n",
      "Epoch 3071 - lr: 0.05000 - Train loss: 1.19329 - Test loss: 1.21890\n",
      "Epoch 3072 - lr: 0.05000 - Train loss: 1.19310 - Test loss: 1.21549\n",
      "Epoch 3073 - lr: 0.05000 - Train loss: 1.20823 - Test loss: 1.27795\n",
      "Epoch 3074 - lr: 0.05000 - Train loss: 1.19452 - Test loss: 1.21555\n",
      "Epoch 3075 - lr: 0.05000 - Train loss: 1.19267 - Test loss: 1.21146\n",
      "Epoch 3076 - lr: 0.05000 - Train loss: 1.20437 - Test loss: 1.21336\n",
      "Epoch 3077 - lr: 0.05000 - Train loss: 1.11944 - Test loss: 0.69042\n",
      "Epoch 3078 - lr: 0.05000 - Train loss: 0.76628 - Test loss: 1.36813\n",
      "Epoch 3079 - lr: 0.05000 - Train loss: 1.22346 - Test loss: 1.23744\n",
      "Epoch 3080 - lr: 0.05000 - Train loss: 1.19721 - Test loss: 1.21931\n",
      "Epoch 3081 - lr: 0.05000 - Train loss: 1.20481 - Test loss: 0.76922\n",
      "Epoch 3082 - lr: 0.05000 - Train loss: 0.94889 - Test loss: 0.67553\n",
      "Epoch 3083 - lr: 0.05000 - Train loss: 1.15288 - Test loss: 1.24608\n",
      "Epoch 3084 - lr: 0.05000 - Train loss: 1.12707 - Test loss: 0.66016\n",
      "Epoch 3085 - lr: 0.05000 - Train loss: 1.14710 - Test loss: 1.25545\n",
      "Epoch 3086 - lr: 0.05000 - Train loss: 1.20503 - Test loss: 1.17298\n",
      "Epoch 3087 - lr: 0.05000 - Train loss: 0.91009 - Test loss: 0.67932\n",
      "Epoch 3088 - lr: 0.05000 - Train loss: 1.13899 - Test loss: 0.81382\n",
      "Epoch 3089 - lr: 0.05000 - Train loss: 0.79195 - Test loss: 0.66969\n",
      "Epoch 3090 - lr: 0.05000 - Train loss: 1.14299 - Test loss: 1.24814\n",
      "Epoch 3091 - lr: 0.05000 - Train loss: 1.19995 - Test loss: 1.22027\n",
      "Epoch 3092 - lr: 0.05000 - Train loss: 1.19428 - Test loss: 1.21853\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3093 - lr: 0.05000 - Train loss: 1.20817 - Test loss: 1.06137\n",
      "Epoch 3094 - lr: 0.05000 - Train loss: 1.02038 - Test loss: 0.72206\n",
      "Epoch 3095 - lr: 0.05000 - Train loss: 1.07796 - Test loss: 0.81450\n",
      "Epoch 3096 - lr: 0.05000 - Train loss: 0.76120 - Test loss: 0.73527\n",
      "Epoch 3097 - lr: 0.05000 - Train loss: 1.26036 - Test loss: 1.40855\n",
      "Epoch 3098 - lr: 0.05000 - Train loss: 1.26326 - Test loss: 1.35379\n",
      "Epoch 3099 - lr: 0.05000 - Train loss: 1.21673 - Test loss: 1.29697\n",
      "Epoch 3100 - lr: 0.05000 - Train loss: 1.20271 - Test loss: 1.27903\n",
      "Epoch 3101 - lr: 0.05000 - Train loss: 1.20093 - Test loss: 1.27842\n",
      "Epoch 3102 - lr: 0.05000 - Train loss: 1.20025 - Test loss: 1.29417\n",
      "Epoch 3103 - lr: 0.05000 - Train loss: 1.19617 - Test loss: 1.31318\n",
      "Epoch 3104 - lr: 0.05000 - Train loss: 1.16116 - Test loss: 1.26010\n",
      "Epoch 3105 - lr: 0.05000 - Train loss: 1.14349 - Test loss: 1.16971\n",
      "Epoch 3106 - lr: 0.05000 - Train loss: 0.95830 - Test loss: 1.07283\n",
      "Epoch 3107 - lr: 0.05000 - Train loss: 0.91150 - Test loss: 1.05505\n",
      "Epoch 3108 - lr: 0.05000 - Train loss: 0.88931 - Test loss: 1.03114\n",
      "Epoch 3109 - lr: 0.05000 - Train loss: 1.00510 - Test loss: 1.04192\n",
      "Epoch 3110 - lr: 0.05000 - Train loss: 0.89708 - Test loss: 0.89436\n",
      "Epoch 3111 - lr: 0.05000 - Train loss: 0.79360 - Test loss: 0.83118\n",
      "Epoch 3112 - lr: 0.05000 - Train loss: 0.80648 - Test loss: 0.93856\n",
      "Epoch 3113 - lr: 0.05000 - Train loss: 1.04068 - Test loss: 0.79035\n",
      "Epoch 3114 - lr: 0.05000 - Train loss: 0.75667 - Test loss: 0.75207\n",
      "Epoch 3115 - lr: 0.05000 - Train loss: 0.77534 - Test loss: 0.80877\n",
      "Epoch 3116 - lr: 0.05000 - Train loss: 1.15481 - Test loss: 0.91029\n",
      "Epoch 3117 - lr: 0.05000 - Train loss: 0.89713 - Test loss: 0.82952\n",
      "Epoch 3118 - lr: 0.05000 - Train loss: 0.82234 - Test loss: 0.92033\n",
      "Epoch 3119 - lr: 0.05000 - Train loss: 0.85787 - Test loss: 0.94300\n",
      "Epoch 3120 - lr: 0.05000 - Train loss: 0.83544 - Test loss: 0.89003\n",
      "Epoch 3121 - lr: 0.05000 - Train loss: 0.87136 - Test loss: 0.75497\n",
      "Epoch 3122 - lr: 0.05000 - Train loss: 0.92485 - Test loss: 0.92274\n",
      "Epoch 3123 - lr: 0.05000 - Train loss: 0.83043 - Test loss: 0.77695\n",
      "Epoch 3124 - lr: 0.05000 - Train loss: 0.80711 - Test loss: 0.90955\n",
      "Epoch 3125 - lr: 0.05000 - Train loss: 0.85567 - Test loss: 0.90383\n",
      "Epoch 3126 - lr: 0.05000 - Train loss: 0.82199 - Test loss: 0.89703\n",
      "Epoch 3127 - lr: 0.05000 - Train loss: 0.86222 - Test loss: 0.92352\n",
      "Epoch 3128 - lr: 0.05000 - Train loss: 0.77225 - Test loss: 0.75880\n",
      "Epoch 3129 - lr: 0.05000 - Train loss: 1.02121 - Test loss: 1.02399\n",
      "Epoch 3130 - lr: 0.05000 - Train loss: 0.91177 - Test loss: 0.83162\n",
      "Epoch 3131 - lr: 0.05000 - Train loss: 0.80674 - Test loss: 0.76471\n",
      "Epoch 3132 - lr: 0.05000 - Train loss: 0.78882 - Test loss: 0.85048\n",
      "Epoch 3133 - lr: 0.05000 - Train loss: 0.92539 - Test loss: 0.86125\n",
      "Epoch 3134 - lr: 0.05000 - Train loss: 0.83667 - Test loss: 0.75954\n",
      "Epoch 3135 - lr: 0.05000 - Train loss: 0.79367 - Test loss: 0.89642\n",
      "Epoch 3136 - lr: 0.05000 - Train loss: 1.26493 - Test loss: 0.85205\n",
      "Epoch 3137 - lr: 0.05000 - Train loss: 0.86917 - Test loss: 0.95209\n",
      "Epoch 3138 - lr: 0.05000 - Train loss: 0.82979 - Test loss: 0.86508\n",
      "Epoch 3139 - lr: 0.05000 - Train loss: 0.92943 - Test loss: 0.89174\n",
      "Epoch 3140 - lr: 0.05000 - Train loss: 0.80560 - Test loss: 0.78362\n",
      "Epoch 3141 - lr: 0.05000 - Train loss: 0.79564 - Test loss: 0.90262\n",
      "Epoch 3142 - lr: 0.05000 - Train loss: 0.82153 - Test loss: 0.81009\n",
      "Epoch 3143 - lr: 0.05000 - Train loss: 0.84958 - Test loss: 0.78788\n",
      "Epoch 3144 - lr: 0.05000 - Train loss: 0.81358 - Test loss: 0.90296\n",
      "Epoch 3145 - lr: 0.05000 - Train loss: 0.87400 - Test loss: 0.88351\n",
      "Epoch 3146 - lr: 0.05000 - Train loss: 1.09560 - Test loss: 0.84490\n",
      "Epoch 3147 - lr: 0.05000 - Train loss: 0.94199 - Test loss: 0.66260\n",
      "Epoch 3148 - lr: 0.05000 - Train loss: 0.76149 - Test loss: 0.68533\n",
      "Epoch 3149 - lr: 0.05000 - Train loss: 1.06002 - Test loss: 0.88271\n",
      "Epoch 3150 - lr: 0.05000 - Train loss: 0.81466 - Test loss: 0.82607\n",
      "Epoch 3151 - lr: 0.05000 - Train loss: 1.10356 - Test loss: 1.06845\n",
      "Epoch 3152 - lr: 0.05000 - Train loss: 0.92442 - Test loss: 0.88125\n",
      "Epoch 3153 - lr: 0.05000 - Train loss: 1.23964 - Test loss: 0.88383\n",
      "Epoch 3154 - lr: 0.05000 - Train loss: 1.22966 - Test loss: 1.04961\n",
      "Epoch 3155 - lr: 0.05000 - Train loss: 0.90060 - Test loss: 0.70788\n",
      "Epoch 3156 - lr: 0.05000 - Train loss: 0.72451 - Test loss: 0.70589\n",
      "Epoch 3157 - lr: 0.05000 - Train loss: 0.66086 - Test loss: 0.70451\n",
      "Epoch 3158 - lr: 0.05000 - Train loss: 0.65442 - Test loss: 0.69840\n",
      "Epoch 3159 - lr: 0.05000 - Train loss: 0.65079 - Test loss: 0.69420\n",
      "Epoch 3160 - lr: 0.05000 - Train loss: 0.64648 - Test loss: 0.69220\n",
      "Epoch 3161 - lr: 0.05000 - Train loss: 0.64566 - Test loss: 0.69004\n",
      "Epoch 3162 - lr: 0.05000 - Train loss: 0.64231 - Test loss: 0.68934\n",
      "Epoch 3163 - lr: 0.05000 - Train loss: 0.64199 - Test loss: 0.68790\n",
      "Epoch 3164 - lr: 0.05000 - Train loss: 0.64000 - Test loss: 0.68708\n",
      "Epoch 3165 - lr: 0.05000 - Train loss: 0.64012 - Test loss: 0.68546\n",
      "Epoch 3166 - lr: 0.05000 - Train loss: 0.63802 - Test loss: 0.68525\n",
      "Epoch 3167 - lr: 0.05000 - Train loss: 0.63871 - Test loss: 0.68373\n",
      "Epoch 3168 - lr: 0.05000 - Train loss: 0.63684 - Test loss: 0.68357\n",
      "Epoch 3169 - lr: 0.05000 - Train loss: 0.63766 - Test loss: 0.68212\n",
      "Epoch 3170 - lr: 0.05000 - Train loss: 0.63599 - Test loss: 0.68204\n",
      "Epoch 3171 - lr: 0.05000 - Train loss: 0.63689 - Test loss: 0.68069\n",
      "Epoch 3172 - lr: 0.05000 - Train loss: 0.63537 - Test loss: 0.68069\n",
      "Epoch 3173 - lr: 0.05000 - Train loss: 0.63630 - Test loss: 0.67946\n",
      "Epoch 3174 - lr: 0.05000 - Train loss: 0.63491 - Test loss: 0.67950\n",
      "Epoch 3175 - lr: 0.05000 - Train loss: 0.63583 - Test loss: 0.67839\n",
      "Epoch 3176 - lr: 0.05000 - Train loss: 0.63456 - Test loss: 0.67848\n",
      "Epoch 3177 - lr: 0.05000 - Train loss: 0.63546 - Test loss: 0.67747\n",
      "Epoch 3178 - lr: 0.05000 - Train loss: 0.63429 - Test loss: 0.67760\n",
      "Epoch 3179 - lr: 0.05000 - Train loss: 0.63516 - Test loss: 0.67670\n",
      "Epoch 3180 - lr: 0.05000 - Train loss: 0.63407 - Test loss: 0.67685\n",
      "Epoch 3181 - lr: 0.05000 - Train loss: 0.63492 - Test loss: 0.67604\n",
      "Epoch 3182 - lr: 0.05000 - Train loss: 0.63390 - Test loss: 0.67621\n",
      "Epoch 3183 - lr: 0.05000 - Train loss: 0.63471 - Test loss: 0.67548\n",
      "Epoch 3184 - lr: 0.05000 - Train loss: 0.63376 - Test loss: 0.67566\n",
      "Epoch 3185 - lr: 0.05000 - Train loss: 0.63453 - Test loss: 0.67500\n",
      "Epoch 3186 - lr: 0.05000 - Train loss: 0.63365 - Test loss: 0.67520\n",
      "Epoch 3187 - lr: 0.05000 - Train loss: 0.63438 - Test loss: 0.67460\n",
      "Epoch 3188 - lr: 0.05000 - Train loss: 0.63355 - Test loss: 0.67480\n",
      "Epoch 3189 - lr: 0.05000 - Train loss: 0.63424 - Test loss: 0.67426\n",
      "Epoch 3190 - lr: 0.05000 - Train loss: 0.63346 - Test loss: 0.67446\n",
      "Epoch 3191 - lr: 0.05000 - Train loss: 0.63412 - Test loss: 0.67396\n",
      "Epoch 3192 - lr: 0.05000 - Train loss: 0.63339 - Test loss: 0.67417\n",
      "Epoch 3193 - lr: 0.05000 - Train loss: 0.63401 - Test loss: 0.67371\n",
      "Epoch 3194 - lr: 0.05000 - Train loss: 0.63332 - Test loss: 0.67392\n",
      "Epoch 3195 - lr: 0.05000 - Train loss: 0.63391 - Test loss: 0.67350\n",
      "Epoch 3196 - lr: 0.05000 - Train loss: 0.63325 - Test loss: 0.67370\n",
      "Epoch 3197 - lr: 0.05000 - Train loss: 0.63382 - Test loss: 0.67331\n",
      "Epoch 3198 - lr: 0.05000 - Train loss: 0.63320 - Test loss: 0.67351\n",
      "Epoch 3199 - lr: 0.05000 - Train loss: 0.63374 - Test loss: 0.67315\n",
      "Epoch 3200 - lr: 0.05000 - Train loss: 0.63314 - Test loss: 0.67335\n",
      "Epoch 3201 - lr: 0.05000 - Train loss: 0.63366 - Test loss: 0.67301\n",
      "Epoch 3202 - lr: 0.05000 - Train loss: 0.63309 - Test loss: 0.67320\n",
      "Epoch 3203 - lr: 0.05000 - Train loss: 0.63358 - Test loss: 0.67288\n",
      "Epoch 3204 - lr: 0.05000 - Train loss: 0.63304 - Test loss: 0.67307\n",
      "Epoch 3205 - lr: 0.05000 - Train loss: 0.63351 - Test loss: 0.67277\n",
      "Epoch 3206 - lr: 0.05000 - Train loss: 0.63299 - Test loss: 0.67296\n",
      "Epoch 3207 - lr: 0.05000 - Train loss: 0.63345 - Test loss: 0.67268\n",
      "Epoch 3208 - lr: 0.05000 - Train loss: 0.63294 - Test loss: 0.67286\n",
      "Epoch 3209 - lr: 0.05000 - Train loss: 0.63338 - Test loss: 0.67259\n",
      "Epoch 3210 - lr: 0.05000 - Train loss: 0.63290 - Test loss: 0.67276\n",
      "Epoch 3211 - lr: 0.05000 - Train loss: 0.63332 - Test loss: 0.67251\n",
      "Epoch 3212 - lr: 0.05000 - Train loss: 0.63285 - Test loss: 0.67268\n",
      "Epoch 3213 - lr: 0.05000 - Train loss: 0.63326 - Test loss: 0.67243\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3214 - lr: 0.05000 - Train loss: 0.63281 - Test loss: 0.67260\n",
      "Epoch 3215 - lr: 0.05000 - Train loss: 0.63320 - Test loss: 0.67237\n",
      "Epoch 3216 - lr: 0.05000 - Train loss: 0.63277 - Test loss: 0.67253\n",
      "Epoch 3217 - lr: 0.05000 - Train loss: 0.63314 - Test loss: 0.67231\n",
      "Epoch 3218 - lr: 0.05000 - Train loss: 0.63272 - Test loss: 0.67246\n",
      "Epoch 3219 - lr: 0.05000 - Train loss: 0.63309 - Test loss: 0.67225\n",
      "Epoch 3220 - lr: 0.05000 - Train loss: 0.63268 - Test loss: 0.67240\n",
      "Epoch 3221 - lr: 0.05000 - Train loss: 0.63303 - Test loss: 0.67220\n",
      "Epoch 3222 - lr: 0.05000 - Train loss: 0.63264 - Test loss: 0.67234\n",
      "Epoch 3223 - lr: 0.05000 - Train loss: 0.63298 - Test loss: 0.67215\n",
      "Epoch 3224 - lr: 0.05000 - Train loss: 0.63260 - Test loss: 0.67229\n",
      "Epoch 3225 - lr: 0.05000 - Train loss: 0.63292 - Test loss: 0.67210\n",
      "Epoch 3226 - lr: 0.05000 - Train loss: 0.63256 - Test loss: 0.67224\n",
      "Epoch 3227 - lr: 0.05000 - Train loss: 0.63287 - Test loss: 0.67205\n",
      "Epoch 3228 - lr: 0.05000 - Train loss: 0.63252 - Test loss: 0.67219\n",
      "Epoch 3229 - lr: 0.05000 - Train loss: 0.63282 - Test loss: 0.67201\n",
      "Epoch 3230 - lr: 0.05000 - Train loss: 0.63248 - Test loss: 0.67214\n",
      "Epoch 3231 - lr: 0.05000 - Train loss: 0.63277 - Test loss: 0.67197\n",
      "Epoch 3232 - lr: 0.05000 - Train loss: 0.63244 - Test loss: 0.67210\n",
      "Epoch 3233 - lr: 0.05000 - Train loss: 0.63272 - Test loss: 0.67193\n",
      "Epoch 3234 - lr: 0.05000 - Train loss: 0.63240 - Test loss: 0.67205\n",
      "Epoch 3235 - lr: 0.05000 - Train loss: 0.63268 - Test loss: 0.67189\n",
      "Epoch 3236 - lr: 0.05000 - Train loss: 0.63237 - Test loss: 0.67201\n",
      "Epoch 3237 - lr: 0.05000 - Train loss: 0.63263 - Test loss: 0.67186\n",
      "Epoch 3238 - lr: 0.05000 - Train loss: 0.63233 - Test loss: 0.67197\n",
      "Epoch 3239 - lr: 0.05000 - Train loss: 0.63258 - Test loss: 0.67182\n",
      "Epoch 3240 - lr: 0.05000 - Train loss: 0.63229 - Test loss: 0.67193\n",
      "Epoch 3241 - lr: 0.05000 - Train loss: 0.63253 - Test loss: 0.67179\n",
      "Epoch 3242 - lr: 0.05000 - Train loss: 0.63226 - Test loss: 0.67189\n",
      "Epoch 3243 - lr: 0.05000 - Train loss: 0.63249 - Test loss: 0.67176\n",
      "Epoch 3244 - lr: 0.05000 - Train loss: 0.63222 - Test loss: 0.67186\n",
      "Epoch 3245 - lr: 0.05000 - Train loss: 0.63244 - Test loss: 0.67173\n",
      "Epoch 3246 - lr: 0.05000 - Train loss: 0.63218 - Test loss: 0.67182\n",
      "Epoch 3247 - lr: 0.05000 - Train loss: 0.63240 - Test loss: 0.67170\n",
      "Epoch 3248 - lr: 0.05000 - Train loss: 0.63215 - Test loss: 0.67179\n",
      "Epoch 3249 - lr: 0.05000 - Train loss: 0.63236 - Test loss: 0.67167\n",
      "Epoch 3250 - lr: 0.05000 - Train loss: 0.63211 - Test loss: 0.67176\n",
      "Epoch 3251 - lr: 0.05000 - Train loss: 0.63231 - Test loss: 0.67164\n",
      "Epoch 3252 - lr: 0.05000 - Train loss: 0.63208 - Test loss: 0.67172\n",
      "Epoch 3253 - lr: 0.05000 - Train loss: 0.63227 - Test loss: 0.67161\n",
      "Epoch 3254 - lr: 0.05000 - Train loss: 0.63204 - Test loss: 0.67169\n",
      "Epoch 3255 - lr: 0.05000 - Train loss: 0.63223 - Test loss: 0.67158\n",
      "Epoch 3256 - lr: 0.05000 - Train loss: 0.63201 - Test loss: 0.67166\n",
      "Epoch 3257 - lr: 0.05000 - Train loss: 0.63219 - Test loss: 0.67155\n",
      "Epoch 3258 - lr: 0.05000 - Train loss: 0.63198 - Test loss: 0.67163\n",
      "Epoch 3259 - lr: 0.05000 - Train loss: 0.63215 - Test loss: 0.67153\n",
      "Epoch 3260 - lr: 0.05000 - Train loss: 0.63194 - Test loss: 0.67160\n",
      "Epoch 3261 - lr: 0.05000 - Train loss: 0.63210 - Test loss: 0.67150\n",
      "Epoch 3262 - lr: 0.05000 - Train loss: 0.63191 - Test loss: 0.67157\n",
      "Epoch 3263 - lr: 0.05000 - Train loss: 0.63206 - Test loss: 0.67148\n",
      "Epoch 3264 - lr: 0.05000 - Train loss: 0.63188 - Test loss: 0.67154\n",
      "Epoch 3265 - lr: 0.05000 - Train loss: 0.63203 - Test loss: 0.67145\n",
      "Epoch 3266 - lr: 0.05000 - Train loss: 0.63185 - Test loss: 0.67151\n",
      "Epoch 3267 - lr: 0.05000 - Train loss: 0.63199 - Test loss: 0.67143\n",
      "Epoch 3268 - lr: 0.05000 - Train loss: 0.63181 - Test loss: 0.67149\n",
      "Epoch 3269 - lr: 0.05000 - Train loss: 0.63195 - Test loss: 0.67140\n",
      "Epoch 3270 - lr: 0.05000 - Train loss: 0.63178 - Test loss: 0.67146\n",
      "Epoch 3271 - lr: 0.05000 - Train loss: 0.63191 - Test loss: 0.67138\n",
      "Epoch 3272 - lr: 0.05000 - Train loss: 0.63175 - Test loss: 0.67143\n",
      "Epoch 3273 - lr: 0.05000 - Train loss: 0.63187 - Test loss: 0.67136\n",
      "Epoch 3274 - lr: 0.05000 - Train loss: 0.63172 - Test loss: 0.67141\n",
      "Epoch 3275 - lr: 0.05000 - Train loss: 0.63183 - Test loss: 0.67133\n",
      "Epoch 3276 - lr: 0.05000 - Train loss: 0.63169 - Test loss: 0.67138\n",
      "Epoch 3277 - lr: 0.05000 - Train loss: 0.63180 - Test loss: 0.67131\n",
      "Epoch 3278 - lr: 0.05000 - Train loss: 0.63166 - Test loss: 0.67136\n",
      "Epoch 3279 - lr: 0.05000 - Train loss: 0.63176 - Test loss: 0.67129\n",
      "Epoch 3280 - lr: 0.05000 - Train loss: 0.63163 - Test loss: 0.67133\n",
      "Epoch 3281 - lr: 0.05000 - Train loss: 0.63173 - Test loss: 0.67127\n",
      "Epoch 3282 - lr: 0.05000 - Train loss: 0.63160 - Test loss: 0.67131\n",
      "Epoch 3283 - lr: 0.05000 - Train loss: 0.63169 - Test loss: 0.67125\n",
      "Epoch 3284 - lr: 0.05000 - Train loss: 0.63157 - Test loss: 0.67129\n",
      "Epoch 3285 - lr: 0.05000 - Train loss: 0.63165 - Test loss: 0.67122\n",
      "Epoch 3286 - lr: 0.05000 - Train loss: 0.63154 - Test loss: 0.67126\n",
      "Epoch 3287 - lr: 0.05000 - Train loss: 0.63162 - Test loss: 0.67120\n",
      "Epoch 3288 - lr: 0.05000 - Train loss: 0.63151 - Test loss: 0.67124\n",
      "Epoch 3289 - lr: 0.05000 - Train loss: 0.63158 - Test loss: 0.67118\n",
      "Epoch 3290 - lr: 0.05000 - Train loss: 0.63148 - Test loss: 0.67122\n",
      "Epoch 3291 - lr: 0.05000 - Train loss: 0.63155 - Test loss: 0.67116\n",
      "Epoch 3292 - lr: 0.05000 - Train loss: 0.63145 - Test loss: 0.67119\n",
      "Epoch 3293 - lr: 0.05000 - Train loss: 0.63152 - Test loss: 0.67114\n",
      "Epoch 3294 - lr: 0.05000 - Train loss: 0.63142 - Test loss: 0.67117\n",
      "Epoch 3295 - lr: 0.05000 - Train loss: 0.63148 - Test loss: 0.67112\n",
      "Epoch 3296 - lr: 0.05000 - Train loss: 0.63139 - Test loss: 0.67115\n",
      "Epoch 3297 - lr: 0.05000 - Train loss: 0.63145 - Test loss: 0.67110\n",
      "Epoch 3298 - lr: 0.05000 - Train loss: 0.63136 - Test loss: 0.67113\n",
      "Epoch 3299 - lr: 0.05000 - Train loss: 0.63142 - Test loss: 0.67108\n",
      "Epoch 3300 - lr: 0.05000 - Train loss: 0.63133 - Test loss: 0.67110\n",
      "Epoch 3301 - lr: 0.05000 - Train loss: 0.63138 - Test loss: 0.67106\n",
      "Epoch 3302 - lr: 0.05000 - Train loss: 0.63130 - Test loss: 0.67108\n",
      "Epoch 3303 - lr: 0.05000 - Train loss: 0.63135 - Test loss: 0.67104\n",
      "Epoch 3304 - lr: 0.05000 - Train loss: 0.63127 - Test loss: 0.67106\n",
      "Epoch 3305 - lr: 0.05000 - Train loss: 0.63132 - Test loss: 0.67102\n",
      "Epoch 3306 - lr: 0.05000 - Train loss: 0.63124 - Test loss: 0.67104\n",
      "Epoch 3307 - lr: 0.05000 - Train loss: 0.63128 - Test loss: 0.67100\n",
      "Epoch 3308 - lr: 0.05000 - Train loss: 0.63121 - Test loss: 0.67102\n",
      "Epoch 3309 - lr: 0.05000 - Train loss: 0.63125 - Test loss: 0.67098\n",
      "Epoch 3310 - lr: 0.05000 - Train loss: 0.63118 - Test loss: 0.67099\n",
      "Epoch 3311 - lr: 0.05000 - Train loss: 0.63122 - Test loss: 0.67096\n",
      "Epoch 3312 - lr: 0.05000 - Train loss: 0.63116 - Test loss: 0.67097\n",
      "Epoch 3313 - lr: 0.05000 - Train loss: 0.63119 - Test loss: 0.67094\n",
      "Epoch 3314 - lr: 0.05000 - Train loss: 0.63113 - Test loss: 0.67095\n",
      "Epoch 3315 - lr: 0.05000 - Train loss: 0.63116 - Test loss: 0.67092\n",
      "Epoch 3316 - lr: 0.05000 - Train loss: 0.63110 - Test loss: 0.67093\n",
      "Epoch 3317 - lr: 0.05000 - Train loss: 0.63112 - Test loss: 0.67090\n",
      "Epoch 3318 - lr: 0.05000 - Train loss: 0.63107 - Test loss: 0.67091\n",
      "Epoch 3319 - lr: 0.05000 - Train loss: 0.63109 - Test loss: 0.67088\n",
      "Epoch 3320 - lr: 0.05000 - Train loss: 0.63104 - Test loss: 0.67089\n",
      "Epoch 3321 - lr: 0.05000 - Train loss: 0.63106 - Test loss: 0.67086\n",
      "Epoch 3322 - lr: 0.05000 - Train loss: 0.63101 - Test loss: 0.67087\n",
      "Epoch 3323 - lr: 0.05000 - Train loss: 0.63103 - Test loss: 0.67084\n",
      "Epoch 3324 - lr: 0.05000 - Train loss: 0.63097 - Test loss: 0.67085\n",
      "Epoch 3325 - lr: 0.05000 - Train loss: 0.63099 - Test loss: 0.67082\n",
      "Epoch 3326 - lr: 0.05000 - Train loss: 0.63094 - Test loss: 0.67083\n",
      "Epoch 3327 - lr: 0.05000 - Train loss: 0.63096 - Test loss: 0.67080\n",
      "Epoch 3328 - lr: 0.05000 - Train loss: 0.63091 - Test loss: 0.67081\n",
      "Epoch 3329 - lr: 0.05000 - Train loss: 0.63093 - Test loss: 0.67078\n",
      "Epoch 3330 - lr: 0.05000 - Train loss: 0.63088 - Test loss: 0.67079\n",
      "Epoch 3331 - lr: 0.05000 - Train loss: 0.63089 - Test loss: 0.67077\n",
      "Epoch 3332 - lr: 0.05000 - Train loss: 0.63085 - Test loss: 0.67077\n",
      "Epoch 3333 - lr: 0.05000 - Train loss: 0.63086 - Test loss: 0.67075\n",
      "Epoch 3334 - lr: 0.05000 - Train loss: 0.63082 - Test loss: 0.67075\n",
      "Epoch 3335 - lr: 0.05000 - Train loss: 0.63082 - Test loss: 0.67073\n",
      "Epoch 3336 - lr: 0.05000 - Train loss: 0.63078 - Test loss: 0.67073\n",
      "Epoch 3337 - lr: 0.05000 - Train loss: 0.63079 - Test loss: 0.67071\n",
      "Epoch 3338 - lr: 0.05000 - Train loss: 0.63075 - Test loss: 0.67071\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3339 - lr: 0.05000 - Train loss: 0.63075 - Test loss: 0.67069\n",
      "Epoch 3340 - lr: 0.05000 - Train loss: 0.63072 - Test loss: 0.67069\n",
      "Epoch 3341 - lr: 0.05000 - Train loss: 0.63072 - Test loss: 0.67067\n",
      "Epoch 3342 - lr: 0.05000 - Train loss: 0.63068 - Test loss: 0.67067\n",
      "Epoch 3343 - lr: 0.05000 - Train loss: 0.63068 - Test loss: 0.67065\n",
      "Epoch 3344 - lr: 0.05000 - Train loss: 0.63064 - Test loss: 0.67065\n",
      "Epoch 3345 - lr: 0.05000 - Train loss: 0.63065 - Test loss: 0.67063\n",
      "Epoch 3346 - lr: 0.05000 - Train loss: 0.63061 - Test loss: 0.67063\n",
      "Epoch 3347 - lr: 0.05000 - Train loss: 0.63061 - Test loss: 0.67062\n",
      "Epoch 3348 - lr: 0.05000 - Train loss: 0.63057 - Test loss: 0.67061\n",
      "Epoch 3349 - lr: 0.05000 - Train loss: 0.63057 - Test loss: 0.67060\n",
      "Epoch 3350 - lr: 0.05000 - Train loss: 0.63053 - Test loss: 0.67060\n",
      "Epoch 3351 - lr: 0.05000 - Train loss: 0.63053 - Test loss: 0.67058\n",
      "Epoch 3352 - lr: 0.05000 - Train loss: 0.63049 - Test loss: 0.67058\n",
      "Epoch 3353 - lr: 0.05000 - Train loss: 0.63049 - Test loss: 0.67057\n",
      "Epoch 3354 - lr: 0.05000 - Train loss: 0.63045 - Test loss: 0.67057\n",
      "Epoch 3355 - lr: 0.05000 - Train loss: 0.63045 - Test loss: 0.67055\n",
      "Epoch 3356 - lr: 0.05000 - Train loss: 0.63041 - Test loss: 0.67055\n",
      "Epoch 3357 - lr: 0.05000 - Train loss: 0.63041 - Test loss: 0.67054\n",
      "Epoch 3358 - lr: 0.05000 - Train loss: 0.63037 - Test loss: 0.67054\n",
      "Epoch 3359 - lr: 0.05000 - Train loss: 0.63036 - Test loss: 0.67053\n",
      "Epoch 3360 - lr: 0.05000 - Train loss: 0.63033 - Test loss: 0.67052\n",
      "Epoch 3361 - lr: 0.05000 - Train loss: 0.63032 - Test loss: 0.67052\n",
      "Epoch 3362 - lr: 0.05000 - Train loss: 0.63028 - Test loss: 0.67051\n",
      "Epoch 3363 - lr: 0.05000 - Train loss: 0.63027 - Test loss: 0.67050\n",
      "Epoch 3364 - lr: 0.05000 - Train loss: 0.63024 - Test loss: 0.67050\n",
      "Epoch 3365 - lr: 0.05000 - Train loss: 0.63023 - Test loss: 0.67050\n",
      "Epoch 3366 - lr: 0.05000 - Train loss: 0.63019 - Test loss: 0.67050\n",
      "Epoch 3367 - lr: 0.05000 - Train loss: 0.63018 - Test loss: 0.67049\n",
      "Epoch 3368 - lr: 0.05000 - Train loss: 0.63014 - Test loss: 0.67049\n",
      "Epoch 3369 - lr: 0.05000 - Train loss: 0.63013 - Test loss: 0.67048\n",
      "Epoch 3370 - lr: 0.05000 - Train loss: 0.63009 - Test loss: 0.67048\n",
      "Epoch 3371 - lr: 0.05000 - Train loss: 0.63008 - Test loss: 0.67048\n",
      "Epoch 3372 - lr: 0.05000 - Train loss: 0.63004 - Test loss: 0.67048\n",
      "Epoch 3373 - lr: 0.05000 - Train loss: 0.63002 - Test loss: 0.67048\n",
      "Epoch 3374 - lr: 0.05000 - Train loss: 0.62998 - Test loss: 0.67048\n",
      "Epoch 3375 - lr: 0.05000 - Train loss: 0.62997 - Test loss: 0.67048\n",
      "Epoch 3376 - lr: 0.05000 - Train loss: 0.62993 - Test loss: 0.67049\n",
      "Epoch 3377 - lr: 0.05000 - Train loss: 0.62991 - Test loss: 0.67049\n",
      "Epoch 3378 - lr: 0.05000 - Train loss: 0.62987 - Test loss: 0.67049\n",
      "Epoch 3379 - lr: 0.05000 - Train loss: 0.62985 - Test loss: 0.67049\n",
      "Epoch 3380 - lr: 0.05000 - Train loss: 0.62981 - Test loss: 0.67050\n",
      "Epoch 3381 - lr: 0.05000 - Train loss: 0.62979 - Test loss: 0.67051\n",
      "Epoch 3382 - lr: 0.05000 - Train loss: 0.62975 - Test loss: 0.67052\n",
      "Epoch 3383 - lr: 0.05000 - Train loss: 0.62972 - Test loss: 0.67052\n",
      "Epoch 3384 - lr: 0.05000 - Train loss: 0.62968 - Test loss: 0.67053\n",
      "Epoch 3385 - lr: 0.05000 - Train loss: 0.62965 - Test loss: 0.67054\n",
      "Epoch 3386 - lr: 0.05000 - Train loss: 0.62961 - Test loss: 0.67055\n",
      "Epoch 3387 - lr: 0.05000 - Train loss: 0.62958 - Test loss: 0.67056\n",
      "Epoch 3388 - lr: 0.05000 - Train loss: 0.62954 - Test loss: 0.67058\n",
      "Epoch 3389 - lr: 0.05000 - Train loss: 0.62951 - Test loss: 0.67059\n",
      "Epoch 3390 - lr: 0.05000 - Train loss: 0.62947 - Test loss: 0.67061\n",
      "Epoch 3391 - lr: 0.05000 - Train loss: 0.62944 - Test loss: 0.67063\n",
      "Epoch 3392 - lr: 0.05000 - Train loss: 0.62939 - Test loss: 0.67065\n",
      "Epoch 3393 - lr: 0.05000 - Train loss: 0.62936 - Test loss: 0.67067\n",
      "Epoch 3394 - lr: 0.05000 - Train loss: 0.62931 - Test loss: 0.67069\n",
      "Epoch 3395 - lr: 0.05000 - Train loss: 0.62927 - Test loss: 0.67072\n",
      "Epoch 3396 - lr: 0.05000 - Train loss: 0.62922 - Test loss: 0.67075\n",
      "Epoch 3397 - lr: 0.05000 - Train loss: 0.62919 - Test loss: 0.67077\n",
      "Epoch 3398 - lr: 0.05000 - Train loss: 0.62914 - Test loss: 0.67080\n",
      "Epoch 3399 - lr: 0.05000 - Train loss: 0.62910 - Test loss: 0.67083\n",
      "Epoch 3400 - lr: 0.05000 - Train loss: 0.62904 - Test loss: 0.67087\n",
      "Epoch 3401 - lr: 0.05000 - Train loss: 0.62900 - Test loss: 0.67091\n",
      "Epoch 3402 - lr: 0.05000 - Train loss: 0.62895 - Test loss: 0.67095\n",
      "Epoch 3403 - lr: 0.05000 - Train loss: 0.62890 - Test loss: 0.67098\n",
      "Epoch 3404 - lr: 0.05000 - Train loss: 0.62885 - Test loss: 0.67103\n",
      "Epoch 3405 - lr: 0.05000 - Train loss: 0.62880 - Test loss: 0.67107\n",
      "Epoch 3406 - lr: 0.05000 - Train loss: 0.62874 - Test loss: 0.67112\n",
      "Epoch 3407 - lr: 0.05000 - Train loss: 0.62869 - Test loss: 0.67117\n",
      "Epoch 3408 - lr: 0.05000 - Train loss: 0.62863 - Test loss: 0.67123\n",
      "Epoch 3409 - lr: 0.05000 - Train loss: 0.62858 - Test loss: 0.67129\n",
      "Epoch 3410 - lr: 0.05000 - Train loss: 0.62851 - Test loss: 0.67135\n",
      "Epoch 3411 - lr: 0.05000 - Train loss: 0.62846 - Test loss: 0.67141\n",
      "Epoch 3412 - lr: 0.05000 - Train loss: 0.62839 - Test loss: 0.67148\n",
      "Epoch 3413 - lr: 0.05000 - Train loss: 0.62833 - Test loss: 0.67154\n",
      "Epoch 3414 - lr: 0.05000 - Train loss: 0.62826 - Test loss: 0.67162\n",
      "Epoch 3415 - lr: 0.05000 - Train loss: 0.62820 - Test loss: 0.67169\n",
      "Epoch 3416 - lr: 0.05000 - Train loss: 0.62813 - Test loss: 0.67178\n",
      "Epoch 3417 - lr: 0.05000 - Train loss: 0.62807 - Test loss: 0.67186\n",
      "Epoch 3418 - lr: 0.05000 - Train loss: 0.62799 - Test loss: 0.67195\n",
      "Epoch 3419 - lr: 0.05000 - Train loss: 0.62792 - Test loss: 0.67204\n",
      "Epoch 3420 - lr: 0.05000 - Train loss: 0.62785 - Test loss: 0.67214\n",
      "Epoch 3421 - lr: 0.05000 - Train loss: 0.62778 - Test loss: 0.67224\n",
      "Epoch 3422 - lr: 0.05000 - Train loss: 0.62769 - Test loss: 0.67234\n",
      "Epoch 3423 - lr: 0.05000 - Train loss: 0.62762 - Test loss: 0.67245\n",
      "Epoch 3424 - lr: 0.05000 - Train loss: 0.62753 - Test loss: 0.67256\n",
      "Epoch 3425 - lr: 0.05000 - Train loss: 0.62745 - Test loss: 0.67268\n",
      "Epoch 3426 - lr: 0.05000 - Train loss: 0.62737 - Test loss: 0.67281\n",
      "Epoch 3427 - lr: 0.05000 - Train loss: 0.62728 - Test loss: 0.67293\n",
      "Epoch 3428 - lr: 0.05000 - Train loss: 0.62719 - Test loss: 0.67307\n",
      "Epoch 3429 - lr: 0.05000 - Train loss: 0.62710 - Test loss: 0.67321\n",
      "Epoch 3430 - lr: 0.05000 - Train loss: 0.62701 - Test loss: 0.67335\n",
      "Epoch 3431 - lr: 0.05000 - Train loss: 0.62692 - Test loss: 0.67350\n",
      "Epoch 3432 - lr: 0.05000 - Train loss: 0.62682 - Test loss: 0.67366\n",
      "Epoch 3433 - lr: 0.05000 - Train loss: 0.62672 - Test loss: 0.67382\n",
      "Epoch 3434 - lr: 0.05000 - Train loss: 0.62662 - Test loss: 0.67398\n",
      "Epoch 3435 - lr: 0.05000 - Train loss: 0.62652 - Test loss: 0.67415\n",
      "Epoch 3436 - lr: 0.05000 - Train loss: 0.62641 - Test loss: 0.67433\n",
      "Epoch 3437 - lr: 0.05000 - Train loss: 0.62631 - Test loss: 0.67452\n",
      "Epoch 3438 - lr: 0.05000 - Train loss: 0.62619 - Test loss: 0.67471\n",
      "Epoch 3439 - lr: 0.05000 - Train loss: 0.62609 - Test loss: 0.67490\n",
      "Epoch 3440 - lr: 0.05000 - Train loss: 0.62597 - Test loss: 0.67510\n",
      "Epoch 3441 - lr: 0.05000 - Train loss: 0.62586 - Test loss: 0.67531\n",
      "Epoch 3442 - lr: 0.05000 - Train loss: 0.62574 - Test loss: 0.67553\n",
      "Epoch 3443 - lr: 0.05000 - Train loss: 0.62562 - Test loss: 0.67575\n",
      "Epoch 3444 - lr: 0.05000 - Train loss: 0.62550 - Test loss: 0.67597\n",
      "Epoch 3445 - lr: 0.05000 - Train loss: 0.62538 - Test loss: 0.67621\n",
      "Epoch 3446 - lr: 0.05000 - Train loss: 0.62525 - Test loss: 0.67644\n",
      "Epoch 3447 - lr: 0.05000 - Train loss: 0.62512 - Test loss: 0.67669\n",
      "Epoch 3448 - lr: 0.05000 - Train loss: 0.62499 - Test loss: 0.67694\n",
      "Epoch 3449 - lr: 0.05000 - Train loss: 0.62486 - Test loss: 0.67720\n",
      "Epoch 3450 - lr: 0.05000 - Train loss: 0.62472 - Test loss: 0.67746\n",
      "Epoch 3451 - lr: 0.05000 - Train loss: 0.62459 - Test loss: 0.67773\n",
      "Epoch 3452 - lr: 0.05000 - Train loss: 0.62445 - Test loss: 0.67800\n",
      "Epoch 3453 - lr: 0.05000 - Train loss: 0.62432 - Test loss: 0.67828\n",
      "Epoch 3454 - lr: 0.05000 - Train loss: 0.62417 - Test loss: 0.67857\n",
      "Epoch 3455 - lr: 0.05000 - Train loss: 0.62404 - Test loss: 0.67885\n",
      "Epoch 3456 - lr: 0.05000 - Train loss: 0.62389 - Test loss: 0.67915\n",
      "Epoch 3457 - lr: 0.05000 - Train loss: 0.62375 - Test loss: 0.67945\n",
      "Epoch 3458 - lr: 0.05000 - Train loss: 0.62360 - Test loss: 0.67975\n",
      "Epoch 3459 - lr: 0.05000 - Train loss: 0.62346 - Test loss: 0.68005\n",
      "Epoch 3460 - lr: 0.05000 - Train loss: 0.62330 - Test loss: 0.68036\n",
      "Epoch 3461 - lr: 0.05000 - Train loss: 0.62316 - Test loss: 0.68067\n",
      "Epoch 3462 - lr: 0.05000 - Train loss: 0.62301 - Test loss: 0.68099\n",
      "Epoch 3463 - lr: 0.05000 - Train loss: 0.62286 - Test loss: 0.68130\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3464 - lr: 0.05000 - Train loss: 0.62271 - Test loss: 0.68162\n",
      "Epoch 3465 - lr: 0.05000 - Train loss: 0.62256 - Test loss: 0.68194\n",
      "Epoch 3466 - lr: 0.05000 - Train loss: 0.62240 - Test loss: 0.68226\n",
      "Epoch 3467 - lr: 0.05000 - Train loss: 0.62226 - Test loss: 0.68258\n",
      "Epoch 3468 - lr: 0.05000 - Train loss: 0.62210 - Test loss: 0.68291\n",
      "Epoch 3469 - lr: 0.05000 - Train loss: 0.62195 - Test loss: 0.68323\n",
      "Epoch 3470 - lr: 0.05000 - Train loss: 0.62180 - Test loss: 0.68355\n",
      "Epoch 3471 - lr: 0.05000 - Train loss: 0.62165 - Test loss: 0.68387\n",
      "Epoch 3472 - lr: 0.05000 - Train loss: 0.62150 - Test loss: 0.68420\n",
      "Epoch 3473 - lr: 0.05000 - Train loss: 0.62135 - Test loss: 0.68452\n",
      "Epoch 3474 - lr: 0.05000 - Train loss: 0.62120 - Test loss: 0.68484\n",
      "Epoch 3475 - lr: 0.05000 - Train loss: 0.62105 - Test loss: 0.68516\n",
      "Epoch 3476 - lr: 0.05000 - Train loss: 0.62090 - Test loss: 0.68548\n",
      "Epoch 3477 - lr: 0.05000 - Train loss: 0.62076 - Test loss: 0.68580\n",
      "Epoch 3478 - lr: 0.05000 - Train loss: 0.62061 - Test loss: 0.68612\n",
      "Epoch 3479 - lr: 0.05000 - Train loss: 0.62047 - Test loss: 0.68644\n",
      "Epoch 3480 - lr: 0.05000 - Train loss: 0.62032 - Test loss: 0.68675\n",
      "Epoch 3481 - lr: 0.05000 - Train loss: 0.62018 - Test loss: 0.68707\n",
      "Epoch 3482 - lr: 0.05000 - Train loss: 0.62003 - Test loss: 0.68738\n",
      "Epoch 3483 - lr: 0.05000 - Train loss: 0.61989 - Test loss: 0.68769\n",
      "Epoch 3484 - lr: 0.05000 - Train loss: 0.61975 - Test loss: 0.68800\n",
      "Epoch 3485 - lr: 0.05000 - Train loss: 0.61961 - Test loss: 0.68831\n",
      "Epoch 3486 - lr: 0.05000 - Train loss: 0.61947 - Test loss: 0.68861\n",
      "Epoch 3487 - lr: 0.05000 - Train loss: 0.61934 - Test loss: 0.68892\n",
      "Epoch 3488 - lr: 0.05000 - Train loss: 0.61920 - Test loss: 0.68922\n",
      "Epoch 3489 - lr: 0.05000 - Train loss: 0.61907 - Test loss: 0.68952\n",
      "Epoch 3490 - lr: 0.05000 - Train loss: 0.61893 - Test loss: 0.68981\n",
      "Epoch 3491 - lr: 0.05000 - Train loss: 0.61880 - Test loss: 0.69011\n",
      "Epoch 3492 - lr: 0.05000 - Train loss: 0.61867 - Test loss: 0.69040\n",
      "Epoch 3493 - lr: 0.05000 - Train loss: 0.61854 - Test loss: 0.69069\n",
      "Epoch 3494 - lr: 0.05000 - Train loss: 0.61841 - Test loss: 0.69098\n",
      "Epoch 3495 - lr: 0.05000 - Train loss: 0.61829 - Test loss: 0.69126\n",
      "Epoch 3496 - lr: 0.05000 - Train loss: 0.61816 - Test loss: 0.69155\n",
      "Epoch 3497 - lr: 0.05000 - Train loss: 0.61804 - Test loss: 0.69183\n",
      "Epoch 3498 - lr: 0.05000 - Train loss: 0.61791 - Test loss: 0.69211\n",
      "Epoch 3499 - lr: 0.05000 - Train loss: 0.61779 - Test loss: 0.69238\n",
      "Epoch 3500 - lr: 0.05000 - Train loss: 0.61767 - Test loss: 0.69265\n",
      "Epoch 3501 - lr: 0.05000 - Train loss: 0.61755 - Test loss: 0.69292\n",
      "Epoch 3502 - lr: 0.05000 - Train loss: 0.61744 - Test loss: 0.69319\n",
      "Epoch 3503 - lr: 0.05000 - Train loss: 0.61732 - Test loss: 0.69346\n",
      "Epoch 3504 - lr: 0.05000 - Train loss: 0.61721 - Test loss: 0.69372\n",
      "Epoch 3505 - lr: 0.05000 - Train loss: 0.61709 - Test loss: 0.69398\n",
      "Epoch 3506 - lr: 0.05000 - Train loss: 0.61698 - Test loss: 0.69423\n",
      "Epoch 3507 - lr: 0.05000 - Train loss: 0.61687 - Test loss: 0.69449\n",
      "Epoch 3508 - lr: 0.05000 - Train loss: 0.61676 - Test loss: 0.69474\n",
      "Epoch 3509 - lr: 0.05000 - Train loss: 0.61665 - Test loss: 0.69498\n",
      "Epoch 3510 - lr: 0.05000 - Train loss: 0.61655 - Test loss: 0.69523\n",
      "Epoch 3511 - lr: 0.05000 - Train loss: 0.61644 - Test loss: 0.69547\n",
      "Epoch 3512 - lr: 0.05000 - Train loss: 0.61634 - Test loss: 0.69571\n",
      "Epoch 3513 - lr: 0.05000 - Train loss: 0.61624 - Test loss: 0.69595\n",
      "Epoch 3514 - lr: 0.05000 - Train loss: 0.61613 - Test loss: 0.69618\n",
      "Epoch 3515 - lr: 0.05000 - Train loss: 0.61603 - Test loss: 0.69641\n",
      "Epoch 3516 - lr: 0.05000 - Train loss: 0.61593 - Test loss: 0.69664\n",
      "Epoch 3517 - lr: 0.05000 - Train loss: 0.61584 - Test loss: 0.69686\n",
      "Epoch 3518 - lr: 0.05000 - Train loss: 0.61574 - Test loss: 0.69708\n",
      "Epoch 3519 - lr: 0.05000 - Train loss: 0.61564 - Test loss: 0.69730\n",
      "Epoch 3520 - lr: 0.05000 - Train loss: 0.61555 - Test loss: 0.69751\n",
      "Epoch 3521 - lr: 0.05000 - Train loss: 0.61545 - Test loss: 0.69773\n",
      "Epoch 3522 - lr: 0.05000 - Train loss: 0.61536 - Test loss: 0.69794\n",
      "Epoch 3523 - lr: 0.05000 - Train loss: 0.61527 - Test loss: 0.69814\n",
      "Epoch 3524 - lr: 0.05000 - Train loss: 0.61518 - Test loss: 0.69835\n",
      "Epoch 3525 - lr: 0.05000 - Train loss: 0.61509 - Test loss: 0.69855\n",
      "Epoch 3526 - lr: 0.05000 - Train loss: 0.61500 - Test loss: 0.69875\n",
      "Epoch 3527 - lr: 0.05000 - Train loss: 0.61491 - Test loss: 0.69894\n",
      "Epoch 3528 - lr: 0.05000 - Train loss: 0.61483 - Test loss: 0.69914\n",
      "Epoch 3529 - lr: 0.05000 - Train loss: 0.61474 - Test loss: 0.69933\n",
      "Epoch 3530 - lr: 0.05000 - Train loss: 0.61465 - Test loss: 0.69951\n",
      "Epoch 3531 - lr: 0.05000 - Train loss: 0.61457 - Test loss: 0.69970\n",
      "Epoch 3532 - lr: 0.05000 - Train loss: 0.61449 - Test loss: 0.69988\n",
      "Epoch 3533 - lr: 0.05000 - Train loss: 0.61441 - Test loss: 0.70006\n",
      "Epoch 3534 - lr: 0.05000 - Train loss: 0.61432 - Test loss: 0.70023\n",
      "Epoch 3535 - lr: 0.05000 - Train loss: 0.61424 - Test loss: 0.70041\n",
      "Epoch 3536 - lr: 0.05000 - Train loss: 0.61416 - Test loss: 0.70058\n",
      "Epoch 3537 - lr: 0.05000 - Train loss: 0.61408 - Test loss: 0.70075\n",
      "Epoch 3538 - lr: 0.05000 - Train loss: 0.61401 - Test loss: 0.70091\n",
      "Epoch 3539 - lr: 0.05000 - Train loss: 0.61393 - Test loss: 0.70108\n",
      "Epoch 3540 - lr: 0.05000 - Train loss: 0.61385 - Test loss: 0.70124\n",
      "Epoch 3541 - lr: 0.05000 - Train loss: 0.61378 - Test loss: 0.70140\n",
      "Epoch 3542 - lr: 0.05000 - Train loss: 0.61370 - Test loss: 0.70155\n",
      "Epoch 3543 - lr: 0.05000 - Train loss: 0.61363 - Test loss: 0.70171\n",
      "Epoch 3544 - lr: 0.05000 - Train loss: 0.61355 - Test loss: 0.70186\n",
      "Epoch 3545 - lr: 0.05000 - Train loss: 0.61348 - Test loss: 0.70201\n",
      "Epoch 3546 - lr: 0.05000 - Train loss: 0.61341 - Test loss: 0.70216\n",
      "Epoch 3547 - lr: 0.05000 - Train loss: 0.61333 - Test loss: 0.70230\n",
      "Epoch 3548 - lr: 0.05000 - Train loss: 0.61326 - Test loss: 0.70244\n",
      "Epoch 3549 - lr: 0.05000 - Train loss: 0.61319 - Test loss: 0.70258\n",
      "Epoch 3550 - lr: 0.05000 - Train loss: 0.61312 - Test loss: 0.70272\n",
      "Epoch 3551 - lr: 0.05000 - Train loss: 0.61305 - Test loss: 0.70286\n",
      "Epoch 3552 - lr: 0.05000 - Train loss: 0.61298 - Test loss: 0.70299\n",
      "Epoch 3553 - lr: 0.05000 - Train loss: 0.61292 - Test loss: 0.70313\n",
      "Epoch 3554 - lr: 0.05000 - Train loss: 0.61285 - Test loss: 0.70326\n",
      "Epoch 3555 - lr: 0.05000 - Train loss: 0.61278 - Test loss: 0.70339\n",
      "Epoch 3556 - lr: 0.05000 - Train loss: 0.61272 - Test loss: 0.70351\n",
      "Epoch 3557 - lr: 0.05000 - Train loss: 0.61265 - Test loss: 0.70364\n",
      "Epoch 3558 - lr: 0.05000 - Train loss: 0.61259 - Test loss: 0.70376\n",
      "Epoch 3559 - lr: 0.05000 - Train loss: 0.61252 - Test loss: 0.70388\n",
      "Epoch 3560 - lr: 0.05000 - Train loss: 0.61246 - Test loss: 0.70400\n",
      "Epoch 3561 - lr: 0.05000 - Train loss: 0.61239 - Test loss: 0.70412\n",
      "Epoch 3562 - lr: 0.05000 - Train loss: 0.61233 - Test loss: 0.70423\n",
      "Epoch 3563 - lr: 0.05000 - Train loss: 0.61227 - Test loss: 0.70435\n",
      "Epoch 3564 - lr: 0.05000 - Train loss: 0.61221 - Test loss: 0.70446\n",
      "Epoch 3565 - lr: 0.05000 - Train loss: 0.61215 - Test loss: 0.70457\n",
      "Epoch 3566 - lr: 0.05000 - Train loss: 0.61209 - Test loss: 0.70468\n",
      "Epoch 3567 - lr: 0.05000 - Train loss: 0.61203 - Test loss: 0.70479\n",
      "Epoch 3568 - lr: 0.05000 - Train loss: 0.61197 - Test loss: 0.70489\n",
      "Epoch 3569 - lr: 0.05000 - Train loss: 0.61191 - Test loss: 0.70500\n",
      "Epoch 3570 - lr: 0.05000 - Train loss: 0.61185 - Test loss: 0.70510\n",
      "Epoch 3571 - lr: 0.05000 - Train loss: 0.61179 - Test loss: 0.70520\n",
      "Epoch 3572 - lr: 0.05000 - Train loss: 0.61173 - Test loss: 0.70531\n",
      "Epoch 3573 - lr: 0.05000 - Train loss: 0.61168 - Test loss: 0.70540\n",
      "Epoch 3574 - lr: 0.05000 - Train loss: 0.61162 - Test loss: 0.70550\n",
      "Epoch 3575 - lr: 0.05000 - Train loss: 0.61157 - Test loss: 0.70560\n",
      "Epoch 3576 - lr: 0.05000 - Train loss: 0.61151 - Test loss: 0.70569\n",
      "Epoch 3577 - lr: 0.05000 - Train loss: 0.61146 - Test loss: 0.70579\n",
      "Epoch 3578 - lr: 0.05000 - Train loss: 0.61140 - Test loss: 0.70588\n",
      "Epoch 3579 - lr: 0.05000 - Train loss: 0.61135 - Test loss: 0.70597\n",
      "Epoch 3580 - lr: 0.05000 - Train loss: 0.61129 - Test loss: 0.70606\n",
      "Epoch 3581 - lr: 0.05000 - Train loss: 0.61124 - Test loss: 0.70615\n",
      "Epoch 3582 - lr: 0.05000 - Train loss: 0.61119 - Test loss: 0.70624\n",
      "Epoch 3583 - lr: 0.05000 - Train loss: 0.61114 - Test loss: 0.70633\n",
      "Epoch 3584 - lr: 0.05000 - Train loss: 0.61108 - Test loss: 0.70642\n",
      "Epoch 3585 - lr: 0.05000 - Train loss: 0.61103 - Test loss: 0.70650\n",
      "Epoch 3586 - lr: 0.05000 - Train loss: 0.61098 - Test loss: 0.70659\n",
      "Epoch 3587 - lr: 0.05000 - Train loss: 0.61093 - Test loss: 0.70667\n",
      "Epoch 3588 - lr: 0.05000 - Train loss: 0.61088 - Test loss: 0.70675\n",
      "Epoch 3589 - lr: 0.05000 - Train loss: 0.61083 - Test loss: 0.70683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3590 - lr: 0.05000 - Train loss: 0.61078 - Test loss: 0.70691\n",
      "Epoch 3591 - lr: 0.05000 - Train loss: 0.61073 - Test loss: 0.70699\n",
      "Epoch 3592 - lr: 0.05000 - Train loss: 0.61069 - Test loss: 0.70707\n",
      "Epoch 3593 - lr: 0.05000 - Train loss: 0.61064 - Test loss: 0.70715\n",
      "Epoch 3594 - lr: 0.05000 - Train loss: 0.61059 - Test loss: 0.70722\n",
      "Epoch 3595 - lr: 0.05000 - Train loss: 0.61054 - Test loss: 0.70730\n",
      "Epoch 3596 - lr: 0.05000 - Train loss: 0.61050 - Test loss: 0.70738\n",
      "Epoch 3597 - lr: 0.05000 - Train loss: 0.61045 - Test loss: 0.70745\n",
      "Epoch 3598 - lr: 0.05000 - Train loss: 0.61041 - Test loss: 0.70752\n",
      "Epoch 3599 - lr: 0.05000 - Train loss: 0.61036 - Test loss: 0.70760\n",
      "Epoch 3600 - lr: 0.05000 - Train loss: 0.61031 - Test loss: 0.70767\n",
      "Epoch 3601 - lr: 0.05000 - Train loss: 0.61027 - Test loss: 0.70774\n",
      "Epoch 3602 - lr: 0.05000 - Train loss: 0.61023 - Test loss: 0.70781\n",
      "Epoch 3603 - lr: 0.05000 - Train loss: 0.61018 - Test loss: 0.70788\n",
      "Epoch 3604 - lr: 0.05000 - Train loss: 0.61014 - Test loss: 0.70795\n",
      "Epoch 3605 - lr: 0.05000 - Train loss: 0.61009 - Test loss: 0.70802\n",
      "Epoch 3606 - lr: 0.05000 - Train loss: 0.61005 - Test loss: 0.70808\n",
      "Epoch 3607 - lr: 0.05000 - Train loss: 0.61001 - Test loss: 0.70815\n",
      "Epoch 3608 - lr: 0.05000 - Train loss: 0.60997 - Test loss: 0.70822\n",
      "Epoch 3609 - lr: 0.05000 - Train loss: 0.60993 - Test loss: 0.70828\n",
      "Epoch 3610 - lr: 0.05000 - Train loss: 0.60988 - Test loss: 0.70835\n",
      "Epoch 3611 - lr: 0.05000 - Train loss: 0.60984 - Test loss: 0.70841\n",
      "Epoch 3612 - lr: 0.05000 - Train loss: 0.60980 - Test loss: 0.70848\n",
      "Epoch 3613 - lr: 0.05000 - Train loss: 0.60976 - Test loss: 0.70854\n",
      "Epoch 3614 - lr: 0.05000 - Train loss: 0.60972 - Test loss: 0.70860\n",
      "Epoch 3615 - lr: 0.05000 - Train loss: 0.60968 - Test loss: 0.70867\n",
      "Epoch 3616 - lr: 0.05000 - Train loss: 0.60964 - Test loss: 0.70873\n",
      "Epoch 3617 - lr: 0.05000 - Train loss: 0.60960 - Test loss: 0.70879\n",
      "Epoch 3618 - lr: 0.05000 - Train loss: 0.60956 - Test loss: 0.70885\n",
      "Epoch 3619 - lr: 0.05000 - Train loss: 0.60952 - Test loss: 0.70891\n",
      "Epoch 3620 - lr: 0.05000 - Train loss: 0.60949 - Test loss: 0.70897\n",
      "Epoch 3621 - lr: 0.05000 - Train loss: 0.60945 - Test loss: 0.70903\n",
      "Epoch 3622 - lr: 0.05000 - Train loss: 0.60941 - Test loss: 0.70908\n",
      "Epoch 3623 - lr: 0.05000 - Train loss: 0.60937 - Test loss: 0.70914\n",
      "Epoch 3624 - lr: 0.05000 - Train loss: 0.60934 - Test loss: 0.70920\n",
      "Epoch 3625 - lr: 0.05000 - Train loss: 0.60930 - Test loss: 0.70926\n",
      "Epoch 3626 - lr: 0.05000 - Train loss: 0.60926 - Test loss: 0.70931\n",
      "Epoch 3627 - lr: 0.05000 - Train loss: 0.60923 - Test loss: 0.70937\n",
      "Epoch 3628 - lr: 0.05000 - Train loss: 0.60919 - Test loss: 0.70942\n",
      "Epoch 3629 - lr: 0.05000 - Train loss: 0.60915 - Test loss: 0.70948\n",
      "Epoch 3630 - lr: 0.05000 - Train loss: 0.60912 - Test loss: 0.70953\n",
      "Epoch 3631 - lr: 0.05000 - Train loss: 0.60908 - Test loss: 0.70959\n",
      "Epoch 3632 - lr: 0.05000 - Train loss: 0.60905 - Test loss: 0.70964\n",
      "Epoch 3633 - lr: 0.05000 - Train loss: 0.60901 - Test loss: 0.70969\n",
      "Epoch 3634 - lr: 0.05000 - Train loss: 0.60898 - Test loss: 0.70974\n",
      "Epoch 3635 - lr: 0.05000 - Train loss: 0.60894 - Test loss: 0.70980\n",
      "Epoch 3636 - lr: 0.05000 - Train loss: 0.60891 - Test loss: 0.70985\n",
      "Epoch 3637 - lr: 0.05000 - Train loss: 0.60888 - Test loss: 0.70990\n",
      "Epoch 3638 - lr: 0.05000 - Train loss: 0.60884 - Test loss: 0.70995\n",
      "Epoch 3639 - lr: 0.05000 - Train loss: 0.60881 - Test loss: 0.71000\n",
      "Epoch 3640 - lr: 0.05000 - Train loss: 0.60878 - Test loss: 0.71005\n",
      "Epoch 3641 - lr: 0.05000 - Train loss: 0.60874 - Test loss: 0.71010\n",
      "Epoch 3642 - lr: 0.05000 - Train loss: 0.60871 - Test loss: 0.71015\n",
      "Epoch 3643 - lr: 0.05000 - Train loss: 0.60868 - Test loss: 0.71019\n",
      "Epoch 3644 - lr: 0.05000 - Train loss: 0.60865 - Test loss: 0.71024\n",
      "Epoch 3645 - lr: 0.05000 - Train loss: 0.60861 - Test loss: 0.71029\n",
      "Epoch 3646 - lr: 0.05000 - Train loss: 0.60858 - Test loss: 0.71034\n",
      "Epoch 3647 - lr: 0.05000 - Train loss: 0.60855 - Test loss: 0.71038\n",
      "Epoch 3648 - lr: 0.05000 - Train loss: 0.60852 - Test loss: 0.71043\n",
      "Epoch 3649 - lr: 0.05000 - Train loss: 0.60849 - Test loss: 0.71047\n",
      "Epoch 3650 - lr: 0.05000 - Train loss: 0.60846 - Test loss: 0.71052\n",
      "Epoch 3651 - lr: 0.05000 - Train loss: 0.60843 - Test loss: 0.71056\n",
      "Epoch 3652 - lr: 0.05000 - Train loss: 0.60840 - Test loss: 0.71061\n",
      "Epoch 3653 - lr: 0.05000 - Train loss: 0.60837 - Test loss: 0.71065\n",
      "Epoch 3654 - lr: 0.05000 - Train loss: 0.60834 - Test loss: 0.71069\n",
      "Epoch 3655 - lr: 0.05000 - Train loss: 0.60831 - Test loss: 0.71074\n",
      "Epoch 3656 - lr: 0.05000 - Train loss: 0.60828 - Test loss: 0.71078\n",
      "Epoch 3657 - lr: 0.05000 - Train loss: 0.60825 - Test loss: 0.71082\n",
      "Epoch 3658 - lr: 0.05000 - Train loss: 0.60822 - Test loss: 0.71086\n",
      "Epoch 3659 - lr: 0.05000 - Train loss: 0.60819 - Test loss: 0.71090\n",
      "Epoch 3660 - lr: 0.05000 - Train loss: 0.60816 - Test loss: 0.71094\n",
      "Epoch 3661 - lr: 0.05000 - Train loss: 0.60813 - Test loss: 0.71098\n",
      "Epoch 3662 - lr: 0.05000 - Train loss: 0.60810 - Test loss: 0.71102\n",
      "Epoch 3663 - lr: 0.05000 - Train loss: 0.60807 - Test loss: 0.71106\n",
      "Epoch 3664 - lr: 0.05000 - Train loss: 0.60805 - Test loss: 0.71110\n",
      "Epoch 3665 - lr: 0.05000 - Train loss: 0.60802 - Test loss: 0.71113\n",
      "Epoch 3666 - lr: 0.05000 - Train loss: 0.60799 - Test loss: 0.71117\n",
      "Epoch 3667 - lr: 0.05000 - Train loss: 0.60796 - Test loss: 0.71121\n",
      "Epoch 3668 - lr: 0.05000 - Train loss: 0.60793 - Test loss: 0.71124\n",
      "Epoch 3669 - lr: 0.05000 - Train loss: 0.60791 - Test loss: 0.71128\n",
      "Epoch 3670 - lr: 0.05000 - Train loss: 0.60788 - Test loss: 0.71131\n",
      "Epoch 3671 - lr: 0.05000 - Train loss: 0.60785 - Test loss: 0.71135\n",
      "Epoch 3672 - lr: 0.05000 - Train loss: 0.60783 - Test loss: 0.71138\n",
      "Epoch 3673 - lr: 0.05000 - Train loss: 0.60780 - Test loss: 0.71142\n",
      "Epoch 3674 - lr: 0.05000 - Train loss: 0.60777 - Test loss: 0.71145\n",
      "Epoch 3675 - lr: 0.05000 - Train loss: 0.60775 - Test loss: 0.71148\n",
      "Epoch 3676 - lr: 0.05000 - Train loss: 0.60772 - Test loss: 0.71151\n",
      "Epoch 3677 - lr: 0.05000 - Train loss: 0.60769 - Test loss: 0.71154\n",
      "Epoch 3678 - lr: 0.05000 - Train loss: 0.60767 - Test loss: 0.71157\n",
      "Epoch 3679 - lr: 0.05000 - Train loss: 0.60764 - Test loss: 0.71160\n",
      "Epoch 3680 - lr: 0.05000 - Train loss: 0.60762 - Test loss: 0.71163\n",
      "Epoch 3681 - lr: 0.05000 - Train loss: 0.60759 - Test loss: 0.71166\n",
      "Epoch 3682 - lr: 0.05000 - Train loss: 0.60757 - Test loss: 0.71168\n",
      "Epoch 3683 - lr: 0.05000 - Train loss: 0.60754 - Test loss: 0.71171\n",
      "Epoch 3684 - lr: 0.05000 - Train loss: 0.60752 - Test loss: 0.71173\n",
      "Epoch 3685 - lr: 0.05000 - Train loss: 0.60749 - Test loss: 0.71176\n",
      "Epoch 3686 - lr: 0.05000 - Train loss: 0.60747 - Test loss: 0.71178\n",
      "Epoch 3687 - lr: 0.05000 - Train loss: 0.60744 - Test loss: 0.71180\n",
      "Epoch 3688 - lr: 0.05000 - Train loss: 0.60742 - Test loss: 0.71183\n",
      "Epoch 3689 - lr: 0.05000 - Train loss: 0.60739 - Test loss: 0.71185\n",
      "Epoch 3690 - lr: 0.05000 - Train loss: 0.60737 - Test loss: 0.71187\n",
      "Epoch 3691 - lr: 0.05000 - Train loss: 0.60735 - Test loss: 0.71188\n",
      "Epoch 3692 - lr: 0.05000 - Train loss: 0.60732 - Test loss: 0.71190\n",
      "Epoch 3693 - lr: 0.05000 - Train loss: 0.60730 - Test loss: 0.71192\n",
      "Epoch 3694 - lr: 0.05000 - Train loss: 0.60728 - Test loss: 0.71193\n",
      "Epoch 3695 - lr: 0.05000 - Train loss: 0.60725 - Test loss: 0.71195\n",
      "Epoch 3696 - lr: 0.05000 - Train loss: 0.60723 - Test loss: 0.71196\n",
      "Epoch 3697 - lr: 0.05000 - Train loss: 0.60721 - Test loss: 0.71197\n",
      "Epoch 3698 - lr: 0.05000 - Train loss: 0.60718 - Test loss: 0.71198\n",
      "Epoch 3699 - lr: 0.05000 - Train loss: 0.60716 - Test loss: 0.71199\n",
      "Epoch 3700 - lr: 0.05000 - Train loss: 0.60714 - Test loss: 0.71200\n",
      "Epoch 3701 - lr: 0.05000 - Train loss: 0.60711 - Test loss: 0.71201\n",
      "Epoch 3702 - lr: 0.05000 - Train loss: 0.60709 - Test loss: 0.71201\n",
      "Epoch 3703 - lr: 0.05000 - Train loss: 0.60707 - Test loss: 0.71201\n",
      "Epoch 3704 - lr: 0.05000 - Train loss: 0.60705 - Test loss: 0.71201\n",
      "Epoch 3705 - lr: 0.05000 - Train loss: 0.60702 - Test loss: 0.71201\n",
      "Epoch 3706 - lr: 0.05000 - Train loss: 0.60700 - Test loss: 0.71201\n",
      "Epoch 3707 - lr: 0.05000 - Train loss: 0.60698 - Test loss: 0.71200\n",
      "Epoch 3708 - lr: 0.05000 - Train loss: 0.60696 - Test loss: 0.71200\n",
      "Epoch 3709 - lr: 0.05000 - Train loss: 0.60694 - Test loss: 0.71199\n",
      "Epoch 3710 - lr: 0.05000 - Train loss: 0.60691 - Test loss: 0.71197\n",
      "Epoch 3711 - lr: 0.05000 - Train loss: 0.60689 - Test loss: 0.71196\n",
      "Epoch 3712 - lr: 0.05000 - Train loss: 0.60687 - Test loss: 0.71194\n",
      "Epoch 3713 - lr: 0.05000 - Train loss: 0.60685 - Test loss: 0.71192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3714 - lr: 0.05000 - Train loss: 0.60683 - Test loss: 0.71190\n",
      "Epoch 3715 - lr: 0.05000 - Train loss: 0.60681 - Test loss: 0.71187\n",
      "Epoch 3716 - lr: 0.05000 - Train loss: 0.60678 - Test loss: 0.71184\n",
      "Epoch 3717 - lr: 0.05000 - Train loss: 0.60676 - Test loss: 0.71181\n",
      "Epoch 3718 - lr: 0.05000 - Train loss: 0.60674 - Test loss: 0.71178\n",
      "Epoch 3719 - lr: 0.05000 - Train loss: 0.60672 - Test loss: 0.71174\n",
      "Epoch 3720 - lr: 0.05000 - Train loss: 0.60670 - Test loss: 0.71169\n",
      "Epoch 3721 - lr: 0.05000 - Train loss: 0.60668 - Test loss: 0.71164\n",
      "Epoch 3722 - lr: 0.05000 - Train loss: 0.60666 - Test loss: 0.71159\n",
      "Epoch 3723 - lr: 0.05000 - Train loss: 0.60663 - Test loss: 0.71153\n",
      "Epoch 3724 - lr: 0.05000 - Train loss: 0.60661 - Test loss: 0.71147\n",
      "Epoch 3725 - lr: 0.05000 - Train loss: 0.60659 - Test loss: 0.71141\n",
      "Epoch 3726 - lr: 0.05000 - Train loss: 0.60657 - Test loss: 0.71133\n",
      "Epoch 3727 - lr: 0.05000 - Train loss: 0.60654 - Test loss: 0.71126\n",
      "Epoch 3728 - lr: 0.05000 - Train loss: 0.60652 - Test loss: 0.71117\n",
      "Epoch 3729 - lr: 0.05000 - Train loss: 0.60650 - Test loss: 0.71109\n",
      "Epoch 3730 - lr: 0.05000 - Train loss: 0.60647 - Test loss: 0.71100\n",
      "Epoch 3731 - lr: 0.05000 - Train loss: 0.60645 - Test loss: 0.71090\n",
      "Epoch 3732 - lr: 0.05000 - Train loss: 0.60642 - Test loss: 0.71080\n",
      "Epoch 3733 - lr: 0.05000 - Train loss: 0.60639 - Test loss: 0.71070\n",
      "Epoch 3734 - lr: 0.05000 - Train loss: 0.60636 - Test loss: 0.71060\n",
      "Epoch 3735 - lr: 0.05000 - Train loss: 0.60633 - Test loss: 0.71049\n",
      "Epoch 3736 - lr: 0.05000 - Train loss: 0.60630 - Test loss: 0.71039\n",
      "Epoch 3737 - lr: 0.05000 - Train loss: 0.60627 - Test loss: 0.71030\n",
      "Epoch 3738 - lr: 0.05000 - Train loss: 0.60623 - Test loss: 0.71021\n",
      "Epoch 3739 - lr: 0.05000 - Train loss: 0.60619 - Test loss: 0.71014\n",
      "Epoch 3740 - lr: 0.05000 - Train loss: 0.60614 - Test loss: 0.71008\n",
      "Epoch 3741 - lr: 0.05000 - Train loss: 0.60610 - Test loss: 0.71004\n",
      "Epoch 3742 - lr: 0.05000 - Train loss: 0.60604 - Test loss: 0.71003\n",
      "Epoch 3743 - lr: 0.05000 - Train loss: 0.60599 - Test loss: 0.71006\n",
      "Epoch 3744 - lr: 0.05000 - Train loss: 0.60592 - Test loss: 0.71012\n",
      "Epoch 3745 - lr: 0.05000 - Train loss: 0.60586 - Test loss: 0.71022\n",
      "Epoch 3746 - lr: 0.05000 - Train loss: 0.60579 - Test loss: 0.71036\n",
      "Epoch 3747 - lr: 0.05000 - Train loss: 0.60571 - Test loss: 0.71054\n",
      "Epoch 3748 - lr: 0.05000 - Train loss: 0.60564 - Test loss: 0.71075\n",
      "Epoch 3749 - lr: 0.05000 - Train loss: 0.60556 - Test loss: 0.71098\n",
      "Epoch 3750 - lr: 0.05000 - Train loss: 0.60548 - Test loss: 0.71124\n",
      "Epoch 3751 - lr: 0.05000 - Train loss: 0.60541 - Test loss: 0.71150\n",
      "Epoch 3752 - lr: 0.05000 - Train loss: 0.60533 - Test loss: 0.71175\n",
      "Epoch 3753 - lr: 0.05000 - Train loss: 0.60526 - Test loss: 0.71199\n",
      "Epoch 3754 - lr: 0.05000 - Train loss: 0.60519 - Test loss: 0.71222\n",
      "Epoch 3755 - lr: 0.05000 - Train loss: 0.60513 - Test loss: 0.71242\n",
      "Epoch 3756 - lr: 0.05000 - Train loss: 0.60507 - Test loss: 0.71260\n",
      "Epoch 3757 - lr: 0.05000 - Train loss: 0.60501 - Test loss: 0.71279\n",
      "Epoch 3758 - lr: 0.05000 - Train loss: 0.60497 - Test loss: 0.71300\n",
      "Epoch 3759 - lr: 0.05000 - Train loss: 0.60493 - Test loss: 0.71322\n",
      "Epoch 3760 - lr: 0.05000 - Train loss: 0.60489 - Test loss: 0.71346\n",
      "Epoch 3761 - lr: 0.05000 - Train loss: 0.60485 - Test loss: 0.71369\n",
      "Epoch 3762 - lr: 0.05000 - Train loss: 0.60481 - Test loss: 0.71391\n",
      "Epoch 3763 - lr: 0.05000 - Train loss: 0.60476 - Test loss: 0.71412\n",
      "Epoch 3764 - lr: 0.05000 - Train loss: 0.60472 - Test loss: 0.71431\n",
      "Epoch 3765 - lr: 0.05000 - Train loss: 0.60467 - Test loss: 0.71449\n",
      "Epoch 3766 - lr: 0.05000 - Train loss: 0.60463 - Test loss: 0.71466\n",
      "Epoch 3767 - lr: 0.05000 - Train loss: 0.60458 - Test loss: 0.71482\n",
      "Epoch 3768 - lr: 0.05000 - Train loss: 0.60454 - Test loss: 0.71497\n",
      "Epoch 3769 - lr: 0.05000 - Train loss: 0.60449 - Test loss: 0.71511\n",
      "Epoch 3770 - lr: 0.05000 - Train loss: 0.60445 - Test loss: 0.71524\n",
      "Epoch 3771 - lr: 0.05000 - Train loss: 0.60441 - Test loss: 0.71536\n",
      "Epoch 3772 - lr: 0.05000 - Train loss: 0.60436 - Test loss: 0.71548\n",
      "Epoch 3773 - lr: 0.05000 - Train loss: 0.60432 - Test loss: 0.71560\n",
      "Epoch 3774 - lr: 0.05000 - Train loss: 0.60428 - Test loss: 0.71570\n",
      "Epoch 3775 - lr: 0.05000 - Train loss: 0.60424 - Test loss: 0.71581\n",
      "Epoch 3776 - lr: 0.05000 - Train loss: 0.60420 - Test loss: 0.71591\n",
      "Epoch 3777 - lr: 0.05000 - Train loss: 0.60416 - Test loss: 0.71601\n",
      "Epoch 3778 - lr: 0.05000 - Train loss: 0.60411 - Test loss: 0.71610\n",
      "Epoch 3779 - lr: 0.05000 - Train loss: 0.60407 - Test loss: 0.71619\n",
      "Epoch 3780 - lr: 0.05000 - Train loss: 0.60403 - Test loss: 0.71628\n",
      "Epoch 3781 - lr: 0.05000 - Train loss: 0.60399 - Test loss: 0.71637\n",
      "Epoch 3782 - lr: 0.05000 - Train loss: 0.60396 - Test loss: 0.71646\n",
      "Epoch 3783 - lr: 0.05000 - Train loss: 0.60392 - Test loss: 0.71654\n",
      "Epoch 3784 - lr: 0.05000 - Train loss: 0.60388 - Test loss: 0.71662\n",
      "Epoch 3785 - lr: 0.05000 - Train loss: 0.60384 - Test loss: 0.71670\n",
      "Epoch 3786 - lr: 0.05000 - Train loss: 0.60380 - Test loss: 0.71678\n",
      "Epoch 3787 - lr: 0.05000 - Train loss: 0.60377 - Test loss: 0.71686\n",
      "Epoch 3788 - lr: 0.05000 - Train loss: 0.60373 - Test loss: 0.71694\n",
      "Epoch 3789 - lr: 0.05000 - Train loss: 0.60369 - Test loss: 0.71702\n",
      "Epoch 3790 - lr: 0.05000 - Train loss: 0.60366 - Test loss: 0.71710\n",
      "Epoch 3791 - lr: 0.05000 - Train loss: 0.60362 - Test loss: 0.71717\n",
      "Epoch 3792 - lr: 0.05000 - Train loss: 0.60359 - Test loss: 0.71725\n",
      "Epoch 3793 - lr: 0.05000 - Train loss: 0.60355 - Test loss: 0.71732\n",
      "Epoch 3794 - lr: 0.05000 - Train loss: 0.60352 - Test loss: 0.71740\n",
      "Epoch 3795 - lr: 0.05000 - Train loss: 0.60348 - Test loss: 0.71747\n",
      "Epoch 3796 - lr: 0.05000 - Train loss: 0.60345 - Test loss: 0.71754\n",
      "Epoch 3797 - lr: 0.05000 - Train loss: 0.60342 - Test loss: 0.71762\n",
      "Epoch 3798 - lr: 0.05000 - Train loss: 0.60338 - Test loss: 0.71769\n",
      "Epoch 3799 - lr: 0.05000 - Train loss: 0.60335 - Test loss: 0.71776\n",
      "Epoch 3800 - lr: 0.05000 - Train loss: 0.60332 - Test loss: 0.71784\n",
      "Epoch 3801 - lr: 0.05000 - Train loss: 0.60328 - Test loss: 0.71791\n",
      "Epoch 3802 - lr: 0.05000 - Train loss: 0.60325 - Test loss: 0.71798\n",
      "Epoch 3803 - lr: 0.05000 - Train loss: 0.60322 - Test loss: 0.71805\n",
      "Epoch 3804 - lr: 0.05000 - Train loss: 0.60319 - Test loss: 0.71812\n",
      "Epoch 3805 - lr: 0.05000 - Train loss: 0.60316 - Test loss: 0.71819\n",
      "Epoch 3806 - lr: 0.05000 - Train loss: 0.60313 - Test loss: 0.71826\n",
      "Epoch 3807 - lr: 0.05000 - Train loss: 0.60310 - Test loss: 0.71834\n",
      "Epoch 3808 - lr: 0.05000 - Train loss: 0.60307 - Test loss: 0.71841\n",
      "Epoch 3809 - lr: 0.05000 - Train loss: 0.60304 - Test loss: 0.71848\n",
      "Epoch 3810 - lr: 0.05000 - Train loss: 0.60301 - Test loss: 0.71854\n",
      "Epoch 3811 - lr: 0.05000 - Train loss: 0.60298 - Test loss: 0.71861\n",
      "Epoch 3812 - lr: 0.05000 - Train loss: 0.60295 - Test loss: 0.71868\n",
      "Epoch 3813 - lr: 0.05000 - Train loss: 0.60292 - Test loss: 0.71875\n",
      "Epoch 3814 - lr: 0.05000 - Train loss: 0.60289 - Test loss: 0.71882\n",
      "Epoch 3815 - lr: 0.05000 - Train loss: 0.60286 - Test loss: 0.71889\n",
      "Epoch 3816 - lr: 0.05000 - Train loss: 0.60283 - Test loss: 0.71896\n",
      "Epoch 3817 - lr: 0.05000 - Train loss: 0.60281 - Test loss: 0.71902\n",
      "Epoch 3818 - lr: 0.05000 - Train loss: 0.60278 - Test loss: 0.71909\n",
      "Epoch 3819 - lr: 0.05000 - Train loss: 0.60275 - Test loss: 0.71916\n",
      "Epoch 3820 - lr: 0.05000 - Train loss: 0.60272 - Test loss: 0.71922\n",
      "Epoch 3821 - lr: 0.05000 - Train loss: 0.60270 - Test loss: 0.71929\n",
      "Epoch 3822 - lr: 0.05000 - Train loss: 0.60267 - Test loss: 0.71936\n",
      "Epoch 3823 - lr: 0.05000 - Train loss: 0.60264 - Test loss: 0.71942\n",
      "Epoch 3824 - lr: 0.05000 - Train loss: 0.60262 - Test loss: 0.71949\n",
      "Epoch 3825 - lr: 0.05000 - Train loss: 0.60259 - Test loss: 0.71955\n",
      "Epoch 3826 - lr: 0.05000 - Train loss: 0.60256 - Test loss: 0.71962\n",
      "Epoch 3827 - lr: 0.05000 - Train loss: 0.60254 - Test loss: 0.71968\n",
      "Epoch 3828 - lr: 0.05000 - Train loss: 0.60251 - Test loss: 0.71975\n",
      "Epoch 3829 - lr: 0.05000 - Train loss: 0.60249 - Test loss: 0.71981\n",
      "Epoch 3830 - lr: 0.05000 - Train loss: 0.60246 - Test loss: 0.71987\n",
      "Epoch 3831 - lr: 0.05000 - Train loss: 0.60244 - Test loss: 0.71993\n",
      "Epoch 3832 - lr: 0.05000 - Train loss: 0.60241 - Test loss: 0.72000\n",
      "Epoch 3833 - lr: 0.05000 - Train loss: 0.60239 - Test loss: 0.72006\n",
      "Epoch 3834 - lr: 0.05000 - Train loss: 0.60236 - Test loss: 0.72012\n",
      "Epoch 3835 - lr: 0.05000 - Train loss: 0.60234 - Test loss: 0.72018\n",
      "Epoch 3836 - lr: 0.05000 - Train loss: 0.60231 - Test loss: 0.72024\n",
      "Epoch 3837 - lr: 0.05000 - Train loss: 0.60229 - Test loss: 0.72030\n",
      "Epoch 3838 - lr: 0.05000 - Train loss: 0.60226 - Test loss: 0.72036\n",
      "Epoch 3839 - lr: 0.05000 - Train loss: 0.60224 - Test loss: 0.72042\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3840 - lr: 0.05000 - Train loss: 0.60222 - Test loss: 0.72048\n",
      "Epoch 3841 - lr: 0.05000 - Train loss: 0.60219 - Test loss: 0.72054\n",
      "Epoch 3842 - lr: 0.05000 - Train loss: 0.60217 - Test loss: 0.72060\n",
      "Epoch 3843 - lr: 0.05000 - Train loss: 0.60215 - Test loss: 0.72066\n",
      "Epoch 3844 - lr: 0.05000 - Train loss: 0.60212 - Test loss: 0.72072\n",
      "Epoch 3845 - lr: 0.05000 - Train loss: 0.60210 - Test loss: 0.72078\n",
      "Epoch 3846 - lr: 0.05000 - Train loss: 0.60208 - Test loss: 0.72083\n",
      "Epoch 3847 - lr: 0.05000 - Train loss: 0.60206 - Test loss: 0.72089\n",
      "Epoch 3848 - lr: 0.05000 - Train loss: 0.60203 - Test loss: 0.72095\n",
      "Epoch 3849 - lr: 0.05000 - Train loss: 0.60201 - Test loss: 0.72100\n",
      "Epoch 3850 - lr: 0.05000 - Train loss: 0.60199 - Test loss: 0.72106\n",
      "Epoch 3851 - lr: 0.05000 - Train loss: 0.60197 - Test loss: 0.72111\n",
      "Epoch 3852 - lr: 0.05000 - Train loss: 0.60194 - Test loss: 0.72117\n",
      "Epoch 3853 - lr: 0.05000 - Train loss: 0.60192 - Test loss: 0.72123\n",
      "Epoch 3854 - lr: 0.05000 - Train loss: 0.60190 - Test loss: 0.72128\n",
      "Epoch 3855 - lr: 0.05000 - Train loss: 0.60188 - Test loss: 0.72133\n",
      "Epoch 3856 - lr: 0.05000 - Train loss: 0.60186 - Test loss: 0.72139\n",
      "Epoch 3857 - lr: 0.05000 - Train loss: 0.60184 - Test loss: 0.72144\n",
      "Epoch 3858 - lr: 0.05000 - Train loss: 0.60182 - Test loss: 0.72150\n",
      "Epoch 3859 - lr: 0.05000 - Train loss: 0.60180 - Test loss: 0.72155\n",
      "Epoch 3860 - lr: 0.05000 - Train loss: 0.60177 - Test loss: 0.72160\n",
      "Epoch 3861 - lr: 0.05000 - Train loss: 0.60175 - Test loss: 0.72166\n",
      "Epoch 3862 - lr: 0.05000 - Train loss: 0.60173 - Test loss: 0.72171\n",
      "Epoch 3863 - lr: 0.05000 - Train loss: 0.60171 - Test loss: 0.72176\n",
      "Epoch 3864 - lr: 0.05000 - Train loss: 0.60169 - Test loss: 0.72181\n",
      "Epoch 3865 - lr: 0.05000 - Train loss: 0.60167 - Test loss: 0.72187\n",
      "Epoch 3866 - lr: 0.05000 - Train loss: 0.60165 - Test loss: 0.72192\n",
      "Epoch 3867 - lr: 0.05000 - Train loss: 0.60163 - Test loss: 0.72197\n",
      "Epoch 3868 - lr: 0.05000 - Train loss: 0.60161 - Test loss: 0.72202\n",
      "Epoch 3869 - lr: 0.05000 - Train loss: 0.60159 - Test loss: 0.72207\n",
      "Epoch 3870 - lr: 0.05000 - Train loss: 0.60157 - Test loss: 0.72212\n",
      "Epoch 3871 - lr: 0.05000 - Train loss: 0.60155 - Test loss: 0.72217\n",
      "Epoch 3872 - lr: 0.05000 - Train loss: 0.60153 - Test loss: 0.72222\n",
      "Epoch 3873 - lr: 0.05000 - Train loss: 0.60152 - Test loss: 0.72227\n",
      "Epoch 3874 - lr: 0.05000 - Train loss: 0.60150 - Test loss: 0.72232\n",
      "Epoch 3875 - lr: 0.05000 - Train loss: 0.60148 - Test loss: 0.72237\n",
      "Epoch 3876 - lr: 0.05000 - Train loss: 0.60146 - Test loss: 0.72242\n",
      "Epoch 3877 - lr: 0.05000 - Train loss: 0.60144 - Test loss: 0.72247\n",
      "Epoch 3878 - lr: 0.05000 - Train loss: 0.60142 - Test loss: 0.72252\n",
      "Epoch 3879 - lr: 0.05000 - Train loss: 0.60140 - Test loss: 0.72257\n",
      "Epoch 3880 - lr: 0.05000 - Train loss: 0.60138 - Test loss: 0.72262\n",
      "Epoch 3881 - lr: 0.05000 - Train loss: 0.60137 - Test loss: 0.72266\n",
      "Epoch 3882 - lr: 0.05000 - Train loss: 0.60135 - Test loss: 0.72271\n",
      "Epoch 3883 - lr: 0.05000 - Train loss: 0.60133 - Test loss: 0.72276\n",
      "Epoch 3884 - lr: 0.05000 - Train loss: 0.60131 - Test loss: 0.72281\n",
      "Epoch 3885 - lr: 0.05000 - Train loss: 0.60129 - Test loss: 0.72285\n",
      "Epoch 3886 - lr: 0.05000 - Train loss: 0.60128 - Test loss: 0.72290\n",
      "Epoch 3887 - lr: 0.05000 - Train loss: 0.60126 - Test loss: 0.72295\n",
      "Epoch 3888 - lr: 0.05000 - Train loss: 0.60124 - Test loss: 0.72300\n",
      "Epoch 3889 - lr: 0.05000 - Train loss: 0.60122 - Test loss: 0.72304\n",
      "Epoch 3890 - lr: 0.05000 - Train loss: 0.60121 - Test loss: 0.72309\n",
      "Epoch 3891 - lr: 0.05000 - Train loss: 0.60119 - Test loss: 0.72314\n",
      "Epoch 3892 - lr: 0.05000 - Train loss: 0.60117 - Test loss: 0.72318\n",
      "Epoch 3893 - lr: 0.05000 - Train loss: 0.60115 - Test loss: 0.72323\n",
      "Epoch 3894 - lr: 0.05000 - Train loss: 0.60114 - Test loss: 0.72328\n",
      "Epoch 3895 - lr: 0.05000 - Train loss: 0.60112 - Test loss: 0.72332\n",
      "Epoch 3896 - lr: 0.05000 - Train loss: 0.60110 - Test loss: 0.72337\n",
      "Epoch 3897 - lr: 0.05000 - Train loss: 0.60109 - Test loss: 0.72341\n",
      "Epoch 3898 - lr: 0.05000 - Train loss: 0.60107 - Test loss: 0.72346\n",
      "Epoch 3899 - lr: 0.05000 - Train loss: 0.60105 - Test loss: 0.72350\n",
      "Epoch 3900 - lr: 0.05000 - Train loss: 0.60104 - Test loss: 0.72355\n",
      "Epoch 3901 - lr: 0.05000 - Train loss: 0.60102 - Test loss: 0.72359\n",
      "Epoch 3902 - lr: 0.05000 - Train loss: 0.60100 - Test loss: 0.72364\n",
      "Epoch 3903 - lr: 0.05000 - Train loss: 0.60099 - Test loss: 0.72368\n",
      "Epoch 3904 - lr: 0.05000 - Train loss: 0.60097 - Test loss: 0.72373\n",
      "Epoch 3905 - lr: 0.05000 - Train loss: 0.60095 - Test loss: 0.72377\n",
      "Epoch 3906 - lr: 0.05000 - Train loss: 0.60094 - Test loss: 0.72382\n",
      "Epoch 3907 - lr: 0.05000 - Train loss: 0.60092 - Test loss: 0.72386\n",
      "Epoch 3908 - lr: 0.05000 - Train loss: 0.60091 - Test loss: 0.72391\n",
      "Epoch 3909 - lr: 0.05000 - Train loss: 0.60089 - Test loss: 0.72395\n",
      "Epoch 3910 - lr: 0.05000 - Train loss: 0.60087 - Test loss: 0.72399\n",
      "Epoch 3911 - lr: 0.05000 - Train loss: 0.60086 - Test loss: 0.72404\n",
      "Epoch 3912 - lr: 0.05000 - Train loss: 0.60084 - Test loss: 0.72408\n",
      "Epoch 3913 - lr: 0.05000 - Train loss: 0.60083 - Test loss: 0.72412\n",
      "Epoch 3914 - lr: 0.05000 - Train loss: 0.60081 - Test loss: 0.72417\n",
      "Epoch 3915 - lr: 0.05000 - Train loss: 0.60080 - Test loss: 0.72421\n",
      "Epoch 3916 - lr: 0.05000 - Train loss: 0.60078 - Test loss: 0.72425\n",
      "Epoch 3917 - lr: 0.05000 - Train loss: 0.60077 - Test loss: 0.72430\n",
      "Epoch 3918 - lr: 0.05000 - Train loss: 0.60075 - Test loss: 0.72434\n",
      "Epoch 3919 - lr: 0.05000 - Train loss: 0.60074 - Test loss: 0.72438\n",
      "Epoch 3920 - lr: 0.05000 - Train loss: 0.60072 - Test loss: 0.72443\n",
      "Epoch 3921 - lr: 0.05000 - Train loss: 0.60071 - Test loss: 0.72447\n",
      "Epoch 3922 - lr: 0.05000 - Train loss: 0.60069 - Test loss: 0.72451\n",
      "Epoch 3923 - lr: 0.05000 - Train loss: 0.60068 - Test loss: 0.72455\n",
      "Epoch 3924 - lr: 0.05000 - Train loss: 0.60066 - Test loss: 0.72460\n",
      "Epoch 3925 - lr: 0.05000 - Train loss: 0.60065 - Test loss: 0.72464\n",
      "Epoch 3926 - lr: 0.05000 - Train loss: 0.60063 - Test loss: 0.72468\n",
      "Epoch 3927 - lr: 0.05000 - Train loss: 0.60062 - Test loss: 0.72472\n",
      "Epoch 3928 - lr: 0.05000 - Train loss: 0.60060 - Test loss: 0.72476\n",
      "Epoch 3929 - lr: 0.05000 - Train loss: 0.60059 - Test loss: 0.72481\n",
      "Epoch 3930 - lr: 0.05000 - Train loss: 0.60058 - Test loss: 0.72485\n",
      "Epoch 3931 - lr: 0.05000 - Train loss: 0.60056 - Test loss: 0.72489\n",
      "Epoch 3932 - lr: 0.05000 - Train loss: 0.60055 - Test loss: 0.72493\n",
      "Epoch 3933 - lr: 0.05000 - Train loss: 0.60053 - Test loss: 0.72497\n",
      "Epoch 3934 - lr: 0.05000 - Train loss: 0.60052 - Test loss: 0.72501\n",
      "Epoch 3935 - lr: 0.05000 - Train loss: 0.60050 - Test loss: 0.72506\n",
      "Epoch 3936 - lr: 0.05000 - Train loss: 0.60049 - Test loss: 0.72510\n",
      "Epoch 3937 - lr: 0.05000 - Train loss: 0.60048 - Test loss: 0.72514\n",
      "Epoch 3938 - lr: 0.05000 - Train loss: 0.60046 - Test loss: 0.72518\n",
      "Epoch 3939 - lr: 0.05000 - Train loss: 0.60045 - Test loss: 0.72522\n",
      "Epoch 3940 - lr: 0.05000 - Train loss: 0.60044 - Test loss: 0.72526\n",
      "Epoch 3941 - lr: 0.05000 - Train loss: 0.60042 - Test loss: 0.72530\n",
      "Epoch 3942 - lr: 0.05000 - Train loss: 0.60041 - Test loss: 0.72534\n",
      "Epoch 3943 - lr: 0.05000 - Train loss: 0.60040 - Test loss: 0.72538\n",
      "Epoch 3944 - lr: 0.05000 - Train loss: 0.60038 - Test loss: 0.72542\n",
      "Epoch 3945 - lr: 0.05000 - Train loss: 0.60037 - Test loss: 0.72546\n",
      "Epoch 3946 - lr: 0.05000 - Train loss: 0.60036 - Test loss: 0.72550\n",
      "Epoch 3947 - lr: 0.05000 - Train loss: 0.60034 - Test loss: 0.72554\n",
      "Epoch 3948 - lr: 0.05000 - Train loss: 0.60033 - Test loss: 0.72558\n",
      "Epoch 3949 - lr: 0.05000 - Train loss: 0.60032 - Test loss: 0.72562\n",
      "Epoch 3950 - lr: 0.05000 - Train loss: 0.60030 - Test loss: 0.72566\n",
      "Epoch 3951 - lr: 0.05000 - Train loss: 0.60029 - Test loss: 0.72570\n",
      "Epoch 3952 - lr: 0.05000 - Train loss: 0.60028 - Test loss: 0.72574\n",
      "Epoch 3953 - lr: 0.05000 - Train loss: 0.60026 - Test loss: 0.72578\n",
      "Epoch 3954 - lr: 0.05000 - Train loss: 0.60025 - Test loss: 0.72582\n",
      "Epoch 3955 - lr: 0.05000 - Train loss: 0.60024 - Test loss: 0.72586\n",
      "Epoch 3956 - lr: 0.05000 - Train loss: 0.60023 - Test loss: 0.72590\n",
      "Epoch 3957 - lr: 0.05000 - Train loss: 0.60021 - Test loss: 0.72594\n",
      "Epoch 3958 - lr: 0.05000 - Train loss: 0.60020 - Test loss: 0.72598\n",
      "Epoch 3959 - lr: 0.05000 - Train loss: 0.60019 - Test loss: 0.72602\n",
      "Epoch 3960 - lr: 0.05000 - Train loss: 0.60017 - Test loss: 0.72606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3961 - lr: 0.05000 - Train loss: 0.60016 - Test loss: 0.72610\n",
      "Epoch 3962 - lr: 0.05000 - Train loss: 0.60015 - Test loss: 0.72614\n",
      "Epoch 3963 - lr: 0.05000 - Train loss: 0.60014 - Test loss: 0.72618\n",
      "Epoch 3964 - lr: 0.05000 - Train loss: 0.60012 - Test loss: 0.72622\n",
      "Epoch 3965 - lr: 0.05000 - Train loss: 0.60011 - Test loss: 0.72626\n",
      "Epoch 3966 - lr: 0.05000 - Train loss: 0.60010 - Test loss: 0.72629\n",
      "Epoch 3967 - lr: 0.05000 - Train loss: 0.60009 - Test loss: 0.72633\n",
      "Epoch 3968 - lr: 0.05000 - Train loss: 0.60008 - Test loss: 0.72637\n",
      "Epoch 3969 - lr: 0.05000 - Train loss: 0.60006 - Test loss: 0.72641\n",
      "Epoch 3970 - lr: 0.05000 - Train loss: 0.60005 - Test loss: 0.72645\n",
      "Epoch 3971 - lr: 0.05000 - Train loss: 0.60004 - Test loss: 0.72649\n",
      "Epoch 3972 - lr: 0.05000 - Train loss: 0.60003 - Test loss: 0.72653\n",
      "Epoch 3973 - lr: 0.05000 - Train loss: 0.60002 - Test loss: 0.72657\n",
      "Epoch 3974 - lr: 0.05000 - Train loss: 0.60000 - Test loss: 0.72660\n",
      "Epoch 3975 - lr: 0.05000 - Train loss: 0.59999 - Test loss: 0.72664\n",
      "Epoch 3976 - lr: 0.05000 - Train loss: 0.59998 - Test loss: 0.72668\n",
      "Epoch 3977 - lr: 0.05000 - Train loss: 0.59997 - Test loss: 0.72672\n",
      "Epoch 3978 - lr: 0.05000 - Train loss: 0.59996 - Test loss: 0.72676\n",
      "Epoch 3979 - lr: 0.05000 - Train loss: 0.59995 - Test loss: 0.72679\n",
      "Epoch 3980 - lr: 0.05000 - Train loss: 0.59993 - Test loss: 0.72683\n",
      "Epoch 3981 - lr: 0.05000 - Train loss: 0.59992 - Test loss: 0.72687\n",
      "Epoch 3982 - lr: 0.05000 - Train loss: 0.59991 - Test loss: 0.72691\n",
      "Epoch 3983 - lr: 0.05000 - Train loss: 0.59990 - Test loss: 0.72695\n",
      "Epoch 3984 - lr: 0.05000 - Train loss: 0.59989 - Test loss: 0.72698\n",
      "Epoch 3985 - lr: 0.05000 - Train loss: 0.59988 - Test loss: 0.72702\n",
      "Epoch 3986 - lr: 0.05000 - Train loss: 0.59986 - Test loss: 0.72706\n",
      "Epoch 3987 - lr: 0.05000 - Train loss: 0.59985 - Test loss: 0.72710\n",
      "Epoch 3988 - lr: 0.05000 - Train loss: 0.59984 - Test loss: 0.72713\n",
      "Epoch 3989 - lr: 0.05000 - Train loss: 0.59983 - Test loss: 0.72717\n",
      "Epoch 3990 - lr: 0.05000 - Train loss: 0.59982 - Test loss: 0.72721\n",
      "Epoch 3991 - lr: 0.05000 - Train loss: 0.59981 - Test loss: 0.72725\n",
      "Epoch 3992 - lr: 0.05000 - Train loss: 0.59980 - Test loss: 0.72728\n",
      "Epoch 3993 - lr: 0.05000 - Train loss: 0.59979 - Test loss: 0.72732\n",
      "Epoch 3994 - lr: 0.05000 - Train loss: 0.59978 - Test loss: 0.72736\n",
      "Epoch 3995 - lr: 0.05000 - Train loss: 0.59976 - Test loss: 0.72740\n",
      "Epoch 3996 - lr: 0.05000 - Train loss: 0.59975 - Test loss: 0.72743\n",
      "Epoch 3997 - lr: 0.05000 - Train loss: 0.59974 - Test loss: 0.72747\n",
      "Epoch 3998 - lr: 0.05000 - Train loss: 0.59973 - Test loss: 0.72751\n",
      "Epoch 3999 - lr: 0.05000 - Train loss: 0.59972 - Test loss: 0.72754\n",
      "Epoch 4000 - lr: 0.05000 - Train loss: 0.59971 - Test loss: 0.72758\n",
      "Epoch 4001 - lr: 0.05000 - Train loss: 0.59970 - Test loss: 0.72762\n",
      "Epoch 4002 - lr: 0.05000 - Train loss: 0.59969 - Test loss: 0.72765\n",
      "Epoch 4003 - lr: 0.05000 - Train loss: 0.59968 - Test loss: 0.72769\n",
      "Epoch 4004 - lr: 0.05000 - Train loss: 0.59967 - Test loss: 0.72773\n",
      "Epoch 4005 - lr: 0.05000 - Train loss: 0.59966 - Test loss: 0.72776\n",
      "Epoch 4006 - lr: 0.05000 - Train loss: 0.59965 - Test loss: 0.72780\n",
      "Epoch 4007 - lr: 0.05000 - Train loss: 0.59964 - Test loss: 0.72784\n",
      "Epoch 4008 - lr: 0.05000 - Train loss: 0.59963 - Test loss: 0.72787\n",
      "Epoch 4009 - lr: 0.05000 - Train loss: 0.59961 - Test loss: 0.72791\n",
      "Epoch 4010 - lr: 0.05000 - Train loss: 0.59960 - Test loss: 0.72795\n",
      "Epoch 4011 - lr: 0.05000 - Train loss: 0.59959 - Test loss: 0.72798\n",
      "Epoch 4012 - lr: 0.05000 - Train loss: 0.59958 - Test loss: 0.72802\n",
      "Epoch 4013 - lr: 0.05000 - Train loss: 0.59957 - Test loss: 0.72805\n",
      "Epoch 4014 - lr: 0.05000 - Train loss: 0.59956 - Test loss: 0.72809\n",
      "Epoch 4015 - lr: 0.05000 - Train loss: 0.59955 - Test loss: 0.72813\n",
      "Epoch 4016 - lr: 0.05000 - Train loss: 0.59954 - Test loss: 0.72816\n",
      "Epoch 4017 - lr: 0.05000 - Train loss: 0.59953 - Test loss: 0.72820\n",
      "Epoch 4018 - lr: 0.05000 - Train loss: 0.59952 - Test loss: 0.72823\n",
      "Epoch 4019 - lr: 0.05000 - Train loss: 0.59951 - Test loss: 0.72827\n",
      "Epoch 4020 - lr: 0.05000 - Train loss: 0.59950 - Test loss: 0.72831\n",
      "Epoch 4021 - lr: 0.05000 - Train loss: 0.59949 - Test loss: 0.72834\n",
      "Epoch 4022 - lr: 0.05000 - Train loss: 0.59948 - Test loss: 0.72838\n",
      "Epoch 4023 - lr: 0.05000 - Train loss: 0.59947 - Test loss: 0.72841\n",
      "Epoch 4024 - lr: 0.05000 - Train loss: 0.59946 - Test loss: 0.72845\n",
      "Epoch 4025 - lr: 0.05000 - Train loss: 0.59945 - Test loss: 0.72848\n",
      "Epoch 4026 - lr: 0.05000 - Train loss: 0.59944 - Test loss: 0.72852\n",
      "Epoch 4027 - lr: 0.05000 - Train loss: 0.59943 - Test loss: 0.72856\n",
      "Epoch 4028 - lr: 0.05000 - Train loss: 0.59942 - Test loss: 0.72859\n",
      "Epoch 4029 - lr: 0.05000 - Train loss: 0.59941 - Test loss: 0.72863\n",
      "Epoch 4030 - lr: 0.05000 - Train loss: 0.59940 - Test loss: 0.72866\n",
      "Epoch 4031 - lr: 0.05000 - Train loss: 0.59939 - Test loss: 0.72870\n",
      "Epoch 4032 - lr: 0.05000 - Train loss: 0.59938 - Test loss: 0.72873\n",
      "Epoch 4033 - lr: 0.05000 - Train loss: 0.59937 - Test loss: 0.72877\n",
      "Epoch 4034 - lr: 0.05000 - Train loss: 0.59936 - Test loss: 0.72880\n",
      "Epoch 4035 - lr: 0.05000 - Train loss: 0.59935 - Test loss: 0.72884\n",
      "Epoch 4036 - lr: 0.05000 - Train loss: 0.59935 - Test loss: 0.72887\n",
      "Epoch 4037 - lr: 0.05000 - Train loss: 0.59934 - Test loss: 0.72891\n",
      "Epoch 4038 - lr: 0.05000 - Train loss: 0.59933 - Test loss: 0.72894\n",
      "Epoch 4039 - lr: 0.05000 - Train loss: 0.59932 - Test loss: 0.72898\n",
      "Epoch 4040 - lr: 0.05000 - Train loss: 0.59931 - Test loss: 0.72901\n",
      "Epoch 4041 - lr: 0.05000 - Train loss: 0.59930 - Test loss: 0.72905\n",
      "Epoch 4042 - lr: 0.05000 - Train loss: 0.59929 - Test loss: 0.72908\n",
      "Epoch 4043 - lr: 0.05000 - Train loss: 0.59928 - Test loss: 0.72912\n",
      "Epoch 4044 - lr: 0.05000 - Train loss: 0.59927 - Test loss: 0.72915\n",
      "Epoch 4045 - lr: 0.05000 - Train loss: 0.59926 - Test loss: 0.72919\n",
      "Epoch 4046 - lr: 0.05000 - Train loss: 0.59925 - Test loss: 0.72922\n",
      "Epoch 4047 - lr: 0.05000 - Train loss: 0.59924 - Test loss: 0.72926\n",
      "Epoch 4048 - lr: 0.05000 - Train loss: 0.59923 - Test loss: 0.72929\n",
      "Epoch 4049 - lr: 0.05000 - Train loss: 0.59922 - Test loss: 0.72933\n",
      "Epoch 4050 - lr: 0.05000 - Train loss: 0.59921 - Test loss: 0.72936\n",
      "Epoch 4051 - lr: 0.05000 - Train loss: 0.59920 - Test loss: 0.72939\n",
      "Epoch 4052 - lr: 0.05000 - Train loss: 0.59920 - Test loss: 0.72943\n",
      "Epoch 4053 - lr: 0.05000 - Train loss: 0.59919 - Test loss: 0.72946\n",
      "Epoch 4054 - lr: 0.05000 - Train loss: 0.59918 - Test loss: 0.72950\n",
      "Epoch 4055 - lr: 0.05000 - Train loss: 0.59917 - Test loss: 0.72953\n",
      "Epoch 4056 - lr: 0.05000 - Train loss: 0.59916 - Test loss: 0.72957\n",
      "Epoch 4057 - lr: 0.05000 - Train loss: 0.59915 - Test loss: 0.72960\n",
      "Epoch 4058 - lr: 0.05000 - Train loss: 0.59914 - Test loss: 0.72963\n",
      "Epoch 4059 - lr: 0.05000 - Train loss: 0.59913 - Test loss: 0.72967\n",
      "Epoch 4060 - lr: 0.05000 - Train loss: 0.59912 - Test loss: 0.72970\n",
      "Epoch 4061 - lr: 0.05000 - Train loss: 0.59911 - Test loss: 0.72974\n",
      "Epoch 4062 - lr: 0.05000 - Train loss: 0.59911 - Test loss: 0.72977\n",
      "Epoch 4063 - lr: 0.05000 - Train loss: 0.59910 - Test loss: 0.72981\n",
      "Epoch 4064 - lr: 0.05000 - Train loss: 0.59909 - Test loss: 0.72984\n",
      "Epoch 4065 - lr: 0.05000 - Train loss: 0.59908 - Test loss: 0.72987\n",
      "Epoch 4066 - lr: 0.05000 - Train loss: 0.59907 - Test loss: 0.72991\n",
      "Epoch 4067 - lr: 0.05000 - Train loss: 0.59906 - Test loss: 0.72994\n",
      "Epoch 4068 - lr: 0.05000 - Train loss: 0.59905 - Test loss: 0.72997\n",
      "Epoch 4069 - lr: 0.05000 - Train loss: 0.59904 - Test loss: 0.73001\n",
      "Epoch 4070 - lr: 0.05000 - Train loss: 0.59904 - Test loss: 0.73004\n",
      "Epoch 4071 - lr: 0.05000 - Train loss: 0.59903 - Test loss: 0.73008\n",
      "Epoch 4072 - lr: 0.05000 - Train loss: 0.59902 - Test loss: 0.73011\n",
      "Epoch 4073 - lr: 0.05000 - Train loss: 0.59901 - Test loss: 0.73014\n",
      "Epoch 4074 - lr: 0.05000 - Train loss: 0.59900 - Test loss: 0.73018\n",
      "Epoch 4075 - lr: 0.05000 - Train loss: 0.59899 - Test loss: 0.73021\n",
      "Epoch 4076 - lr: 0.05000 - Train loss: 0.59898 - Test loss: 0.73024\n",
      "Epoch 4077 - lr: 0.05000 - Train loss: 0.59898 - Test loss: 0.73028\n",
      "Epoch 4078 - lr: 0.05000 - Train loss: 0.59897 - Test loss: 0.73031\n",
      "Epoch 4079 - lr: 0.05000 - Train loss: 0.59896 - Test loss: 0.73034\n",
      "Epoch 4080 - lr: 0.05000 - Train loss: 0.59895 - Test loss: 0.73038\n",
      "Epoch 4081 - lr: 0.05000 - Train loss: 0.59894 - Test loss: 0.73041\n",
      "Epoch 4082 - lr: 0.05000 - Train loss: 0.59893 - Test loss: 0.73044\n",
      "Epoch 4083 - lr: 0.05000 - Train loss: 0.59893 - Test loss: 0.73048\n",
      "Epoch 4084 - lr: 0.05000 - Train loss: 0.59892 - Test loss: 0.73051\n",
      "Epoch 4085 - lr: 0.05000 - Train loss: 0.59891 - Test loss: 0.73054\n",
      "Epoch 4086 - lr: 0.05000 - Train loss: 0.59890 - Test loss: 0.73058\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4087 - lr: 0.05000 - Train loss: 0.59889 - Test loss: 0.73061\n",
      "Epoch 4088 - lr: 0.05000 - Train loss: 0.59888 - Test loss: 0.73064\n",
      "Epoch 4089 - lr: 0.05000 - Train loss: 0.59888 - Test loss: 0.73068\n",
      "Epoch 4090 - lr: 0.05000 - Train loss: 0.59887 - Test loss: 0.73071\n",
      "Epoch 4091 - lr: 0.05000 - Train loss: 0.59886 - Test loss: 0.73074\n",
      "Epoch 4092 - lr: 0.05000 - Train loss: 0.59885 - Test loss: 0.73077\n",
      "Epoch 4093 - lr: 0.05000 - Train loss: 0.59884 - Test loss: 0.73081\n",
      "Epoch 4094 - lr: 0.05000 - Train loss: 0.59883 - Test loss: 0.73084\n",
      "Epoch 4095 - lr: 0.05000 - Train loss: 0.59883 - Test loss: 0.73087\n",
      "Epoch 4096 - lr: 0.05000 - Train loss: 0.59882 - Test loss: 0.73091\n",
      "Epoch 4097 - lr: 0.05000 - Train loss: 0.59881 - Test loss: 0.73094\n",
      "Epoch 4098 - lr: 0.05000 - Train loss: 0.59880 - Test loss: 0.73097\n",
      "Epoch 4099 - lr: 0.05000 - Train loss: 0.59879 - Test loss: 0.73100\n",
      "Epoch 4100 - lr: 0.05000 - Train loss: 0.59879 - Test loss: 0.73104\n",
      "Epoch 4101 - lr: 0.05000 - Train loss: 0.59878 - Test loss: 0.73107\n",
      "Epoch 4102 - lr: 0.05000 - Train loss: 0.59877 - Test loss: 0.73110\n",
      "Epoch 4103 - lr: 0.05000 - Train loss: 0.59876 - Test loss: 0.73113\n",
      "Epoch 4104 - lr: 0.05000 - Train loss: 0.59875 - Test loss: 0.73117\n",
      "Epoch 4105 - lr: 0.05000 - Train loss: 0.59875 - Test loss: 0.73120\n",
      "Epoch 4106 - lr: 0.05000 - Train loss: 0.59874 - Test loss: 0.73123\n",
      "Epoch 4107 - lr: 0.05000 - Train loss: 0.59873 - Test loss: 0.73126\n",
      "Epoch 4108 - lr: 0.05000 - Train loss: 0.59872 - Test loss: 0.73130\n",
      "Epoch 4109 - lr: 0.05000 - Train loss: 0.59872 - Test loss: 0.73133\n",
      "Epoch 4110 - lr: 0.05000 - Train loss: 0.59871 - Test loss: 0.73136\n",
      "Epoch 4111 - lr: 0.05000 - Train loss: 0.59870 - Test loss: 0.73139\n",
      "Epoch 4112 - lr: 0.05000 - Train loss: 0.59869 - Test loss: 0.73142\n",
      "Epoch 4113 - lr: 0.05000 - Train loss: 0.59868 - Test loss: 0.73146\n",
      "Epoch 4114 - lr: 0.05000 - Train loss: 0.59868 - Test loss: 0.73149\n",
      "Epoch 4115 - lr: 0.05000 - Train loss: 0.59867 - Test loss: 0.73152\n",
      "Epoch 4116 - lr: 0.05000 - Train loss: 0.59866 - Test loss: 0.73155\n",
      "Epoch 4117 - lr: 0.05000 - Train loss: 0.59865 - Test loss: 0.73159\n",
      "Epoch 4118 - lr: 0.05000 - Train loss: 0.59865 - Test loss: 0.73162\n",
      "Epoch 4119 - lr: 0.05000 - Train loss: 0.59864 - Test loss: 0.73165\n",
      "Epoch 4120 - lr: 0.05000 - Train loss: 0.59863 - Test loss: 0.73168\n",
      "Epoch 4121 - lr: 0.05000 - Train loss: 0.59862 - Test loss: 0.73171\n",
      "Epoch 4122 - lr: 0.05000 - Train loss: 0.59862 - Test loss: 0.73175\n",
      "Epoch 4123 - lr: 0.05000 - Train loss: 0.59861 - Test loss: 0.73178\n",
      "Epoch 4124 - lr: 0.05000 - Train loss: 0.59860 - Test loss: 0.73181\n",
      "Epoch 4125 - lr: 0.05000 - Train loss: 0.59859 - Test loss: 0.73184\n",
      "Epoch 4126 - lr: 0.05000 - Train loss: 0.59859 - Test loss: 0.73187\n",
      "Epoch 4127 - lr: 0.05000 - Train loss: 0.59858 - Test loss: 0.73190\n",
      "Epoch 4128 - lr: 0.05000 - Train loss: 0.59857 - Test loss: 0.73194\n",
      "Epoch 4129 - lr: 0.05000 - Train loss: 0.59856 - Test loss: 0.73197\n",
      "Epoch 4130 - lr: 0.05000 - Train loss: 0.59856 - Test loss: 0.73200\n",
      "Epoch 4131 - lr: 0.05000 - Train loss: 0.59855 - Test loss: 0.73203\n",
      "Epoch 4132 - lr: 0.05000 - Train loss: 0.59854 - Test loss: 0.73206\n",
      "Epoch 4133 - lr: 0.05000 - Train loss: 0.59853 - Test loss: 0.73209\n",
      "Epoch 4134 - lr: 0.05000 - Train loss: 0.59853 - Test loss: 0.73213\n",
      "Epoch 4135 - lr: 0.05000 - Train loss: 0.59852 - Test loss: 0.73216\n",
      "Epoch 4136 - lr: 0.05000 - Train loss: 0.59851 - Test loss: 0.73219\n",
      "Epoch 4137 - lr: 0.05000 - Train loss: 0.59850 - Test loss: 0.73222\n",
      "Epoch 4138 - lr: 0.05000 - Train loss: 0.59850 - Test loss: 0.73225\n",
      "Epoch 4139 - lr: 0.05000 - Train loss: 0.59849 - Test loss: 0.73228\n",
      "Epoch 4140 - lr: 0.05000 - Train loss: 0.59848 - Test loss: 0.73231\n",
      "Epoch 4141 - lr: 0.05000 - Train loss: 0.59847 - Test loss: 0.73234\n",
      "Epoch 4142 - lr: 0.05000 - Train loss: 0.59847 - Test loss: 0.73238\n",
      "Epoch 4143 - lr: 0.05000 - Train loss: 0.59846 - Test loss: 0.73241\n",
      "Epoch 4144 - lr: 0.05000 - Train loss: 0.59845 - Test loss: 0.73244\n",
      "Epoch 4145 - lr: 0.05000 - Train loss: 0.59845 - Test loss: 0.73247\n",
      "Epoch 4146 - lr: 0.05000 - Train loss: 0.59844 - Test loss: 0.73250\n",
      "Epoch 4147 - lr: 0.05000 - Train loss: 0.59843 - Test loss: 0.73253\n",
      "Epoch 4148 - lr: 0.05000 - Train loss: 0.59842 - Test loss: 0.73256\n",
      "Epoch 4149 - lr: 0.05000 - Train loss: 0.59842 - Test loss: 0.73259\n",
      "Epoch 4150 - lr: 0.05000 - Train loss: 0.59841 - Test loss: 0.73262\n",
      "Epoch 4151 - lr: 0.05000 - Train loss: 0.59840 - Test loss: 0.73266\n",
      "Epoch 4152 - lr: 0.05000 - Train loss: 0.59840 - Test loss: 0.73269\n",
      "Epoch 4153 - lr: 0.05000 - Train loss: 0.59839 - Test loss: 0.73272\n",
      "Epoch 4154 - lr: 0.05000 - Train loss: 0.59838 - Test loss: 0.73275\n",
      "Epoch 4155 - lr: 0.05000 - Train loss: 0.59837 - Test loss: 0.73278\n",
      "Epoch 4156 - lr: 0.05000 - Train loss: 0.59837 - Test loss: 0.73281\n",
      "Epoch 4157 - lr: 0.05000 - Train loss: 0.59836 - Test loss: 0.73284\n",
      "Epoch 4158 - lr: 0.05000 - Train loss: 0.59835 - Test loss: 0.73287\n",
      "Epoch 4159 - lr: 0.05000 - Train loss: 0.59835 - Test loss: 0.73290\n",
      "Epoch 4160 - lr: 0.05000 - Train loss: 0.59834 - Test loss: 0.73293\n",
      "Epoch 4161 - lr: 0.05000 - Train loss: 0.59833 - Test loss: 0.73296\n",
      "Epoch 4162 - lr: 0.05000 - Train loss: 0.59833 - Test loss: 0.73299\n",
      "Epoch 4163 - lr: 0.05000 - Train loss: 0.59832 - Test loss: 0.73302\n",
      "Epoch 4164 - lr: 0.05000 - Train loss: 0.59831 - Test loss: 0.73306\n",
      "Epoch 4165 - lr: 0.05000 - Train loss: 0.59830 - Test loss: 0.73309\n",
      "Epoch 4166 - lr: 0.05000 - Train loss: 0.59830 - Test loss: 0.73312\n",
      "Epoch 4167 - lr: 0.05000 - Train loss: 0.59829 - Test loss: 0.73315\n",
      "Epoch 4168 - lr: 0.05000 - Train loss: 0.59828 - Test loss: 0.73318\n",
      "Epoch 4169 - lr: 0.05000 - Train loss: 0.59828 - Test loss: 0.73321\n",
      "Epoch 4170 - lr: 0.05000 - Train loss: 0.59827 - Test loss: 0.73324\n",
      "Epoch 4171 - lr: 0.05000 - Train loss: 0.59826 - Test loss: 0.73327\n",
      "Epoch 4172 - lr: 0.05000 - Train loss: 0.59826 - Test loss: 0.73330\n",
      "Epoch 4173 - lr: 0.05000 - Train loss: 0.59825 - Test loss: 0.73333\n",
      "Epoch 4174 - lr: 0.05000 - Train loss: 0.59824 - Test loss: 0.73336\n",
      "Epoch 4175 - lr: 0.05000 - Train loss: 0.59824 - Test loss: 0.73339\n",
      "Epoch 4176 - lr: 0.05000 - Train loss: 0.59823 - Test loss: 0.73342\n",
      "Epoch 4177 - lr: 0.05000 - Train loss: 0.59822 - Test loss: 0.73345\n",
      "Epoch 4178 - lr: 0.05000 - Train loss: 0.59822 - Test loss: 0.73348\n",
      "Epoch 4179 - lr: 0.05000 - Train loss: 0.59821 - Test loss: 0.73351\n",
      "Epoch 4180 - lr: 0.05000 - Train loss: 0.59820 - Test loss: 0.73354\n",
      "Epoch 4181 - lr: 0.05000 - Train loss: 0.59820 - Test loss: 0.73357\n",
      "Epoch 4182 - lr: 0.05000 - Train loss: 0.59819 - Test loss: 0.73360\n",
      "Epoch 4183 - lr: 0.05000 - Train loss: 0.59818 - Test loss: 0.73363\n",
      "Epoch 4184 - lr: 0.05000 - Train loss: 0.59818 - Test loss: 0.73366\n",
      "Epoch 4185 - lr: 0.05000 - Train loss: 0.59817 - Test loss: 0.73369\n",
      "Epoch 4186 - lr: 0.05000 - Train loss: 0.59816 - Test loss: 0.73372\n",
      "Epoch 4187 - lr: 0.05000 - Train loss: 0.59816 - Test loss: 0.73375\n",
      "Epoch 4188 - lr: 0.05000 - Train loss: 0.59815 - Test loss: 0.73378\n",
      "Epoch 4189 - lr: 0.05000 - Train loss: 0.59814 - Test loss: 0.73381\n",
      "Epoch 4190 - lr: 0.05000 - Train loss: 0.59814 - Test loss: 0.73384\n",
      "Epoch 4191 - lr: 0.05000 - Train loss: 0.59813 - Test loss: 0.73387\n",
      "Epoch 4192 - lr: 0.05000 - Train loss: 0.59812 - Test loss: 0.73390\n",
      "Epoch 4193 - lr: 0.05000 - Train loss: 0.59812 - Test loss: 0.73393\n",
      "Epoch 4194 - lr: 0.05000 - Train loss: 0.59811 - Test loss: 0.73396\n",
      "Epoch 4195 - lr: 0.05000 - Train loss: 0.59810 - Test loss: 0.73399\n",
      "Epoch 4196 - lr: 0.05000 - Train loss: 0.59810 - Test loss: 0.73402\n",
      "Epoch 4197 - lr: 0.05000 - Train loss: 0.59809 - Test loss: 0.73405\n",
      "Epoch 4198 - lr: 0.05000 - Train loss: 0.59809 - Test loss: 0.73408\n",
      "Epoch 4199 - lr: 0.05000 - Train loss: 0.59808 - Test loss: 0.73411\n",
      "Epoch 4200 - lr: 0.05000 - Train loss: 0.59807 - Test loss: 0.73414\n",
      "Epoch 4201 - lr: 0.05000 - Train loss: 0.59807 - Test loss: 0.73417\n",
      "Epoch 4202 - lr: 0.05000 - Train loss: 0.59806 - Test loss: 0.73420\n",
      "Epoch 4203 - lr: 0.05000 - Train loss: 0.59805 - Test loss: 0.73423\n",
      "Epoch 4204 - lr: 0.05000 - Train loss: 0.59805 - Test loss: 0.73426\n",
      "Epoch 4205 - lr: 0.05000 - Train loss: 0.59804 - Test loss: 0.73428\n",
      "Epoch 4206 - lr: 0.05000 - Train loss: 0.59803 - Test loss: 0.73431\n",
      "Epoch 4207 - lr: 0.05000 - Train loss: 0.59803 - Test loss: 0.73434\n",
      "Epoch 4208 - lr: 0.05000 - Train loss: 0.59802 - Test loss: 0.73437\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4209 - lr: 0.05000 - Train loss: 0.59802 - Test loss: 0.73440\n",
      "Epoch 4210 - lr: 0.05000 - Train loss: 0.59801 - Test loss: 0.73443\n",
      "Epoch 4211 - lr: 0.05000 - Train loss: 0.59800 - Test loss: 0.73446\n",
      "Epoch 4212 - lr: 0.05000 - Train loss: 0.59800 - Test loss: 0.73449\n",
      "Epoch 4213 - lr: 0.05000 - Train loss: 0.59799 - Test loss: 0.73452\n",
      "Epoch 4214 - lr: 0.05000 - Train loss: 0.59798 - Test loss: 0.73455\n",
      "Epoch 4215 - lr: 0.05000 - Train loss: 0.59798 - Test loss: 0.73458\n",
      "Epoch 4216 - lr: 0.05000 - Train loss: 0.59797 - Test loss: 0.73461\n",
      "Epoch 4217 - lr: 0.05000 - Train loss: 0.59797 - Test loss: 0.73464\n",
      "Epoch 4218 - lr: 0.05000 - Train loss: 0.59796 - Test loss: 0.73466\n",
      "Epoch 4219 - lr: 0.05000 - Train loss: 0.59795 - Test loss: 0.73469\n",
      "Epoch 4220 - lr: 0.05000 - Train loss: 0.59795 - Test loss: 0.73472\n",
      "Epoch 4221 - lr: 0.05000 - Train loss: 0.59794 - Test loss: 0.73475\n",
      "Epoch 4222 - lr: 0.05000 - Train loss: 0.59793 - Test loss: 0.73478\n",
      "Epoch 4223 - lr: 0.05000 - Train loss: 0.59793 - Test loss: 0.73481\n",
      "Epoch 4224 - lr: 0.05000 - Train loss: 0.59792 - Test loss: 0.73484\n",
      "Epoch 4225 - lr: 0.05000 - Train loss: 0.59792 - Test loss: 0.73487\n",
      "Epoch 4226 - lr: 0.05000 - Train loss: 0.59791 - Test loss: 0.73490\n",
      "Epoch 4227 - lr: 0.05000 - Train loss: 0.59790 - Test loss: 0.73493\n",
      "Epoch 4228 - lr: 0.05000 - Train loss: 0.59790 - Test loss: 0.73495\n",
      "Epoch 4229 - lr: 0.05000 - Train loss: 0.59789 - Test loss: 0.73498\n",
      "Epoch 4230 - lr: 0.05000 - Train loss: 0.59789 - Test loss: 0.73501\n",
      "Epoch 4231 - lr: 0.05000 - Train loss: 0.59788 - Test loss: 0.73504\n",
      "Epoch 4232 - lr: 0.05000 - Train loss: 0.59787 - Test loss: 0.73507\n",
      "Epoch 4233 - lr: 0.05000 - Train loss: 0.59787 - Test loss: 0.73510\n",
      "Epoch 4234 - lr: 0.05000 - Train loss: 0.59786 - Test loss: 0.73513\n",
      "Epoch 4235 - lr: 0.05000 - Train loss: 0.59785 - Test loss: 0.73516\n",
      "Epoch 4236 - lr: 0.05000 - Train loss: 0.59785 - Test loss: 0.73518\n",
      "Epoch 4237 - lr: 0.05000 - Train loss: 0.59784 - Test loss: 0.73521\n",
      "Epoch 4238 - lr: 0.05000 - Train loss: 0.59784 - Test loss: 0.73524\n",
      "Epoch 4239 - lr: 0.05000 - Train loss: 0.59783 - Test loss: 0.73527\n",
      "Epoch 4240 - lr: 0.05000 - Train loss: 0.59782 - Test loss: 0.73530\n",
      "Epoch 4241 - lr: 0.05000 - Train loss: 0.59782 - Test loss: 0.73533\n",
      "Epoch 4242 - lr: 0.05000 - Train loss: 0.59781 - Test loss: 0.73535\n",
      "Epoch 4243 - lr: 0.05000 - Train loss: 0.59781 - Test loss: 0.73538\n",
      "Epoch 4244 - lr: 0.05000 - Train loss: 0.59780 - Test loss: 0.73541\n",
      "Epoch 4245 - lr: 0.05000 - Train loss: 0.59780 - Test loss: 0.73544\n",
      "Epoch 4246 - lr: 0.05000 - Train loss: 0.59779 - Test loss: 0.73547\n",
      "Epoch 4247 - lr: 0.05000 - Train loss: 0.59778 - Test loss: 0.73550\n",
      "Epoch 4248 - lr: 0.05000 - Train loss: 0.59778 - Test loss: 0.73553\n",
      "Epoch 4249 - lr: 0.05000 - Train loss: 0.59777 - Test loss: 0.73555\n",
      "Epoch 4250 - lr: 0.05000 - Train loss: 0.59777 - Test loss: 0.73558\n",
      "Epoch 4251 - lr: 0.05000 - Train loss: 0.59776 - Test loss: 0.73561\n",
      "Epoch 4252 - lr: 0.05000 - Train loss: 0.59775 - Test loss: 0.73564\n",
      "Epoch 4253 - lr: 0.05000 - Train loss: 0.59775 - Test loss: 0.73567\n",
      "Epoch 4254 - lr: 0.05000 - Train loss: 0.59774 - Test loss: 0.73569\n",
      "Epoch 4255 - lr: 0.05000 - Train loss: 0.59774 - Test loss: 0.73572\n",
      "Epoch 4256 - lr: 0.05000 - Train loss: 0.59773 - Test loss: 0.73575\n",
      "Epoch 4257 - lr: 0.05000 - Train loss: 0.59772 - Test loss: 0.73578\n",
      "Epoch 4258 - lr: 0.05000 - Train loss: 0.59772 - Test loss: 0.73581\n",
      "Epoch 4259 - lr: 0.05000 - Train loss: 0.59771 - Test loss: 0.73584\n",
      "Epoch 4260 - lr: 0.05000 - Train loss: 0.59771 - Test loss: 0.73586\n",
      "Epoch 4261 - lr: 0.05000 - Train loss: 0.59770 - Test loss: 0.73589\n",
      "Epoch 4262 - lr: 0.05000 - Train loss: 0.59770 - Test loss: 0.73592\n",
      "Epoch 4263 - lr: 0.05000 - Train loss: 0.59769 - Test loss: 0.73595\n",
      "Epoch 4264 - lr: 0.05000 - Train loss: 0.59768 - Test loss: 0.73597\n",
      "Epoch 4265 - lr: 0.05000 - Train loss: 0.59768 - Test loss: 0.73600\n",
      "Epoch 4266 - lr: 0.05000 - Train loss: 0.59767 - Test loss: 0.73603\n",
      "Epoch 4267 - lr: 0.05000 - Train loss: 0.59767 - Test loss: 0.73606\n",
      "Epoch 4268 - lr: 0.05000 - Train loss: 0.59766 - Test loss: 0.73609\n",
      "Epoch 4269 - lr: 0.05000 - Train loss: 0.59766 - Test loss: 0.73611\n",
      "Epoch 4270 - lr: 0.05000 - Train loss: 0.59765 - Test loss: 0.73614\n",
      "Epoch 4271 - lr: 0.05000 - Train loss: 0.59764 - Test loss: 0.73617\n",
      "Epoch 4272 - lr: 0.05000 - Train loss: 0.59764 - Test loss: 0.73620\n",
      "Epoch 4273 - lr: 0.05000 - Train loss: 0.59763 - Test loss: 0.73623\n",
      "Epoch 4274 - lr: 0.05000 - Train loss: 0.59763 - Test loss: 0.73625\n",
      "Epoch 4275 - lr: 0.05000 - Train loss: 0.59762 - Test loss: 0.73628\n",
      "Epoch 4276 - lr: 0.05000 - Train loss: 0.59762 - Test loss: 0.73631\n",
      "Epoch 4277 - lr: 0.05000 - Train loss: 0.59761 - Test loss: 0.73634\n",
      "Epoch 4278 - lr: 0.05000 - Train loss: 0.59761 - Test loss: 0.73636\n",
      "Epoch 4279 - lr: 0.05000 - Train loss: 0.59760 - Test loss: 0.73639\n",
      "Epoch 4280 - lr: 0.05000 - Train loss: 0.59759 - Test loss: 0.73642\n",
      "Epoch 4281 - lr: 0.05000 - Train loss: 0.59759 - Test loss: 0.73645\n",
      "Epoch 4282 - lr: 0.05000 - Train loss: 0.59758 - Test loss: 0.73647\n",
      "Epoch 4283 - lr: 0.05000 - Train loss: 0.59758 - Test loss: 0.73650\n",
      "Epoch 4284 - lr: 0.05000 - Train loss: 0.59757 - Test loss: 0.73653\n",
      "Epoch 4285 - lr: 0.05000 - Train loss: 0.59757 - Test loss: 0.73656\n",
      "Epoch 4286 - lr: 0.05000 - Train loss: 0.59756 - Test loss: 0.73658\n",
      "Epoch 4287 - lr: 0.05000 - Train loss: 0.59756 - Test loss: 0.73661\n",
      "Epoch 4288 - lr: 0.05000 - Train loss: 0.59755 - Test loss: 0.73664\n",
      "Epoch 4289 - lr: 0.05000 - Train loss: 0.59754 - Test loss: 0.73667\n",
      "Epoch 4290 - lr: 0.05000 - Train loss: 0.59754 - Test loss: 0.73669\n",
      "Epoch 4291 - lr: 0.05000 - Train loss: 0.59753 - Test loss: 0.73672\n",
      "Epoch 4292 - lr: 0.05000 - Train loss: 0.59753 - Test loss: 0.73675\n",
      "Epoch 4293 - lr: 0.05000 - Train loss: 0.59752 - Test loss: 0.73677\n",
      "Epoch 4294 - lr: 0.05000 - Train loss: 0.59752 - Test loss: 0.73680\n",
      "Epoch 4295 - lr: 0.05000 - Train loss: 0.59751 - Test loss: 0.73683\n",
      "Epoch 4296 - lr: 0.05000 - Train loss: 0.59751 - Test loss: 0.73686\n",
      "Epoch 4297 - lr: 0.05000 - Train loss: 0.59750 - Test loss: 0.73688\n",
      "Epoch 4298 - lr: 0.05000 - Train loss: 0.59750 - Test loss: 0.73691\n",
      "Epoch 4299 - lr: 0.05000 - Train loss: 0.59749 - Test loss: 0.73694\n",
      "Epoch 4300 - lr: 0.05000 - Train loss: 0.59748 - Test loss: 0.73696\n",
      "Epoch 4301 - lr: 0.05000 - Train loss: 0.59748 - Test loss: 0.73699\n",
      "Epoch 4302 - lr: 0.05000 - Train loss: 0.59747 - Test loss: 0.73702\n",
      "Epoch 4303 - lr: 0.05000 - Train loss: 0.59747 - Test loss: 0.73705\n",
      "Epoch 4304 - lr: 0.05000 - Train loss: 0.59746 - Test loss: 0.73707\n",
      "Epoch 4305 - lr: 0.05000 - Train loss: 0.59746 - Test loss: 0.73710\n",
      "Epoch 4306 - lr: 0.05000 - Train loss: 0.59745 - Test loss: 0.73713\n",
      "Epoch 4307 - lr: 0.05000 - Train loss: 0.59745 - Test loss: 0.73715\n",
      "Epoch 4308 - lr: 0.05000 - Train loss: 0.59744 - Test loss: 0.73718\n",
      "Epoch 4309 - lr: 0.05000 - Train loss: 0.59744 - Test loss: 0.73721\n",
      "Epoch 4310 - lr: 0.05000 - Train loss: 0.59743 - Test loss: 0.73723\n",
      "Epoch 4311 - lr: 0.05000 - Train loss: 0.59743 - Test loss: 0.73726\n",
      "Epoch 4312 - lr: 0.05000 - Train loss: 0.59742 - Test loss: 0.73729\n",
      "Epoch 4313 - lr: 0.05000 - Train loss: 0.59741 - Test loss: 0.73731\n",
      "Epoch 4314 - lr: 0.05000 - Train loss: 0.59741 - Test loss: 0.73734\n",
      "Epoch 4315 - lr: 0.05000 - Train loss: 0.59740 - Test loss: 0.73737\n",
      "Epoch 4316 - lr: 0.05000 - Train loss: 0.59740 - Test loss: 0.73739\n",
      "Epoch 4317 - lr: 0.05000 - Train loss: 0.59739 - Test loss: 0.73742\n",
      "Epoch 4318 - lr: 0.05000 - Train loss: 0.59739 - Test loss: 0.73745\n",
      "Epoch 4319 - lr: 0.05000 - Train loss: 0.59738 - Test loss: 0.73747\n",
      "Epoch 4320 - lr: 0.05000 - Train loss: 0.59738 - Test loss: 0.73750\n",
      "Epoch 4321 - lr: 0.05000 - Train loss: 0.59737 - Test loss: 0.73753\n",
      "Epoch 4322 - lr: 0.05000 - Train loss: 0.59737 - Test loss: 0.73755\n",
      "Epoch 4323 - lr: 0.05000 - Train loss: 0.59736 - Test loss: 0.73758\n",
      "Epoch 4324 - lr: 0.05000 - Train loss: 0.59736 - Test loss: 0.73761\n",
      "Epoch 4325 - lr: 0.05000 - Train loss: 0.59735 - Test loss: 0.73763\n",
      "Epoch 4326 - lr: 0.05000 - Train loss: 0.59735 - Test loss: 0.73766\n",
      "Epoch 4327 - lr: 0.05000 - Train loss: 0.59734 - Test loss: 0.73769\n",
      "Epoch 4328 - lr: 0.05000 - Train loss: 0.59734 - Test loss: 0.73771\n",
      "Epoch 4329 - lr: 0.05000 - Train loss: 0.59733 - Test loss: 0.73774\n",
      "Epoch 4330 - lr: 0.05000 - Train loss: 0.59733 - Test loss: 0.73777\n",
      "Epoch 4331 - lr: 0.05000 - Train loss: 0.59732 - Test loss: 0.73779\n",
      "Epoch 4332 - lr: 0.05000 - Train loss: 0.59731 - Test loss: 0.73782\n",
      "Epoch 4333 - lr: 0.05000 - Train loss: 0.59731 - Test loss: 0.73785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4334 - lr: 0.05000 - Train loss: 0.59730 - Test loss: 0.73787\n",
      "Epoch 4335 - lr: 0.05000 - Train loss: 0.59730 - Test loss: 0.73790\n",
      "Epoch 4336 - lr: 0.05000 - Train loss: 0.59729 - Test loss: 0.73792\n",
      "Epoch 4337 - lr: 0.05000 - Train loss: 0.59729 - Test loss: 0.73795\n",
      "Epoch 4338 - lr: 0.05000 - Train loss: 0.59728 - Test loss: 0.73798\n",
      "Epoch 4339 - lr: 0.05000 - Train loss: 0.59728 - Test loss: 0.73800\n",
      "Epoch 4340 - lr: 0.05000 - Train loss: 0.59727 - Test loss: 0.73803\n",
      "Epoch 4341 - lr: 0.05000 - Train loss: 0.59727 - Test loss: 0.73806\n",
      "Epoch 4342 - lr: 0.05000 - Train loss: 0.59726 - Test loss: 0.73808\n",
      "Epoch 4343 - lr: 0.05000 - Train loss: 0.59726 - Test loss: 0.73811\n",
      "Epoch 4344 - lr: 0.05000 - Train loss: 0.59725 - Test loss: 0.73813\n",
      "Epoch 4345 - lr: 0.05000 - Train loss: 0.59725 - Test loss: 0.73816\n",
      "Epoch 4346 - lr: 0.05000 - Train loss: 0.59724 - Test loss: 0.73819\n",
      "Epoch 4347 - lr: 0.05000 - Train loss: 0.59724 - Test loss: 0.73821\n",
      "Epoch 4348 - lr: 0.05000 - Train loss: 0.59723 - Test loss: 0.73824\n",
      "Epoch 4349 - lr: 0.05000 - Train loss: 0.59723 - Test loss: 0.73826\n",
      "Epoch 4350 - lr: 0.05000 - Train loss: 0.59722 - Test loss: 0.73829\n",
      "Epoch 4351 - lr: 0.05000 - Train loss: 0.59722 - Test loss: 0.73832\n",
      "Epoch 4352 - lr: 0.05000 - Train loss: 0.59721 - Test loss: 0.73834\n",
      "Epoch 4353 - lr: 0.05000 - Train loss: 0.59721 - Test loss: 0.73837\n",
      "Epoch 4354 - lr: 0.05000 - Train loss: 0.59720 - Test loss: 0.73839\n",
      "Epoch 4355 - lr: 0.05000 - Train loss: 0.59720 - Test loss: 0.73842\n",
      "Epoch 4356 - lr: 0.05000 - Train loss: 0.59719 - Test loss: 0.73845\n",
      "Epoch 4357 - lr: 0.05000 - Train loss: 0.59719 - Test loss: 0.73847\n",
      "Epoch 4358 - lr: 0.05000 - Train loss: 0.59718 - Test loss: 0.73850\n",
      "Epoch 4359 - lr: 0.05000 - Train loss: 0.59718 - Test loss: 0.73852\n",
      "Epoch 4360 - lr: 0.05000 - Train loss: 0.59717 - Test loss: 0.73855\n",
      "Epoch 4361 - lr: 0.05000 - Train loss: 0.59717 - Test loss: 0.73857\n",
      "Epoch 4362 - lr: 0.05000 - Train loss: 0.59716 - Test loss: 0.73860\n",
      "Epoch 4363 - lr: 0.05000 - Train loss: 0.59716 - Test loss: 0.73863\n",
      "Epoch 4364 - lr: 0.05000 - Train loss: 0.59715 - Test loss: 0.73865\n",
      "Epoch 4365 - lr: 0.05000 - Train loss: 0.59715 - Test loss: 0.73868\n",
      "Epoch 4366 - lr: 0.05000 - Train loss: 0.59714 - Test loss: 0.73870\n",
      "Epoch 4367 - lr: 0.05000 - Train loss: 0.59714 - Test loss: 0.73873\n",
      "Epoch 4368 - lr: 0.05000 - Train loss: 0.59713 - Test loss: 0.73875\n",
      "Epoch 4369 - lr: 0.05000 - Train loss: 0.59713 - Test loss: 0.73878\n",
      "Epoch 4370 - lr: 0.05000 - Train loss: 0.59712 - Test loss: 0.73881\n",
      "Epoch 4371 - lr: 0.05000 - Train loss: 0.59712 - Test loss: 0.73883\n",
      "Epoch 4372 - lr: 0.05000 - Train loss: 0.59711 - Test loss: 0.73886\n",
      "Epoch 4373 - lr: 0.05000 - Train loss: 0.59711 - Test loss: 0.73888\n",
      "Epoch 4374 - lr: 0.05000 - Train loss: 0.59710 - Test loss: 0.73891\n",
      "Epoch 4375 - lr: 0.05000 - Train loss: 0.59710 - Test loss: 0.73893\n",
      "Epoch 4376 - lr: 0.05000 - Train loss: 0.59709 - Test loss: 0.73896\n",
      "Epoch 4377 - lr: 0.05000 - Train loss: 0.59709 - Test loss: 0.73898\n",
      "Epoch 4378 - lr: 0.05000 - Train loss: 0.59708 - Test loss: 0.73901\n",
      "Epoch 4379 - lr: 0.05000 - Train loss: 0.59708 - Test loss: 0.73903\n",
      "Epoch 4380 - lr: 0.05000 - Train loss: 0.59707 - Test loss: 0.73906\n",
      "Epoch 4381 - lr: 0.05000 - Train loss: 0.59707 - Test loss: 0.73909\n",
      "Epoch 4382 - lr: 0.05000 - Train loss: 0.59707 - Test loss: 0.73911\n",
      "Epoch 4383 - lr: 0.05000 - Train loss: 0.59706 - Test loss: 0.73914\n",
      "Epoch 4384 - lr: 0.05000 - Train loss: 0.59706 - Test loss: 0.73916\n",
      "Epoch 4385 - lr: 0.05000 - Train loss: 0.59705 - Test loss: 0.73919\n",
      "Epoch 4386 - lr: 0.05000 - Train loss: 0.59705 - Test loss: 0.73921\n",
      "Epoch 4387 - lr: 0.05000 - Train loss: 0.59704 - Test loss: 0.73924\n",
      "Epoch 4388 - lr: 0.05000 - Train loss: 0.59704 - Test loss: 0.73926\n",
      "Epoch 4389 - lr: 0.05000 - Train loss: 0.59703 - Test loss: 0.73929\n",
      "Epoch 4390 - lr: 0.05000 - Train loss: 0.59703 - Test loss: 0.73931\n",
      "Epoch 4391 - lr: 0.05000 - Train loss: 0.59702 - Test loss: 0.73934\n",
      "Epoch 4392 - lr: 0.05000 - Train loss: 0.59702 - Test loss: 0.73936\n",
      "Epoch 4393 - lr: 0.05000 - Train loss: 0.59701 - Test loss: 0.73939\n",
      "Epoch 4394 - lr: 0.05000 - Train loss: 0.59701 - Test loss: 0.73941\n",
      "Epoch 4395 - lr: 0.05000 - Train loss: 0.59700 - Test loss: 0.73944\n",
      "Epoch 4396 - lr: 0.05000 - Train loss: 0.59700 - Test loss: 0.73946\n",
      "Epoch 4397 - lr: 0.05000 - Train loss: 0.59699 - Test loss: 0.73949\n",
      "Epoch 4398 - lr: 0.05000 - Train loss: 0.59699 - Test loss: 0.73951\n",
      "Epoch 4399 - lr: 0.05000 - Train loss: 0.59698 - Test loss: 0.73954\n",
      "Epoch 4400 - lr: 0.05000 - Train loss: 0.59698 - Test loss: 0.73956\n",
      "Epoch 4401 - lr: 0.05000 - Train loss: 0.59697 - Test loss: 0.73959\n",
      "Epoch 4402 - lr: 0.05000 - Train loss: 0.59697 - Test loss: 0.73961\n",
      "Epoch 4403 - lr: 0.05000 - Train loss: 0.59697 - Test loss: 0.73964\n",
      "Epoch 4404 - lr: 0.05000 - Train loss: 0.59696 - Test loss: 0.73966\n",
      "Epoch 4405 - lr: 0.05000 - Train loss: 0.59696 - Test loss: 0.73969\n",
      "Epoch 4406 - lr: 0.05000 - Train loss: 0.59695 - Test loss: 0.73971\n",
      "Epoch 4407 - lr: 0.05000 - Train loss: 0.59695 - Test loss: 0.73974\n",
      "Epoch 4408 - lr: 0.05000 - Train loss: 0.59694 - Test loss: 0.73976\n",
      "Epoch 4409 - lr: 0.05000 - Train loss: 0.59694 - Test loss: 0.73979\n",
      "Epoch 4410 - lr: 0.05000 - Train loss: 0.59693 - Test loss: 0.73981\n",
      "Epoch 4411 - lr: 0.05000 - Train loss: 0.59693 - Test loss: 0.73984\n",
      "Epoch 4412 - lr: 0.05000 - Train loss: 0.59692 - Test loss: 0.73986\n",
      "Epoch 4413 - lr: 0.05000 - Train loss: 0.59692 - Test loss: 0.73989\n",
      "Epoch 4414 - lr: 0.05000 - Train loss: 0.59691 - Test loss: 0.73991\n",
      "Epoch 4415 - lr: 0.05000 - Train loss: 0.59691 - Test loss: 0.73994\n",
      "Epoch 4416 - lr: 0.05000 - Train loss: 0.59690 - Test loss: 0.73996\n",
      "Epoch 4417 - lr: 0.05000 - Train loss: 0.59690 - Test loss: 0.73998\n",
      "Epoch 4418 - lr: 0.05000 - Train loss: 0.59690 - Test loss: 0.74001\n",
      "Epoch 4419 - lr: 0.05000 - Train loss: 0.59689 - Test loss: 0.74003\n",
      "Epoch 4420 - lr: 0.05000 - Train loss: 0.59689 - Test loss: 0.74006\n",
      "Epoch 4421 - lr: 0.05000 - Train loss: 0.59688 - Test loss: 0.74008\n",
      "Epoch 4422 - lr: 0.05000 - Train loss: 0.59688 - Test loss: 0.74011\n",
      "Epoch 4423 - lr: 0.05000 - Train loss: 0.59687 - Test loss: 0.74013\n",
      "Epoch 4424 - lr: 0.05000 - Train loss: 0.59687 - Test loss: 0.74016\n",
      "Epoch 4425 - lr: 0.05000 - Train loss: 0.59686 - Test loss: 0.74018\n",
      "Epoch 4426 - lr: 0.05000 - Train loss: 0.59686 - Test loss: 0.74021\n",
      "Epoch 4427 - lr: 0.05000 - Train loss: 0.59685 - Test loss: 0.74023\n",
      "Epoch 4428 - lr: 0.05000 - Train loss: 0.59685 - Test loss: 0.74025\n",
      "Epoch 4429 - lr: 0.05000 - Train loss: 0.59684 - Test loss: 0.74028\n",
      "Epoch 4430 - lr: 0.05000 - Train loss: 0.59684 - Test loss: 0.74030\n",
      "Epoch 4431 - lr: 0.05000 - Train loss: 0.59684 - Test loss: 0.74033\n",
      "Epoch 4432 - lr: 0.05000 - Train loss: 0.59683 - Test loss: 0.74035\n",
      "Epoch 4433 - lr: 0.05000 - Train loss: 0.59683 - Test loss: 0.74038\n",
      "Epoch 4434 - lr: 0.05000 - Train loss: 0.59682 - Test loss: 0.74040\n",
      "Epoch 4435 - lr: 0.05000 - Train loss: 0.59682 - Test loss: 0.74042\n",
      "Epoch 4436 - lr: 0.05000 - Train loss: 0.59681 - Test loss: 0.74045\n",
      "Epoch 4437 - lr: 0.05000 - Train loss: 0.59681 - Test loss: 0.74047\n",
      "Epoch 4438 - lr: 0.05000 - Train loss: 0.59680 - Test loss: 0.74050\n",
      "Epoch 4439 - lr: 0.05000 - Train loss: 0.59680 - Test loss: 0.74052\n",
      "Epoch 4440 - lr: 0.05000 - Train loss: 0.59680 - Test loss: 0.74055\n",
      "Epoch 4441 - lr: 0.05000 - Train loss: 0.59679 - Test loss: 0.74057\n",
      "Epoch 4442 - lr: 0.05000 - Train loss: 0.59679 - Test loss: 0.74059\n",
      "Epoch 4443 - lr: 0.05000 - Train loss: 0.59678 - Test loss: 0.74062\n",
      "Epoch 4444 - lr: 0.05000 - Train loss: 0.59678 - Test loss: 0.74064\n",
      "Epoch 4445 - lr: 0.05000 - Train loss: 0.59677 - Test loss: 0.74067\n",
      "Epoch 4446 - lr: 0.05000 - Train loss: 0.59677 - Test loss: 0.74069\n",
      "Epoch 4447 - lr: 0.05000 - Train loss: 0.59676 - Test loss: 0.74071\n",
      "Epoch 4448 - lr: 0.05000 - Train loss: 0.59676 - Test loss: 0.74074\n",
      "Epoch 4449 - lr: 0.05000 - Train loss: 0.59675 - Test loss: 0.74076\n",
      "Epoch 4450 - lr: 0.05000 - Train loss: 0.59675 - Test loss: 0.74079\n",
      "Epoch 4451 - lr: 0.05000 - Train loss: 0.59675 - Test loss: 0.74081\n",
      "Epoch 4452 - lr: 0.05000 - Train loss: 0.59674 - Test loss: 0.74083\n",
      "Epoch 4453 - lr: 0.05000 - Train loss: 0.59674 - Test loss: 0.74086\n",
      "Epoch 4454 - lr: 0.05000 - Train loss: 0.59673 - Test loss: 0.74088\n",
      "Epoch 4455 - lr: 0.05000 - Train loss: 0.59673 - Test loss: 0.74091\n",
      "Epoch 4456 - lr: 0.05000 - Train loss: 0.59672 - Test loss: 0.74093\n",
      "Epoch 4457 - lr: 0.05000 - Train loss: 0.59672 - Test loss: 0.74095\n",
      "Epoch 4458 - lr: 0.05000 - Train loss: 0.59672 - Test loss: 0.74098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4459 - lr: 0.05000 - Train loss: 0.59671 - Test loss: 0.74100\n",
      "Epoch 4460 - lr: 0.05000 - Train loss: 0.59671 - Test loss: 0.74103\n",
      "Epoch 4461 - lr: 0.05000 - Train loss: 0.59670 - Test loss: 0.74105\n",
      "Epoch 4462 - lr: 0.05000 - Train loss: 0.59670 - Test loss: 0.74107\n",
      "Epoch 4463 - lr: 0.05000 - Train loss: 0.59669 - Test loss: 0.74110\n",
      "Epoch 4464 - lr: 0.05000 - Train loss: 0.59669 - Test loss: 0.74112\n",
      "Epoch 4465 - lr: 0.05000 - Train loss: 0.59668 - Test loss: 0.74114\n",
      "Epoch 4466 - lr: 0.05000 - Train loss: 0.59668 - Test loss: 0.74117\n",
      "Epoch 4467 - lr: 0.05000 - Train loss: 0.59668 - Test loss: 0.74119\n",
      "Epoch 4468 - lr: 0.05000 - Train loss: 0.59667 - Test loss: 0.74121\n",
      "Epoch 4469 - lr: 0.05000 - Train loss: 0.59667 - Test loss: 0.74124\n",
      "Epoch 4470 - lr: 0.05000 - Train loss: 0.59666 - Test loss: 0.74126\n",
      "Epoch 4471 - lr: 0.05000 - Train loss: 0.59666 - Test loss: 0.74129\n",
      "Epoch 4472 - lr: 0.05000 - Train loss: 0.59665 - Test loss: 0.74131\n",
      "Epoch 4473 - lr: 0.05000 - Train loss: 0.59665 - Test loss: 0.74133\n",
      "Epoch 4474 - lr: 0.05000 - Train loss: 0.59665 - Test loss: 0.74136\n",
      "Epoch 4475 - lr: 0.05000 - Train loss: 0.59664 - Test loss: 0.74138\n",
      "Epoch 4476 - lr: 0.05000 - Train loss: 0.59664 - Test loss: 0.74140\n",
      "Epoch 4477 - lr: 0.05000 - Train loss: 0.59663 - Test loss: 0.74143\n",
      "Epoch 4478 - lr: 0.05000 - Train loss: 0.59663 - Test loss: 0.74145\n",
      "Epoch 4479 - lr: 0.05000 - Train loss: 0.59662 - Test loss: 0.74147\n",
      "Epoch 4480 - lr: 0.05000 - Train loss: 0.59662 - Test loss: 0.74150\n",
      "Epoch 4481 - lr: 0.05000 - Train loss: 0.59662 - Test loss: 0.74152\n",
      "Epoch 4482 - lr: 0.05000 - Train loss: 0.59661 - Test loss: 0.74154\n",
      "Epoch 4483 - lr: 0.05000 - Train loss: 0.59661 - Test loss: 0.74157\n",
      "Epoch 4484 - lr: 0.05000 - Train loss: 0.59660 - Test loss: 0.74159\n",
      "Epoch 4485 - lr: 0.05000 - Train loss: 0.59660 - Test loss: 0.74161\n",
      "Epoch 4486 - lr: 0.05000 - Train loss: 0.59659 - Test loss: 0.74164\n",
      "Epoch 4487 - lr: 0.05000 - Train loss: 0.59659 - Test loss: 0.74166\n",
      "Epoch 4488 - lr: 0.05000 - Train loss: 0.59659 - Test loss: 0.74168\n",
      "Epoch 4489 - lr: 0.05000 - Train loss: 0.59658 - Test loss: 0.74171\n",
      "Epoch 4490 - lr: 0.05000 - Train loss: 0.59658 - Test loss: 0.74173\n",
      "Epoch 4491 - lr: 0.05000 - Train loss: 0.59657 - Test loss: 0.74175\n",
      "Epoch 4492 - lr: 0.05000 - Train loss: 0.59657 - Test loss: 0.74178\n",
      "Epoch 4493 - lr: 0.05000 - Train loss: 0.59656 - Test loss: 0.74180\n",
      "Epoch 4494 - lr: 0.05000 - Train loss: 0.59656 - Test loss: 0.74182\n",
      "Epoch 4495 - lr: 0.05000 - Train loss: 0.59656 - Test loss: 0.74185\n",
      "Epoch 4496 - lr: 0.05000 - Train loss: 0.59655 - Test loss: 0.74187\n",
      "Epoch 4497 - lr: 0.05000 - Train loss: 0.59655 - Test loss: 0.74189\n",
      "Epoch 4498 - lr: 0.05000 - Train loss: 0.59654 - Test loss: 0.74192\n",
      "Epoch 4499 - lr: 0.05000 - Train loss: 0.59654 - Test loss: 0.74194\n",
      "Epoch 4500 - lr: 0.05000 - Train loss: 0.59653 - Test loss: 0.74196\n",
      "Epoch 4501 - lr: 0.05000 - Train loss: 0.59653 - Test loss: 0.74198\n",
      "Epoch 4502 - lr: 0.05000 - Train loss: 0.59653 - Test loss: 0.74201\n",
      "Epoch 4503 - lr: 0.05000 - Train loss: 0.59652 - Test loss: 0.74203\n",
      "Epoch 4504 - lr: 0.05000 - Train loss: 0.59652 - Test loss: 0.74205\n",
      "Epoch 4505 - lr: 0.05000 - Train loss: 0.59651 - Test loss: 0.74208\n",
      "Epoch 4506 - lr: 0.05000 - Train loss: 0.59651 - Test loss: 0.74210\n",
      "Epoch 4507 - lr: 0.05000 - Train loss: 0.59651 - Test loss: 0.74212\n",
      "Epoch 4508 - lr: 0.05000 - Train loss: 0.59650 - Test loss: 0.74215\n",
      "Epoch 4509 - lr: 0.05000 - Train loss: 0.59650 - Test loss: 0.74217\n",
      "Epoch 4510 - lr: 0.05000 - Train loss: 0.59649 - Test loss: 0.74219\n",
      "Epoch 4511 - lr: 0.05000 - Train loss: 0.59649 - Test loss: 0.74221\n",
      "Epoch 4512 - lr: 0.05000 - Train loss: 0.59648 - Test loss: 0.74224\n",
      "Epoch 4513 - lr: 0.05000 - Train loss: 0.59648 - Test loss: 0.74226\n",
      "Epoch 4514 - lr: 0.05000 - Train loss: 0.59648 - Test loss: 0.74228\n",
      "Epoch 4515 - lr: 0.05000 - Train loss: 0.59647 - Test loss: 0.74230\n",
      "Epoch 4516 - lr: 0.05000 - Train loss: 0.59647 - Test loss: 0.74233\n",
      "Epoch 4517 - lr: 0.05000 - Train loss: 0.59646 - Test loss: 0.74235\n",
      "Epoch 4518 - lr: 0.05000 - Train loss: 0.59646 - Test loss: 0.74237\n",
      "Epoch 4519 - lr: 0.05000 - Train loss: 0.59646 - Test loss: 0.74240\n",
      "Epoch 4520 - lr: 0.05000 - Train loss: 0.59645 - Test loss: 0.74242\n",
      "Epoch 4521 - lr: 0.05000 - Train loss: 0.59645 - Test loss: 0.74244\n",
      "Epoch 4522 - lr: 0.05000 - Train loss: 0.59644 - Test loss: 0.74246\n",
      "Epoch 4523 - lr: 0.05000 - Train loss: 0.59644 - Test loss: 0.74249\n",
      "Epoch 4524 - lr: 0.05000 - Train loss: 0.59643 - Test loss: 0.74251\n",
      "Epoch 4525 - lr: 0.05000 - Train loss: 0.59643 - Test loss: 0.74253\n",
      "Epoch 4526 - lr: 0.05000 - Train loss: 0.59643 - Test loss: 0.74255\n",
      "Epoch 4527 - lr: 0.05000 - Train loss: 0.59642 - Test loss: 0.74258\n",
      "Epoch 4528 - lr: 0.05000 - Train loss: 0.59642 - Test loss: 0.74260\n",
      "Epoch 4529 - lr: 0.05000 - Train loss: 0.59641 - Test loss: 0.74262\n",
      "Epoch 4530 - lr: 0.05000 - Train loss: 0.59641 - Test loss: 0.74264\n",
      "Epoch 4531 - lr: 0.05000 - Train loss: 0.59641 - Test loss: 0.74267\n",
      "Epoch 4532 - lr: 0.05000 - Train loss: 0.59640 - Test loss: 0.74269\n",
      "Epoch 4533 - lr: 0.05000 - Train loss: 0.59640 - Test loss: 0.74271\n",
      "Epoch 4534 - lr: 0.05000 - Train loss: 0.59639 - Test loss: 0.74273\n",
      "Epoch 4535 - lr: 0.05000 - Train loss: 0.59639 - Test loss: 0.74276\n",
      "Epoch 4536 - lr: 0.05000 - Train loss: 0.59639 - Test loss: 0.74278\n",
      "Epoch 4537 - lr: 0.05000 - Train loss: 0.59638 - Test loss: 0.74280\n",
      "Epoch 4538 - lr: 0.05000 - Train loss: 0.59638 - Test loss: 0.74282\n",
      "Epoch 4539 - lr: 0.05000 - Train loss: 0.59637 - Test loss: 0.74285\n",
      "Epoch 4540 - lr: 0.05000 - Train loss: 0.59637 - Test loss: 0.74287\n",
      "Epoch 4541 - lr: 0.05000 - Train loss: 0.59637 - Test loss: 0.74289\n",
      "Epoch 4542 - lr: 0.05000 - Train loss: 0.59636 - Test loss: 0.74291\n",
      "Epoch 4543 - lr: 0.05000 - Train loss: 0.59636 - Test loss: 0.74293\n",
      "Epoch 4544 - lr: 0.05000 - Train loss: 0.59635 - Test loss: 0.74296\n",
      "Epoch 4545 - lr: 0.05000 - Train loss: 0.59635 - Test loss: 0.74298\n",
      "Epoch 4546 - lr: 0.05000 - Train loss: 0.59635 - Test loss: 0.74300\n",
      "Epoch 4547 - lr: 0.05000 - Train loss: 0.59634 - Test loss: 0.74302\n",
      "Epoch 4548 - lr: 0.05000 - Train loss: 0.59634 - Test loss: 0.74305\n",
      "Epoch 4549 - lr: 0.05000 - Train loss: 0.59633 - Test loss: 0.74307\n",
      "Epoch 4550 - lr: 0.05000 - Train loss: 0.59633 - Test loss: 0.74309\n",
      "Epoch 4551 - lr: 0.05000 - Train loss: 0.59633 - Test loss: 0.74311\n",
      "Epoch 4552 - lr: 0.05000 - Train loss: 0.59632 - Test loss: 0.74313\n",
      "Epoch 4553 - lr: 0.05000 - Train loss: 0.59632 - Test loss: 0.74316\n",
      "Epoch 4554 - lr: 0.05000 - Train loss: 0.59631 - Test loss: 0.74318\n",
      "Epoch 4555 - lr: 0.05000 - Train loss: 0.59631 - Test loss: 0.74320\n",
      "Epoch 4556 - lr: 0.05000 - Train loss: 0.59631 - Test loss: 0.74322\n",
      "Epoch 4557 - lr: 0.05000 - Train loss: 0.59630 - Test loss: 0.74324\n",
      "Epoch 4558 - lr: 0.05000 - Train loss: 0.59630 - Test loss: 0.74327\n",
      "Epoch 4559 - lr: 0.05000 - Train loss: 0.59629 - Test loss: 0.74329\n",
      "Epoch 4560 - lr: 0.05000 - Train loss: 0.59629 - Test loss: 0.74331\n",
      "Epoch 4561 - lr: 0.05000 - Train loss: 0.59629 - Test loss: 0.74333\n",
      "Epoch 4562 - lr: 0.05000 - Train loss: 0.59628 - Test loss: 0.74335\n",
      "Epoch 4563 - lr: 0.05000 - Train loss: 0.59628 - Test loss: 0.74338\n",
      "Epoch 4564 - lr: 0.05000 - Train loss: 0.59627 - Test loss: 0.74340\n",
      "Epoch 4565 - lr: 0.05000 - Train loss: 0.59627 - Test loss: 0.74342\n",
      "Epoch 4566 - lr: 0.05000 - Train loss: 0.59627 - Test loss: 0.74344\n",
      "Epoch 4567 - lr: 0.05000 - Train loss: 0.59626 - Test loss: 0.74346\n",
      "Epoch 4568 - lr: 0.05000 - Train loss: 0.59626 - Test loss: 0.74348\n",
      "Epoch 4569 - lr: 0.05000 - Train loss: 0.59626 - Test loss: 0.74351\n",
      "Epoch 4570 - lr: 0.05000 - Train loss: 0.59625 - Test loss: 0.74353\n",
      "Epoch 4571 - lr: 0.05000 - Train loss: 0.59625 - Test loss: 0.74355\n",
      "Epoch 4572 - lr: 0.05000 - Train loss: 0.59624 - Test loss: 0.74357\n",
      "Epoch 4573 - lr: 0.05000 - Train loss: 0.59624 - Test loss: 0.74359\n",
      "Epoch 4574 - lr: 0.05000 - Train loss: 0.59624 - Test loss: 0.74362\n",
      "Epoch 4575 - lr: 0.05000 - Train loss: 0.59623 - Test loss: 0.74364\n",
      "Epoch 4576 - lr: 0.05000 - Train loss: 0.59623 - Test loss: 0.74366\n",
      "Epoch 4577 - lr: 0.05000 - Train loss: 0.59622 - Test loss: 0.74368\n",
      "Epoch 4578 - lr: 0.05000 - Train loss: 0.59622 - Test loss: 0.74370\n",
      "Epoch 4579 - lr: 0.05000 - Train loss: 0.59622 - Test loss: 0.74372\n",
      "Epoch 4580 - lr: 0.05000 - Train loss: 0.59621 - Test loss: 0.74374\n",
      "Epoch 4581 - lr: 0.05000 - Train loss: 0.59621 - Test loss: 0.74377\n",
      "Epoch 4582 - lr: 0.05000 - Train loss: 0.59620 - Test loss: 0.74379\n",
      "Epoch 4583 - lr: 0.05000 - Train loss: 0.59620 - Test loss: 0.74381\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4584 - lr: 0.05000 - Train loss: 0.59620 - Test loss: 0.74383\n",
      "Epoch 4585 - lr: 0.05000 - Train loss: 0.59619 - Test loss: 0.74385\n",
      "Epoch 4586 - lr: 0.05000 - Train loss: 0.59619 - Test loss: 0.74387\n",
      "Epoch 4587 - lr: 0.05000 - Train loss: 0.59619 - Test loss: 0.74390\n",
      "Epoch 4588 - lr: 0.05000 - Train loss: 0.59618 - Test loss: 0.74392\n",
      "Epoch 4589 - lr: 0.05000 - Train loss: 0.59618 - Test loss: 0.74394\n",
      "Epoch 4590 - lr: 0.05000 - Train loss: 0.59617 - Test loss: 0.74396\n",
      "Epoch 4591 - lr: 0.05000 - Train loss: 0.59617 - Test loss: 0.74398\n",
      "Epoch 4592 - lr: 0.05000 - Train loss: 0.59617 - Test loss: 0.74400\n",
      "Epoch 4593 - lr: 0.05000 - Train loss: 0.59616 - Test loss: 0.74402\n",
      "Epoch 4594 - lr: 0.05000 - Train loss: 0.59616 - Test loss: 0.74404\n",
      "Epoch 4595 - lr: 0.05000 - Train loss: 0.59615 - Test loss: 0.74407\n",
      "Epoch 4596 - lr: 0.05000 - Train loss: 0.59615 - Test loss: 0.74409\n",
      "Epoch 4597 - lr: 0.05000 - Train loss: 0.59615 - Test loss: 0.74411\n",
      "Epoch 4598 - lr: 0.05000 - Train loss: 0.59614 - Test loss: 0.74413\n",
      "Epoch 4599 - lr: 0.05000 - Train loss: 0.59614 - Test loss: 0.74415\n",
      "Epoch 4600 - lr: 0.05000 - Train loss: 0.59614 - Test loss: 0.74417\n",
      "Epoch 4601 - lr: 0.05000 - Train loss: 0.59613 - Test loss: 0.74419\n",
      "Epoch 4602 - lr: 0.05000 - Train loss: 0.59613 - Test loss: 0.74421\n",
      "Epoch 4603 - lr: 0.05000 - Train loss: 0.59612 - Test loss: 0.74424\n",
      "Epoch 4604 - lr: 0.05000 - Train loss: 0.59612 - Test loss: 0.74426\n",
      "Epoch 4605 - lr: 0.05000 - Train loss: 0.59612 - Test loss: 0.74428\n",
      "Epoch 4606 - lr: 0.05000 - Train loss: 0.59611 - Test loss: 0.74430\n",
      "Epoch 4607 - lr: 0.05000 - Train loss: 0.59611 - Test loss: 0.74432\n",
      "Epoch 4608 - lr: 0.05000 - Train loss: 0.59611 - Test loss: 0.74434\n",
      "Epoch 4609 - lr: 0.05000 - Train loss: 0.59610 - Test loss: 0.74436\n",
      "Epoch 4610 - lr: 0.05000 - Train loss: 0.59610 - Test loss: 0.74438\n",
      "Epoch 4611 - lr: 0.05000 - Train loss: 0.59609 - Test loss: 0.74440\n",
      "Epoch 4612 - lr: 0.05000 - Train loss: 0.59609 - Test loss: 0.74442\n",
      "Epoch 4613 - lr: 0.05000 - Train loss: 0.59609 - Test loss: 0.74445\n",
      "Epoch 4614 - lr: 0.05000 - Train loss: 0.59608 - Test loss: 0.74447\n",
      "Epoch 4615 - lr: 0.05000 - Train loss: 0.59608 - Test loss: 0.74449\n",
      "Epoch 4616 - lr: 0.05000 - Train loss: 0.59608 - Test loss: 0.74451\n",
      "Epoch 4617 - lr: 0.05000 - Train loss: 0.59607 - Test loss: 0.74453\n",
      "Epoch 4618 - lr: 0.05000 - Train loss: 0.59607 - Test loss: 0.74455\n",
      "Epoch 4619 - lr: 0.05000 - Train loss: 0.59606 - Test loss: 0.74457\n",
      "Epoch 4620 - lr: 0.05000 - Train loss: 0.59606 - Test loss: 0.74459\n",
      "Epoch 4621 - lr: 0.05000 - Train loss: 0.59606 - Test loss: 0.74461\n",
      "Epoch 4622 - lr: 0.05000 - Train loss: 0.59605 - Test loss: 0.74463\n",
      "Epoch 4623 - lr: 0.05000 - Train loss: 0.59605 - Test loss: 0.74465\n",
      "Epoch 4624 - lr: 0.05000 - Train loss: 0.59605 - Test loss: 0.74467\n",
      "Epoch 4625 - lr: 0.05000 - Train loss: 0.59604 - Test loss: 0.74470\n",
      "Epoch 4626 - lr: 0.05000 - Train loss: 0.59604 - Test loss: 0.74472\n",
      "Epoch 4627 - lr: 0.05000 - Train loss: 0.59604 - Test loss: 0.74474\n",
      "Epoch 4628 - lr: 0.05000 - Train loss: 0.59603 - Test loss: 0.74476\n",
      "Epoch 4629 - lr: 0.05000 - Train loss: 0.59603 - Test loss: 0.74478\n",
      "Epoch 4630 - lr: 0.05000 - Train loss: 0.59602 - Test loss: 0.74480\n",
      "Epoch 4631 - lr: 0.05000 - Train loss: 0.59602 - Test loss: 0.74482\n",
      "Epoch 4632 - lr: 0.05000 - Train loss: 0.59602 - Test loss: 0.74484\n",
      "Epoch 4633 - lr: 0.05000 - Train loss: 0.59601 - Test loss: 0.74486\n",
      "Epoch 4634 - lr: 0.05000 - Train loss: 0.59601 - Test loss: 0.74488\n",
      "Epoch 4635 - lr: 0.05000 - Train loss: 0.59601 - Test loss: 0.74490\n",
      "Epoch 4636 - lr: 0.05000 - Train loss: 0.59600 - Test loss: 0.74492\n",
      "Epoch 4637 - lr: 0.05000 - Train loss: 0.59600 - Test loss: 0.74494\n",
      "Epoch 4638 - lr: 0.05000 - Train loss: 0.59599 - Test loss: 0.74496\n",
      "Epoch 4639 - lr: 0.05000 - Train loss: 0.59599 - Test loss: 0.74498\n",
      "Epoch 4640 - lr: 0.05000 - Train loss: 0.59599 - Test loss: 0.74500\n",
      "Epoch 4641 - lr: 0.05000 - Train loss: 0.59598 - Test loss: 0.74502\n",
      "Epoch 4642 - lr: 0.05000 - Train loss: 0.59598 - Test loss: 0.74504\n",
      "Epoch 4643 - lr: 0.05000 - Train loss: 0.59598 - Test loss: 0.74507\n",
      "Epoch 4644 - lr: 0.05000 - Train loss: 0.59597 - Test loss: 0.74509\n",
      "Epoch 4645 - lr: 0.05000 - Train loss: 0.59597 - Test loss: 0.74511\n",
      "Epoch 4646 - lr: 0.05000 - Train loss: 0.59597 - Test loss: 0.74513\n",
      "Epoch 4647 - lr: 0.05000 - Train loss: 0.59596 - Test loss: 0.74515\n",
      "Epoch 4648 - lr: 0.05000 - Train loss: 0.59596 - Test loss: 0.74517\n",
      "Epoch 4649 - lr: 0.05000 - Train loss: 0.59595 - Test loss: 0.74519\n",
      "Epoch 4650 - lr: 0.05000 - Train loss: 0.59595 - Test loss: 0.74521\n",
      "Epoch 4651 - lr: 0.05000 - Train loss: 0.59595 - Test loss: 0.74523\n",
      "Epoch 4652 - lr: 0.05000 - Train loss: 0.59594 - Test loss: 0.74525\n",
      "Epoch 4653 - lr: 0.05000 - Train loss: 0.59594 - Test loss: 0.74527\n",
      "Epoch 4654 - lr: 0.05000 - Train loss: 0.59594 - Test loss: 0.74529\n",
      "Epoch 4655 - lr: 0.05000 - Train loss: 0.59593 - Test loss: 0.74531\n",
      "Epoch 4656 - lr: 0.05000 - Train loss: 0.59593 - Test loss: 0.74533\n",
      "Epoch 4657 - lr: 0.05000 - Train loss: 0.59593 - Test loss: 0.74535\n",
      "Epoch 4658 - lr: 0.05000 - Train loss: 0.59592 - Test loss: 0.74537\n",
      "Epoch 4659 - lr: 0.05000 - Train loss: 0.59592 - Test loss: 0.74539\n",
      "Epoch 4660 - lr: 0.05000 - Train loss: 0.59592 - Test loss: 0.74541\n",
      "Epoch 4661 - lr: 0.05000 - Train loss: 0.59591 - Test loss: 0.74543\n",
      "Epoch 4662 - lr: 0.05000 - Train loss: 0.59591 - Test loss: 0.74545\n",
      "Epoch 4663 - lr: 0.05000 - Train loss: 0.59590 - Test loss: 0.74547\n",
      "Epoch 4664 - lr: 0.05000 - Train loss: 0.59590 - Test loss: 0.74549\n",
      "Epoch 4665 - lr: 0.05000 - Train loss: 0.59590 - Test loss: 0.74551\n",
      "Epoch 4666 - lr: 0.05000 - Train loss: 0.59589 - Test loss: 0.74553\n",
      "Epoch 4667 - lr: 0.05000 - Train loss: 0.59589 - Test loss: 0.74555\n",
      "Epoch 4668 - lr: 0.05000 - Train loss: 0.59589 - Test loss: 0.74557\n",
      "Epoch 4669 - lr: 0.05000 - Train loss: 0.59588 - Test loss: 0.74559\n",
      "Epoch 4670 - lr: 0.05000 - Train loss: 0.59588 - Test loss: 0.74561\n",
      "Epoch 4671 - lr: 0.05000 - Train loss: 0.59588 - Test loss: 0.74563\n",
      "Epoch 4672 - lr: 0.05000 - Train loss: 0.59587 - Test loss: 0.74565\n",
      "Epoch 4673 - lr: 0.05000 - Train loss: 0.59587 - Test loss: 0.74567\n",
      "Epoch 4674 - lr: 0.05000 - Train loss: 0.59587 - Test loss: 0.74569\n",
      "Epoch 4675 - lr: 0.05000 - Train loss: 0.59586 - Test loss: 0.74571\n",
      "Epoch 4676 - lr: 0.05000 - Train loss: 0.59586 - Test loss: 0.74573\n",
      "Epoch 4677 - lr: 0.05000 - Train loss: 0.59586 - Test loss: 0.74575\n",
      "Epoch 4678 - lr: 0.05000 - Train loss: 0.59585 - Test loss: 0.74577\n",
      "Epoch 4679 - lr: 0.05000 - Train loss: 0.59585 - Test loss: 0.74579\n",
      "Epoch 4680 - lr: 0.05000 - Train loss: 0.59584 - Test loss: 0.74580\n",
      "Epoch 4681 - lr: 0.05000 - Train loss: 0.59584 - Test loss: 0.74582\n",
      "Epoch 4682 - lr: 0.05000 - Train loss: 0.59584 - Test loss: 0.74584\n",
      "Epoch 4683 - lr: 0.05000 - Train loss: 0.59583 - Test loss: 0.74586\n",
      "Epoch 4684 - lr: 0.05000 - Train loss: 0.59583 - Test loss: 0.74588\n",
      "Epoch 4685 - lr: 0.05000 - Train loss: 0.59583 - Test loss: 0.74590\n",
      "Epoch 4686 - lr: 0.05000 - Train loss: 0.59582 - Test loss: 0.74592\n",
      "Epoch 4687 - lr: 0.05000 - Train loss: 0.59582 - Test loss: 0.74594\n",
      "Epoch 4688 - lr: 0.05000 - Train loss: 0.59582 - Test loss: 0.74596\n",
      "Epoch 4689 - lr: 0.05000 - Train loss: 0.59581 - Test loss: 0.74598\n",
      "Epoch 4690 - lr: 0.05000 - Train loss: 0.59581 - Test loss: 0.74600\n",
      "Epoch 4691 - lr: 0.05000 - Train loss: 0.59581 - Test loss: 0.74602\n",
      "Epoch 4692 - lr: 0.05000 - Train loss: 0.59580 - Test loss: 0.74604\n",
      "Epoch 4693 - lr: 0.05000 - Train loss: 0.59580 - Test loss: 0.74606\n",
      "Epoch 4694 - lr: 0.05000 - Train loss: 0.59580 - Test loss: 0.74608\n",
      "Epoch 4695 - lr: 0.05000 - Train loss: 0.59579 - Test loss: 0.74610\n",
      "Epoch 4696 - lr: 0.05000 - Train loss: 0.59579 - Test loss: 0.74612\n",
      "Epoch 4697 - lr: 0.05000 - Train loss: 0.59579 - Test loss: 0.74614\n",
      "Epoch 4698 - lr: 0.05000 - Train loss: 0.59578 - Test loss: 0.74615\n",
      "Epoch 4699 - lr: 0.05000 - Train loss: 0.59578 - Test loss: 0.74617\n",
      "Epoch 4700 - lr: 0.05000 - Train loss: 0.59578 - Test loss: 0.74619\n",
      "Epoch 4701 - lr: 0.05000 - Train loss: 0.59577 - Test loss: 0.74621\n",
      "Epoch 4702 - lr: 0.05000 - Train loss: 0.59577 - Test loss: 0.74623\n",
      "Epoch 4703 - lr: 0.05000 - Train loss: 0.59577 - Test loss: 0.74625\n",
      "Epoch 4704 - lr: 0.05000 - Train loss: 0.59576 - Test loss: 0.74627\n",
      "Epoch 4705 - lr: 0.05000 - Train loss: 0.59576 - Test loss: 0.74629\n",
      "Epoch 4706 - lr: 0.05000 - Train loss: 0.59576 - Test loss: 0.74631\n",
      "Epoch 4707 - lr: 0.05000 - Train loss: 0.59575 - Test loss: 0.74633\n",
      "Epoch 4708 - lr: 0.05000 - Train loss: 0.59575 - Test loss: 0.74635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4709 - lr: 0.05000 - Train loss: 0.59574 - Test loss: 0.74637\n",
      "Epoch 4710 - lr: 0.05000 - Train loss: 0.59574 - Test loss: 0.74638\n",
      "Epoch 4711 - lr: 0.05000 - Train loss: 0.59574 - Test loss: 0.74640\n",
      "Epoch 4712 - lr: 0.05000 - Train loss: 0.59573 - Test loss: 0.74642\n",
      "Epoch 4713 - lr: 0.05000 - Train loss: 0.59573 - Test loss: 0.74644\n",
      "Epoch 4714 - lr: 0.05000 - Train loss: 0.59573 - Test loss: 0.74646\n",
      "Epoch 4715 - lr: 0.05000 - Train loss: 0.59572 - Test loss: 0.74648\n",
      "Epoch 4716 - lr: 0.05000 - Train loss: 0.59572 - Test loss: 0.74650\n",
      "Epoch 4717 - lr: 0.05000 - Train loss: 0.59572 - Test loss: 0.74652\n",
      "Epoch 4718 - lr: 0.05000 - Train loss: 0.59571 - Test loss: 0.74654\n",
      "Epoch 4719 - lr: 0.05000 - Train loss: 0.59571 - Test loss: 0.74655\n",
      "Epoch 4720 - lr: 0.05000 - Train loss: 0.59571 - Test loss: 0.74657\n",
      "Epoch 4721 - lr: 0.05000 - Train loss: 0.59570 - Test loss: 0.74659\n",
      "Epoch 4722 - lr: 0.05000 - Train loss: 0.59570 - Test loss: 0.74661\n",
      "Epoch 4723 - lr: 0.05000 - Train loss: 0.59570 - Test loss: 0.74663\n",
      "Epoch 4724 - lr: 0.05000 - Train loss: 0.59569 - Test loss: 0.74665\n",
      "Epoch 4725 - lr: 0.05000 - Train loss: 0.59569 - Test loss: 0.74667\n",
      "Epoch 4726 - lr: 0.05000 - Train loss: 0.59569 - Test loss: 0.74669\n",
      "Epoch 4727 - lr: 0.05000 - Train loss: 0.59568 - Test loss: 0.74670\n",
      "Epoch 4728 - lr: 0.05000 - Train loss: 0.59568 - Test loss: 0.74672\n",
      "Epoch 4729 - lr: 0.05000 - Train loss: 0.59568 - Test loss: 0.74674\n",
      "Epoch 4730 - lr: 0.05000 - Train loss: 0.59567 - Test loss: 0.74676\n",
      "Epoch 4731 - lr: 0.05000 - Train loss: 0.59567 - Test loss: 0.74678\n",
      "Epoch 4732 - lr: 0.05000 - Train loss: 0.59567 - Test loss: 0.74680\n",
      "Epoch 4733 - lr: 0.05000 - Train loss: 0.59566 - Test loss: 0.74682\n",
      "Epoch 4734 - lr: 0.05000 - Train loss: 0.59566 - Test loss: 0.74684\n",
      "Epoch 4735 - lr: 0.05000 - Train loss: 0.59566 - Test loss: 0.74685\n",
      "Epoch 4736 - lr: 0.05000 - Train loss: 0.59565 - Test loss: 0.74687\n",
      "Epoch 4737 - lr: 0.05000 - Train loss: 0.59565 - Test loss: 0.74689\n",
      "Epoch 4738 - lr: 0.05000 - Train loss: 0.59565 - Test loss: 0.74691\n",
      "Epoch 4739 - lr: 0.05000 - Train loss: 0.59564 - Test loss: 0.74693\n",
      "Epoch 4740 - lr: 0.05000 - Train loss: 0.59564 - Test loss: 0.74695\n",
      "Epoch 4741 - lr: 0.05000 - Train loss: 0.59564 - Test loss: 0.74696\n",
      "Epoch 4742 - lr: 0.05000 - Train loss: 0.59563 - Test loss: 0.74698\n",
      "Epoch 4743 - lr: 0.05000 - Train loss: 0.59563 - Test loss: 0.74700\n",
      "Epoch 4744 - lr: 0.05000 - Train loss: 0.59563 - Test loss: 0.74702\n",
      "Epoch 4745 - lr: 0.05000 - Train loss: 0.59562 - Test loss: 0.74704\n",
      "Epoch 4746 - lr: 0.05000 - Train loss: 0.59562 - Test loss: 0.74706\n",
      "Epoch 4747 - lr: 0.05000 - Train loss: 0.59562 - Test loss: 0.74707\n",
      "Epoch 4748 - lr: 0.05000 - Train loss: 0.59561 - Test loss: 0.74709\n",
      "Epoch 4749 - lr: 0.05000 - Train loss: 0.59561 - Test loss: 0.74711\n",
      "Epoch 4750 - lr: 0.05000 - Train loss: 0.59561 - Test loss: 0.74713\n",
      "Epoch 4751 - lr: 0.05000 - Train loss: 0.59560 - Test loss: 0.74715\n",
      "Epoch 4752 - lr: 0.05000 - Train loss: 0.59560 - Test loss: 0.74716\n",
      "Epoch 4753 - lr: 0.05000 - Train loss: 0.59560 - Test loss: 0.74718\n",
      "Epoch 4754 - lr: 0.05000 - Train loss: 0.59560 - Test loss: 0.74720\n",
      "Epoch 4755 - lr: 0.05000 - Train loss: 0.59559 - Test loss: 0.74722\n",
      "Epoch 4756 - lr: 0.05000 - Train loss: 0.59559 - Test loss: 0.74724\n",
      "Epoch 4757 - lr: 0.05000 - Train loss: 0.59559 - Test loss: 0.74726\n",
      "Epoch 4758 - lr: 0.05000 - Train loss: 0.59558 - Test loss: 0.74727\n",
      "Epoch 4759 - lr: 0.05000 - Train loss: 0.59558 - Test loss: 0.74729\n",
      "Epoch 4760 - lr: 0.05000 - Train loss: 0.59558 - Test loss: 0.74731\n",
      "Epoch 4761 - lr: 0.05000 - Train loss: 0.59557 - Test loss: 0.74733\n",
      "Epoch 4762 - lr: 0.05000 - Train loss: 0.59557 - Test loss: 0.74735\n",
      "Epoch 4763 - lr: 0.05000 - Train loss: 0.59557 - Test loss: 0.74736\n",
      "Epoch 4764 - lr: 0.05000 - Train loss: 0.59556 - Test loss: 0.74738\n",
      "Epoch 4765 - lr: 0.05000 - Train loss: 0.59556 - Test loss: 0.74740\n",
      "Epoch 4766 - lr: 0.05000 - Train loss: 0.59556 - Test loss: 0.74742\n",
      "Epoch 4767 - lr: 0.05000 - Train loss: 0.59555 - Test loss: 0.74743\n",
      "Epoch 4768 - lr: 0.05000 - Train loss: 0.59555 - Test loss: 0.74745\n",
      "Epoch 4769 - lr: 0.05000 - Train loss: 0.59555 - Test loss: 0.74747\n",
      "Epoch 4770 - lr: 0.05000 - Train loss: 0.59554 - Test loss: 0.74749\n",
      "Epoch 4771 - lr: 0.05000 - Train loss: 0.59554 - Test loss: 0.74751\n",
      "Epoch 4772 - lr: 0.05000 - Train loss: 0.59554 - Test loss: 0.74752\n",
      "Epoch 4773 - lr: 0.05000 - Train loss: 0.59553 - Test loss: 0.74754\n",
      "Epoch 4774 - lr: 0.05000 - Train loss: 0.59553 - Test loss: 0.74756\n",
      "Epoch 4775 - lr: 0.05000 - Train loss: 0.59553 - Test loss: 0.74758\n",
      "Epoch 4776 - lr: 0.05000 - Train loss: 0.59552 - Test loss: 0.74759\n",
      "Epoch 4777 - lr: 0.05000 - Train loss: 0.59552 - Test loss: 0.74761\n",
      "Epoch 4778 - lr: 0.05000 - Train loss: 0.59552 - Test loss: 0.74763\n",
      "Epoch 4779 - lr: 0.05000 - Train loss: 0.59551 - Test loss: 0.74765\n",
      "Epoch 4780 - lr: 0.05000 - Train loss: 0.59551 - Test loss: 0.74766\n",
      "Epoch 4781 - lr: 0.05000 - Train loss: 0.59551 - Test loss: 0.74768\n",
      "Epoch 4782 - lr: 0.05000 - Train loss: 0.59551 - Test loss: 0.74770\n",
      "Epoch 4783 - lr: 0.05000 - Train loss: 0.59550 - Test loss: 0.74772\n",
      "Epoch 4784 - lr: 0.05000 - Train loss: 0.59550 - Test loss: 0.74773\n",
      "Epoch 4785 - lr: 0.05000 - Train loss: 0.59550 - Test loss: 0.74775\n",
      "Epoch 4786 - lr: 0.05000 - Train loss: 0.59549 - Test loss: 0.74777\n",
      "Epoch 4787 - lr: 0.05000 - Train loss: 0.59549 - Test loss: 0.74779\n",
      "Epoch 4788 - lr: 0.05000 - Train loss: 0.59549 - Test loss: 0.74780\n",
      "Epoch 4789 - lr: 0.05000 - Train loss: 0.59548 - Test loss: 0.74782\n",
      "Epoch 4790 - lr: 0.05000 - Train loss: 0.59548 - Test loss: 0.74784\n",
      "Epoch 4791 - lr: 0.05000 - Train loss: 0.59548 - Test loss: 0.74786\n",
      "Epoch 4792 - lr: 0.05000 - Train loss: 0.59547 - Test loss: 0.74787\n",
      "Epoch 4793 - lr: 0.05000 - Train loss: 0.59547 - Test loss: 0.74789\n",
      "Epoch 4794 - lr: 0.05000 - Train loss: 0.59547 - Test loss: 0.74791\n",
      "Epoch 4795 - lr: 0.05000 - Train loss: 0.59546 - Test loss: 0.74792\n",
      "Epoch 4796 - lr: 0.05000 - Train loss: 0.59546 - Test loss: 0.74794\n",
      "Epoch 4797 - lr: 0.05000 - Train loss: 0.59546 - Test loss: 0.74796\n",
      "Epoch 4798 - lr: 0.05000 - Train loss: 0.59545 - Test loss: 0.74798\n",
      "Epoch 4799 - lr: 0.05000 - Train loss: 0.59545 - Test loss: 0.74799\n",
      "Epoch 4800 - lr: 0.05000 - Train loss: 0.59545 - Test loss: 0.74801\n",
      "Epoch 4801 - lr: 0.05000 - Train loss: 0.59545 - Test loss: 0.74803\n",
      "Epoch 4802 - lr: 0.05000 - Train loss: 0.59544 - Test loss: 0.74804\n",
      "Epoch 4803 - lr: 0.05000 - Train loss: 0.59544 - Test loss: 0.74806\n",
      "Epoch 4804 - lr: 0.05000 - Train loss: 0.59544 - Test loss: 0.74808\n",
      "Epoch 4805 - lr: 0.05000 - Train loss: 0.59543 - Test loss: 0.74809\n",
      "Epoch 4806 - lr: 0.05000 - Train loss: 0.59543 - Test loss: 0.74811\n",
      "Epoch 4807 - lr: 0.05000 - Train loss: 0.59543 - Test loss: 0.74813\n",
      "Epoch 4808 - lr: 0.05000 - Train loss: 0.59542 - Test loss: 0.74815\n",
      "Epoch 4809 - lr: 0.05000 - Train loss: 0.59542 - Test loss: 0.74816\n",
      "Epoch 4810 - lr: 0.05000 - Train loss: 0.59542 - Test loss: 0.74818\n",
      "Epoch 4811 - lr: 0.05000 - Train loss: 0.59541 - Test loss: 0.74820\n",
      "Epoch 4812 - lr: 0.05000 - Train loss: 0.59541 - Test loss: 0.74821\n",
      "Epoch 4813 - lr: 0.05000 - Train loss: 0.59541 - Test loss: 0.74823\n",
      "Epoch 4814 - lr: 0.05000 - Train loss: 0.59541 - Test loss: 0.74825\n",
      "Epoch 4815 - lr: 0.05000 - Train loss: 0.59540 - Test loss: 0.74826\n",
      "Epoch 4816 - lr: 0.05000 - Train loss: 0.59540 - Test loss: 0.74828\n",
      "Epoch 4817 - lr: 0.05000 - Train loss: 0.59540 - Test loss: 0.74830\n",
      "Epoch 4818 - lr: 0.05000 - Train loss: 0.59539 - Test loss: 0.74831\n",
      "Epoch 4819 - lr: 0.05000 - Train loss: 0.59539 - Test loss: 0.74833\n",
      "Epoch 4820 - lr: 0.05000 - Train loss: 0.59539 - Test loss: 0.74835\n",
      "Epoch 4821 - lr: 0.05000 - Train loss: 0.59538 - Test loss: 0.74836\n",
      "Epoch 4822 - lr: 0.05000 - Train loss: 0.59538 - Test loss: 0.74838\n",
      "Epoch 4823 - lr: 0.05000 - Train loss: 0.59538 - Test loss: 0.74840\n",
      "Epoch 4824 - lr: 0.05000 - Train loss: 0.59537 - Test loss: 0.74841\n",
      "Epoch 4825 - lr: 0.05000 - Train loss: 0.59537 - Test loss: 0.74843\n",
      "Epoch 4826 - lr: 0.05000 - Train loss: 0.59537 - Test loss: 0.74844\n",
      "Epoch 4827 - lr: 0.05000 - Train loss: 0.59537 - Test loss: 0.74846\n",
      "Epoch 4828 - lr: 0.05000 - Train loss: 0.59536 - Test loss: 0.74848\n",
      "Epoch 4829 - lr: 0.05000 - Train loss: 0.59536 - Test loss: 0.74849\n",
      "Epoch 4830 - lr: 0.05000 - Train loss: 0.59536 - Test loss: 0.74851\n",
      "Epoch 4831 - lr: 0.05000 - Train loss: 0.59535 - Test loss: 0.74853\n",
      "Epoch 4832 - lr: 0.05000 - Train loss: 0.59535 - Test loss: 0.74854\n",
      "Epoch 4833 - lr: 0.05000 - Train loss: 0.59535 - Test loss: 0.74856\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4834 - lr: 0.05000 - Train loss: 0.59534 - Test loss: 0.74858\n",
      "Epoch 4835 - lr: 0.05000 - Train loss: 0.59534 - Test loss: 0.74859\n",
      "Epoch 4836 - lr: 0.05000 - Train loss: 0.59534 - Test loss: 0.74861\n",
      "Epoch 4837 - lr: 0.05000 - Train loss: 0.59534 - Test loss: 0.74862\n",
      "Epoch 4838 - lr: 0.05000 - Train loss: 0.59533 - Test loss: 0.74864\n",
      "Epoch 4839 - lr: 0.05000 - Train loss: 0.59533 - Test loss: 0.74866\n",
      "Epoch 4840 - lr: 0.05000 - Train loss: 0.59533 - Test loss: 0.74867\n",
      "Epoch 4841 - lr: 0.05000 - Train loss: 0.59532 - Test loss: 0.74869\n",
      "Epoch 4842 - lr: 0.05000 - Train loss: 0.59532 - Test loss: 0.74870\n",
      "Epoch 4843 - lr: 0.05000 - Train loss: 0.59532 - Test loss: 0.74872\n",
      "Epoch 4844 - lr: 0.05000 - Train loss: 0.59531 - Test loss: 0.74874\n",
      "Epoch 4845 - lr: 0.05000 - Train loss: 0.59531 - Test loss: 0.74875\n",
      "Epoch 4846 - lr: 0.05000 - Train loss: 0.59531 - Test loss: 0.74877\n",
      "Epoch 4847 - lr: 0.05000 - Train loss: 0.59531 - Test loss: 0.74878\n",
      "Epoch 4848 - lr: 0.05000 - Train loss: 0.59530 - Test loss: 0.74880\n",
      "Epoch 4849 - lr: 0.05000 - Train loss: 0.59530 - Test loss: 0.74882\n",
      "Epoch 4850 - lr: 0.05000 - Train loss: 0.59530 - Test loss: 0.74883\n",
      "Epoch 4851 - lr: 0.05000 - Train loss: 0.59529 - Test loss: 0.74885\n",
      "Epoch 4852 - lr: 0.05000 - Train loss: 0.59529 - Test loss: 0.74886\n",
      "Epoch 4853 - lr: 0.05000 - Train loss: 0.59529 - Test loss: 0.74888\n",
      "Epoch 4854 - lr: 0.05000 - Train loss: 0.59528 - Test loss: 0.74890\n",
      "Epoch 4855 - lr: 0.05000 - Train loss: 0.59528 - Test loss: 0.74891\n",
      "Epoch 4856 - lr: 0.05000 - Train loss: 0.59528 - Test loss: 0.74893\n",
      "Epoch 4857 - lr: 0.05000 - Train loss: 0.59528 - Test loss: 0.74894\n",
      "Epoch 4858 - lr: 0.05000 - Train loss: 0.59527 - Test loss: 0.74896\n",
      "Epoch 4859 - lr: 0.05000 - Train loss: 0.59527 - Test loss: 0.74897\n",
      "Epoch 4860 - lr: 0.05000 - Train loss: 0.59527 - Test loss: 0.74899\n",
      "Epoch 4861 - lr: 0.05000 - Train loss: 0.59526 - Test loss: 0.74901\n",
      "Epoch 4862 - lr: 0.05000 - Train loss: 0.59526 - Test loss: 0.74902\n",
      "Epoch 4863 - lr: 0.05000 - Train loss: 0.59526 - Test loss: 0.74904\n",
      "Epoch 4864 - lr: 0.05000 - Train loss: 0.59526 - Test loss: 0.74905\n",
      "Epoch 4865 - lr: 0.05000 - Train loss: 0.59525 - Test loss: 0.74907\n",
      "Epoch 4866 - lr: 0.05000 - Train loss: 0.59525 - Test loss: 0.74908\n",
      "Epoch 4867 - lr: 0.05000 - Train loss: 0.59525 - Test loss: 0.74910\n",
      "Epoch 4868 - lr: 0.05000 - Train loss: 0.59524 - Test loss: 0.74911\n",
      "Epoch 4869 - lr: 0.05000 - Train loss: 0.59524 - Test loss: 0.74913\n",
      "Epoch 4870 - lr: 0.05000 - Train loss: 0.59524 - Test loss: 0.74914\n",
      "Epoch 4871 - lr: 0.05000 - Train loss: 0.59524 - Test loss: 0.74916\n",
      "Epoch 4872 - lr: 0.05000 - Train loss: 0.59523 - Test loss: 0.74918\n",
      "Epoch 4873 - lr: 0.05000 - Train loss: 0.59523 - Test loss: 0.74919\n",
      "Epoch 4874 - lr: 0.05000 - Train loss: 0.59523 - Test loss: 0.74921\n",
      "Epoch 4875 - lr: 0.05000 - Train loss: 0.59522 - Test loss: 0.74922\n",
      "Epoch 4876 - lr: 0.05000 - Train loss: 0.59522 - Test loss: 0.74924\n",
      "Epoch 4877 - lr: 0.05000 - Train loss: 0.59522 - Test loss: 0.74925\n",
      "Epoch 4878 - lr: 0.05000 - Train loss: 0.59521 - Test loss: 0.74927\n",
      "Epoch 4879 - lr: 0.05000 - Train loss: 0.59521 - Test loss: 0.74928\n",
      "Epoch 4880 - lr: 0.05000 - Train loss: 0.59521 - Test loss: 0.74930\n",
      "Epoch 4881 - lr: 0.05000 - Train loss: 0.59521 - Test loss: 0.74931\n",
      "Epoch 4882 - lr: 0.05000 - Train loss: 0.59520 - Test loss: 0.74933\n",
      "Epoch 4883 - lr: 0.05000 - Train loss: 0.59520 - Test loss: 0.74934\n",
      "Epoch 4884 - lr: 0.05000 - Train loss: 0.59520 - Test loss: 0.74936\n",
      "Epoch 4885 - lr: 0.05000 - Train loss: 0.59519 - Test loss: 0.74937\n",
      "Epoch 4886 - lr: 0.05000 - Train loss: 0.59519 - Test loss: 0.74939\n",
      "Epoch 4887 - lr: 0.05000 - Train loss: 0.59519 - Test loss: 0.74940\n",
      "Epoch 4888 - lr: 0.05000 - Train loss: 0.59519 - Test loss: 0.74942\n",
      "Epoch 4889 - lr: 0.05000 - Train loss: 0.59518 - Test loss: 0.74943\n",
      "Epoch 4890 - lr: 0.05000 - Train loss: 0.59518 - Test loss: 0.74945\n",
      "Epoch 4891 - lr: 0.05000 - Train loss: 0.59518 - Test loss: 0.74946\n",
      "Epoch 4892 - lr: 0.05000 - Train loss: 0.59517 - Test loss: 0.74948\n",
      "Epoch 4893 - lr: 0.05000 - Train loss: 0.59517 - Test loss: 0.74949\n",
      "Epoch 4894 - lr: 0.05000 - Train loss: 0.59517 - Test loss: 0.74951\n",
      "Epoch 4895 - lr: 0.05000 - Train loss: 0.59517 - Test loss: 0.74952\n",
      "Epoch 4896 - lr: 0.05000 - Train loss: 0.59516 - Test loss: 0.74953\n",
      "Epoch 4897 - lr: 0.05000 - Train loss: 0.59516 - Test loss: 0.74955\n",
      "Epoch 4898 - lr: 0.05000 - Train loss: 0.59516 - Test loss: 0.74956\n",
      "Epoch 4899 - lr: 0.05000 - Train loss: 0.59515 - Test loss: 0.74958\n",
      "Epoch 4900 - lr: 0.05000 - Train loss: 0.59515 - Test loss: 0.74959\n",
      "Epoch 4901 - lr: 0.05000 - Train loss: 0.59515 - Test loss: 0.74961\n",
      "Epoch 4902 - lr: 0.05000 - Train loss: 0.59515 - Test loss: 0.74962\n",
      "Epoch 4903 - lr: 0.05000 - Train loss: 0.59514 - Test loss: 0.74964\n",
      "Epoch 4904 - lr: 0.05000 - Train loss: 0.59514 - Test loss: 0.74965\n",
      "Epoch 4905 - lr: 0.05000 - Train loss: 0.59514 - Test loss: 0.74967\n",
      "Epoch 4906 - lr: 0.05000 - Train loss: 0.59514 - Test loss: 0.74968\n",
      "Epoch 4907 - lr: 0.05000 - Train loss: 0.59513 - Test loss: 0.74969\n",
      "Epoch 4908 - lr: 0.05000 - Train loss: 0.59513 - Test loss: 0.74971\n",
      "Epoch 4909 - lr: 0.05000 - Train loss: 0.59513 - Test loss: 0.74972\n",
      "Epoch 4910 - lr: 0.05000 - Train loss: 0.59512 - Test loss: 0.74974\n",
      "Epoch 4911 - lr: 0.05000 - Train loss: 0.59512 - Test loss: 0.74975\n",
      "Epoch 4912 - lr: 0.05000 - Train loss: 0.59512 - Test loss: 0.74977\n",
      "Epoch 4913 - lr: 0.05000 - Train loss: 0.59512 - Test loss: 0.74978\n",
      "Epoch 4914 - lr: 0.05000 - Train loss: 0.59511 - Test loss: 0.74979\n",
      "Epoch 4915 - lr: 0.05000 - Train loss: 0.59511 - Test loss: 0.74981\n",
      "Epoch 4916 - lr: 0.05000 - Train loss: 0.59511 - Test loss: 0.74982\n",
      "Epoch 4917 - lr: 0.05000 - Train loss: 0.59510 - Test loss: 0.74984\n",
      "Epoch 4918 - lr: 0.05000 - Train loss: 0.59510 - Test loss: 0.74985\n",
      "Epoch 4919 - lr: 0.05000 - Train loss: 0.59510 - Test loss: 0.74986\n",
      "Epoch 4920 - lr: 0.05000 - Train loss: 0.59510 - Test loss: 0.74988\n",
      "Epoch 4921 - lr: 0.05000 - Train loss: 0.59509 - Test loss: 0.74989\n",
      "Epoch 4922 - lr: 0.05000 - Train loss: 0.59509 - Test loss: 0.74991\n",
      "Epoch 4923 - lr: 0.05000 - Train loss: 0.59509 - Test loss: 0.74992\n",
      "Epoch 4924 - lr: 0.05000 - Train loss: 0.59509 - Test loss: 0.74993\n",
      "Epoch 4925 - lr: 0.05000 - Train loss: 0.59508 - Test loss: 0.74995\n",
      "Epoch 4926 - lr: 0.05000 - Train loss: 0.59508 - Test loss: 0.74996\n",
      "Epoch 4927 - lr: 0.05000 - Train loss: 0.59508 - Test loss: 0.74998\n",
      "Epoch 4928 - lr: 0.05000 - Train loss: 0.59507 - Test loss: 0.74999\n",
      "Epoch 4929 - lr: 0.05000 - Train loss: 0.59507 - Test loss: 0.75000\n",
      "Epoch 4930 - lr: 0.05000 - Train loss: 0.59507 - Test loss: 0.75002\n",
      "Epoch 4931 - lr: 0.05000 - Train loss: 0.59507 - Test loss: 0.75003\n",
      "Epoch 4932 - lr: 0.05000 - Train loss: 0.59506 - Test loss: 0.75005\n",
      "Epoch 4933 - lr: 0.05000 - Train loss: 0.59506 - Test loss: 0.75006\n",
      "Epoch 4934 - lr: 0.05000 - Train loss: 0.59506 - Test loss: 0.75007\n",
      "Epoch 4935 - lr: 0.05000 - Train loss: 0.59506 - Test loss: 0.75009\n",
      "Epoch 4936 - lr: 0.05000 - Train loss: 0.59505 - Test loss: 0.75010\n",
      "Epoch 4937 - lr: 0.05000 - Train loss: 0.59505 - Test loss: 0.75011\n",
      "Epoch 4938 - lr: 0.05000 - Train loss: 0.59505 - Test loss: 0.75013\n",
      "Epoch 4939 - lr: 0.05000 - Train loss: 0.59504 - Test loss: 0.75014\n",
      "Epoch 4940 - lr: 0.05000 - Train loss: 0.59504 - Test loss: 0.75015\n",
      "Epoch 4941 - lr: 0.05000 - Train loss: 0.59504 - Test loss: 0.75017\n",
      "Epoch 4942 - lr: 0.05000 - Train loss: 0.59504 - Test loss: 0.75018\n",
      "Epoch 4943 - lr: 0.05000 - Train loss: 0.59503 - Test loss: 0.75019\n",
      "Epoch 4944 - lr: 0.05000 - Train loss: 0.59503 - Test loss: 0.75021\n",
      "Epoch 4945 - lr: 0.05000 - Train loss: 0.59503 - Test loss: 0.75022\n",
      "Epoch 4946 - lr: 0.05000 - Train loss: 0.59503 - Test loss: 0.75023\n",
      "Epoch 4947 - lr: 0.05000 - Train loss: 0.59502 - Test loss: 0.75025\n",
      "Epoch 4948 - lr: 0.05000 - Train loss: 0.59502 - Test loss: 0.75026\n",
      "Epoch 4949 - lr: 0.05000 - Train loss: 0.59502 - Test loss: 0.75027\n",
      "Epoch 4950 - lr: 0.05000 - Train loss: 0.59501 - Test loss: 0.75029\n",
      "Epoch 4951 - lr: 0.05000 - Train loss: 0.59501 - Test loss: 0.75030\n",
      "Epoch 4952 - lr: 0.05000 - Train loss: 0.59501 - Test loss: 0.75031\n",
      "Epoch 4953 - lr: 0.05000 - Train loss: 0.59501 - Test loss: 0.75033\n",
      "Epoch 4954 - lr: 0.05000 - Train loss: 0.59500 - Test loss: 0.75034\n",
      "Epoch 4955 - lr: 0.05000 - Train loss: 0.59500 - Test loss: 0.75035\n",
      "Epoch 4956 - lr: 0.05000 - Train loss: 0.59500 - Test loss: 0.75037\n",
      "Epoch 4957 - lr: 0.05000 - Train loss: 0.59500 - Test loss: 0.75038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4958 - lr: 0.05000 - Train loss: 0.59499 - Test loss: 0.75039\n",
      "Epoch 4959 - lr: 0.05000 - Train loss: 0.59499 - Test loss: 0.75040\n",
      "Epoch 4960 - lr: 0.05000 - Train loss: 0.59499 - Test loss: 0.75042\n",
      "Epoch 4961 - lr: 0.05000 - Train loss: 0.59499 - Test loss: 0.75043\n",
      "Epoch 4962 - lr: 0.05000 - Train loss: 0.59498 - Test loss: 0.75044\n",
      "Epoch 4963 - lr: 0.05000 - Train loss: 0.59498 - Test loss: 0.75046\n",
      "Epoch 4964 - lr: 0.05000 - Train loss: 0.59498 - Test loss: 0.75047\n",
      "Epoch 4965 - lr: 0.05000 - Train loss: 0.59497 - Test loss: 0.75048\n",
      "Epoch 4966 - lr: 0.05000 - Train loss: 0.59497 - Test loss: 0.75049\n",
      "Epoch 4967 - lr: 0.05000 - Train loss: 0.59497 - Test loss: 0.75051\n",
      "Epoch 4968 - lr: 0.05000 - Train loss: 0.59497 - Test loss: 0.75052\n",
      "Epoch 4969 - lr: 0.05000 - Train loss: 0.59496 - Test loss: 0.75053\n",
      "Epoch 4970 - lr: 0.05000 - Train loss: 0.59496 - Test loss: 0.75055\n",
      "Epoch 4971 - lr: 0.05000 - Train loss: 0.59496 - Test loss: 0.75056\n",
      "Epoch 4972 - lr: 0.05000 - Train loss: 0.59496 - Test loss: 0.75057\n",
      "Epoch 4973 - lr: 0.05000 - Train loss: 0.59495 - Test loss: 0.75058\n",
      "Epoch 4974 - lr: 0.05000 - Train loss: 0.59495 - Test loss: 0.75060\n",
      "Epoch 4975 - lr: 0.05000 - Train loss: 0.59495 - Test loss: 0.75061\n",
      "Epoch 4976 - lr: 0.05000 - Train loss: 0.59495 - Test loss: 0.75062\n",
      "Epoch 4977 - lr: 0.05000 - Train loss: 0.59494 - Test loss: 0.75063\n",
      "Epoch 4978 - lr: 0.05000 - Train loss: 0.59494 - Test loss: 0.75064\n",
      "Epoch 4979 - lr: 0.05000 - Train loss: 0.59494 - Test loss: 0.75066\n",
      "Epoch 4980 - lr: 0.05000 - Train loss: 0.59494 - Test loss: 0.75067\n",
      "Epoch 4981 - lr: 0.05000 - Train loss: 0.59493 - Test loss: 0.75068\n",
      "Epoch 4982 - lr: 0.05000 - Train loss: 0.59493 - Test loss: 0.75069\n",
      "Epoch 4983 - lr: 0.05000 - Train loss: 0.59493 - Test loss: 0.75071\n",
      "Epoch 4984 - lr: 0.05000 - Train loss: 0.59492 - Test loss: 0.75072\n",
      "Epoch 4985 - lr: 0.05000 - Train loss: 0.59492 - Test loss: 0.75073\n",
      "Epoch 4986 - lr: 0.05000 - Train loss: 0.59492 - Test loss: 0.75074\n",
      "Epoch 4987 - lr: 0.05000 - Train loss: 0.59492 - Test loss: 0.75076\n",
      "Epoch 4988 - lr: 0.05000 - Train loss: 0.59491 - Test loss: 0.75077\n",
      "Epoch 4989 - lr: 0.05000 - Train loss: 0.59491 - Test loss: 0.75078\n",
      "Epoch 4990 - lr: 0.05000 - Train loss: 0.59491 - Test loss: 0.75079\n",
      "Epoch 4991 - lr: 0.05000 - Train loss: 0.59491 - Test loss: 0.75080\n",
      "Epoch 4992 - lr: 0.05000 - Train loss: 0.59490 - Test loss: 0.75082\n",
      "Epoch 4993 - lr: 0.05000 - Train loss: 0.59490 - Test loss: 0.75083\n",
      "Epoch 4994 - lr: 0.05000 - Train loss: 0.59490 - Test loss: 0.75084\n",
      "Epoch 4995 - lr: 0.05000 - Train loss: 0.59490 - Test loss: 0.75085\n",
      "Epoch 4996 - lr: 0.05000 - Train loss: 0.59489 - Test loss: 0.75086\n",
      "Epoch 4997 - lr: 0.05000 - Train loss: 0.59489 - Test loss: 0.75087\n",
      "Epoch 4998 - lr: 0.05000 - Train loss: 0.59489 - Test loss: 0.75089\n",
      "Epoch 4999 - lr: 0.05000 - Train loss: 0.59489 - Test loss: 0.75090\n",
      "Epoch 5000 - lr: 0.05000 - Train loss: 0.59488 - Test loss: 0.75091\n",
      "number of hidden neurons in first layer is: 240 number of hidden neurons in second layer is: 360\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of hidden neurons in first layer is: 360 number of hidden neurons in second layer is: 240\n",
      "Epoch 1 - lr: 0.05000 - Train loss: 12.20424 - Test loss: 225.97802\n",
      "Epoch 2 - lr: 0.05000 - Train loss: 8.48852 - Test loss: 119.59114\n",
      "Epoch 3 - lr: 0.05000 - Train loss: 7.79191 - Test loss: 119.00872\n",
      "Epoch 4 - lr: 0.05000 - Train loss: 7.78443 - Test loss: 118.08917\n",
      "Epoch 5 - lr: 0.05000 - Train loss: 7.75626 - Test loss: 116.61036\n",
      "Epoch 6 - lr: 0.05000 - Train loss: 7.70385 - Test loss: 114.10529\n",
      "Epoch 7 - lr: 0.05000 - Train loss: 7.61113 - Test loss: 109.63859\n",
      "Epoch 8 - lr: 0.05000 - Train loss: 7.44609 - Test loss: 101.50585\n",
      "Epoch 9 - lr: 0.05000 - Train loss: 7.15459 - Test loss: 87.38249\n",
      "Epoch 10 - lr: 0.05000 - Train loss: 6.61698 - Test loss: 65.34228\n",
      "Epoch 11 - lr: 0.05000 - Train loss: 5.76252 - Test loss: 38.24813\n",
      "Epoch 12 - lr: 0.05000 - Train loss: 4.86746 - Test loss: 19.49518\n",
      "Epoch 13 - lr: 0.05000 - Train loss: 4.17564 - Test loss: 10.72570\n",
      "Epoch 14 - lr: 0.05000 - Train loss: 3.62950 - Test loss: 6.43644\n",
      "Epoch 15 - lr: 0.05000 - Train loss: 3.21944 - Test loss: 4.40262\n",
      "Epoch 16 - lr: 0.05000 - Train loss: 2.93903 - Test loss: 3.43009\n",
      "Epoch 17 - lr: 0.05000 - Train loss: 2.75804 - Test loss: 2.93321\n",
      "Epoch 18 - lr: 0.05000 - Train loss: 2.64220 - Test loss: 2.65279\n",
      "Epoch 19 - lr: 0.05000 - Train loss: 2.56622 - Test loss: 2.47817\n",
      "Epoch 20 - lr: 0.05000 - Train loss: 2.51446 - Test loss: 2.36074\n",
      "Epoch 21 - lr: 0.05000 - Train loss: 2.47783 - Test loss: 2.27739\n",
      "Epoch 22 - lr: 0.05000 - Train loss: 2.45102 - Test loss: 2.21599\n",
      "Epoch 23 - lr: 0.05000 - Train loss: 2.43083 - Test loss: 2.16954\n",
      "Epoch 24 - lr: 0.05000 - Train loss: 2.41525 - Test loss: 2.13370\n",
      "Epoch 25 - lr: 0.05000 - Train loss: 2.40300 - Test loss: 2.10564\n",
      "Epoch 26 - lr: 0.05000 - Train loss: 2.39320 - Test loss: 2.08340\n",
      "Epoch 27 - lr: 0.05000 - Train loss: 2.38526 - Test loss: 2.06560\n",
      "Epoch 28 - lr: 0.05000 - Train loss: 2.37874 - Test loss: 2.05124\n",
      "Epoch 29 - lr: 0.05000 - Train loss: 2.37334 - Test loss: 2.03957\n",
      "Epoch 30 - lr: 0.05000 - Train loss: 2.36882 - Test loss: 2.03004\n",
      "Epoch 31 - lr: 0.05000 - Train loss: 2.36502 - Test loss: 2.02221\n",
      "Epoch 32 - lr: 0.05000 - Train loss: 2.36179 - Test loss: 2.01574\n",
      "Epoch 33 - lr: 0.05000 - Train loss: 2.35904 - Test loss: 2.01039\n",
      "Epoch 34 - lr: 0.05000 - Train loss: 2.35668 - Test loss: 2.00593\n",
      "Epoch 35 - lr: 0.05000 - Train loss: 2.35465 - Test loss: 2.00222\n",
      "Epoch 36 - lr: 0.05000 - Train loss: 2.35289 - Test loss: 1.99912\n",
      "Epoch 37 - lr: 0.05000 - Train loss: 2.35136 - Test loss: 1.99651\n",
      "Epoch 38 - lr: 0.05000 - Train loss: 2.35002 - Test loss: 1.99433\n",
      "Epoch 39 - lr: 0.05000 - Train loss: 2.34886 - Test loss: 1.99250\n",
      "Epoch 40 - lr: 0.05000 - Train loss: 2.34783 - Test loss: 1.99095\n",
      "Epoch 41 - lr: 0.05000 - Train loss: 2.34693 - Test loss: 1.98966\n",
      "Epoch 42 - lr: 0.05000 - Train loss: 2.34614 - Test loss: 1.98857\n",
      "Epoch 43 - lr: 0.05000 - Train loss: 2.34543 - Test loss: 1.98765\n",
      "Epoch 44 - lr: 0.05000 - Train loss: 2.34481 - Test loss: 1.98689\n",
      "Epoch 45 - lr: 0.05000 - Train loss: 2.34426 - Test loss: 1.98625\n",
      "Epoch 46 - lr: 0.05000 - Train loss: 2.34377 - Test loss: 1.98573\n",
      "Epoch 47 - lr: 0.05000 - Train loss: 2.34333 - Test loss: 1.98530\n",
      "Epoch 48 - lr: 0.05000 - Train loss: 2.34293 - Test loss: 1.98495\n",
      "Epoch 49 - lr: 0.05000 - Train loss: 2.34258 - Test loss: 1.98467\n",
      "Epoch 50 - lr: 0.05000 - Train loss: 2.34226 - Test loss: 1.98445\n",
      "Epoch 51 - lr: 0.05000 - Train loss: 2.34198 - Test loss: 1.98429\n",
      "Epoch 52 - lr: 0.05000 - Train loss: 2.34172 - Test loss: 1.98417\n",
      "Epoch 53 - lr: 0.05000 - Train loss: 2.34149 - Test loss: 1.98409\n",
      "Epoch 54 - lr: 0.05000 - Train loss: 2.34127 - Test loss: 1.98405\n",
      "Epoch 55 - lr: 0.05000 - Train loss: 2.34108 - Test loss: 1.98404\n",
      "Epoch 56 - lr: 0.05000 - Train loss: 2.34090 - Test loss: 1.98406\n",
      "Epoch 57 - lr: 0.05000 - Train loss: 2.34074 - Test loss: 1.98410\n",
      "Epoch 58 - lr: 0.05000 - Train loss: 2.34059 - Test loss: 1.98416\n",
      "Epoch 59 - lr: 0.05000 - Train loss: 2.34045 - Test loss: 1.98424\n",
      "Epoch 60 - lr: 0.05000 - Train loss: 2.34032 - Test loss: 1.98433\n",
      "Epoch 61 - lr: 0.05000 - Train loss: 2.34020 - Test loss: 1.98444\n",
      "Epoch 62 - lr: 0.05000 - Train loss: 2.34009 - Test loss: 1.98456\n",
      "Epoch 63 - lr: 0.05000 - Train loss: 2.33999 - Test loss: 1.98469\n",
      "Epoch 64 - lr: 0.05000 - Train loss: 2.33989 - Test loss: 1.98482\n",
      "Epoch 65 - lr: 0.05000 - Train loss: 2.33979 - Test loss: 1.98496\n",
      "Epoch 66 - lr: 0.05000 - Train loss: 2.33970 - Test loss: 1.98511\n",
      "Epoch 67 - lr: 0.05000 - Train loss: 2.33962 - Test loss: 1.98526\n",
      "Epoch 68 - lr: 0.05000 - Train loss: 2.33954 - Test loss: 1.98542\n",
      "Epoch 69 - lr: 0.05000 - Train loss: 2.33946 - Test loss: 1.98557\n",
      "Epoch 70 - lr: 0.05000 - Train loss: 2.33938 - Test loss: 1.98573\n",
      "Epoch 71 - lr: 0.05000 - Train loss: 2.33930 - Test loss: 1.98589\n",
      "Epoch 72 - lr: 0.05000 - Train loss: 2.33923 - Test loss: 1.98605\n",
      "Epoch 73 - lr: 0.05000 - Train loss: 2.33916 - Test loss: 1.98621\n",
      "Epoch 74 - lr: 0.05000 - Train loss: 2.33909 - Test loss: 1.98637\n",
      "Epoch 75 - lr: 0.05000 - Train loss: 2.33903 - Test loss: 1.98653\n",
      "Epoch 76 - lr: 0.05000 - Train loss: 2.33896 - Test loss: 1.98668\n",
      "Epoch 77 - lr: 0.05000 - Train loss: 2.33889 - Test loss: 1.98684\n",
      "Epoch 78 - lr: 0.05000 - Train loss: 2.33883 - Test loss: 1.98699\n",
      "Epoch 79 - lr: 0.05000 - Train loss: 2.33876 - Test loss: 1.98714\n",
      "Epoch 80 - lr: 0.05000 - Train loss: 2.33870 - Test loss: 1.98729\n",
      "Epoch 81 - lr: 0.05000 - Train loss: 2.33864 - Test loss: 1.98743\n",
      "Epoch 82 - lr: 0.05000 - Train loss: 2.33858 - Test loss: 1.98757\n",
      "Epoch 83 - lr: 0.05000 - Train loss: 2.33851 - Test loss: 1.98771\n",
      "Epoch 84 - lr: 0.05000 - Train loss: 2.33845 - Test loss: 1.98784\n",
      "Epoch 85 - lr: 0.05000 - Train loss: 2.33839 - Test loss: 1.98797\n",
      "Epoch 86 - lr: 0.05000 - Train loss: 2.33833 - Test loss: 1.98810\n",
      "Epoch 87 - lr: 0.05000 - Train loss: 2.33827 - Test loss: 1.98822\n",
      "Epoch 88 - lr: 0.05000 - Train loss: 2.33821 - Test loss: 1.98835\n",
      "Epoch 89 - lr: 0.05000 - Train loss: 2.33815 - Test loss: 1.98846\n",
      "Epoch 90 - lr: 0.05000 - Train loss: 2.33809 - Test loss: 1.98858\n",
      "Epoch 91 - lr: 0.05000 - Train loss: 2.33803 - Test loss: 1.98869\n",
      "Epoch 92 - lr: 0.05000 - Train loss: 2.33797 - Test loss: 1.98879\n",
      "Epoch 93 - lr: 0.05000 - Train loss: 2.33791 - Test loss: 1.98889\n",
      "Epoch 94 - lr: 0.05000 - Train loss: 2.33785 - Test loss: 1.98899\n",
      "Epoch 95 - lr: 0.05000 - Train loss: 2.33779 - Test loss: 1.98909\n",
      "Epoch 96 - lr: 0.05000 - Train loss: 2.33774 - Test loss: 1.98918\n",
      "Epoch 97 - lr: 0.05000 - Train loss: 2.33768 - Test loss: 1.98927\n",
      "Epoch 98 - lr: 0.05000 - Train loss: 2.33762 - Test loss: 1.98936\n",
      "Epoch 99 - lr: 0.05000 - Train loss: 2.33756 - Test loss: 1.98944\n",
      "Epoch 100 - lr: 0.05000 - Train loss: 2.33750 - Test loss: 1.98952\n",
      "Epoch 101 - lr: 0.05000 - Train loss: 2.33744 - Test loss: 1.98960\n",
      "Epoch 102 - lr: 0.05000 - Train loss: 2.33738 - Test loss: 1.98967\n",
      "Epoch 103 - lr: 0.05000 - Train loss: 2.33732 - Test loss: 1.98974\n",
      "Epoch 104 - lr: 0.05000 - Train loss: 2.33726 - Test loss: 1.98981\n",
      "Epoch 105 - lr: 0.05000 - Train loss: 2.33720 - Test loss: 1.98987\n",
      "Epoch 106 - lr: 0.05000 - Train loss: 2.33714 - Test loss: 1.98994\n",
      "Epoch 107 - lr: 0.05000 - Train loss: 2.33708 - Test loss: 1.99000\n",
      "Epoch 108 - lr: 0.05000 - Train loss: 2.33702 - Test loss: 1.99005\n",
      "Epoch 109 - lr: 0.05000 - Train loss: 2.33696 - Test loss: 1.99011\n",
      "Epoch 110 - lr: 0.05000 - Train loss: 2.33690 - Test loss: 1.99016\n",
      "Epoch 111 - lr: 0.05000 - Train loss: 2.33684 - Test loss: 1.99021\n",
      "Epoch 112 - lr: 0.05000 - Train loss: 2.33678 - Test loss: 1.99026\n",
      "Epoch 113 - lr: 0.05000 - Train loss: 2.33672 - Test loss: 1.99030\n",
      "Epoch 114 - lr: 0.05000 - Train loss: 2.33666 - Test loss: 1.99035\n",
      "Epoch 115 - lr: 0.05000 - Train loss: 2.33659 - Test loss: 1.99039\n",
      "Epoch 116 - lr: 0.05000 - Train loss: 2.33653 - Test loss: 1.99043\n",
      "Epoch 117 - lr: 0.05000 - Train loss: 2.33646 - Test loss: 1.99048\n",
      "Epoch 118 - lr: 0.05000 - Train loss: 2.33639 - Test loss: 1.99052\n",
      "Epoch 119 - lr: 0.05000 - Train loss: 2.33632 - Test loss: 1.99056\n",
      "Epoch 120 - lr: 0.05000 - Train loss: 2.33624 - Test loss: 1.99060\n",
      "Epoch 121 - lr: 0.05000 - Train loss: 2.33616 - Test loss: 1.99064\n",
      "Epoch 122 - lr: 0.05000 - Train loss: 2.33606 - Test loss: 1.99068\n",
      "Epoch 123 - lr: 0.05000 - Train loss: 2.33596 - Test loss: 1.99073\n",
      "Epoch 124 - lr: 0.05000 - Train loss: 2.33584 - Test loss: 1.99079\n",
      "Epoch 125 - lr: 0.05000 - Train loss: 2.33569 - Test loss: 1.99086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 126 - lr: 0.05000 - Train loss: 2.33547 - Test loss: 1.99095\n",
      "Epoch 127 - lr: 0.05000 - Train loss: 2.33508 - Test loss: 1.99108\n",
      "Epoch 128 - lr: 0.05000 - Train loss: 2.33402 - Test loss: 1.99120\n",
      "Epoch 129 - lr: 0.05000 - Train loss: 2.31234 - Test loss: 1.98674\n",
      "Epoch 130 - lr: 0.05000 - Train loss: 2.25797 - Test loss: 1.98383\n",
      "Epoch 131 - lr: 0.05000 - Train loss: 2.25178 - Test loss: 1.98092\n",
      "Epoch 132 - lr: 0.05000 - Train loss: 2.24908 - Test loss: 1.97893\n",
      "Epoch 133 - lr: 0.05000 - Train loss: 2.24672 - Test loss: 1.97753\n",
      "Epoch 134 - lr: 0.05000 - Train loss: 2.22459 - Test loss: 1.97299\n",
      "Epoch 135 - lr: 0.05000 - Train loss: 2.17402 - Test loss: 1.96958\n",
      "Epoch 136 - lr: 0.05000 - Train loss: 2.16862 - Test loss: 1.96592\n",
      "Epoch 137 - lr: 0.05000 - Train loss: 2.16657 - Test loss: 1.96321\n",
      "Epoch 138 - lr: 0.05000 - Train loss: 2.16577 - Test loss: 1.96204\n",
      "Epoch 139 - lr: 0.05000 - Train loss: 2.16567 - Test loss: 1.96285\n",
      "Epoch 140 - lr: 0.05000 - Train loss: 2.16613 - Test loss: 1.96463\n",
      "Epoch 141 - lr: 0.05000 - Train loss: 2.16710 - Test loss: 1.96607\n",
      "Epoch 142 - lr: 0.05000 - Train loss: 2.16793 - Test loss: 1.96660\n",
      "Epoch 143 - lr: 0.05000 - Train loss: 2.16825 - Test loss: 1.96618\n",
      "Epoch 144 - lr: 0.05000 - Train loss: 2.16895 - Test loss: 1.96621\n",
      "Epoch 145 - lr: 0.05000 - Train loss: 2.16836 - Test loss: 1.96307\n",
      "Epoch 146 - lr: 0.05000 - Train loss: 2.17135 - Test loss: 1.96618\n",
      "Epoch 147 - lr: 0.05000 - Train loss: 2.16995 - Test loss: 1.97679\n",
      "Epoch 148 - lr: 0.05000 - Train loss: 2.16709 - Test loss: 1.95471\n",
      "Epoch 149 - lr: 0.05000 - Train loss: 2.17683 - Test loss: 1.96717\n",
      "Epoch 150 - lr: 0.05000 - Train loss: 2.17634 - Test loss: 1.98449\n",
      "Epoch 151 - lr: 0.05000 - Train loss: 2.20925 - Test loss: 1.97895\n",
      "Epoch 152 - lr: 0.05000 - Train loss: 2.17087 - Test loss: 1.96664\n",
      "Epoch 153 - lr: 0.05000 - Train loss: 2.17418 - Test loss: 1.96853\n",
      "Epoch 154 - lr: 0.05000 - Train loss: 2.17166 - Test loss: 1.96487\n",
      "Epoch 155 - lr: 0.05000 - Train loss: 2.17125 - Test loss: 1.96533\n",
      "Epoch 156 - lr: 0.05000 - Train loss: 2.18167 - Test loss: 1.96805\n",
      "Epoch 157 - lr: 0.05000 - Train loss: 2.18320 - Test loss: 1.97307\n",
      "Epoch 158 - lr: 0.05000 - Train loss: 2.19187 - Test loss: 1.98765\n",
      "Epoch 159 - lr: 0.05000 - Train loss: 2.16775 - Test loss: 1.95515\n",
      "Epoch 160 - lr: 0.05000 - Train loss: 2.18052 - Test loss: 1.96515\n",
      "Epoch 161 - lr: 0.05000 - Train loss: 2.17992 - Test loss: 1.97063\n",
      "Epoch 162 - lr: 0.05000 - Train loss: 2.19473 - Test loss: 1.95729\n",
      "Epoch 163 - lr: 0.05000 - Train loss: 2.17737 - Test loss: 1.95969\n",
      "Epoch 164 - lr: 0.05000 - Train loss: 2.16886 - Test loss: 1.95527\n",
      "Epoch 165 - lr: 0.05000 - Train loss: 2.18301 - Test loss: 1.97271\n",
      "Epoch 166 - lr: 0.05000 - Train loss: 2.18278 - Test loss: 1.97657\n",
      "Epoch 167 - lr: 0.05000 - Train loss: 2.16965 - Test loss: 1.95557\n",
      "Epoch 168 - lr: 0.05000 - Train loss: 2.10876 - Test loss: 1.97158\n",
      "Epoch 169 - lr: 0.05000 - Train loss: 2.11516 - Test loss: 1.96696\n",
      "Epoch 170 - lr: 0.05000 - Train loss: 2.09937 - Test loss: 1.95525\n",
      "Epoch 171 - lr: 0.05000 - Train loss: 2.11464 - Test loss: 1.96854\n",
      "Epoch 172 - lr: 0.05000 - Train loss: 2.10071 - Test loss: 1.95344\n",
      "Epoch 173 - lr: 0.05000 - Train loss: 2.10289 - Test loss: 1.94601\n",
      "Epoch 174 - lr: 0.05000 - Train loss: 2.09992 - Test loss: 1.94065\n",
      "Epoch 175 - lr: 0.05000 - Train loss: 2.11080 - Test loss: 1.96486\n",
      "Epoch 176 - lr: 0.05000 - Train loss: 2.09967 - Test loss: 1.93526\n",
      "Epoch 177 - lr: 0.05000 - Train loss: 2.11091 - Test loss: 1.95850\n",
      "Epoch 178 - lr: 0.05000 - Train loss: 2.10103 - Test loss: 1.95327\n",
      "Epoch 179 - lr: 0.05000 - Train loss: 2.11141 - Test loss: 1.94092\n",
      "Epoch 180 - lr: 0.05000 - Train loss: 2.10090 - Test loss: 1.96449\n",
      "Epoch 181 - lr: 0.05000 - Train loss: 2.12392 - Test loss: 1.95271\n",
      "Epoch 182 - lr: 0.05000 - Train loss: 2.09728 - Test loss: 1.96356\n",
      "Epoch 183 - lr: 0.05000 - Train loss: 2.12514 - Test loss: 1.93320\n",
      "Epoch 184 - lr: 0.05000 - Train loss: 2.09500 - Test loss: 1.97498\n",
      "Epoch 185 - lr: 0.05000 - Train loss: 2.08272 - Test loss: 1.94064\n",
      "Epoch 186 - lr: 0.05000 - Train loss: 2.04615 - Test loss: 1.94802\n",
      "Epoch 187 - lr: 0.05000 - Train loss: 2.02661 - Test loss: 1.94807\n",
      "Epoch 188 - lr: 0.05000 - Train loss: 2.05386 - Test loss: 1.94519\n",
      "Epoch 189 - lr: 0.05000 - Train loss: 2.04162 - Test loss: 1.96822\n",
      "Epoch 190 - lr: 0.05000 - Train loss: 2.04126 - Test loss: 1.95980\n",
      "Epoch 191 - lr: 0.05000 - Train loss: 2.02740 - Test loss: 1.96863\n",
      "Epoch 192 - lr: 0.05000 - Train loss: 2.07024 - Test loss: 1.92190\n",
      "Epoch 193 - lr: 0.05000 - Train loss: 2.05958 - Test loss: 1.91779\n",
      "Epoch 194 - lr: 0.05000 - Train loss: 2.01915 - Test loss: 1.88328\n",
      "Epoch 195 - lr: 0.05000 - Train loss: 2.04027 - Test loss: 1.97581\n",
      "Epoch 196 - lr: 0.05000 - Train loss: 2.02858 - Test loss: 1.94906\n",
      "Epoch 197 - lr: 0.05000 - Train loss: 2.01025 - Test loss: 1.82387\n",
      "Epoch 198 - lr: 0.05000 - Train loss: 2.05444 - Test loss: 1.99869\n",
      "Epoch 199 - lr: 0.05000 - Train loss: 2.03107 - Test loss: 1.95749\n",
      "Epoch 200 - lr: 0.05000 - Train loss: 2.03257 - Test loss: 1.98354\n",
      "Epoch 201 - lr: 0.05000 - Train loss: 2.01849 - Test loss: 1.97771\n",
      "Epoch 202 - lr: 0.05000 - Train loss: 2.01162 - Test loss: 1.99355\n",
      "Epoch 203 - lr: 0.05000 - Train loss: 1.94841 - Test loss: 1.75883\n",
      "Epoch 204 - lr: 0.05000 - Train loss: 1.99934 - Test loss: 1.93807\n",
      "Epoch 205 - lr: 0.05000 - Train loss: 1.98534 - Test loss: 1.98558\n",
      "Epoch 206 - lr: 0.05000 - Train loss: 2.04255 - Test loss: 2.07167\n",
      "Epoch 207 - lr: 0.05000 - Train loss: 2.16538 - Test loss: 2.04817\n",
      "Epoch 208 - lr: 0.05000 - Train loss: 2.13966 - Test loss: 2.02935\n",
      "Epoch 209 - lr: 0.05000 - Train loss: 2.12866 - Test loss: 2.01802\n",
      "Epoch 210 - lr: 0.05000 - Train loss: 2.12369 - Test loss: 2.01090\n",
      "Epoch 211 - lr: 0.05000 - Train loss: 2.12070 - Test loss: 2.00597\n",
      "Epoch 212 - lr: 0.05000 - Train loss: 2.11859 - Test loss: 2.00233\n",
      "Epoch 213 - lr: 0.05000 - Train loss: 2.11696 - Test loss: 1.99949\n",
      "Epoch 214 - lr: 0.05000 - Train loss: 2.11557 - Test loss: 1.99713\n",
      "Epoch 215 - lr: 0.05000 - Train loss: 2.11417 - Test loss: 1.99492\n",
      "Epoch 216 - lr: 0.05000 - Train loss: 2.11203 - Test loss: 1.99132\n",
      "Epoch 217 - lr: 0.05000 - Train loss: 2.13524 - Test loss: 2.00501\n",
      "Epoch 218 - lr: 0.05000 - Train loss: 2.18927 - Test loss: 1.99697\n",
      "Epoch 219 - lr: 0.05000 - Train loss: 2.18540 - Test loss: 1.99440\n",
      "Epoch 220 - lr: 0.05000 - Train loss: 2.18414 - Test loss: 1.99313\n",
      "Epoch 221 - lr: 0.05000 - Train loss: 2.18338 - Test loss: 1.99238\n",
      "Epoch 222 - lr: 0.05000 - Train loss: 2.18266 - Test loss: 1.99197\n",
      "Epoch 223 - lr: 0.05000 - Train loss: 2.18094 - Test loss: 1.99195\n",
      "Epoch 224 - lr: 0.05000 - Train loss: 2.13439 - Test loss: 1.97646\n",
      "Epoch 225 - lr: 0.05000 - Train loss: 2.10826 - Test loss: 1.98230\n",
      "Epoch 226 - lr: 0.05000 - Train loss: 2.11112 - Test loss: 1.98050\n",
      "Epoch 227 - lr: 0.05000 - Train loss: 2.11092 - Test loss: 1.97909\n",
      "Epoch 228 - lr: 0.05000 - Train loss: 2.11017 - Test loss: 1.97048\n",
      "Epoch 229 - lr: 0.05000 - Train loss: 2.10509 - Test loss: 1.94835\n",
      "Epoch 230 - lr: 0.05000 - Train loss: 2.09999 - Test loss: 1.93117\n",
      "Epoch 231 - lr: 0.05000 - Train loss: 2.07799 - Test loss: 1.93369\n",
      "Epoch 232 - lr: 0.05000 - Train loss: 2.09837 - Test loss: 1.89656\n",
      "Epoch 233 - lr: 0.05000 - Train loss: 2.10855 - Test loss: 1.98963\n",
      "Epoch 234 - lr: 0.05000 - Train loss: 2.09532 - Test loss: 1.88560\n",
      "Epoch 235 - lr: 0.05000 - Train loss: 2.09311 - Test loss: 1.98226\n",
      "Epoch 236 - lr: 0.05000 - Train loss: 2.09534 - Test loss: 1.99545\n",
      "Epoch 237 - lr: 0.05000 - Train loss: 2.11558 - Test loss: 1.99355\n",
      "Epoch 238 - lr: 0.05000 - Train loss: 2.11468 - Test loss: 1.99100\n",
      "Epoch 239 - lr: 0.05000 - Train loss: 2.08644 - Test loss: 1.82413\n",
      "Epoch 240 - lr: 0.05000 - Train loss: 2.11688 - Test loss: 1.98273\n",
      "Epoch 241 - lr: 0.05000 - Train loss: 2.02940 - Test loss: 1.99799\n",
      "Epoch 242 - lr: 0.05000 - Train loss: 2.02198 - Test loss: 1.82830\n",
      "Epoch 243 - lr: 0.05000 - Train loss: 2.01224 - Test loss: 1.99121\n",
      "Epoch 244 - lr: 0.05000 - Train loss: 2.02276 - Test loss: 1.87955\n",
      "Epoch 245 - lr: 0.05000 - Train loss: 1.97786 - Test loss: 1.82800\n",
      "Epoch 246 - lr: 0.05000 - Train loss: 1.94889 - Test loss: 1.87350\n",
      "Epoch 247 - lr: 0.05000 - Train loss: 1.95339 - Test loss: 1.86411\n",
      "Epoch 248 - lr: 0.05000 - Train loss: 1.85406 - Test loss: 1.94581\n",
      "Epoch 249 - lr: 0.05000 - Train loss: 1.75357 - Test loss: 1.94818\n",
      "Epoch 250 - lr: 0.05000 - Train loss: 2.03710 - Test loss: 2.00242\n",
      "Epoch 251 - lr: 0.05000 - Train loss: 2.04487 - Test loss: 1.87959\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 252 - lr: 0.05000 - Train loss: 2.01377 - Test loss: 1.84536\n",
      "Epoch 253 - lr: 0.05000 - Train loss: 1.99393 - Test loss: 1.83959\n",
      "Epoch 254 - lr: 0.05000 - Train loss: 2.01613 - Test loss: 1.77496\n",
      "Epoch 255 - lr: 0.05000 - Train loss: 2.01669 - Test loss: 1.77461\n",
      "Epoch 256 - lr: 0.05000 - Train loss: 2.00331 - Test loss: 1.98349\n",
      "Epoch 257 - lr: 0.05000 - Train loss: 2.05434 - Test loss: 1.89669\n",
      "Epoch 258 - lr: 0.05000 - Train loss: 2.05154 - Test loss: 1.90388\n",
      "Epoch 259 - lr: 0.05000 - Train loss: 1.98636 - Test loss: 1.80775\n",
      "Epoch 260 - lr: 0.05000 - Train loss: 2.01087 - Test loss: 1.80731\n",
      "Epoch 261 - lr: 0.05000 - Train loss: 1.99810 - Test loss: 1.86715\n",
      "Epoch 262 - lr: 0.05000 - Train loss: 1.99265 - Test loss: 2.03148\n",
      "Epoch 263 - lr: 0.05000 - Train loss: 2.11190 - Test loss: 1.96292\n",
      "Epoch 264 - lr: 0.05000 - Train loss: 1.86382 - Test loss: 1.97637\n",
      "Epoch 265 - lr: 0.05000 - Train loss: 1.79423 - Test loss: 2.05796\n",
      "Epoch 266 - lr: 0.05000 - Train loss: 2.00211 - Test loss: 2.05899\n",
      "Epoch 267 - lr: 0.05000 - Train loss: 2.06979 - Test loss: 2.04741\n",
      "Epoch 268 - lr: 0.05000 - Train loss: 2.06792 - Test loss: 2.04382\n",
      "Epoch 269 - lr: 0.05000 - Train loss: 2.06719 - Test loss: 2.04221\n",
      "Epoch 270 - lr: 0.05000 - Train loss: 2.06632 - Test loss: 2.04035\n",
      "Epoch 271 - lr: 0.05000 - Train loss: 2.04902 - Test loss: 2.01306\n",
      "Epoch 272 - lr: 0.05000 - Train loss: 1.89989 - Test loss: 1.99122\n",
      "Epoch 273 - lr: 0.05000 - Train loss: 1.89558 - Test loss: 1.97901\n",
      "Epoch 274 - lr: 0.05000 - Train loss: 1.84473 - Test loss: 2.06192\n",
      "Epoch 275 - lr: 0.05000 - Train loss: 2.00564 - Test loss: 2.05543\n",
      "Epoch 276 - lr: 0.05000 - Train loss: 1.99998 - Test loss: 2.04714\n",
      "Epoch 277 - lr: 0.05000 - Train loss: 1.99522 - Test loss: 2.02893\n",
      "Epoch 278 - lr: 0.05000 - Train loss: 1.91522 - Test loss: 2.01835\n",
      "Epoch 279 - lr: 0.05000 - Train loss: 2.05188 - Test loss: 2.01107\n",
      "Epoch 280 - lr: 0.05000 - Train loss: 1.88836 - Test loss: 1.99986\n",
      "Epoch 281 - lr: 0.05000 - Train loss: 1.92635 - Test loss: 1.89641\n",
      "Epoch 282 - lr: 0.05000 - Train loss: 1.91734 - Test loss: 1.91402\n",
      "Epoch 283 - lr: 0.05000 - Train loss: 1.87002 - Test loss: 1.94239\n",
      "Epoch 284 - lr: 0.05000 - Train loss: 1.81659 - Test loss: 1.98670\n",
      "Epoch 285 - lr: 0.05000 - Train loss: 1.78372 - Test loss: 1.99655\n",
      "Epoch 286 - lr: 0.05000 - Train loss: 1.80289 - Test loss: 2.05005\n",
      "Epoch 287 - lr: 0.05000 - Train loss: 1.99762 - Test loss: 2.04114\n",
      "Epoch 288 - lr: 0.05000 - Train loss: 1.99595 - Test loss: 2.03847\n",
      "Epoch 289 - lr: 0.05000 - Train loss: 1.99455 - Test loss: 2.03633\n",
      "Epoch 290 - lr: 0.05000 - Train loss: 1.99339 - Test loss: 2.03444\n",
      "Epoch 291 - lr: 0.05000 - Train loss: 1.99239 - Test loss: 2.03275\n",
      "Epoch 292 - lr: 0.05000 - Train loss: 1.99150 - Test loss: 2.03123\n",
      "Epoch 293 - lr: 0.05000 - Train loss: 1.99070 - Test loss: 2.02984\n",
      "Epoch 294 - lr: 0.05000 - Train loss: 1.98997 - Test loss: 2.02858\n",
      "Epoch 295 - lr: 0.05000 - Train loss: 1.98929 - Test loss: 2.02744\n",
      "Epoch 296 - lr: 0.05000 - Train loss: 1.98867 - Test loss: 2.02639\n",
      "Epoch 297 - lr: 0.05000 - Train loss: 1.98809 - Test loss: 2.02543\n",
      "Epoch 298 - lr: 0.05000 - Train loss: 1.98755 - Test loss: 2.02455\n",
      "Epoch 299 - lr: 0.05000 - Train loss: 1.98705 - Test loss: 2.02374\n",
      "Epoch 300 - lr: 0.05000 - Train loss: 1.98658 - Test loss: 2.02300\n",
      "Epoch 301 - lr: 0.05000 - Train loss: 1.98614 - Test loss: 2.02233\n",
      "Epoch 302 - lr: 0.05000 - Train loss: 1.98572 - Test loss: 2.02171\n",
      "Epoch 303 - lr: 0.05000 - Train loss: 1.98533 - Test loss: 2.02114\n",
      "Epoch 304 - lr: 0.05000 - Train loss: 1.98496 - Test loss: 2.02061\n",
      "Epoch 305 - lr: 0.05000 - Train loss: 1.98461 - Test loss: 2.02013\n",
      "Epoch 306 - lr: 0.05000 - Train loss: 1.98428 - Test loss: 2.01969\n",
      "Epoch 307 - lr: 0.05000 - Train loss: 1.98397 - Test loss: 2.01929\n",
      "Epoch 308 - lr: 0.05000 - Train loss: 1.98367 - Test loss: 2.01891\n",
      "Epoch 309 - lr: 0.05000 - Train loss: 1.98338 - Test loss: 2.01856\n",
      "Epoch 310 - lr: 0.05000 - Train loss: 1.98310 - Test loss: 2.01823\n",
      "Epoch 311 - lr: 0.05000 - Train loss: 1.98281 - Test loss: 2.01791\n",
      "Epoch 312 - lr: 0.05000 - Train loss: 1.98251 - Test loss: 2.01757\n",
      "Epoch 313 - lr: 0.05000 - Train loss: 1.98213 - Test loss: 2.01712\n",
      "Epoch 314 - lr: 0.05000 - Train loss: 1.98126 - Test loss: 2.01521\n",
      "Epoch 315 - lr: 0.05000 - Train loss: 1.96872 - Test loss: 2.01324\n",
      "Epoch 316 - lr: 0.05000 - Train loss: 1.73527 - Test loss: 2.00964\n",
      "Epoch 317 - lr: 0.05000 - Train loss: 1.98392 - Test loss: 2.01678\n",
      "Epoch 318 - lr: 0.05000 - Train loss: 1.98311 - Test loss: 2.01674\n",
      "Epoch 319 - lr: 0.05000 - Train loss: 1.98271 - Test loss: 2.01651\n",
      "Epoch 320 - lr: 0.05000 - Train loss: 1.98236 - Test loss: 2.01632\n",
      "Epoch 321 - lr: 0.05000 - Train loss: 1.98204 - Test loss: 2.01617\n",
      "Epoch 322 - lr: 0.05000 - Train loss: 1.98173 - Test loss: 2.01601\n",
      "Epoch 323 - lr: 0.05000 - Train loss: 1.98139 - Test loss: 2.01577\n",
      "Epoch 324 - lr: 0.05000 - Train loss: 1.98079 - Test loss: 2.01493\n",
      "Epoch 325 - lr: 0.05000 - Train loss: 1.97514 - Test loss: 2.08896\n",
      "Epoch 326 - lr: 0.05000 - Train loss: 2.06115 - Test loss: 2.01759\n",
      "Epoch 327 - lr: 0.05000 - Train loss: 2.04678 - Test loss: 2.00800\n",
      "Epoch 328 - lr: 0.05000 - Train loss: 2.04467 - Test loss: 2.00461\n",
      "Epoch 329 - lr: 0.05000 - Train loss: 2.04406 - Test loss: 2.00293\n",
      "Epoch 330 - lr: 0.05000 - Train loss: 2.04381 - Test loss: 2.00196\n",
      "Epoch 331 - lr: 0.05000 - Train loss: 2.04367 - Test loss: 2.00133\n",
      "Epoch 332 - lr: 0.05000 - Train loss: 2.04357 - Test loss: 2.00089\n",
      "Epoch 333 - lr: 0.05000 - Train loss: 2.04348 - Test loss: 2.00057\n",
      "Epoch 334 - lr: 0.05000 - Train loss: 2.04339 - Test loss: 2.00033\n",
      "Epoch 335 - lr: 0.05000 - Train loss: 2.04331 - Test loss: 2.00014\n",
      "Epoch 336 - lr: 0.05000 - Train loss: 2.04322 - Test loss: 1.99999\n",
      "Epoch 337 - lr: 0.05000 - Train loss: 2.04314 - Test loss: 1.99988\n",
      "Epoch 338 - lr: 0.05000 - Train loss: 2.04306 - Test loss: 1.99980\n",
      "Epoch 339 - lr: 0.05000 - Train loss: 2.04298 - Test loss: 1.99974\n",
      "Epoch 340 - lr: 0.05000 - Train loss: 2.04290 - Test loss: 1.99970\n",
      "Epoch 341 - lr: 0.05000 - Train loss: 2.04282 - Test loss: 1.99968\n",
      "Epoch 342 - lr: 0.05000 - Train loss: 2.04275 - Test loss: 1.99968\n",
      "Epoch 343 - lr: 0.05000 - Train loss: 2.04269 - Test loss: 1.99970\n",
      "Epoch 344 - lr: 0.05000 - Train loss: 2.04263 - Test loss: 1.99974\n",
      "Epoch 345 - lr: 0.05000 - Train loss: 2.04257 - Test loss: 1.99979\n",
      "Epoch 346 - lr: 0.05000 - Train loss: 2.04252 - Test loss: 1.99986\n",
      "Epoch 347 - lr: 0.05000 - Train loss: 2.04248 - Test loss: 1.99994\n",
      "Epoch 348 - lr: 0.05000 - Train loss: 2.04244 - Test loss: 2.00004\n",
      "Epoch 349 - lr: 0.05000 - Train loss: 2.04241 - Test loss: 2.00016\n",
      "Epoch 350 - lr: 0.05000 - Train loss: 2.04238 - Test loss: 2.00030\n",
      "Epoch 351 - lr: 0.05000 - Train loss: 2.04237 - Test loss: 2.00045\n",
      "Epoch 352 - lr: 0.05000 - Train loss: 2.04235 - Test loss: 2.00062\n",
      "Epoch 353 - lr: 0.05000 - Train loss: 2.04235 - Test loss: 2.00080\n",
      "Epoch 354 - lr: 0.05000 - Train loss: 2.04236 - Test loss: 2.00100\n",
      "Epoch 355 - lr: 0.05000 - Train loss: 2.04237 - Test loss: 2.00120\n",
      "Epoch 356 - lr: 0.05000 - Train loss: 2.04237 - Test loss: 2.00140\n",
      "Epoch 357 - lr: 0.05000 - Train loss: 2.04235 - Test loss: 2.00156\n",
      "Epoch 358 - lr: 0.05000 - Train loss: 2.04223 - Test loss: 2.00166\n",
      "Epoch 359 - lr: 0.05000 - Train loss: 2.04170 - Test loss: 2.00164\n",
      "Epoch 360 - lr: 0.05000 - Train loss: 2.03792 - Test loss: 1.99804\n",
      "Epoch 361 - lr: 0.05000 - Train loss: 2.00377 - Test loss: 1.94747\n",
      "Epoch 362 - lr: 0.05000 - Train loss: 1.76493 - Test loss: 2.00807\n",
      "Epoch 363 - lr: 0.05000 - Train loss: 1.98025 - Test loss: 2.01230\n",
      "Epoch 364 - lr: 0.05000 - Train loss: 1.97966 - Test loss: 2.01244\n",
      "Epoch 365 - lr: 0.05000 - Train loss: 1.97935 - Test loss: 2.01249\n",
      "Epoch 366 - lr: 0.05000 - Train loss: 1.97910 - Test loss: 2.01258\n",
      "Epoch 367 - lr: 0.05000 - Train loss: 1.97889 - Test loss: 2.01271\n",
      "Epoch 368 - lr: 0.05000 - Train loss: 1.97872 - Test loss: 2.01285\n",
      "Epoch 369 - lr: 0.05000 - Train loss: 1.97857 - Test loss: 2.01300\n",
      "Epoch 370 - lr: 0.05000 - Train loss: 1.97844 - Test loss: 2.01314\n",
      "Epoch 371 - lr: 0.05000 - Train loss: 1.97833 - Test loss: 2.01329\n",
      "Epoch 372 - lr: 0.05000 - Train loss: 1.97822 - Test loss: 2.01342\n",
      "Epoch 373 - lr: 0.05000 - Train loss: 1.97813 - Test loss: 2.01355\n",
      "Epoch 374 - lr: 0.05000 - Train loss: 1.97803 - Test loss: 2.01366\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 375 - lr: 0.05000 - Train loss: 1.97793 - Test loss: 2.01374\n",
      "Epoch 376 - lr: 0.05000 - Train loss: 1.97779 - Test loss: 2.01376\n",
      "Epoch 377 - lr: 0.05000 - Train loss: 1.97755 - Test loss: 2.01357\n",
      "Epoch 378 - lr: 0.05000 - Train loss: 1.97621 - Test loss: 2.00665\n",
      "Epoch 379 - lr: 0.05000 - Train loss: 1.69292 - Test loss: 2.01307\n",
      "Epoch 380 - lr: 0.05000 - Train loss: 1.97800 - Test loss: 2.01424\n",
      "Epoch 381 - lr: 0.05000 - Train loss: 1.97791 - Test loss: 2.01440\n",
      "Epoch 382 - lr: 0.05000 - Train loss: 1.97788 - Test loss: 2.01451\n",
      "Epoch 383 - lr: 0.05000 - Train loss: 1.97784 - Test loss: 2.01462\n",
      "Epoch 384 - lr: 0.05000 - Train loss: 1.97781 - Test loss: 2.01472\n",
      "Epoch 385 - lr: 0.05000 - Train loss: 1.97778 - Test loss: 2.01482\n",
      "Epoch 386 - lr: 0.05000 - Train loss: 1.97776 - Test loss: 2.01492\n",
      "Epoch 387 - lr: 0.05000 - Train loss: 1.97773 - Test loss: 2.01501\n",
      "Epoch 388 - lr: 0.05000 - Train loss: 1.97771 - Test loss: 2.01511\n",
      "Epoch 389 - lr: 0.05000 - Train loss: 1.97769 - Test loss: 2.01520\n",
      "Epoch 390 - lr: 0.05000 - Train loss: 1.97766 - Test loss: 2.01528\n",
      "Epoch 391 - lr: 0.05000 - Train loss: 1.97764 - Test loss: 2.01537\n",
      "Epoch 392 - lr: 0.05000 - Train loss: 1.97762 - Test loss: 2.01545\n",
      "Epoch 393 - lr: 0.05000 - Train loss: 1.97761 - Test loss: 2.01554\n",
      "Epoch 394 - lr: 0.05000 - Train loss: 1.97759 - Test loss: 2.01562\n",
      "Epoch 395 - lr: 0.05000 - Train loss: 1.97757 - Test loss: 2.01570\n",
      "Epoch 396 - lr: 0.05000 - Train loss: 1.97755 - Test loss: 2.01578\n",
      "Epoch 397 - lr: 0.05000 - Train loss: 1.97754 - Test loss: 2.01586\n",
      "Epoch 398 - lr: 0.05000 - Train loss: 1.97752 - Test loss: 2.01594\n",
      "Epoch 399 - lr: 0.05000 - Train loss: 1.97750 - Test loss: 2.01601\n",
      "Epoch 400 - lr: 0.05000 - Train loss: 1.97749 - Test loss: 2.01609\n",
      "Epoch 401 - lr: 0.05000 - Train loss: 1.97747 - Test loss: 2.01617\n",
      "Epoch 402 - lr: 0.05000 - Train loss: 1.97746 - Test loss: 2.01625\n",
      "Epoch 403 - lr: 0.05000 - Train loss: 1.97745 - Test loss: 2.01632\n",
      "Epoch 404 - lr: 0.05000 - Train loss: 1.97743 - Test loss: 2.01640\n",
      "Epoch 405 - lr: 0.05000 - Train loss: 1.97742 - Test loss: 2.01647\n",
      "Epoch 406 - lr: 0.05000 - Train loss: 1.97741 - Test loss: 2.01655\n",
      "Epoch 407 - lr: 0.05000 - Train loss: 1.97739 - Test loss: 2.01662\n",
      "Epoch 408 - lr: 0.05000 - Train loss: 1.97738 - Test loss: 2.01670\n",
      "Epoch 409 - lr: 0.05000 - Train loss: 1.97737 - Test loss: 2.01677\n",
      "Epoch 410 - lr: 0.05000 - Train loss: 1.97736 - Test loss: 2.01685\n",
      "Epoch 411 - lr: 0.05000 - Train loss: 1.97735 - Test loss: 2.01692\n",
      "Epoch 412 - lr: 0.05000 - Train loss: 1.97734 - Test loss: 2.01700\n",
      "Epoch 413 - lr: 0.05000 - Train loss: 1.97733 - Test loss: 2.01708\n",
      "Epoch 414 - lr: 0.05000 - Train loss: 1.97732 - Test loss: 2.01715\n",
      "Epoch 415 - lr: 0.05000 - Train loss: 1.97731 - Test loss: 2.01723\n",
      "Epoch 416 - lr: 0.05000 - Train loss: 1.97730 - Test loss: 2.01730\n",
      "Epoch 417 - lr: 0.05000 - Train loss: 1.97729 - Test loss: 2.01738\n",
      "Epoch 418 - lr: 0.05000 - Train loss: 1.97728 - Test loss: 2.01745\n",
      "Epoch 419 - lr: 0.05000 - Train loss: 1.97727 - Test loss: 2.01753\n",
      "Epoch 420 - lr: 0.05000 - Train loss: 1.97727 - Test loss: 2.01761\n",
      "Epoch 421 - lr: 0.05000 - Train loss: 1.97726 - Test loss: 2.01768\n",
      "Epoch 422 - lr: 0.05000 - Train loss: 1.97725 - Test loss: 2.01776\n",
      "Epoch 423 - lr: 0.05000 - Train loss: 1.97725 - Test loss: 2.01784\n",
      "Epoch 424 - lr: 0.05000 - Train loss: 1.97724 - Test loss: 2.01791\n",
      "Epoch 425 - lr: 0.05000 - Train loss: 1.97723 - Test loss: 2.01799\n",
      "Epoch 426 - lr: 0.05000 - Train loss: 1.97723 - Test loss: 2.01807\n",
      "Epoch 427 - lr: 0.05000 - Train loss: 1.97722 - Test loss: 2.01815\n",
      "Epoch 428 - lr: 0.05000 - Train loss: 1.97722 - Test loss: 2.01823\n",
      "Epoch 429 - lr: 0.05000 - Train loss: 1.97721 - Test loss: 2.01831\n",
      "Epoch 430 - lr: 0.05000 - Train loss: 1.97721 - Test loss: 2.01839\n",
      "Epoch 431 - lr: 0.05000 - Train loss: 1.97720 - Test loss: 2.01847\n",
      "Epoch 432 - lr: 0.05000 - Train loss: 1.97720 - Test loss: 2.01855\n",
      "Epoch 433 - lr: 0.05000 - Train loss: 1.97719 - Test loss: 2.01863\n",
      "Epoch 434 - lr: 0.05000 - Train loss: 1.97719 - Test loss: 2.01871\n",
      "Epoch 435 - lr: 0.05000 - Train loss: 1.97719 - Test loss: 2.01879\n",
      "Epoch 436 - lr: 0.05000 - Train loss: 1.97718 - Test loss: 2.01888\n",
      "Epoch 437 - lr: 0.05000 - Train loss: 1.97718 - Test loss: 2.01896\n",
      "Epoch 438 - lr: 0.05000 - Train loss: 1.97718 - Test loss: 2.01905\n",
      "Epoch 439 - lr: 0.05000 - Train loss: 1.97718 - Test loss: 2.01913\n",
      "Epoch 440 - lr: 0.05000 - Train loss: 1.97718 - Test loss: 2.01922\n",
      "Epoch 441 - lr: 0.05000 - Train loss: 1.97717 - Test loss: 2.01931\n",
      "Epoch 442 - lr: 0.05000 - Train loss: 1.97717 - Test loss: 2.01940\n",
      "Epoch 443 - lr: 0.05000 - Train loss: 1.97717 - Test loss: 2.01949\n",
      "Epoch 444 - lr: 0.05000 - Train loss: 1.97717 - Test loss: 2.01958\n",
      "Epoch 445 - lr: 0.05000 - Train loss: 1.97717 - Test loss: 2.01967\n",
      "Epoch 446 - lr: 0.05000 - Train loss: 1.97718 - Test loss: 2.01976\n",
      "Epoch 447 - lr: 0.05000 - Train loss: 1.97718 - Test loss: 2.01986\n",
      "Epoch 448 - lr: 0.05000 - Train loss: 1.97718 - Test loss: 2.01996\n",
      "Epoch 449 - lr: 0.05000 - Train loss: 1.97718 - Test loss: 2.02006\n",
      "Epoch 450 - lr: 0.05000 - Train loss: 1.97718 - Test loss: 2.02016\n",
      "Epoch 451 - lr: 0.05000 - Train loss: 1.97719 - Test loss: 2.02026\n",
      "Epoch 452 - lr: 0.05000 - Train loss: 1.97719 - Test loss: 2.02036\n",
      "Epoch 453 - lr: 0.05000 - Train loss: 1.97720 - Test loss: 2.02047\n",
      "Epoch 454 - lr: 0.05000 - Train loss: 1.97720 - Test loss: 2.02058\n",
      "Epoch 455 - lr: 0.05000 - Train loss: 1.97721 - Test loss: 2.02069\n",
      "Epoch 456 - lr: 0.05000 - Train loss: 1.97722 - Test loss: 2.02081\n",
      "Epoch 457 - lr: 0.05000 - Train loss: 1.97722 - Test loss: 2.02092\n",
      "Epoch 458 - lr: 0.05000 - Train loss: 1.97723 - Test loss: 2.02105\n",
      "Epoch 459 - lr: 0.05000 - Train loss: 1.97724 - Test loss: 2.02117\n",
      "Epoch 460 - lr: 0.05000 - Train loss: 1.97725 - Test loss: 2.02130\n",
      "Epoch 461 - lr: 0.05000 - Train loss: 1.97726 - Test loss: 2.02143\n",
      "Epoch 462 - lr: 0.05000 - Train loss: 1.97728 - Test loss: 2.02157\n",
      "Epoch 463 - lr: 0.05000 - Train loss: 1.97729 - Test loss: 2.02172\n",
      "Epoch 464 - lr: 0.05000 - Train loss: 1.97731 - Test loss: 2.02187\n",
      "Epoch 465 - lr: 0.05000 - Train loss: 1.97732 - Test loss: 2.02202\n",
      "Epoch 466 - lr: 0.05000 - Train loss: 1.97734 - Test loss: 2.02219\n",
      "Epoch 467 - lr: 0.05000 - Train loss: 1.97736 - Test loss: 2.02236\n",
      "Epoch 468 - lr: 0.05000 - Train loss: 1.97739 - Test loss: 2.02254\n",
      "Epoch 469 - lr: 0.05000 - Train loss: 1.97741 - Test loss: 2.02274\n",
      "Epoch 470 - lr: 0.05000 - Train loss: 1.97745 - Test loss: 2.02294\n",
      "Epoch 471 - lr: 0.05000 - Train loss: 1.97748 - Test loss: 2.02317\n",
      "Epoch 472 - lr: 0.05000 - Train loss: 1.97752 - Test loss: 2.02340\n",
      "Epoch 473 - lr: 0.05000 - Train loss: 1.97756 - Test loss: 2.02366\n",
      "Epoch 474 - lr: 0.05000 - Train loss: 1.97761 - Test loss: 2.02395\n",
      "Epoch 475 - lr: 0.05000 - Train loss: 1.97767 - Test loss: 2.02426\n",
      "Epoch 476 - lr: 0.05000 - Train loss: 1.97774 - Test loss: 2.02461\n",
      "Epoch 477 - lr: 0.05000 - Train loss: 1.97782 - Test loss: 2.02500\n",
      "Epoch 478 - lr: 0.05000 - Train loss: 1.97792 - Test loss: 2.02545\n",
      "Epoch 479 - lr: 0.05000 - Train loss: 1.97802 - Test loss: 2.02597\n",
      "Epoch 480 - lr: 0.05000 - Train loss: 1.97810 - Test loss: 2.02656\n",
      "Epoch 481 - lr: 0.05000 - Train loss: 1.97797 - Test loss: 2.02710\n",
      "Epoch 482 - lr: 0.05000 - Train loss: 1.97491 - Test loss: 2.02196\n",
      "Epoch 483 - lr: 0.05000 - Train loss: 1.89706 - Test loss: 1.84716\n",
      "Epoch 484 - lr: 0.05000 - Train loss: 1.61884 - Test loss: 2.03405\n",
      "Epoch 485 - lr: 0.05000 - Train loss: 1.91783 - Test loss: 2.04585\n",
      "Epoch 486 - lr: 0.05000 - Train loss: 1.91752 - Test loss: 2.04679\n",
      "Epoch 487 - lr: 0.05000 - Train loss: 1.91741 - Test loss: 2.04707\n",
      "Epoch 488 - lr: 0.05000 - Train loss: 1.91734 - Test loss: 2.04731\n",
      "Epoch 489 - lr: 0.05000 - Train loss: 1.91729 - Test loss: 2.04756\n",
      "Epoch 490 - lr: 0.05000 - Train loss: 1.91725 - Test loss: 2.04780\n",
      "Epoch 491 - lr: 0.05000 - Train loss: 1.91722 - Test loss: 2.04802\n",
      "Epoch 492 - lr: 0.05000 - Train loss: 1.91719 - Test loss: 2.04824\n",
      "Epoch 493 - lr: 0.05000 - Train loss: 1.91717 - Test loss: 2.04844\n",
      "Epoch 494 - lr: 0.05000 - Train loss: 1.91715 - Test loss: 2.04862\n",
      "Epoch 495 - lr: 0.05000 - Train loss: 1.91713 - Test loss: 2.04879\n",
      "Epoch 496 - lr: 0.05000 - Train loss: 1.91711 - Test loss: 2.04894\n",
      "Epoch 497 - lr: 0.05000 - Train loss: 1.91708 - Test loss: 2.04907\n",
      "Epoch 498 - lr: 0.05000 - Train loss: 1.91705 - Test loss: 2.04918\n",
      "Epoch 499 - lr: 0.05000 - Train loss: 1.91700 - Test loss: 2.04925\n",
      "Epoch 500 - lr: 0.05000 - Train loss: 1.91692 - Test loss: 2.04925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 501 - lr: 0.05000 - Train loss: 1.91672 - Test loss: 2.04900\n",
      "Epoch 502 - lr: 0.05000 - Train loss: 1.91519 - Test loss: 2.03621\n",
      "Epoch 503 - lr: 0.05000 - Train loss: 1.94340 - Test loss: 1.55602\n",
      "Epoch 504 - lr: 0.05000 - Train loss: 1.52054 - Test loss: 2.15049\n",
      "Epoch 505 - lr: 0.05000 - Train loss: 1.98850 - Test loss: 2.05090\n",
      "Epoch 506 - lr: 0.05000 - Train loss: 1.97927 - Test loss: 2.03749\n",
      "Epoch 507 - lr: 0.05000 - Train loss: 1.97761 - Test loss: 2.03149\n",
      "Epoch 508 - lr: 0.05000 - Train loss: 1.97693 - Test loss: 2.02794\n",
      "Epoch 509 - lr: 0.05000 - Train loss: 1.97658 - Test loss: 2.02561\n",
      "Epoch 510 - lr: 0.05000 - Train loss: 1.97639 - Test loss: 2.02400\n",
      "Epoch 511 - lr: 0.05000 - Train loss: 1.97627 - Test loss: 2.02283\n",
      "Epoch 512 - lr: 0.05000 - Train loss: 1.97620 - Test loss: 2.02197\n",
      "Epoch 513 - lr: 0.05000 - Train loss: 1.97615 - Test loss: 2.02132\n",
      "Epoch 514 - lr: 0.05000 - Train loss: 1.97611 - Test loss: 2.02082\n",
      "Epoch 515 - lr: 0.05000 - Train loss: 1.97609 - Test loss: 2.02043\n",
      "Epoch 516 - lr: 0.05000 - Train loss: 1.97607 - Test loss: 2.02012\n",
      "Epoch 517 - lr: 0.05000 - Train loss: 1.97605 - Test loss: 2.01988\n",
      "Epoch 518 - lr: 0.05000 - Train loss: 1.97604 - Test loss: 2.01969\n",
      "Epoch 519 - lr: 0.05000 - Train loss: 1.97603 - Test loss: 2.01954\n",
      "Epoch 520 - lr: 0.05000 - Train loss: 1.97602 - Test loss: 2.01943\n",
      "Epoch 521 - lr: 0.05000 - Train loss: 1.97601 - Test loss: 2.01933\n",
      "Epoch 522 - lr: 0.05000 - Train loss: 1.97600 - Test loss: 2.01926\n",
      "Epoch 523 - lr: 0.05000 - Train loss: 1.97600 - Test loss: 2.01921\n",
      "Epoch 524 - lr: 0.05000 - Train loss: 1.97599 - Test loss: 2.01917\n",
      "Epoch 525 - lr: 0.05000 - Train loss: 1.97598 - Test loss: 2.01914\n",
      "Epoch 526 - lr: 0.05000 - Train loss: 1.97598 - Test loss: 2.01912\n",
      "Epoch 527 - lr: 0.05000 - Train loss: 1.97597 - Test loss: 2.01911\n",
      "Epoch 528 - lr: 0.05000 - Train loss: 1.97597 - Test loss: 2.01911\n",
      "Epoch 529 - lr: 0.05000 - Train loss: 1.97596 - Test loss: 2.01911\n",
      "Epoch 530 - lr: 0.05000 - Train loss: 1.97596 - Test loss: 2.01911\n",
      "Epoch 531 - lr: 0.05000 - Train loss: 1.97595 - Test loss: 2.01912\n",
      "Epoch 532 - lr: 0.05000 - Train loss: 1.97595 - Test loss: 2.01914\n",
      "Epoch 533 - lr: 0.05000 - Train loss: 1.97594 - Test loss: 2.01916\n",
      "Epoch 534 - lr: 0.05000 - Train loss: 1.97594 - Test loss: 2.01918\n",
      "Epoch 535 - lr: 0.05000 - Train loss: 1.97594 - Test loss: 2.01920\n",
      "Epoch 536 - lr: 0.05000 - Train loss: 1.97594 - Test loss: 2.01922\n",
      "Epoch 537 - lr: 0.05000 - Train loss: 1.97593 - Test loss: 2.01925\n",
      "Epoch 538 - lr: 0.05000 - Train loss: 1.97593 - Test loss: 2.01928\n",
      "Epoch 539 - lr: 0.05000 - Train loss: 1.97593 - Test loss: 2.01931\n",
      "Epoch 540 - lr: 0.05000 - Train loss: 1.97592 - Test loss: 2.01934\n",
      "Epoch 541 - lr: 0.05000 - Train loss: 1.97592 - Test loss: 2.01937\n",
      "Epoch 542 - lr: 0.05000 - Train loss: 1.97592 - Test loss: 2.01940\n",
      "Epoch 543 - lr: 0.05000 - Train loss: 1.97592 - Test loss: 2.01943\n",
      "Epoch 544 - lr: 0.05000 - Train loss: 1.97592 - Test loss: 2.01947\n",
      "Epoch 545 - lr: 0.05000 - Train loss: 1.97591 - Test loss: 2.01950\n",
      "Epoch 546 - lr: 0.05000 - Train loss: 1.97591 - Test loss: 2.01954\n",
      "Epoch 547 - lr: 0.05000 - Train loss: 1.97591 - Test loss: 2.01957\n",
      "Epoch 548 - lr: 0.05000 - Train loss: 1.97591 - Test loss: 2.01961\n",
      "Epoch 549 - lr: 0.05000 - Train loss: 1.97591 - Test loss: 2.01965\n",
      "Epoch 550 - lr: 0.05000 - Train loss: 1.97591 - Test loss: 2.01968\n",
      "Epoch 551 - lr: 0.05000 - Train loss: 1.97590 - Test loss: 2.01972\n",
      "Epoch 552 - lr: 0.05000 - Train loss: 1.97590 - Test loss: 2.01976\n",
      "Epoch 553 - lr: 0.05000 - Train loss: 1.97590 - Test loss: 2.01980\n",
      "Epoch 554 - lr: 0.05000 - Train loss: 1.97590 - Test loss: 2.01983\n",
      "Epoch 555 - lr: 0.05000 - Train loss: 1.97590 - Test loss: 2.01987\n",
      "Epoch 556 - lr: 0.05000 - Train loss: 1.97590 - Test loss: 2.01991\n",
      "Epoch 557 - lr: 0.05000 - Train loss: 1.97590 - Test loss: 2.01995\n",
      "Epoch 558 - lr: 0.05000 - Train loss: 1.97590 - Test loss: 2.01999\n",
      "Epoch 559 - lr: 0.05000 - Train loss: 1.97590 - Test loss: 2.02003\n",
      "Epoch 560 - lr: 0.05000 - Train loss: 1.97590 - Test loss: 2.02007\n",
      "Epoch 561 - lr: 0.05000 - Train loss: 1.97589 - Test loss: 2.02011\n",
      "Epoch 562 - lr: 0.05000 - Train loss: 1.97589 - Test loss: 2.02015\n",
      "Epoch 563 - lr: 0.05000 - Train loss: 1.97589 - Test loss: 2.02019\n",
      "Epoch 564 - lr: 0.05000 - Train loss: 1.97589 - Test loss: 2.02023\n",
      "Epoch 565 - lr: 0.05000 - Train loss: 1.97589 - Test loss: 2.02027\n",
      "Epoch 566 - lr: 0.05000 - Train loss: 1.97589 - Test loss: 2.02031\n",
      "Epoch 567 - lr: 0.05000 - Train loss: 1.97589 - Test loss: 2.02035\n",
      "Epoch 568 - lr: 0.05000 - Train loss: 1.97589 - Test loss: 2.02039\n",
      "Epoch 569 - lr: 0.05000 - Train loss: 1.97589 - Test loss: 2.02043\n",
      "Epoch 570 - lr: 0.05000 - Train loss: 1.97589 - Test loss: 2.02047\n",
      "Epoch 571 - lr: 0.05000 - Train loss: 1.97589 - Test loss: 2.02051\n",
      "Epoch 572 - lr: 0.05000 - Train loss: 1.97589 - Test loss: 2.02055\n",
      "Epoch 573 - lr: 0.05000 - Train loss: 1.97589 - Test loss: 2.02059\n",
      "Epoch 574 - lr: 0.05000 - Train loss: 1.97589 - Test loss: 2.02063\n",
      "Epoch 575 - lr: 0.05000 - Train loss: 1.97589 - Test loss: 2.02067\n",
      "Epoch 576 - lr: 0.05000 - Train loss: 1.97589 - Test loss: 2.02071\n",
      "Epoch 577 - lr: 0.05000 - Train loss: 1.97589 - Test loss: 2.02075\n",
      "Epoch 578 - lr: 0.05000 - Train loss: 1.97589 - Test loss: 2.02079\n",
      "Epoch 579 - lr: 0.05000 - Train loss: 1.97589 - Test loss: 2.02083\n",
      "Epoch 580 - lr: 0.05000 - Train loss: 1.97589 - Test loss: 2.02087\n",
      "Epoch 581 - lr: 0.05000 - Train loss: 1.97589 - Test loss: 2.02090\n",
      "Epoch 582 - lr: 0.05000 - Train loss: 1.97589 - Test loss: 2.02094\n",
      "Epoch 583 - lr: 0.05000 - Train loss: 1.97589 - Test loss: 2.02098\n",
      "Epoch 584 - lr: 0.05000 - Train loss: 1.97589 - Test loss: 2.02102\n",
      "Epoch 585 - lr: 0.05000 - Train loss: 1.97589 - Test loss: 2.02106\n",
      "Epoch 586 - lr: 0.05000 - Train loss: 1.97589 - Test loss: 2.02110\n",
      "Epoch 587 - lr: 0.05000 - Train loss: 1.97589 - Test loss: 2.02114\n",
      "Epoch 588 - lr: 0.05000 - Train loss: 1.97589 - Test loss: 2.02118\n",
      "Epoch 589 - lr: 0.05000 - Train loss: 1.97589 - Test loss: 2.02122\n",
      "Epoch 590 - lr: 0.05000 - Train loss: 1.97589 - Test loss: 2.02126\n",
      "Epoch 591 - lr: 0.05000 - Train loss: 1.97589 - Test loss: 2.02130\n",
      "Epoch 592 - lr: 0.05000 - Train loss: 1.97589 - Test loss: 2.02134\n",
      "Epoch 593 - lr: 0.05000 - Train loss: 1.97589 - Test loss: 2.02138\n",
      "Epoch 594 - lr: 0.05000 - Train loss: 1.97589 - Test loss: 2.02142\n",
      "Epoch 595 - lr: 0.05000 - Train loss: 1.97589 - Test loss: 2.02146\n",
      "Epoch 596 - lr: 0.05000 - Train loss: 1.97589 - Test loss: 2.02150\n",
      "Epoch 597 - lr: 0.05000 - Train loss: 1.97589 - Test loss: 2.02154\n",
      "Epoch 598 - lr: 0.05000 - Train loss: 1.97589 - Test loss: 2.02158\n",
      "Epoch 599 - lr: 0.05000 - Train loss: 1.97589 - Test loss: 2.02162\n",
      "Epoch 600 - lr: 0.05000 - Train loss: 1.97589 - Test loss: 2.02165\n",
      "Epoch 601 - lr: 0.05000 - Train loss: 1.97589 - Test loss: 2.02169\n",
      "Epoch 602 - lr: 0.05000 - Train loss: 1.97589 - Test loss: 2.02173\n",
      "Epoch 603 - lr: 0.05000 - Train loss: 1.97589 - Test loss: 2.02177\n",
      "Epoch 604 - lr: 0.05000 - Train loss: 1.97589 - Test loss: 2.02181\n",
      "Epoch 605 - lr: 0.05000 - Train loss: 1.97589 - Test loss: 2.02185\n",
      "Epoch 606 - lr: 0.05000 - Train loss: 1.97589 - Test loss: 2.02189\n",
      "Epoch 607 - lr: 0.05000 - Train loss: 1.97589 - Test loss: 2.02192\n",
      "Epoch 608 - lr: 0.05000 - Train loss: 1.97589 - Test loss: 2.02196\n",
      "Epoch 609 - lr: 0.05000 - Train loss: 1.97589 - Test loss: 2.02200\n",
      "Epoch 610 - lr: 0.05000 - Train loss: 1.97590 - Test loss: 2.02204\n",
      "Epoch 611 - lr: 0.05000 - Train loss: 1.97590 - Test loss: 2.02208\n",
      "Epoch 612 - lr: 0.05000 - Train loss: 1.97590 - Test loss: 2.02212\n",
      "Epoch 613 - lr: 0.05000 - Train loss: 1.97590 - Test loss: 2.02215\n",
      "Epoch 614 - lr: 0.05000 - Train loss: 1.97590 - Test loss: 2.02219\n",
      "Epoch 615 - lr: 0.05000 - Train loss: 1.97590 - Test loss: 2.02223\n",
      "Epoch 616 - lr: 0.05000 - Train loss: 1.97590 - Test loss: 2.02227\n",
      "Epoch 617 - lr: 0.05000 - Train loss: 1.97590 - Test loss: 2.02231\n",
      "Epoch 618 - lr: 0.05000 - Train loss: 1.97590 - Test loss: 2.02234\n",
      "Epoch 619 - lr: 0.05000 - Train loss: 1.97590 - Test loss: 2.02238\n",
      "Epoch 620 - lr: 0.05000 - Train loss: 1.97590 - Test loss: 2.02242\n",
      "Epoch 621 - lr: 0.05000 - Train loss: 1.97590 - Test loss: 2.02246\n",
      "Epoch 622 - lr: 0.05000 - Train loss: 1.97590 - Test loss: 2.02249\n",
      "Epoch 623 - lr: 0.05000 - Train loss: 1.97590 - Test loss: 2.02253\n",
      "Epoch 624 - lr: 0.05000 - Train loss: 1.97590 - Test loss: 2.02257\n",
      "Epoch 625 - lr: 0.05000 - Train loss: 1.97591 - Test loss: 2.02261\n",
      "Epoch 626 - lr: 0.05000 - Train loss: 1.97591 - Test loss: 2.02264\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 627 - lr: 0.05000 - Train loss: 1.97591 - Test loss: 2.02268\n",
      "Epoch 628 - lr: 0.05000 - Train loss: 1.97591 - Test loss: 2.02272\n",
      "Epoch 629 - lr: 0.05000 - Train loss: 1.97591 - Test loss: 2.02275\n",
      "Epoch 630 - lr: 0.05000 - Train loss: 1.97591 - Test loss: 2.02279\n",
      "Epoch 631 - lr: 0.05000 - Train loss: 1.97591 - Test loss: 2.02283\n",
      "Epoch 632 - lr: 0.05000 - Train loss: 1.97591 - Test loss: 2.02287\n",
      "Epoch 633 - lr: 0.05000 - Train loss: 1.97591 - Test loss: 2.02290\n",
      "Epoch 634 - lr: 0.05000 - Train loss: 1.97591 - Test loss: 2.02294\n",
      "Epoch 635 - lr: 0.05000 - Train loss: 1.97591 - Test loss: 2.02298\n",
      "Epoch 636 - lr: 0.05000 - Train loss: 1.97592 - Test loss: 2.02301\n",
      "Epoch 637 - lr: 0.05000 - Train loss: 1.97592 - Test loss: 2.02305\n",
      "Epoch 638 - lr: 0.05000 - Train loss: 1.97592 - Test loss: 2.02309\n",
      "Epoch 639 - lr: 0.05000 - Train loss: 1.97592 - Test loss: 2.02312\n",
      "Epoch 640 - lr: 0.05000 - Train loss: 1.97592 - Test loss: 2.02316\n",
      "Epoch 641 - lr: 0.05000 - Train loss: 1.97592 - Test loss: 2.02320\n",
      "Epoch 642 - lr: 0.05000 - Train loss: 1.97592 - Test loss: 2.02323\n",
      "Epoch 643 - lr: 0.05000 - Train loss: 1.97592 - Test loss: 2.02327\n",
      "Epoch 644 - lr: 0.05000 - Train loss: 1.97592 - Test loss: 2.02330\n",
      "Epoch 645 - lr: 0.05000 - Train loss: 1.97592 - Test loss: 2.02334\n",
      "Epoch 646 - lr: 0.05000 - Train loss: 1.97592 - Test loss: 2.02338\n",
      "Epoch 647 - lr: 0.05000 - Train loss: 1.97593 - Test loss: 2.02341\n",
      "Epoch 648 - lr: 0.05000 - Train loss: 1.97593 - Test loss: 2.02345\n",
      "Epoch 649 - lr: 0.05000 - Train loss: 1.97593 - Test loss: 2.02348\n",
      "Epoch 650 - lr: 0.05000 - Train loss: 1.97593 - Test loss: 2.02352\n",
      "Epoch 651 - lr: 0.05000 - Train loss: 1.97593 - Test loss: 2.02356\n",
      "Epoch 652 - lr: 0.05000 - Train loss: 1.97593 - Test loss: 2.02359\n",
      "Epoch 653 - lr: 0.05000 - Train loss: 1.97593 - Test loss: 2.02363\n",
      "Epoch 654 - lr: 0.05000 - Train loss: 1.97593 - Test loss: 2.02366\n",
      "Epoch 655 - lr: 0.05000 - Train loss: 1.97593 - Test loss: 2.02370\n",
      "Epoch 656 - lr: 0.05000 - Train loss: 1.97594 - Test loss: 2.02373\n",
      "Epoch 657 - lr: 0.05000 - Train loss: 1.97594 - Test loss: 2.02377\n",
      "Epoch 658 - lr: 0.05000 - Train loss: 1.97594 - Test loss: 2.02380\n",
      "Epoch 659 - lr: 0.05000 - Train loss: 1.97594 - Test loss: 2.02384\n",
      "Epoch 660 - lr: 0.05000 - Train loss: 1.97594 - Test loss: 2.02388\n",
      "Epoch 661 - lr: 0.05000 - Train loss: 1.97594 - Test loss: 2.02391\n",
      "Epoch 662 - lr: 0.05000 - Train loss: 1.97594 - Test loss: 2.02395\n",
      "Epoch 663 - lr: 0.05000 - Train loss: 1.97594 - Test loss: 2.02398\n",
      "Epoch 664 - lr: 0.05000 - Train loss: 1.97594 - Test loss: 2.02402\n",
      "Epoch 665 - lr: 0.05000 - Train loss: 1.97595 - Test loss: 2.02405\n",
      "Epoch 666 - lr: 0.05000 - Train loss: 1.97595 - Test loss: 2.02409\n",
      "Epoch 667 - lr: 0.05000 - Train loss: 1.97595 - Test loss: 2.02412\n",
      "Epoch 668 - lr: 0.05000 - Train loss: 1.97595 - Test loss: 2.02416\n",
      "Epoch 669 - lr: 0.05000 - Train loss: 1.97595 - Test loss: 2.02419\n",
      "Epoch 670 - lr: 0.05000 - Train loss: 1.97595 - Test loss: 2.02423\n",
      "Epoch 671 - lr: 0.05000 - Train loss: 1.97595 - Test loss: 2.02426\n",
      "Epoch 672 - lr: 0.05000 - Train loss: 1.97595 - Test loss: 2.02429\n",
      "Epoch 673 - lr: 0.05000 - Train loss: 1.97596 - Test loss: 2.02433\n",
      "Epoch 674 - lr: 0.05000 - Train loss: 1.97596 - Test loss: 2.02436\n",
      "Epoch 675 - lr: 0.05000 - Train loss: 1.97596 - Test loss: 2.02440\n",
      "Epoch 676 - lr: 0.05000 - Train loss: 1.97596 - Test loss: 2.02443\n",
      "Epoch 677 - lr: 0.05000 - Train loss: 1.97596 - Test loss: 2.02447\n",
      "Epoch 678 - lr: 0.05000 - Train loss: 1.97596 - Test loss: 2.02450\n",
      "Epoch 679 - lr: 0.05000 - Train loss: 1.97596 - Test loss: 2.02454\n",
      "Epoch 680 - lr: 0.05000 - Train loss: 1.97596 - Test loss: 2.02457\n",
      "Epoch 681 - lr: 0.05000 - Train loss: 1.97597 - Test loss: 2.02461\n",
      "Epoch 682 - lr: 0.05000 - Train loss: 1.97597 - Test loss: 2.02464\n",
      "Epoch 683 - lr: 0.05000 - Train loss: 1.97597 - Test loss: 2.02467\n",
      "Epoch 684 - lr: 0.05000 - Train loss: 1.97597 - Test loss: 2.02471\n",
      "Epoch 685 - lr: 0.05000 - Train loss: 1.97597 - Test loss: 2.02474\n",
      "Epoch 686 - lr: 0.05000 - Train loss: 1.97597 - Test loss: 2.02478\n",
      "Epoch 687 - lr: 0.05000 - Train loss: 1.97597 - Test loss: 2.02481\n",
      "Epoch 688 - lr: 0.05000 - Train loss: 1.97598 - Test loss: 2.02484\n",
      "Epoch 689 - lr: 0.05000 - Train loss: 1.97598 - Test loss: 2.02488\n",
      "Epoch 690 - lr: 0.05000 - Train loss: 1.97598 - Test loss: 2.02491\n",
      "Epoch 691 - lr: 0.05000 - Train loss: 1.97598 - Test loss: 2.02495\n",
      "Epoch 692 - lr: 0.05000 - Train loss: 1.97598 - Test loss: 2.02498\n",
      "Epoch 693 - lr: 0.05000 - Train loss: 1.97598 - Test loss: 2.02501\n",
      "Epoch 694 - lr: 0.05000 - Train loss: 1.97598 - Test loss: 2.02505\n",
      "Epoch 695 - lr: 0.05000 - Train loss: 1.97598 - Test loss: 2.02508\n",
      "Epoch 696 - lr: 0.05000 - Train loss: 1.97599 - Test loss: 2.02512\n",
      "Epoch 697 - lr: 0.05000 - Train loss: 1.97599 - Test loss: 2.02515\n",
      "Epoch 698 - lr: 0.05000 - Train loss: 1.97599 - Test loss: 2.02518\n",
      "Epoch 699 - lr: 0.05000 - Train loss: 1.97599 - Test loss: 2.02522\n",
      "Epoch 700 - lr: 0.05000 - Train loss: 1.97599 - Test loss: 2.02525\n",
      "Epoch 701 - lr: 0.05000 - Train loss: 1.97599 - Test loss: 2.02528\n",
      "Epoch 702 - lr: 0.05000 - Train loss: 1.97599 - Test loss: 2.02532\n",
      "Epoch 703 - lr: 0.05000 - Train loss: 1.97600 - Test loss: 2.02535\n",
      "Epoch 704 - lr: 0.05000 - Train loss: 1.97600 - Test loss: 2.02538\n",
      "Epoch 705 - lr: 0.05000 - Train loss: 1.97600 - Test loss: 2.02542\n",
      "Epoch 706 - lr: 0.05000 - Train loss: 1.97600 - Test loss: 2.02545\n",
      "Epoch 707 - lr: 0.05000 - Train loss: 1.97600 - Test loss: 2.02548\n",
      "Epoch 708 - lr: 0.05000 - Train loss: 1.97600 - Test loss: 2.02552\n",
      "Epoch 709 - lr: 0.05000 - Train loss: 1.97600 - Test loss: 2.02555\n",
      "Epoch 710 - lr: 0.05000 - Train loss: 1.97601 - Test loss: 2.02558\n",
      "Epoch 711 - lr: 0.05000 - Train loss: 1.97601 - Test loss: 2.02562\n",
      "Epoch 712 - lr: 0.05000 - Train loss: 1.97601 - Test loss: 2.02565\n",
      "Epoch 713 - lr: 0.05000 - Train loss: 1.97601 - Test loss: 2.02568\n",
      "Epoch 714 - lr: 0.05000 - Train loss: 1.97601 - Test loss: 2.02572\n",
      "Epoch 715 - lr: 0.05000 - Train loss: 1.97601 - Test loss: 2.02575\n",
      "Epoch 716 - lr: 0.05000 - Train loss: 1.97601 - Test loss: 2.02578\n",
      "Epoch 717 - lr: 0.05000 - Train loss: 1.97602 - Test loss: 2.02582\n",
      "Epoch 718 - lr: 0.05000 - Train loss: 1.97602 - Test loss: 2.02585\n",
      "Epoch 719 - lr: 0.05000 - Train loss: 1.97602 - Test loss: 2.02588\n",
      "Epoch 720 - lr: 0.05000 - Train loss: 1.97602 - Test loss: 2.02592\n",
      "Epoch 721 - lr: 0.05000 - Train loss: 1.97602 - Test loss: 2.02595\n",
      "Epoch 722 - lr: 0.05000 - Train loss: 1.97602 - Test loss: 2.02598\n",
      "Epoch 723 - lr: 0.05000 - Train loss: 1.97603 - Test loss: 2.02601\n",
      "Epoch 724 - lr: 0.05000 - Train loss: 1.97603 - Test loss: 2.02605\n",
      "Epoch 725 - lr: 0.05000 - Train loss: 1.97603 - Test loss: 2.02608\n",
      "Epoch 726 - lr: 0.05000 - Train loss: 1.97603 - Test loss: 2.02611\n",
      "Epoch 727 - lr: 0.05000 - Train loss: 1.97603 - Test loss: 2.02615\n",
      "Epoch 728 - lr: 0.05000 - Train loss: 1.97603 - Test loss: 2.02618\n",
      "Epoch 729 - lr: 0.05000 - Train loss: 1.97603 - Test loss: 2.02621\n",
      "Epoch 730 - lr: 0.05000 - Train loss: 1.97604 - Test loss: 2.02624\n",
      "Epoch 731 - lr: 0.05000 - Train loss: 1.97604 - Test loss: 2.02628\n",
      "Epoch 732 - lr: 0.05000 - Train loss: 1.97604 - Test loss: 2.02631\n",
      "Epoch 733 - lr: 0.05000 - Train loss: 1.97604 - Test loss: 2.02634\n",
      "Epoch 734 - lr: 0.05000 - Train loss: 1.97604 - Test loss: 2.02638\n",
      "Epoch 735 - lr: 0.05000 - Train loss: 1.97604 - Test loss: 2.02641\n",
      "Epoch 736 - lr: 0.05000 - Train loss: 1.97605 - Test loss: 2.02644\n",
      "Epoch 737 - lr: 0.05000 - Train loss: 1.97605 - Test loss: 2.02647\n",
      "Epoch 738 - lr: 0.05000 - Train loss: 1.97605 - Test loss: 2.02651\n",
      "Epoch 739 - lr: 0.05000 - Train loss: 1.97605 - Test loss: 2.02654\n",
      "Epoch 740 - lr: 0.05000 - Train loss: 1.97605 - Test loss: 2.02657\n",
      "Epoch 741 - lr: 0.05000 - Train loss: 1.97605 - Test loss: 2.02660\n",
      "Epoch 742 - lr: 0.05000 - Train loss: 1.97605 - Test loss: 2.02664\n",
      "Epoch 743 - lr: 0.05000 - Train loss: 1.97606 - Test loss: 2.02667\n",
      "Epoch 744 - lr: 0.05000 - Train loss: 1.97606 - Test loss: 2.02670\n",
      "Epoch 745 - lr: 0.05000 - Train loss: 1.97606 - Test loss: 2.02673\n",
      "Epoch 746 - lr: 0.05000 - Train loss: 1.97606 - Test loss: 2.02677\n",
      "Epoch 747 - lr: 0.05000 - Train loss: 1.97606 - Test loss: 2.02680\n",
      "Epoch 748 - lr: 0.05000 - Train loss: 1.97606 - Test loss: 2.02683\n",
      "Epoch 749 - lr: 0.05000 - Train loss: 1.97607 - Test loss: 2.02686\n",
      "Epoch 750 - lr: 0.05000 - Train loss: 1.97607 - Test loss: 2.02690\n",
      "Epoch 751 - lr: 0.05000 - Train loss: 1.97607 - Test loss: 2.02693\n",
      "Epoch 752 - lr: 0.05000 - Train loss: 1.97607 - Test loss: 2.02696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 753 - lr: 0.05000 - Train loss: 1.97607 - Test loss: 2.02699\n",
      "Epoch 754 - lr: 0.05000 - Train loss: 1.97607 - Test loss: 2.02703\n",
      "Epoch 755 - lr: 0.05000 - Train loss: 1.97608 - Test loss: 2.02706\n",
      "Epoch 756 - lr: 0.05000 - Train loss: 1.97608 - Test loss: 2.02709\n",
      "Epoch 757 - lr: 0.05000 - Train loss: 1.97608 - Test loss: 2.02712\n",
      "Epoch 758 - lr: 0.05000 - Train loss: 1.97608 - Test loss: 2.02716\n",
      "Epoch 759 - lr: 0.05000 - Train loss: 1.97608 - Test loss: 2.02719\n",
      "Epoch 760 - lr: 0.05000 - Train loss: 1.97608 - Test loss: 2.02722\n",
      "Epoch 761 - lr: 0.05000 - Train loss: 1.97609 - Test loss: 2.02725\n",
      "Epoch 762 - lr: 0.05000 - Train loss: 1.97609 - Test loss: 2.02729\n",
      "Epoch 763 - lr: 0.05000 - Train loss: 1.97609 - Test loss: 2.02732\n",
      "Epoch 764 - lr: 0.05000 - Train loss: 1.97609 - Test loss: 2.02735\n",
      "Epoch 765 - lr: 0.05000 - Train loss: 1.97609 - Test loss: 2.02738\n",
      "Epoch 766 - lr: 0.05000 - Train loss: 1.97609 - Test loss: 2.02742\n",
      "Epoch 767 - lr: 0.05000 - Train loss: 1.97610 - Test loss: 2.02745\n",
      "Epoch 768 - lr: 0.05000 - Train loss: 1.97610 - Test loss: 2.02748\n",
      "Epoch 769 - lr: 0.05000 - Train loss: 1.97610 - Test loss: 2.02751\n",
      "Epoch 770 - lr: 0.05000 - Train loss: 1.97610 - Test loss: 2.02755\n",
      "Epoch 771 - lr: 0.05000 - Train loss: 1.97610 - Test loss: 2.02758\n",
      "Epoch 772 - lr: 0.05000 - Train loss: 1.97611 - Test loss: 2.02761\n",
      "Epoch 773 - lr: 0.05000 - Train loss: 1.97611 - Test loss: 2.02765\n",
      "Epoch 774 - lr: 0.05000 - Train loss: 1.97611 - Test loss: 2.02768\n",
      "Epoch 775 - lr: 0.05000 - Train loss: 1.97611 - Test loss: 2.02771\n",
      "Epoch 776 - lr: 0.05000 - Train loss: 1.97611 - Test loss: 2.02774\n",
      "Epoch 777 - lr: 0.05000 - Train loss: 1.97611 - Test loss: 2.02778\n",
      "Epoch 778 - lr: 0.05000 - Train loss: 1.97612 - Test loss: 2.02781\n",
      "Epoch 779 - lr: 0.05000 - Train loss: 1.97612 - Test loss: 2.02784\n",
      "Epoch 780 - lr: 0.05000 - Train loss: 1.97612 - Test loss: 2.02787\n",
      "Epoch 781 - lr: 0.05000 - Train loss: 1.97612 - Test loss: 2.02791\n",
      "Epoch 782 - lr: 0.05000 - Train loss: 1.97612 - Test loss: 2.02794\n",
      "Epoch 783 - lr: 0.05000 - Train loss: 1.97613 - Test loss: 2.02797\n",
      "Epoch 784 - lr: 0.05000 - Train loss: 1.97613 - Test loss: 2.02800\n",
      "Epoch 785 - lr: 0.05000 - Train loss: 1.97613 - Test loss: 2.02804\n",
      "Epoch 786 - lr: 0.05000 - Train loss: 1.97613 - Test loss: 2.02807\n",
      "Epoch 787 - lr: 0.05000 - Train loss: 1.97613 - Test loss: 2.02810\n",
      "Epoch 788 - lr: 0.05000 - Train loss: 1.97613 - Test loss: 2.02814\n",
      "Epoch 789 - lr: 0.05000 - Train loss: 1.97614 - Test loss: 2.02817\n",
      "Epoch 790 - lr: 0.05000 - Train loss: 1.97614 - Test loss: 2.02820\n",
      "Epoch 791 - lr: 0.05000 - Train loss: 1.97614 - Test loss: 2.02823\n",
      "Epoch 792 - lr: 0.05000 - Train loss: 1.97614 - Test loss: 2.02827\n",
      "Epoch 793 - lr: 0.05000 - Train loss: 1.97614 - Test loss: 2.02830\n",
      "Epoch 794 - lr: 0.05000 - Train loss: 1.97615 - Test loss: 2.02833\n",
      "Epoch 795 - lr: 0.05000 - Train loss: 1.97615 - Test loss: 2.02837\n",
      "Epoch 796 - lr: 0.05000 - Train loss: 1.97615 - Test loss: 2.02840\n",
      "Epoch 797 - lr: 0.05000 - Train loss: 1.97615 - Test loss: 2.02843\n",
      "Epoch 798 - lr: 0.05000 - Train loss: 1.97615 - Test loss: 2.02847\n",
      "Epoch 799 - lr: 0.05000 - Train loss: 1.97616 - Test loss: 2.02850\n",
      "Epoch 800 - lr: 0.05000 - Train loss: 1.97616 - Test loss: 2.02853\n",
      "Epoch 801 - lr: 0.05000 - Train loss: 1.97616 - Test loss: 2.02857\n",
      "Epoch 802 - lr: 0.05000 - Train loss: 1.97616 - Test loss: 2.02860\n",
      "Epoch 803 - lr: 0.05000 - Train loss: 1.97616 - Test loss: 2.02863\n",
      "Epoch 804 - lr: 0.05000 - Train loss: 1.97617 - Test loss: 2.02867\n",
      "Epoch 805 - lr: 0.05000 - Train loss: 1.97617 - Test loss: 2.02870\n",
      "Epoch 806 - lr: 0.05000 - Train loss: 1.97617 - Test loss: 2.02873\n",
      "Epoch 807 - lr: 0.05000 - Train loss: 1.97617 - Test loss: 2.02877\n",
      "Epoch 808 - lr: 0.05000 - Train loss: 1.97618 - Test loss: 2.02880\n",
      "Epoch 809 - lr: 0.05000 - Train loss: 1.97618 - Test loss: 2.02884\n",
      "Epoch 810 - lr: 0.05000 - Train loss: 1.97618 - Test loss: 2.02887\n",
      "Epoch 811 - lr: 0.05000 - Train loss: 1.97618 - Test loss: 2.02890\n",
      "Epoch 812 - lr: 0.05000 - Train loss: 1.97618 - Test loss: 2.02894\n",
      "Epoch 813 - lr: 0.05000 - Train loss: 1.97619 - Test loss: 2.02897\n",
      "Epoch 814 - lr: 0.05000 - Train loss: 1.97619 - Test loss: 2.02901\n",
      "Epoch 815 - lr: 0.05000 - Train loss: 1.97619 - Test loss: 2.02904\n",
      "Epoch 816 - lr: 0.05000 - Train loss: 1.97619 - Test loss: 2.02907\n",
      "Epoch 817 - lr: 0.05000 - Train loss: 1.97620 - Test loss: 2.02911\n",
      "Epoch 818 - lr: 0.05000 - Train loss: 1.97620 - Test loss: 2.02914\n",
      "Epoch 819 - lr: 0.05000 - Train loss: 1.97620 - Test loss: 2.02918\n",
      "Epoch 820 - lr: 0.05000 - Train loss: 1.97620 - Test loss: 2.02921\n",
      "Epoch 821 - lr: 0.05000 - Train loss: 1.97621 - Test loss: 2.02925\n",
      "Epoch 822 - lr: 0.05000 - Train loss: 1.97621 - Test loss: 2.02928\n",
      "Epoch 823 - lr: 0.05000 - Train loss: 1.97621 - Test loss: 2.02932\n",
      "Epoch 824 - lr: 0.05000 - Train loss: 1.97621 - Test loss: 2.02935\n",
      "Epoch 825 - lr: 0.05000 - Train loss: 1.97622 - Test loss: 2.02939\n",
      "Epoch 826 - lr: 0.05000 - Train loss: 1.97622 - Test loss: 2.02942\n",
      "Epoch 827 - lr: 0.05000 - Train loss: 1.97622 - Test loss: 2.02946\n",
      "Epoch 828 - lr: 0.05000 - Train loss: 1.97622 - Test loss: 2.02949\n",
      "Epoch 829 - lr: 0.05000 - Train loss: 1.97623 - Test loss: 2.02953\n",
      "Epoch 830 - lr: 0.05000 - Train loss: 1.97623 - Test loss: 2.02957\n",
      "Epoch 831 - lr: 0.05000 - Train loss: 1.97623 - Test loss: 2.02960\n",
      "Epoch 832 - lr: 0.05000 - Train loss: 1.97623 - Test loss: 2.02964\n",
      "Epoch 833 - lr: 0.05000 - Train loss: 1.97624 - Test loss: 2.02967\n",
      "Epoch 834 - lr: 0.05000 - Train loss: 1.97624 - Test loss: 2.02971\n",
      "Epoch 835 - lr: 0.05000 - Train loss: 1.97624 - Test loss: 2.02975\n",
      "Epoch 836 - lr: 0.05000 - Train loss: 1.97624 - Test loss: 2.02978\n",
      "Epoch 837 - lr: 0.05000 - Train loss: 1.97625 - Test loss: 2.02982\n",
      "Epoch 838 - lr: 0.05000 - Train loss: 1.97625 - Test loss: 2.02986\n",
      "Epoch 839 - lr: 0.05000 - Train loss: 1.97625 - Test loss: 2.02989\n",
      "Epoch 840 - lr: 0.05000 - Train loss: 1.97626 - Test loss: 2.02993\n",
      "Epoch 841 - lr: 0.05000 - Train loss: 1.97626 - Test loss: 2.02997\n",
      "Epoch 842 - lr: 0.05000 - Train loss: 1.97626 - Test loss: 2.03001\n",
      "Epoch 843 - lr: 0.05000 - Train loss: 1.97627 - Test loss: 2.03005\n",
      "Epoch 844 - lr: 0.05000 - Train loss: 1.97627 - Test loss: 2.03008\n",
      "Epoch 845 - lr: 0.05000 - Train loss: 1.97627 - Test loss: 2.03012\n",
      "Epoch 846 - lr: 0.05000 - Train loss: 1.97628 - Test loss: 2.03016\n",
      "Epoch 847 - lr: 0.05000 - Train loss: 1.97628 - Test loss: 2.03020\n",
      "Epoch 848 - lr: 0.05000 - Train loss: 1.97628 - Test loss: 2.03024\n",
      "Epoch 849 - lr: 0.05000 - Train loss: 1.97629 - Test loss: 2.03028\n",
      "Epoch 850 - lr: 0.05000 - Train loss: 1.97629 - Test loss: 2.03032\n",
      "Epoch 851 - lr: 0.05000 - Train loss: 1.97629 - Test loss: 2.03036\n",
      "Epoch 852 - lr: 0.05000 - Train loss: 1.97630 - Test loss: 2.03040\n",
      "Epoch 853 - lr: 0.05000 - Train loss: 1.97630 - Test loss: 2.03044\n",
      "Epoch 854 - lr: 0.05000 - Train loss: 1.97630 - Test loss: 2.03048\n",
      "Epoch 855 - lr: 0.05000 - Train loss: 1.97631 - Test loss: 2.03052\n",
      "Epoch 856 - lr: 0.05000 - Train loss: 1.97631 - Test loss: 2.03056\n",
      "Epoch 857 - lr: 0.05000 - Train loss: 1.97632 - Test loss: 2.03060\n",
      "Epoch 858 - lr: 0.05000 - Train loss: 1.97632 - Test loss: 2.03064\n",
      "Epoch 859 - lr: 0.05000 - Train loss: 1.97632 - Test loss: 2.03068\n",
      "Epoch 860 - lr: 0.05000 - Train loss: 1.97633 - Test loss: 2.03073\n",
      "Epoch 861 - lr: 0.05000 - Train loss: 1.97633 - Test loss: 2.03077\n",
      "Epoch 862 - lr: 0.05000 - Train loss: 1.97634 - Test loss: 2.03081\n",
      "Epoch 863 - lr: 0.05000 - Train loss: 1.97634 - Test loss: 2.03085\n",
      "Epoch 864 - lr: 0.05000 - Train loss: 1.97635 - Test loss: 2.03090\n",
      "Epoch 865 - lr: 0.05000 - Train loss: 1.97635 - Test loss: 2.03094\n",
      "Epoch 866 - lr: 0.05000 - Train loss: 1.97636 - Test loss: 2.03098\n",
      "Epoch 867 - lr: 0.05000 - Train loss: 1.97636 - Test loss: 2.03103\n",
      "Epoch 868 - lr: 0.05000 - Train loss: 1.97637 - Test loss: 2.03107\n",
      "Epoch 869 - lr: 0.05000 - Train loss: 1.97637 - Test loss: 2.03112\n",
      "Epoch 870 - lr: 0.05000 - Train loss: 1.97638 - Test loss: 2.03116\n",
      "Epoch 871 - lr: 0.05000 - Train loss: 1.97638 - Test loss: 2.03121\n",
      "Epoch 872 - lr: 0.05000 - Train loss: 1.97639 - Test loss: 2.03125\n",
      "Epoch 873 - lr: 0.05000 - Train loss: 1.97639 - Test loss: 2.03130\n",
      "Epoch 874 - lr: 0.05000 - Train loss: 1.97640 - Test loss: 2.03135\n",
      "Epoch 875 - lr: 0.05000 - Train loss: 1.97641 - Test loss: 2.03139\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 876 - lr: 0.05000 - Train loss: 1.97641 - Test loss: 2.03144\n",
      "Epoch 877 - lr: 0.05000 - Train loss: 1.97642 - Test loss: 2.03149\n",
      "Epoch 878 - lr: 0.05000 - Train loss: 1.97642 - Test loss: 2.03153\n",
      "Epoch 879 - lr: 0.05000 - Train loss: 1.97643 - Test loss: 2.03158\n",
      "Epoch 880 - lr: 0.05000 - Train loss: 1.97644 - Test loss: 2.03162\n",
      "Epoch 881 - lr: 0.05000 - Train loss: 1.97645 - Test loss: 2.03167\n",
      "Epoch 882 - lr: 0.05000 - Train loss: 1.97645 - Test loss: 2.03171\n",
      "Epoch 883 - lr: 0.05000 - Train loss: 1.97646 - Test loss: 2.03175\n",
      "Epoch 884 - lr: 0.05000 - Train loss: 1.97647 - Test loss: 2.03180\n",
      "Epoch 885 - lr: 0.05000 - Train loss: 1.97647 - Test loss: 2.03183\n",
      "Epoch 886 - lr: 0.05000 - Train loss: 1.97648 - Test loss: 2.03187\n",
      "Epoch 887 - lr: 0.05000 - Train loss: 1.97649 - Test loss: 2.03190\n",
      "Epoch 888 - lr: 0.05000 - Train loss: 1.97650 - Test loss: 2.03193\n",
      "Epoch 889 - lr: 0.05000 - Train loss: 1.97650 - Test loss: 2.03195\n",
      "Epoch 890 - lr: 0.05000 - Train loss: 1.97651 - Test loss: 2.03196\n",
      "Epoch 891 - lr: 0.05000 - Train loss: 1.97651 - Test loss: 2.03196\n",
      "Epoch 892 - lr: 0.05000 - Train loss: 1.97652 - Test loss: 2.03194\n",
      "Epoch 893 - lr: 0.05000 - Train loss: 1.97652 - Test loss: 2.03191\n",
      "Epoch 894 - lr: 0.05000 - Train loss: 1.97651 - Test loss: 2.03184\n",
      "Epoch 895 - lr: 0.05000 - Train loss: 1.97650 - Test loss: 2.03174\n",
      "Epoch 896 - lr: 0.05000 - Train loss: 1.97648 - Test loss: 2.03158\n",
      "Epoch 897 - lr: 0.05000 - Train loss: 1.97645 - Test loss: 2.03134\n",
      "Epoch 898 - lr: 0.05000 - Train loss: 1.97639 - Test loss: 2.03100\n",
      "Epoch 899 - lr: 0.05000 - Train loss: 1.97629 - Test loss: 2.03052\n",
      "Epoch 900 - lr: 0.05000 - Train loss: 1.97614 - Test loss: 2.02986\n",
      "Epoch 901 - lr: 0.05000 - Train loss: 1.97588 - Test loss: 2.02902\n",
      "Epoch 902 - lr: 0.05000 - Train loss: 1.97547 - Test loss: 2.02820\n",
      "Epoch 903 - lr: 0.05000 - Train loss: 1.97490 - Test loss: 2.02808\n",
      "Epoch 904 - lr: 0.05000 - Train loss: 1.97422 - Test loss: 2.03029\n",
      "Epoch 905 - lr: 0.05000 - Train loss: 1.97264 - Test loss: 2.03646\n",
      "Epoch 906 - lr: 0.05000 - Train loss: 1.95557 - Test loss: 2.02059\n",
      "Epoch 907 - lr: 0.05000 - Train loss: 1.69575 - Test loss: 1.71770\n",
      "Epoch 908 - lr: 0.05000 - Train loss: 1.51566 - Test loss: 2.20138\n",
      "Epoch 909 - lr: 0.05000 - Train loss: 1.99351 - Test loss: 2.06956\n",
      "Epoch 910 - lr: 0.05000 - Train loss: 1.98032 - Test loss: 2.05277\n",
      "Epoch 911 - lr: 0.05000 - Train loss: 1.97805 - Test loss: 2.04566\n",
      "Epoch 912 - lr: 0.05000 - Train loss: 1.97700 - Test loss: 2.04143\n",
      "Epoch 913 - lr: 0.05000 - Train loss: 1.97641 - Test loss: 2.03859\n",
      "Epoch 914 - lr: 0.05000 - Train loss: 1.97604 - Test loss: 2.03657\n",
      "Epoch 915 - lr: 0.05000 - Train loss: 1.97578 - Test loss: 2.03506\n",
      "Epoch 916 - lr: 0.05000 - Train loss: 1.97561 - Test loss: 2.03391\n",
      "Epoch 917 - lr: 0.05000 - Train loss: 1.97548 - Test loss: 2.03300\n",
      "Epoch 918 - lr: 0.05000 - Train loss: 1.97538 - Test loss: 2.03229\n",
      "Epoch 919 - lr: 0.05000 - Train loss: 1.97531 - Test loss: 2.03171\n",
      "Epoch 920 - lr: 0.05000 - Train loss: 1.97526 - Test loss: 2.03124\n",
      "Epoch 921 - lr: 0.05000 - Train loss: 1.97521 - Test loss: 2.03085\n",
      "Epoch 922 - lr: 0.05000 - Train loss: 1.97518 - Test loss: 2.03053\n",
      "Epoch 923 - lr: 0.05000 - Train loss: 1.97515 - Test loss: 2.03026\n",
      "Epoch 924 - lr: 0.05000 - Train loss: 1.97513 - Test loss: 2.03003\n",
      "Epoch 925 - lr: 0.05000 - Train loss: 1.97511 - Test loss: 2.02984\n",
      "Epoch 926 - lr: 0.05000 - Train loss: 1.97510 - Test loss: 2.02967\n",
      "Epoch 927 - lr: 0.05000 - Train loss: 1.97509 - Test loss: 2.02953\n",
      "Epoch 928 - lr: 0.05000 - Train loss: 1.97508 - Test loss: 2.02941\n",
      "Epoch 929 - lr: 0.05000 - Train loss: 1.97507 - Test loss: 2.02931\n",
      "Epoch 930 - lr: 0.05000 - Train loss: 1.97506 - Test loss: 2.02922\n",
      "Epoch 931 - lr: 0.05000 - Train loss: 1.97506 - Test loss: 2.02914\n",
      "Epoch 932 - lr: 0.05000 - Train loss: 1.97505 - Test loss: 2.02907\n",
      "Epoch 933 - lr: 0.05000 - Train loss: 1.97505 - Test loss: 2.02901\n",
      "Epoch 934 - lr: 0.05000 - Train loss: 1.97504 - Test loss: 2.02895\n",
      "Epoch 935 - lr: 0.05000 - Train loss: 1.97504 - Test loss: 2.02890\n",
      "Epoch 936 - lr: 0.05000 - Train loss: 1.97503 - Test loss: 2.02886\n",
      "Epoch 937 - lr: 0.05000 - Train loss: 1.97503 - Test loss: 2.02882\n",
      "Epoch 938 - lr: 0.05000 - Train loss: 1.97502 - Test loss: 2.02878\n",
      "Epoch 939 - lr: 0.05000 - Train loss: 1.97501 - Test loss: 2.02874\n",
      "Epoch 940 - lr: 0.05000 - Train loss: 1.97500 - Test loss: 2.02870\n",
      "Epoch 941 - lr: 0.05000 - Train loss: 1.97499 - Test loss: 2.02867\n",
      "Epoch 942 - lr: 0.05000 - Train loss: 1.97498 - Test loss: 2.02862\n",
      "Epoch 943 - lr: 0.05000 - Train loss: 1.97496 - Test loss: 2.02857\n",
      "Epoch 944 - lr: 0.05000 - Train loss: 1.97494 - Test loss: 2.02851\n",
      "Epoch 945 - lr: 0.05000 - Train loss: 1.97489 - Test loss: 2.02841\n",
      "Epoch 946 - lr: 0.05000 - Train loss: 1.97482 - Test loss: 2.02823\n",
      "Epoch 947 - lr: 0.05000 - Train loss: 1.97464 - Test loss: 2.02775\n",
      "Epoch 948 - lr: 0.05000 - Train loss: 1.97339 - Test loss: 2.01696\n",
      "Epoch 949 - lr: 0.05000 - Train loss: 1.97662 - Test loss: 2.03226\n",
      "Epoch 950 - lr: 0.05000 - Train loss: 1.97515 - Test loss: 2.03130\n",
      "Epoch 951 - lr: 0.05000 - Train loss: 1.97513 - Test loss: 2.03095\n",
      "Epoch 952 - lr: 0.05000 - Train loss: 1.97510 - Test loss: 2.03069\n",
      "Epoch 953 - lr: 0.05000 - Train loss: 1.97507 - Test loss: 2.03047\n",
      "Epoch 954 - lr: 0.05000 - Train loss: 1.97505 - Test loss: 2.03028\n",
      "Epoch 955 - lr: 0.05000 - Train loss: 1.97503 - Test loss: 2.03012\n",
      "Epoch 956 - lr: 0.05000 - Train loss: 1.97501 - Test loss: 2.02998\n",
      "Epoch 957 - lr: 0.05000 - Train loss: 1.97500 - Test loss: 2.02986\n",
      "Epoch 958 - lr: 0.05000 - Train loss: 1.97499 - Test loss: 2.02975\n",
      "Epoch 959 - lr: 0.05000 - Train loss: 1.97498 - Test loss: 2.02966\n",
      "Epoch 960 - lr: 0.05000 - Train loss: 1.97497 - Test loss: 2.02959\n",
      "Epoch 961 - lr: 0.05000 - Train loss: 1.97496 - Test loss: 2.02951\n",
      "Epoch 962 - lr: 0.05000 - Train loss: 1.97495 - Test loss: 2.02945\n",
      "Epoch 963 - lr: 0.05000 - Train loss: 1.97494 - Test loss: 2.02939\n",
      "Epoch 964 - lr: 0.05000 - Train loss: 1.97492 - Test loss: 2.02934\n",
      "Epoch 965 - lr: 0.05000 - Train loss: 1.97491 - Test loss: 2.02928\n",
      "Epoch 966 - lr: 0.05000 - Train loss: 1.97489 - Test loss: 2.02922\n",
      "Epoch 967 - lr: 0.05000 - Train loss: 1.97486 - Test loss: 2.02915\n",
      "Epoch 968 - lr: 0.05000 - Train loss: 1.97481 - Test loss: 2.02904\n",
      "Epoch 969 - lr: 0.05000 - Train loss: 1.97472 - Test loss: 2.02885\n",
      "Epoch 970 - lr: 0.05000 - Train loss: 1.97450 - Test loss: 2.02825\n",
      "Epoch 971 - lr: 0.05000 - Train loss: 1.98051 - Test loss: 2.02474\n",
      "Epoch 972 - lr: 0.05000 - Train loss: 1.97523 - Test loss: 2.02894\n",
      "Epoch 973 - lr: 0.05000 - Train loss: 1.97505 - Test loss: 2.02918\n",
      "Epoch 974 - lr: 0.05000 - Train loss: 1.97504 - Test loss: 2.02919\n",
      "Epoch 975 - lr: 0.05000 - Train loss: 1.97504 - Test loss: 2.02919\n",
      "Epoch 976 - lr: 0.05000 - Train loss: 1.97504 - Test loss: 2.02919\n",
      "Epoch 977 - lr: 0.05000 - Train loss: 1.97504 - Test loss: 2.02919\n",
      "Epoch 978 - lr: 0.05000 - Train loss: 1.97504 - Test loss: 2.02919\n",
      "Epoch 979 - lr: 0.05000 - Train loss: 1.97504 - Test loss: 2.02920\n",
      "Epoch 980 - lr: 0.05000 - Train loss: 1.97504 - Test loss: 2.02920\n",
      "Epoch 981 - lr: 0.05000 - Train loss: 1.97504 - Test loss: 2.02921\n",
      "Epoch 982 - lr: 0.05000 - Train loss: 1.97504 - Test loss: 2.02921\n",
      "Epoch 983 - lr: 0.05000 - Train loss: 1.97504 - Test loss: 2.02922\n",
      "Epoch 984 - lr: 0.05000 - Train loss: 1.97504 - Test loss: 2.02923\n",
      "Epoch 985 - lr: 0.05000 - Train loss: 1.97504 - Test loss: 2.02924\n",
      "Epoch 986 - lr: 0.05000 - Train loss: 1.97504 - Test loss: 2.02925\n",
      "Epoch 987 - lr: 0.05000 - Train loss: 1.97503 - Test loss: 2.02926\n",
      "Epoch 988 - lr: 0.05000 - Train loss: 1.97503 - Test loss: 2.02927\n",
      "Epoch 989 - lr: 0.05000 - Train loss: 1.97503 - Test loss: 2.02928\n",
      "Epoch 990 - lr: 0.05000 - Train loss: 1.97503 - Test loss: 2.02929\n",
      "Epoch 991 - lr: 0.05000 - Train loss: 1.97503 - Test loss: 2.02930\n",
      "Epoch 992 - lr: 0.05000 - Train loss: 1.97503 - Test loss: 2.02931\n",
      "Epoch 993 - lr: 0.05000 - Train loss: 1.97503 - Test loss: 2.02932\n",
      "Epoch 994 - lr: 0.05000 - Train loss: 1.97503 - Test loss: 2.02933\n",
      "Epoch 995 - lr: 0.05000 - Train loss: 1.97503 - Test loss: 2.02935\n",
      "Epoch 996 - lr: 0.05000 - Train loss: 1.97503 - Test loss: 2.02936\n",
      "Epoch 997 - lr: 0.05000 - Train loss: 1.97503 - Test loss: 2.02937\n",
      "Epoch 998 - lr: 0.05000 - Train loss: 1.97503 - Test loss: 2.02939\n",
      "Epoch 999 - lr: 0.05000 - Train loss: 1.97503 - Test loss: 2.02940\n",
      "Epoch 1000 - lr: 0.05000 - Train loss: 1.97503 - Test loss: 2.02941\n",
      "Epoch 1001 - lr: 0.05000 - Train loss: 1.97503 - Test loss: 2.02943\n",
      "Epoch 1002 - lr: 0.05000 - Train loss: 1.97503 - Test loss: 2.02944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1003 - lr: 0.05000 - Train loss: 1.97503 - Test loss: 2.02946\n",
      "Epoch 1004 - lr: 0.05000 - Train loss: 1.97503 - Test loss: 2.02947\n",
      "Epoch 1005 - lr: 0.05000 - Train loss: 1.97503 - Test loss: 2.02949\n",
      "Epoch 1006 - lr: 0.05000 - Train loss: 1.97503 - Test loss: 2.02950\n",
      "Epoch 1007 - lr: 0.05000 - Train loss: 1.97503 - Test loss: 2.02952\n",
      "Epoch 1008 - lr: 0.05000 - Train loss: 1.97503 - Test loss: 2.02953\n",
      "Epoch 1009 - lr: 0.05000 - Train loss: 1.97503 - Test loss: 2.02955\n",
      "Epoch 1010 - lr: 0.05000 - Train loss: 1.97502 - Test loss: 2.02956\n",
      "Epoch 1011 - lr: 0.05000 - Train loss: 1.97502 - Test loss: 2.02958\n",
      "Epoch 1012 - lr: 0.05000 - Train loss: 1.97502 - Test loss: 2.02960\n",
      "Epoch 1013 - lr: 0.05000 - Train loss: 1.97502 - Test loss: 2.02961\n",
      "Epoch 1014 - lr: 0.05000 - Train loss: 1.97502 - Test loss: 2.02963\n",
      "Epoch 1015 - lr: 0.05000 - Train loss: 1.97502 - Test loss: 2.02964\n",
      "Epoch 1016 - lr: 0.05000 - Train loss: 1.97502 - Test loss: 2.02966\n",
      "Epoch 1017 - lr: 0.05000 - Train loss: 1.97502 - Test loss: 2.02968\n",
      "Epoch 1018 - lr: 0.05000 - Train loss: 1.97502 - Test loss: 2.02969\n",
      "Epoch 1019 - lr: 0.05000 - Train loss: 1.97502 - Test loss: 2.02971\n",
      "Epoch 1020 - lr: 0.05000 - Train loss: 1.97502 - Test loss: 2.02972\n",
      "Epoch 1021 - lr: 0.05000 - Train loss: 1.97502 - Test loss: 2.02974\n",
      "Epoch 1022 - lr: 0.05000 - Train loss: 1.97502 - Test loss: 2.02976\n",
      "Epoch 1023 - lr: 0.05000 - Train loss: 1.97502 - Test loss: 2.02977\n",
      "Epoch 1024 - lr: 0.05000 - Train loss: 1.97502 - Test loss: 2.02979\n",
      "Epoch 1025 - lr: 0.05000 - Train loss: 1.97501 - Test loss: 2.02981\n",
      "Epoch 1026 - lr: 0.05000 - Train loss: 1.97501 - Test loss: 2.02982\n",
      "Epoch 1027 - lr: 0.05000 - Train loss: 1.97501 - Test loss: 2.02984\n",
      "Epoch 1028 - lr: 0.05000 - Train loss: 1.97501 - Test loss: 2.02986\n",
      "Epoch 1029 - lr: 0.05000 - Train loss: 1.97501 - Test loss: 2.02987\n",
      "Epoch 1030 - lr: 0.05000 - Train loss: 1.97501 - Test loss: 2.02989\n",
      "Epoch 1031 - lr: 0.05000 - Train loss: 1.97501 - Test loss: 2.02991\n",
      "Epoch 1032 - lr: 0.05000 - Train loss: 1.97501 - Test loss: 2.02992\n",
      "Epoch 1033 - lr: 0.05000 - Train loss: 1.97501 - Test loss: 2.02994\n",
      "Epoch 1034 - lr: 0.05000 - Train loss: 1.97501 - Test loss: 2.02996\n",
      "Epoch 1035 - lr: 0.05000 - Train loss: 1.97501 - Test loss: 2.02997\n",
      "Epoch 1036 - lr: 0.05000 - Train loss: 1.97501 - Test loss: 2.02999\n",
      "Epoch 1037 - lr: 0.05000 - Train loss: 1.97501 - Test loss: 2.03001\n",
      "Epoch 1038 - lr: 0.05000 - Train loss: 1.97500 - Test loss: 2.03003\n",
      "Epoch 1039 - lr: 0.05000 - Train loss: 1.97500 - Test loss: 2.03004\n",
      "Epoch 1040 - lr: 0.05000 - Train loss: 1.97500 - Test loss: 2.03006\n",
      "Epoch 1041 - lr: 0.05000 - Train loss: 1.97500 - Test loss: 2.03008\n",
      "Epoch 1042 - lr: 0.05000 - Train loss: 1.97500 - Test loss: 2.03009\n",
      "Epoch 1043 - lr: 0.05000 - Train loss: 1.97500 - Test loss: 2.03011\n",
      "Epoch 1044 - lr: 0.05000 - Train loss: 1.97500 - Test loss: 2.03013\n",
      "Epoch 1045 - lr: 0.05000 - Train loss: 1.97500 - Test loss: 2.03014\n",
      "Epoch 1046 - lr: 0.05000 - Train loss: 1.97500 - Test loss: 2.03016\n",
      "Epoch 1047 - lr: 0.05000 - Train loss: 1.97500 - Test loss: 2.03018\n",
      "Epoch 1048 - lr: 0.05000 - Train loss: 1.97500 - Test loss: 2.03019\n",
      "Epoch 1049 - lr: 0.05000 - Train loss: 1.97499 - Test loss: 2.03021\n",
      "Epoch 1050 - lr: 0.05000 - Train loss: 1.97499 - Test loss: 2.03023\n",
      "Epoch 1051 - lr: 0.05000 - Train loss: 1.97499 - Test loss: 2.03024\n",
      "Epoch 1052 - lr: 0.05000 - Train loss: 1.97499 - Test loss: 2.03026\n",
      "Epoch 1053 - lr: 0.05000 - Train loss: 1.97499 - Test loss: 2.03028\n",
      "Epoch 1054 - lr: 0.05000 - Train loss: 1.97499 - Test loss: 2.03029\n",
      "Epoch 1055 - lr: 0.05000 - Train loss: 1.97499 - Test loss: 2.03031\n",
      "Epoch 1056 - lr: 0.05000 - Train loss: 1.97499 - Test loss: 2.03033\n",
      "Epoch 1057 - lr: 0.05000 - Train loss: 1.97499 - Test loss: 2.03035\n",
      "Epoch 1058 - lr: 0.05000 - Train loss: 1.97499 - Test loss: 2.03036\n",
      "Epoch 1059 - lr: 0.05000 - Train loss: 1.97498 - Test loss: 2.03038\n",
      "Epoch 1060 - lr: 0.05000 - Train loss: 1.97498 - Test loss: 2.03040\n",
      "Epoch 1061 - lr: 0.05000 - Train loss: 1.97498 - Test loss: 2.03041\n",
      "Epoch 1062 - lr: 0.05000 - Train loss: 1.97498 - Test loss: 2.03043\n",
      "Epoch 1063 - lr: 0.05000 - Train loss: 1.97498 - Test loss: 2.03045\n",
      "Epoch 1064 - lr: 0.05000 - Train loss: 1.97498 - Test loss: 2.03046\n",
      "Epoch 1065 - lr: 0.05000 - Train loss: 1.97498 - Test loss: 2.03048\n",
      "Epoch 1066 - lr: 0.05000 - Train loss: 1.97498 - Test loss: 2.03050\n",
      "Epoch 1067 - lr: 0.05000 - Train loss: 1.97498 - Test loss: 2.03051\n",
      "Epoch 1068 - lr: 0.05000 - Train loss: 1.97497 - Test loss: 2.03053\n",
      "Epoch 1069 - lr: 0.05000 - Train loss: 1.97497 - Test loss: 2.03054\n",
      "Epoch 1070 - lr: 0.05000 - Train loss: 1.97497 - Test loss: 2.03056\n",
      "Epoch 1071 - lr: 0.05000 - Train loss: 1.97497 - Test loss: 2.03058\n",
      "Epoch 1072 - lr: 0.05000 - Train loss: 1.97497 - Test loss: 2.03059\n",
      "Epoch 1073 - lr: 0.05000 - Train loss: 1.97497 - Test loss: 2.03061\n",
      "Epoch 1074 - lr: 0.05000 - Train loss: 1.97497 - Test loss: 2.03063\n",
      "Epoch 1075 - lr: 0.05000 - Train loss: 1.97497 - Test loss: 2.03064\n",
      "Epoch 1076 - lr: 0.05000 - Train loss: 1.97497 - Test loss: 2.03066\n",
      "Epoch 1077 - lr: 0.05000 - Train loss: 1.97496 - Test loss: 2.03068\n",
      "Epoch 1078 - lr: 0.05000 - Train loss: 1.97496 - Test loss: 2.03069\n",
      "Epoch 1079 - lr: 0.05000 - Train loss: 1.97496 - Test loss: 2.03071\n",
      "Epoch 1080 - lr: 0.05000 - Train loss: 1.97496 - Test loss: 2.03073\n",
      "Epoch 1081 - lr: 0.05000 - Train loss: 1.97496 - Test loss: 2.03074\n",
      "Epoch 1082 - lr: 0.05000 - Train loss: 1.97496 - Test loss: 2.03076\n",
      "Epoch 1083 - lr: 0.05000 - Train loss: 1.97496 - Test loss: 2.03077\n",
      "Epoch 1084 - lr: 0.05000 - Train loss: 1.97496 - Test loss: 2.03079\n",
      "Epoch 1085 - lr: 0.05000 - Train loss: 1.97495 - Test loss: 2.03081\n",
      "Epoch 1086 - lr: 0.05000 - Train loss: 1.97495 - Test loss: 2.03082\n",
      "Epoch 1087 - lr: 0.05000 - Train loss: 1.97495 - Test loss: 2.03084\n",
      "Epoch 1088 - lr: 0.05000 - Train loss: 1.97495 - Test loss: 2.03086\n",
      "Epoch 1089 - lr: 0.05000 - Train loss: 1.97495 - Test loss: 2.03087\n",
      "Epoch 1090 - lr: 0.05000 - Train loss: 1.97495 - Test loss: 2.03089\n",
      "Epoch 1091 - lr: 0.05000 - Train loss: 1.97495 - Test loss: 2.03090\n",
      "Epoch 1092 - lr: 0.05000 - Train loss: 1.97495 - Test loss: 2.03092\n",
      "Epoch 1093 - lr: 0.05000 - Train loss: 1.97494 - Test loss: 2.03094\n",
      "Epoch 1094 - lr: 0.05000 - Train loss: 1.97494 - Test loss: 2.03095\n",
      "Epoch 1095 - lr: 0.05000 - Train loss: 1.97494 - Test loss: 2.03097\n",
      "Epoch 1096 - lr: 0.05000 - Train loss: 1.97494 - Test loss: 2.03098\n",
      "Epoch 1097 - lr: 0.05000 - Train loss: 1.97494 - Test loss: 2.03100\n",
      "Epoch 1098 - lr: 0.05000 - Train loss: 1.97494 - Test loss: 2.03102\n",
      "Epoch 1099 - lr: 0.05000 - Train loss: 1.97494 - Test loss: 2.03103\n",
      "Epoch 1100 - lr: 0.05000 - Train loss: 1.97494 - Test loss: 2.03105\n",
      "Epoch 1101 - lr: 0.05000 - Train loss: 1.97493 - Test loss: 2.03106\n",
      "Epoch 1102 - lr: 0.05000 - Train loss: 1.97493 - Test loss: 2.03108\n",
      "Epoch 1103 - lr: 0.05000 - Train loss: 1.97493 - Test loss: 2.03109\n",
      "Epoch 1104 - lr: 0.05000 - Train loss: 1.97493 - Test loss: 2.03111\n",
      "Epoch 1105 - lr: 0.05000 - Train loss: 1.97493 - Test loss: 2.03113\n",
      "Epoch 1106 - lr: 0.05000 - Train loss: 1.97493 - Test loss: 2.03114\n",
      "Epoch 1107 - lr: 0.05000 - Train loss: 1.97493 - Test loss: 2.03116\n",
      "Epoch 1108 - lr: 0.05000 - Train loss: 1.97492 - Test loss: 2.03117\n",
      "Epoch 1109 - lr: 0.05000 - Train loss: 1.97492 - Test loss: 2.03119\n",
      "Epoch 1110 - lr: 0.05000 - Train loss: 1.97492 - Test loss: 2.03120\n",
      "Epoch 1111 - lr: 0.05000 - Train loss: 1.97492 - Test loss: 2.03122\n",
      "Epoch 1112 - lr: 0.05000 - Train loss: 1.97492 - Test loss: 2.03123\n",
      "Epoch 1113 - lr: 0.05000 - Train loss: 1.97492 - Test loss: 2.03125\n",
      "Epoch 1114 - lr: 0.05000 - Train loss: 1.97492 - Test loss: 2.03127\n",
      "Epoch 1115 - lr: 0.05000 - Train loss: 1.97491 - Test loss: 2.03128\n",
      "Epoch 1116 - lr: 0.05000 - Train loss: 1.97491 - Test loss: 2.03130\n",
      "Epoch 1117 - lr: 0.05000 - Train loss: 1.97491 - Test loss: 2.03131\n",
      "Epoch 1118 - lr: 0.05000 - Train loss: 1.97491 - Test loss: 2.03133\n",
      "Epoch 1119 - lr: 0.05000 - Train loss: 1.97491 - Test loss: 2.03134\n",
      "Epoch 1120 - lr: 0.05000 - Train loss: 1.97491 - Test loss: 2.03136\n",
      "Epoch 1121 - lr: 0.05000 - Train loss: 1.97491 - Test loss: 2.03137\n",
      "Epoch 1122 - lr: 0.05000 - Train loss: 1.97490 - Test loss: 2.03139\n",
      "Epoch 1123 - lr: 0.05000 - Train loss: 1.97490 - Test loss: 2.03140\n",
      "Epoch 1124 - lr: 0.05000 - Train loss: 1.97490 - Test loss: 2.03142\n",
      "Epoch 1125 - lr: 0.05000 - Train loss: 1.97490 - Test loss: 2.03143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1126 - lr: 0.05000 - Train loss: 1.97490 - Test loss: 2.03145\n",
      "Epoch 1127 - lr: 0.05000 - Train loss: 1.97490 - Test loss: 2.03146\n",
      "Epoch 1128 - lr: 0.05000 - Train loss: 1.97490 - Test loss: 2.03148\n",
      "Epoch 1129 - lr: 0.05000 - Train loss: 1.97489 - Test loss: 2.03149\n",
      "Epoch 1130 - lr: 0.05000 - Train loss: 1.97489 - Test loss: 2.03151\n",
      "Epoch 1131 - lr: 0.05000 - Train loss: 1.97489 - Test loss: 2.03152\n",
      "Epoch 1132 - lr: 0.05000 - Train loss: 1.97489 - Test loss: 2.03154\n",
      "Epoch 1133 - lr: 0.05000 - Train loss: 1.97489 - Test loss: 2.03155\n",
      "Epoch 1134 - lr: 0.05000 - Train loss: 1.97489 - Test loss: 2.03157\n",
      "Epoch 1135 - lr: 0.05000 - Train loss: 1.97488 - Test loss: 2.03158\n",
      "Epoch 1136 - lr: 0.05000 - Train loss: 1.97488 - Test loss: 2.03160\n",
      "Epoch 1137 - lr: 0.05000 - Train loss: 1.97488 - Test loss: 2.03161\n",
      "Epoch 1138 - lr: 0.05000 - Train loss: 1.97488 - Test loss: 2.03163\n",
      "Epoch 1139 - lr: 0.05000 - Train loss: 1.97488 - Test loss: 2.03164\n",
      "Epoch 1140 - lr: 0.05000 - Train loss: 1.97488 - Test loss: 2.03166\n",
      "Epoch 1141 - lr: 0.05000 - Train loss: 1.97488 - Test loss: 2.03167\n",
      "Epoch 1142 - lr: 0.05000 - Train loss: 1.97487 - Test loss: 2.03169\n",
      "Epoch 1143 - lr: 0.05000 - Train loss: 1.97487 - Test loss: 2.03170\n",
      "Epoch 1144 - lr: 0.05000 - Train loss: 1.97487 - Test loss: 2.03172\n",
      "Epoch 1145 - lr: 0.05000 - Train loss: 1.97487 - Test loss: 2.03173\n",
      "Epoch 1146 - lr: 0.05000 - Train loss: 1.97487 - Test loss: 2.03174\n",
      "Epoch 1147 - lr: 0.05000 - Train loss: 1.97487 - Test loss: 2.03176\n",
      "Epoch 1148 - lr: 0.05000 - Train loss: 1.97486 - Test loss: 2.03177\n",
      "Epoch 1149 - lr: 0.05000 - Train loss: 1.97486 - Test loss: 2.03179\n",
      "Epoch 1150 - lr: 0.05000 - Train loss: 1.97486 - Test loss: 2.03180\n",
      "Epoch 1151 - lr: 0.05000 - Train loss: 1.97486 - Test loss: 2.03182\n",
      "Epoch 1152 - lr: 0.05000 - Train loss: 1.97486 - Test loss: 2.03183\n",
      "Epoch 1153 - lr: 0.05000 - Train loss: 1.97486 - Test loss: 2.03185\n",
      "Epoch 1154 - lr: 0.05000 - Train loss: 1.97485 - Test loss: 2.03186\n",
      "Epoch 1155 - lr: 0.05000 - Train loss: 1.97485 - Test loss: 2.03187\n",
      "Epoch 1156 - lr: 0.05000 - Train loss: 1.97485 - Test loss: 2.03189\n",
      "Epoch 1157 - lr: 0.05000 - Train loss: 1.97485 - Test loss: 2.03190\n",
      "Epoch 1158 - lr: 0.05000 - Train loss: 1.97485 - Test loss: 2.03192\n",
      "Epoch 1159 - lr: 0.05000 - Train loss: 1.97485 - Test loss: 2.03193\n",
      "Epoch 1160 - lr: 0.05000 - Train loss: 1.97484 - Test loss: 2.03195\n",
      "Epoch 1161 - lr: 0.05000 - Train loss: 1.97484 - Test loss: 2.03196\n",
      "Epoch 1162 - lr: 0.05000 - Train loss: 1.97484 - Test loss: 2.03197\n",
      "Epoch 1163 - lr: 0.05000 - Train loss: 1.97484 - Test loss: 2.03199\n",
      "Epoch 1164 - lr: 0.05000 - Train loss: 1.97484 - Test loss: 2.03200\n",
      "Epoch 1165 - lr: 0.05000 - Train loss: 1.97484 - Test loss: 2.03202\n",
      "Epoch 1166 - lr: 0.05000 - Train loss: 1.97483 - Test loss: 2.03203\n",
      "Epoch 1167 - lr: 0.05000 - Train loss: 1.97483 - Test loss: 2.03204\n",
      "Epoch 1168 - lr: 0.05000 - Train loss: 1.97483 - Test loss: 2.03206\n",
      "Epoch 1169 - lr: 0.05000 - Train loss: 1.97483 - Test loss: 2.03207\n",
      "Epoch 1170 - lr: 0.05000 - Train loss: 1.97483 - Test loss: 2.03209\n",
      "Epoch 1171 - lr: 0.05000 - Train loss: 1.97482 - Test loss: 2.03210\n",
      "Epoch 1172 - lr: 0.05000 - Train loss: 1.97482 - Test loss: 2.03211\n",
      "Epoch 1173 - lr: 0.05000 - Train loss: 1.97482 - Test loss: 2.03213\n",
      "Epoch 1174 - lr: 0.05000 - Train loss: 1.97482 - Test loss: 2.03214\n",
      "Epoch 1175 - lr: 0.05000 - Train loss: 1.97482 - Test loss: 2.03215\n",
      "Epoch 1176 - lr: 0.05000 - Train loss: 1.97482 - Test loss: 2.03217\n",
      "Epoch 1177 - lr: 0.05000 - Train loss: 1.97481 - Test loss: 2.03218\n",
      "Epoch 1178 - lr: 0.05000 - Train loss: 1.97481 - Test loss: 2.03220\n",
      "Epoch 1179 - lr: 0.05000 - Train loss: 1.97481 - Test loss: 2.03221\n",
      "Epoch 1180 - lr: 0.05000 - Train loss: 1.97481 - Test loss: 2.03222\n",
      "Epoch 1181 - lr: 0.05000 - Train loss: 1.97481 - Test loss: 2.03224\n",
      "Epoch 1182 - lr: 0.05000 - Train loss: 1.97480 - Test loss: 2.03225\n",
      "Epoch 1183 - lr: 0.05000 - Train loss: 1.97480 - Test loss: 2.03226\n",
      "Epoch 1184 - lr: 0.05000 - Train loss: 1.97480 - Test loss: 2.03228\n",
      "Epoch 1185 - lr: 0.05000 - Train loss: 1.97480 - Test loss: 2.03229\n",
      "Epoch 1186 - lr: 0.05000 - Train loss: 1.97480 - Test loss: 2.03230\n",
      "Epoch 1187 - lr: 0.05000 - Train loss: 1.97479 - Test loss: 2.03232\n",
      "Epoch 1188 - lr: 0.05000 - Train loss: 1.97479 - Test loss: 2.03233\n",
      "Epoch 1189 - lr: 0.05000 - Train loss: 1.97479 - Test loss: 2.03234\n",
      "Epoch 1190 - lr: 0.05000 - Train loss: 1.97479 - Test loss: 2.03236\n",
      "Epoch 1191 - lr: 0.05000 - Train loss: 1.97479 - Test loss: 2.03237\n",
      "Epoch 1192 - lr: 0.05000 - Train loss: 1.97478 - Test loss: 2.03238\n",
      "Epoch 1193 - lr: 0.05000 - Train loss: 1.97478 - Test loss: 2.03240\n",
      "Epoch 1194 - lr: 0.05000 - Train loss: 1.97478 - Test loss: 2.03241\n",
      "Epoch 1195 - lr: 0.05000 - Train loss: 1.97478 - Test loss: 2.03242\n",
      "Epoch 1196 - lr: 0.05000 - Train loss: 1.97478 - Test loss: 2.03244\n",
      "Epoch 1197 - lr: 0.05000 - Train loss: 1.97478 - Test loss: 2.03245\n",
      "Epoch 1198 - lr: 0.05000 - Train loss: 1.97477 - Test loss: 2.03246\n",
      "Epoch 1199 - lr: 0.05000 - Train loss: 1.97477 - Test loss: 2.03248\n",
      "Epoch 1200 - lr: 0.05000 - Train loss: 1.97477 - Test loss: 2.03249\n",
      "Epoch 1201 - lr: 0.05000 - Train loss: 1.97477 - Test loss: 2.03250\n",
      "Epoch 1202 - lr: 0.05000 - Train loss: 1.97477 - Test loss: 2.03252\n",
      "Epoch 1203 - lr: 0.05000 - Train loss: 1.97476 - Test loss: 2.03253\n",
      "Epoch 1204 - lr: 0.05000 - Train loss: 1.97476 - Test loss: 2.03254\n",
      "Epoch 1205 - lr: 0.05000 - Train loss: 1.97476 - Test loss: 2.03255\n",
      "Epoch 1206 - lr: 0.05000 - Train loss: 1.97476 - Test loss: 2.03257\n",
      "Epoch 1207 - lr: 0.05000 - Train loss: 1.97475 - Test loss: 2.03258\n",
      "Epoch 1208 - lr: 0.05000 - Train loss: 1.97475 - Test loss: 2.03259\n",
      "Epoch 1209 - lr: 0.05000 - Train loss: 1.97475 - Test loss: 2.03261\n",
      "Epoch 1210 - lr: 0.05000 - Train loss: 1.97475 - Test loss: 2.03262\n",
      "Epoch 1211 - lr: 0.05000 - Train loss: 1.97475 - Test loss: 2.03263\n",
      "Epoch 1212 - lr: 0.05000 - Train loss: 1.97474 - Test loss: 2.03264\n",
      "Epoch 1213 - lr: 0.05000 - Train loss: 1.97474 - Test loss: 2.03266\n",
      "Epoch 1214 - lr: 0.05000 - Train loss: 1.97474 - Test loss: 2.03267\n",
      "Epoch 1215 - lr: 0.05000 - Train loss: 1.97474 - Test loss: 2.03268\n",
      "Epoch 1216 - lr: 0.05000 - Train loss: 1.97474 - Test loss: 2.03269\n",
      "Epoch 1217 - lr: 0.05000 - Train loss: 1.97473 - Test loss: 2.03271\n",
      "Epoch 1218 - lr: 0.05000 - Train loss: 1.97473 - Test loss: 2.03272\n",
      "Epoch 1219 - lr: 0.05000 - Train loss: 1.97473 - Test loss: 2.03273\n",
      "Epoch 1220 - lr: 0.05000 - Train loss: 1.97473 - Test loss: 2.03274\n",
      "Epoch 1221 - lr: 0.05000 - Train loss: 1.97473 - Test loss: 2.03276\n",
      "Epoch 1222 - lr: 0.05000 - Train loss: 1.97472 - Test loss: 2.03277\n",
      "Epoch 1223 - lr: 0.05000 - Train loss: 1.97472 - Test loss: 2.03278\n",
      "Epoch 1224 - lr: 0.05000 - Train loss: 1.97472 - Test loss: 2.03279\n",
      "Epoch 1225 - lr: 0.05000 - Train loss: 1.97472 - Test loss: 2.03281\n",
      "Epoch 1226 - lr: 0.05000 - Train loss: 1.97471 - Test loss: 2.03282\n",
      "Epoch 1227 - lr: 0.05000 - Train loss: 1.97471 - Test loss: 2.03283\n",
      "Epoch 1228 - lr: 0.05000 - Train loss: 1.97471 - Test loss: 2.03284\n",
      "Epoch 1229 - lr: 0.05000 - Train loss: 1.97471 - Test loss: 2.03286\n",
      "Epoch 1230 - lr: 0.05000 - Train loss: 1.97471 - Test loss: 2.03287\n",
      "Epoch 1231 - lr: 0.05000 - Train loss: 1.97470 - Test loss: 2.03288\n",
      "Epoch 1232 - lr: 0.05000 - Train loss: 1.97470 - Test loss: 2.03289\n",
      "Epoch 1233 - lr: 0.05000 - Train loss: 1.97470 - Test loss: 2.03290\n",
      "Epoch 1234 - lr: 0.05000 - Train loss: 1.97470 - Test loss: 2.03292\n",
      "Epoch 1235 - lr: 0.05000 - Train loss: 1.97469 - Test loss: 2.03293\n",
      "Epoch 1236 - lr: 0.05000 - Train loss: 1.97469 - Test loss: 2.03294\n",
      "Epoch 1237 - lr: 0.05000 - Train loss: 1.97469 - Test loss: 2.03295\n",
      "Epoch 1238 - lr: 0.05000 - Train loss: 1.97469 - Test loss: 2.03296\n",
      "Epoch 1239 - lr: 0.05000 - Train loss: 1.97469 - Test loss: 2.03298\n",
      "Epoch 1240 - lr: 0.05000 - Train loss: 1.97468 - Test loss: 2.03299\n",
      "Epoch 1241 - lr: 0.05000 - Train loss: 1.97468 - Test loss: 2.03300\n",
      "Epoch 1242 - lr: 0.05000 - Train loss: 1.97468 - Test loss: 2.03301\n",
      "Epoch 1243 - lr: 0.05000 - Train loss: 1.97468 - Test loss: 2.03302\n",
      "Epoch 1244 - lr: 0.05000 - Train loss: 1.97467 - Test loss: 2.03303\n",
      "Epoch 1245 - lr: 0.05000 - Train loss: 1.97467 - Test loss: 2.03305\n",
      "Epoch 1246 - lr: 0.05000 - Train loss: 1.97467 - Test loss: 2.03306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1247 - lr: 0.05000 - Train loss: 1.97467 - Test loss: 2.03307\n",
      "Epoch 1248 - lr: 0.05000 - Train loss: 1.97466 - Test loss: 2.03308\n",
      "Epoch 1249 - lr: 0.05000 - Train loss: 1.97466 - Test loss: 2.03309\n",
      "Epoch 1250 - lr: 0.05000 - Train loss: 1.97466 - Test loss: 2.03310\n",
      "Epoch 1251 - lr: 0.05000 - Train loss: 1.97466 - Test loss: 2.03312\n",
      "Epoch 1252 - lr: 0.05000 - Train loss: 1.97466 - Test loss: 2.03313\n",
      "Epoch 1253 - lr: 0.05000 - Train loss: 1.97465 - Test loss: 2.03314\n",
      "Epoch 1254 - lr: 0.05000 - Train loss: 1.97465 - Test loss: 2.03315\n",
      "Epoch 1255 - lr: 0.05000 - Train loss: 1.97465 - Test loss: 2.03316\n",
      "Epoch 1256 - lr: 0.05000 - Train loss: 1.97465 - Test loss: 2.03317\n",
      "Epoch 1257 - lr: 0.05000 - Train loss: 1.97464 - Test loss: 2.03318\n",
      "Epoch 1258 - lr: 0.05000 - Train loss: 1.97464 - Test loss: 2.03319\n",
      "Epoch 1259 - lr: 0.05000 - Train loss: 1.97464 - Test loss: 2.03321\n",
      "Epoch 1260 - lr: 0.05000 - Train loss: 1.97464 - Test loss: 2.03322\n",
      "Epoch 1261 - lr: 0.05000 - Train loss: 1.97463 - Test loss: 2.03323\n",
      "Epoch 1262 - lr: 0.05000 - Train loss: 1.97463 - Test loss: 2.03324\n",
      "Epoch 1263 - lr: 0.05000 - Train loss: 1.97463 - Test loss: 2.03325\n",
      "Epoch 1264 - lr: 0.05000 - Train loss: 1.97463 - Test loss: 2.03326\n",
      "Epoch 1265 - lr: 0.05000 - Train loss: 1.97462 - Test loss: 2.03327\n",
      "Epoch 1266 - lr: 0.05000 - Train loss: 1.97462 - Test loss: 2.03328\n",
      "Epoch 1267 - lr: 0.05000 - Train loss: 1.97462 - Test loss: 2.03329\n",
      "Epoch 1268 - lr: 0.05000 - Train loss: 1.97462 - Test loss: 2.03330\n",
      "Epoch 1269 - lr: 0.05000 - Train loss: 1.97461 - Test loss: 2.03331\n",
      "Epoch 1270 - lr: 0.05000 - Train loss: 1.97461 - Test loss: 2.03333\n",
      "Epoch 1271 - lr: 0.05000 - Train loss: 1.97461 - Test loss: 2.03334\n",
      "Epoch 1272 - lr: 0.05000 - Train loss: 1.97461 - Test loss: 2.03335\n",
      "Epoch 1273 - lr: 0.05000 - Train loss: 1.97460 - Test loss: 2.03336\n",
      "Epoch 1274 - lr: 0.05000 - Train loss: 1.97460 - Test loss: 2.03337\n",
      "Epoch 1275 - lr: 0.05000 - Train loss: 1.97460 - Test loss: 2.03338\n",
      "Epoch 1276 - lr: 0.05000 - Train loss: 1.97460 - Test loss: 2.03339\n",
      "Epoch 1277 - lr: 0.05000 - Train loss: 1.97459 - Test loss: 2.03340\n",
      "Epoch 1278 - lr: 0.05000 - Train loss: 1.97459 - Test loss: 2.03341\n",
      "Epoch 1279 - lr: 0.05000 - Train loss: 1.97459 - Test loss: 2.03342\n",
      "Epoch 1280 - lr: 0.05000 - Train loss: 1.97458 - Test loss: 2.03343\n",
      "Epoch 1281 - lr: 0.05000 - Train loss: 1.97458 - Test loss: 2.03344\n",
      "Epoch 1282 - lr: 0.05000 - Train loss: 1.97458 - Test loss: 2.03345\n",
      "Epoch 1283 - lr: 0.05000 - Train loss: 1.97458 - Test loss: 2.03346\n",
      "Epoch 1284 - lr: 0.05000 - Train loss: 1.97457 - Test loss: 2.03347\n",
      "Epoch 1285 - lr: 0.05000 - Train loss: 1.97457 - Test loss: 2.03348\n",
      "Epoch 1286 - lr: 0.05000 - Train loss: 1.97457 - Test loss: 2.03349\n",
      "Epoch 1287 - lr: 0.05000 - Train loss: 1.97457 - Test loss: 2.03350\n",
      "Epoch 1288 - lr: 0.05000 - Train loss: 1.97456 - Test loss: 2.03351\n",
      "Epoch 1289 - lr: 0.05000 - Train loss: 1.97456 - Test loss: 2.03352\n",
      "Epoch 1290 - lr: 0.05000 - Train loss: 1.97456 - Test loss: 2.03353\n",
      "Epoch 1291 - lr: 0.05000 - Train loss: 1.97455 - Test loss: 2.03354\n",
      "Epoch 1292 - lr: 0.05000 - Train loss: 1.97455 - Test loss: 2.03355\n",
      "Epoch 1293 - lr: 0.05000 - Train loss: 1.97455 - Test loss: 2.03356\n",
      "Epoch 1294 - lr: 0.05000 - Train loss: 1.97455 - Test loss: 2.03357\n",
      "Epoch 1295 - lr: 0.05000 - Train loss: 1.97454 - Test loss: 2.03358\n",
      "Epoch 1296 - lr: 0.05000 - Train loss: 1.97454 - Test loss: 2.03358\n",
      "Epoch 1297 - lr: 0.05000 - Train loss: 1.97454 - Test loss: 2.03359\n",
      "Epoch 1298 - lr: 0.05000 - Train loss: 1.97453 - Test loss: 2.03360\n",
      "Epoch 1299 - lr: 0.05000 - Train loss: 1.97453 - Test loss: 2.03361\n",
      "Epoch 1300 - lr: 0.05000 - Train loss: 1.97453 - Test loss: 2.03362\n",
      "Epoch 1301 - lr: 0.05000 - Train loss: 1.97452 - Test loss: 2.03363\n",
      "Epoch 1302 - lr: 0.05000 - Train loss: 1.97452 - Test loss: 2.03364\n",
      "Epoch 1303 - lr: 0.05000 - Train loss: 1.97452 - Test loss: 2.03365\n",
      "Epoch 1304 - lr: 0.05000 - Train loss: 1.97451 - Test loss: 2.03365\n",
      "Epoch 1305 - lr: 0.05000 - Train loss: 1.97451 - Test loss: 2.03366\n",
      "Epoch 1306 - lr: 0.05000 - Train loss: 1.97451 - Test loss: 2.03367\n",
      "Epoch 1307 - lr: 0.05000 - Train loss: 1.97450 - Test loss: 2.03368\n",
      "Epoch 1308 - lr: 0.05000 - Train loss: 1.97450 - Test loss: 2.03368\n",
      "Epoch 1309 - lr: 0.05000 - Train loss: 1.97450 - Test loss: 2.03369\n",
      "Epoch 1310 - lr: 0.05000 - Train loss: 1.97449 - Test loss: 2.03370\n",
      "Epoch 1311 - lr: 0.05000 - Train loss: 1.97449 - Test loss: 2.03371\n",
      "Epoch 1312 - lr: 0.05000 - Train loss: 1.97449 - Test loss: 2.03371\n",
      "Epoch 1313 - lr: 0.05000 - Train loss: 1.97448 - Test loss: 2.03372\n",
      "Epoch 1314 - lr: 0.05000 - Train loss: 1.97448 - Test loss: 2.03373\n",
      "Epoch 1315 - lr: 0.05000 - Train loss: 1.97447 - Test loss: 2.03373\n",
      "Epoch 1316 - lr: 0.05000 - Train loss: 1.97447 - Test loss: 2.03374\n",
      "Epoch 1317 - lr: 0.05000 - Train loss: 1.97447 - Test loss: 2.03374\n",
      "Epoch 1318 - lr: 0.05000 - Train loss: 1.97446 - Test loss: 2.03375\n",
      "Epoch 1319 - lr: 0.05000 - Train loss: 1.97446 - Test loss: 2.03375\n",
      "Epoch 1320 - lr: 0.05000 - Train loss: 1.97445 - Test loss: 2.03375\n",
      "Epoch 1321 - lr: 0.05000 - Train loss: 1.97445 - Test loss: 2.03376\n",
      "Epoch 1322 - lr: 0.05000 - Train loss: 1.97444 - Test loss: 2.03376\n",
      "Epoch 1323 - lr: 0.05000 - Train loss: 1.97444 - Test loss: 2.03376\n",
      "Epoch 1324 - lr: 0.05000 - Train loss: 1.97443 - Test loss: 2.03376\n",
      "Epoch 1325 - lr: 0.05000 - Train loss: 1.97442 - Test loss: 2.03376\n",
      "Epoch 1326 - lr: 0.05000 - Train loss: 1.97442 - Test loss: 2.03375\n",
      "Epoch 1327 - lr: 0.05000 - Train loss: 1.97441 - Test loss: 2.03375\n",
      "Epoch 1328 - lr: 0.05000 - Train loss: 1.97440 - Test loss: 2.03374\n",
      "Epoch 1329 - lr: 0.05000 - Train loss: 1.97439 - Test loss: 2.03373\n",
      "Epoch 1330 - lr: 0.05000 - Train loss: 1.97438 - Test loss: 2.03372\n",
      "Epoch 1331 - lr: 0.05000 - Train loss: 1.97436 - Test loss: 2.03369\n",
      "Epoch 1332 - lr: 0.05000 - Train loss: 1.97435 - Test loss: 2.03366\n",
      "Epoch 1333 - lr: 0.05000 - Train loss: 1.97432 - Test loss: 2.03362\n",
      "Epoch 1334 - lr: 0.05000 - Train loss: 1.97429 - Test loss: 2.03355\n",
      "Epoch 1335 - lr: 0.05000 - Train loss: 1.97424 - Test loss: 2.03344\n",
      "Epoch 1336 - lr: 0.05000 - Train loss: 1.97416 - Test loss: 2.03322\n",
      "Epoch 1337 - lr: 0.05000 - Train loss: 1.97395 - Test loss: 2.03256\n",
      "Epoch 1338 - lr: 0.05000 - Train loss: 1.97161 - Test loss: 1.97805\n",
      "Epoch 1339 - lr: 0.05000 - Train loss: 1.71625 - Test loss: 2.03357\n",
      "Epoch 1340 - lr: 0.05000 - Train loss: 1.97388 - Test loss: 2.03151\n",
      "Epoch 1341 - lr: 0.05000 - Train loss: 1.97375 - Test loss: 2.03234\n",
      "Epoch 1342 - lr: 0.05000 - Train loss: 1.98099 - Test loss: 2.36014\n",
      "Epoch 1343 - lr: 0.05000 - Train loss: 1.95481 - Test loss: 2.08474\n",
      "Epoch 1344 - lr: 0.05000 - Train loss: 1.91945 - Test loss: 2.07740\n",
      "Epoch 1345 - lr: 0.05000 - Train loss: 1.91758 - Test loss: 2.07467\n",
      "Epoch 1346 - lr: 0.05000 - Train loss: 1.91678 - Test loss: 2.07300\n",
      "Epoch 1347 - lr: 0.05000 - Train loss: 1.91632 - Test loss: 2.07186\n",
      "Epoch 1348 - lr: 0.05000 - Train loss: 1.91604 - Test loss: 2.07102\n",
      "Epoch 1349 - lr: 0.05000 - Train loss: 1.91585 - Test loss: 2.07039\n",
      "Epoch 1350 - lr: 0.05000 - Train loss: 1.91572 - Test loss: 2.06989\n",
      "Epoch 1351 - lr: 0.05000 - Train loss: 1.91564 - Test loss: 2.06950\n",
      "Epoch 1352 - lr: 0.05000 - Train loss: 1.91558 - Test loss: 2.06918\n",
      "Epoch 1353 - lr: 0.05000 - Train loss: 1.91555 - Test loss: 2.06891\n",
      "Epoch 1354 - lr: 0.05000 - Train loss: 1.91553 - Test loss: 2.06869\n",
      "Epoch 1355 - lr: 0.05000 - Train loss: 1.91552 - Test loss: 2.06851\n",
      "Epoch 1356 - lr: 0.05000 - Train loss: 1.91552 - Test loss: 2.06835\n",
      "Epoch 1357 - lr: 0.05000 - Train loss: 1.91553 - Test loss: 2.06822\n",
      "Epoch 1358 - lr: 0.05000 - Train loss: 1.91554 - Test loss: 2.06811\n",
      "Epoch 1359 - lr: 0.05000 - Train loss: 1.91555 - Test loss: 2.06801\n",
      "Epoch 1360 - lr: 0.05000 - Train loss: 1.91556 - Test loss: 2.06793\n",
      "Epoch 1361 - lr: 0.05000 - Train loss: 1.91558 - Test loss: 2.06786\n",
      "Epoch 1362 - lr: 0.05000 - Train loss: 1.91559 - Test loss: 2.06780\n",
      "Epoch 1363 - lr: 0.05000 - Train loss: 1.91561 - Test loss: 2.06775\n",
      "Epoch 1364 - lr: 0.05000 - Train loss: 1.91563 - Test loss: 2.06771\n",
      "Epoch 1365 - lr: 0.05000 - Train loss: 1.91564 - Test loss: 2.06768\n",
      "Epoch 1366 - lr: 0.05000 - Train loss: 1.91566 - Test loss: 2.06765\n",
      "Epoch 1367 - lr: 0.05000 - Train loss: 1.91568 - Test loss: 2.06763\n",
      "Epoch 1368 - lr: 0.05000 - Train loss: 1.91569 - Test loss: 2.06761\n",
      "Epoch 1369 - lr: 0.05000 - Train loss: 1.91571 - Test loss: 2.06760\n",
      "Epoch 1370 - lr: 0.05000 - Train loss: 1.91572 - Test loss: 2.06760\n",
      "Epoch 1371 - lr: 0.05000 - Train loss: 1.91574 - Test loss: 2.06760\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1372 - lr: 0.05000 - Train loss: 1.91575 - Test loss: 2.06760\n",
      "Epoch 1373 - lr: 0.05000 - Train loss: 1.91577 - Test loss: 2.06761\n",
      "Epoch 1374 - lr: 0.05000 - Train loss: 1.91578 - Test loss: 2.06762\n",
      "Epoch 1375 - lr: 0.05000 - Train loss: 1.91579 - Test loss: 2.06763\n",
      "Epoch 1376 - lr: 0.05000 - Train loss: 1.91581 - Test loss: 2.06765\n",
      "Epoch 1377 - lr: 0.05000 - Train loss: 1.91582 - Test loss: 2.06767\n",
      "Epoch 1378 - lr: 0.05000 - Train loss: 1.91583 - Test loss: 2.06769\n",
      "Epoch 1379 - lr: 0.05000 - Train loss: 1.91584 - Test loss: 2.06771\n",
      "Epoch 1380 - lr: 0.05000 - Train loss: 1.91585 - Test loss: 2.06774\n",
      "Epoch 1381 - lr: 0.05000 - Train loss: 1.91586 - Test loss: 2.06776\n",
      "Epoch 1382 - lr: 0.05000 - Train loss: 1.91587 - Test loss: 2.06779\n",
      "Epoch 1383 - lr: 0.05000 - Train loss: 1.91588 - Test loss: 2.06782\n",
      "Epoch 1384 - lr: 0.05000 - Train loss: 1.91589 - Test loss: 2.06785\n",
      "Epoch 1385 - lr: 0.05000 - Train loss: 1.91590 - Test loss: 2.06788\n",
      "Epoch 1386 - lr: 0.05000 - Train loss: 1.91590 - Test loss: 2.06792\n",
      "Epoch 1387 - lr: 0.05000 - Train loss: 1.91591 - Test loss: 2.06795\n",
      "Epoch 1388 - lr: 0.05000 - Train loss: 1.91592 - Test loss: 2.06798\n",
      "Epoch 1389 - lr: 0.05000 - Train loss: 1.91592 - Test loss: 2.06802\n",
      "Epoch 1390 - lr: 0.05000 - Train loss: 1.91593 - Test loss: 2.06805\n",
      "Epoch 1391 - lr: 0.05000 - Train loss: 1.91593 - Test loss: 2.06809\n",
      "Epoch 1392 - lr: 0.05000 - Train loss: 1.91594 - Test loss: 2.06812\n",
      "Epoch 1393 - lr: 0.05000 - Train loss: 1.91594 - Test loss: 2.06816\n",
      "Epoch 1394 - lr: 0.05000 - Train loss: 1.91595 - Test loss: 2.06820\n",
      "Epoch 1395 - lr: 0.05000 - Train loss: 1.91595 - Test loss: 2.06823\n",
      "Epoch 1396 - lr: 0.05000 - Train loss: 1.91595 - Test loss: 2.06827\n",
      "Epoch 1397 - lr: 0.05000 - Train loss: 1.91596 - Test loss: 2.06831\n",
      "Epoch 1398 - lr: 0.05000 - Train loss: 1.91596 - Test loss: 2.06834\n",
      "Epoch 1399 - lr: 0.05000 - Train loss: 1.91596 - Test loss: 2.06838\n",
      "Epoch 1400 - lr: 0.05000 - Train loss: 1.91596 - Test loss: 2.06842\n",
      "Epoch 1401 - lr: 0.05000 - Train loss: 1.91597 - Test loss: 2.06845\n",
      "Epoch 1402 - lr: 0.05000 - Train loss: 1.91597 - Test loss: 2.06849\n",
      "Epoch 1403 - lr: 0.05000 - Train loss: 1.91597 - Test loss: 2.06853\n",
      "Epoch 1404 - lr: 0.05000 - Train loss: 1.91597 - Test loss: 2.06856\n",
      "Epoch 1405 - lr: 0.05000 - Train loss: 1.91597 - Test loss: 2.06860\n",
      "Epoch 1406 - lr: 0.05000 - Train loss: 1.91597 - Test loss: 2.06864\n",
      "Epoch 1407 - lr: 0.05000 - Train loss: 1.91597 - Test loss: 2.06867\n",
      "Epoch 1408 - lr: 0.05000 - Train loss: 1.91597 - Test loss: 2.06871\n",
      "Epoch 1409 - lr: 0.05000 - Train loss: 1.91597 - Test loss: 2.06874\n",
      "Epoch 1410 - lr: 0.05000 - Train loss: 1.91597 - Test loss: 2.06878\n",
      "Epoch 1411 - lr: 0.05000 - Train loss: 1.91597 - Test loss: 2.06881\n",
      "Epoch 1412 - lr: 0.05000 - Train loss: 1.91597 - Test loss: 2.06885\n",
      "Epoch 1413 - lr: 0.05000 - Train loss: 1.91596 - Test loss: 2.06888\n",
      "Epoch 1414 - lr: 0.05000 - Train loss: 1.91596 - Test loss: 2.06892\n",
      "Epoch 1415 - lr: 0.05000 - Train loss: 1.91596 - Test loss: 2.06895\n",
      "Epoch 1416 - lr: 0.05000 - Train loss: 1.91596 - Test loss: 2.06899\n",
      "Epoch 1417 - lr: 0.05000 - Train loss: 1.91596 - Test loss: 2.06902\n",
      "Epoch 1418 - lr: 0.05000 - Train loss: 1.91595 - Test loss: 2.06905\n",
      "Epoch 1419 - lr: 0.05000 - Train loss: 1.91595 - Test loss: 2.06909\n",
      "Epoch 1420 - lr: 0.05000 - Train loss: 1.91595 - Test loss: 2.06912\n",
      "Epoch 1421 - lr: 0.05000 - Train loss: 1.91595 - Test loss: 2.06916\n",
      "Epoch 1422 - lr: 0.05000 - Train loss: 1.91594 - Test loss: 2.06919\n",
      "Epoch 1423 - lr: 0.05000 - Train loss: 1.91594 - Test loss: 2.06922\n",
      "Epoch 1424 - lr: 0.05000 - Train loss: 1.91594 - Test loss: 2.06926\n",
      "Epoch 1425 - lr: 0.05000 - Train loss: 1.91594 - Test loss: 2.06929\n",
      "Epoch 1426 - lr: 0.05000 - Train loss: 1.91593 - Test loss: 2.06932\n",
      "Epoch 1427 - lr: 0.05000 - Train loss: 1.91593 - Test loss: 2.06936\n",
      "Epoch 1428 - lr: 0.05000 - Train loss: 1.91593 - Test loss: 2.06939\n",
      "Epoch 1429 - lr: 0.05000 - Train loss: 1.91592 - Test loss: 2.06942\n",
      "Epoch 1430 - lr: 0.05000 - Train loss: 1.91592 - Test loss: 2.06946\n",
      "Epoch 1431 - lr: 0.05000 - Train loss: 1.91592 - Test loss: 2.06949\n",
      "Epoch 1432 - lr: 0.05000 - Train loss: 1.91591 - Test loss: 2.06952\n",
      "Epoch 1433 - lr: 0.05000 - Train loss: 1.91591 - Test loss: 2.06956\n",
      "Epoch 1434 - lr: 0.05000 - Train loss: 1.91591 - Test loss: 2.06959\n",
      "Epoch 1435 - lr: 0.05000 - Train loss: 1.91590 - Test loss: 2.06962\n",
      "Epoch 1436 - lr: 0.05000 - Train loss: 1.91590 - Test loss: 2.06966\n",
      "Epoch 1437 - lr: 0.05000 - Train loss: 1.91590 - Test loss: 2.06969\n",
      "Epoch 1438 - lr: 0.05000 - Train loss: 1.91589 - Test loss: 2.06972\n",
      "Epoch 1439 - lr: 0.05000 - Train loss: 1.91589 - Test loss: 2.06976\n",
      "Epoch 1440 - lr: 0.05000 - Train loss: 1.91588 - Test loss: 2.06979\n",
      "Epoch 1441 - lr: 0.05000 - Train loss: 1.91588 - Test loss: 2.06982\n",
      "Epoch 1442 - lr: 0.05000 - Train loss: 1.91588 - Test loss: 2.06986\n",
      "Epoch 1443 - lr: 0.05000 - Train loss: 1.91587 - Test loss: 2.06989\n",
      "Epoch 1444 - lr: 0.05000 - Train loss: 1.91587 - Test loss: 2.06993\n",
      "Epoch 1445 - lr: 0.05000 - Train loss: 1.91587 - Test loss: 2.06996\n",
      "Epoch 1446 - lr: 0.05000 - Train loss: 1.91586 - Test loss: 2.07000\n",
      "Epoch 1447 - lr: 0.05000 - Train loss: 1.91586 - Test loss: 2.07003\n",
      "Epoch 1448 - lr: 0.05000 - Train loss: 1.91585 - Test loss: 2.07006\n",
      "Epoch 1449 - lr: 0.05000 - Train loss: 1.91585 - Test loss: 2.07010\n",
      "Epoch 1450 - lr: 0.05000 - Train loss: 1.91585 - Test loss: 2.07013\n",
      "Epoch 1451 - lr: 0.05000 - Train loss: 1.91584 - Test loss: 2.07017\n",
      "Epoch 1452 - lr: 0.05000 - Train loss: 1.91584 - Test loss: 2.07021\n",
      "Epoch 1453 - lr: 0.05000 - Train loss: 1.91583 - Test loss: 2.07024\n",
      "Epoch 1454 - lr: 0.05000 - Train loss: 1.91583 - Test loss: 2.07028\n",
      "Epoch 1455 - lr: 0.05000 - Train loss: 1.91583 - Test loss: 2.07031\n",
      "Epoch 1456 - lr: 0.05000 - Train loss: 1.91582 - Test loss: 2.07035\n",
      "Epoch 1457 - lr: 0.05000 - Train loss: 1.91582 - Test loss: 2.07039\n",
      "Epoch 1458 - lr: 0.05000 - Train loss: 1.91582 - Test loss: 2.07042\n",
      "Epoch 1459 - lr: 0.05000 - Train loss: 1.91581 - Test loss: 2.07046\n",
      "Epoch 1460 - lr: 0.05000 - Train loss: 1.91581 - Test loss: 2.07050\n",
      "Epoch 1461 - lr: 0.05000 - Train loss: 1.91581 - Test loss: 2.07054\n",
      "Epoch 1462 - lr: 0.05000 - Train loss: 1.91580 - Test loss: 2.07058\n",
      "Epoch 1463 - lr: 0.05000 - Train loss: 1.91580 - Test loss: 2.07061\n",
      "Epoch 1464 - lr: 0.05000 - Train loss: 1.91580 - Test loss: 2.07065\n",
      "Epoch 1465 - lr: 0.05000 - Train loss: 1.91579 - Test loss: 2.07069\n",
      "Epoch 1466 - lr: 0.05000 - Train loss: 1.91579 - Test loss: 2.07073\n",
      "Epoch 1467 - lr: 0.05000 - Train loss: 1.91579 - Test loss: 2.07077\n",
      "Epoch 1468 - lr: 0.05000 - Train loss: 1.91578 - Test loss: 2.07081\n",
      "Epoch 1469 - lr: 0.05000 - Train loss: 1.91578 - Test loss: 2.07086\n",
      "Epoch 1470 - lr: 0.05000 - Train loss: 1.91578 - Test loss: 2.07090\n",
      "Epoch 1471 - lr: 0.05000 - Train loss: 1.91577 - Test loss: 2.07094\n",
      "Epoch 1472 - lr: 0.05000 - Train loss: 1.91577 - Test loss: 2.07098\n",
      "Epoch 1473 - lr: 0.05000 - Train loss: 1.91577 - Test loss: 2.07103\n",
      "Epoch 1474 - lr: 0.05000 - Train loss: 1.91576 - Test loss: 2.07107\n",
      "Epoch 1475 - lr: 0.05000 - Train loss: 1.91576 - Test loss: 2.07111\n",
      "Epoch 1476 - lr: 0.05000 - Train loss: 1.91576 - Test loss: 2.07116\n",
      "Epoch 1477 - lr: 0.05000 - Train loss: 1.91576 - Test loss: 2.07120\n",
      "Epoch 1478 - lr: 0.05000 - Train loss: 1.91575 - Test loss: 2.07125\n",
      "Epoch 1479 - lr: 0.05000 - Train loss: 1.91575 - Test loss: 2.07130\n",
      "Epoch 1480 - lr: 0.05000 - Train loss: 1.91575 - Test loss: 2.07134\n",
      "Epoch 1481 - lr: 0.05000 - Train loss: 1.91575 - Test loss: 2.07139\n",
      "Epoch 1482 - lr: 0.05000 - Train loss: 1.91574 - Test loss: 2.07144\n",
      "Epoch 1483 - lr: 0.05000 - Train loss: 1.91574 - Test loss: 2.07149\n",
      "Epoch 1484 - lr: 0.05000 - Train loss: 1.91574 - Test loss: 2.07154\n",
      "Epoch 1485 - lr: 0.05000 - Train loss: 1.91574 - Test loss: 2.07159\n",
      "Epoch 1486 - lr: 0.05000 - Train loss: 1.91574 - Test loss: 2.07164\n",
      "Epoch 1487 - lr: 0.05000 - Train loss: 1.91574 - Test loss: 2.07169\n",
      "Epoch 1488 - lr: 0.05000 - Train loss: 1.91573 - Test loss: 2.07174\n",
      "Epoch 1489 - lr: 0.05000 - Train loss: 1.91573 - Test loss: 2.07180\n",
      "Epoch 1490 - lr: 0.05000 - Train loss: 1.91573 - Test loss: 2.07185\n",
      "Epoch 1491 - lr: 0.05000 - Train loss: 1.91573 - Test loss: 2.07190\n",
      "Epoch 1492 - lr: 0.05000 - Train loss: 1.91573 - Test loss: 2.07196\n",
      "Epoch 1493 - lr: 0.05000 - Train loss: 1.91573 - Test loss: 2.07201\n",
      "Epoch 1494 - lr: 0.05000 - Train loss: 1.91573 - Test loss: 2.07207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1495 - lr: 0.05000 - Train loss: 1.91573 - Test loss: 2.07212\n",
      "Epoch 1496 - lr: 0.05000 - Train loss: 1.91573 - Test loss: 2.07217\n",
      "Epoch 1497 - lr: 0.05000 - Train loss: 1.91573 - Test loss: 2.07223\n",
      "Epoch 1498 - lr: 0.05000 - Train loss: 1.91573 - Test loss: 2.07228\n",
      "Epoch 1499 - lr: 0.05000 - Train loss: 1.91573 - Test loss: 2.07234\n",
      "Epoch 1500 - lr: 0.05000 - Train loss: 1.91573 - Test loss: 2.07239\n",
      "Epoch 1501 - lr: 0.05000 - Train loss: 1.91574 - Test loss: 2.07244\n",
      "Epoch 1502 - lr: 0.05000 - Train loss: 1.91574 - Test loss: 2.07249\n",
      "Epoch 1503 - lr: 0.05000 - Train loss: 1.91574 - Test loss: 2.07254\n",
      "Epoch 1504 - lr: 0.05000 - Train loss: 1.91574 - Test loss: 2.07258\n",
      "Epoch 1505 - lr: 0.05000 - Train loss: 1.91574 - Test loss: 2.07262\n",
      "Epoch 1506 - lr: 0.05000 - Train loss: 1.91574 - Test loss: 2.07265\n",
      "Epoch 1507 - lr: 0.05000 - Train loss: 1.91574 - Test loss: 2.07268\n",
      "Epoch 1508 - lr: 0.05000 - Train loss: 1.91575 - Test loss: 2.07269\n",
      "Epoch 1509 - lr: 0.05000 - Train loss: 1.91575 - Test loss: 2.07269\n",
      "Epoch 1510 - lr: 0.05000 - Train loss: 1.91575 - Test loss: 2.07268\n",
      "Epoch 1511 - lr: 0.05000 - Train loss: 1.91574 - Test loss: 2.07265\n",
      "Epoch 1512 - lr: 0.05000 - Train loss: 1.91574 - Test loss: 2.07259\n",
      "Epoch 1513 - lr: 0.05000 - Train loss: 1.91573 - Test loss: 2.07250\n",
      "Epoch 1514 - lr: 0.05000 - Train loss: 1.91572 - Test loss: 2.07237\n",
      "Epoch 1515 - lr: 0.05000 - Train loss: 1.91570 - Test loss: 2.07218\n",
      "Epoch 1516 - lr: 0.05000 - Train loss: 1.91568 - Test loss: 2.07192\n",
      "Epoch 1517 - lr: 0.05000 - Train loss: 1.91564 - Test loss: 2.07157\n",
      "Epoch 1518 - lr: 0.05000 - Train loss: 1.91558 - Test loss: 2.07111\n",
      "Epoch 1519 - lr: 0.05000 - Train loss: 1.91549 - Test loss: 2.07051\n",
      "Epoch 1520 - lr: 0.05000 - Train loss: 1.91536 - Test loss: 2.06975\n",
      "Epoch 1521 - lr: 0.05000 - Train loss: 1.91518 - Test loss: 2.06884\n",
      "Epoch 1522 - lr: 0.05000 - Train loss: 1.91492 - Test loss: 2.06784\n",
      "Epoch 1523 - lr: 0.05000 - Train loss: 1.91460 - Test loss: 2.06690\n",
      "Epoch 1524 - lr: 0.05000 - Train loss: 1.91424 - Test loss: 2.06631\n",
      "Epoch 1525 - lr: 0.05000 - Train loss: 1.91392 - Test loss: 2.06645\n",
      "Epoch 1526 - lr: 0.05000 - Train loss: 1.91373 - Test loss: 2.06755\n",
      "Epoch 1527 - lr: 0.05000 - Train loss: 1.91371 - Test loss: 2.06974\n",
      "Epoch 1528 - lr: 0.05000 - Train loss: 1.91378 - Test loss: 2.07327\n",
      "Epoch 1529 - lr: 0.05000 - Train loss: 1.91363 - Test loss: 2.07913\n",
      "Epoch 1530 - lr: 0.05000 - Train loss: 1.91044 - Test loss: 2.08624\n",
      "Epoch 1531 - lr: 0.05000 - Train loss: 1.84583 - Test loss: 1.79860\n",
      "Epoch 1532 - lr: 0.05000 - Train loss: 1.51857 - Test loss: 2.03210\n",
      "Epoch 1533 - lr: 0.05000 - Train loss: 1.51489 - Test loss: 2.02025\n",
      "Epoch 1534 - lr: 0.05000 - Train loss: 1.85856 - Test loss: 2.09942\n",
      "Epoch 1535 - lr: 0.05000 - Train loss: 1.85793 - Test loss: 2.10747\n",
      "Epoch 1536 - lr: 0.05000 - Train loss: 1.85831 - Test loss: 2.10816\n",
      "Epoch 1537 - lr: 0.05000 - Train loss: 1.85841 - Test loss: 2.10836\n",
      "Epoch 1538 - lr: 0.05000 - Train loss: 1.85849 - Test loss: 2.10851\n",
      "Epoch 1539 - lr: 0.05000 - Train loss: 1.85856 - Test loss: 2.10865\n",
      "Epoch 1540 - lr: 0.05000 - Train loss: 1.85863 - Test loss: 2.10877\n",
      "Epoch 1541 - lr: 0.05000 - Train loss: 1.85868 - Test loss: 2.10889\n",
      "Epoch 1542 - lr: 0.05000 - Train loss: 1.85873 - Test loss: 2.10899\n",
      "Epoch 1543 - lr: 0.05000 - Train loss: 1.85877 - Test loss: 2.10909\n",
      "Epoch 1544 - lr: 0.05000 - Train loss: 1.85881 - Test loss: 2.10918\n",
      "Epoch 1545 - lr: 0.05000 - Train loss: 1.85884 - Test loss: 2.10925\n",
      "Epoch 1546 - lr: 0.05000 - Train loss: 1.85887 - Test loss: 2.10933\n",
      "Epoch 1547 - lr: 0.05000 - Train loss: 1.85889 - Test loss: 2.10939\n",
      "Epoch 1548 - lr: 0.05000 - Train loss: 1.85891 - Test loss: 2.10945\n",
      "Epoch 1549 - lr: 0.05000 - Train loss: 1.85893 - Test loss: 2.10951\n",
      "Epoch 1550 - lr: 0.05000 - Train loss: 1.85894 - Test loss: 2.10956\n",
      "Epoch 1551 - lr: 0.05000 - Train loss: 1.85895 - Test loss: 2.10960\n",
      "Epoch 1552 - lr: 0.05000 - Train loss: 1.85896 - Test loss: 2.10965\n",
      "Epoch 1553 - lr: 0.05000 - Train loss: 1.85897 - Test loss: 2.10969\n",
      "Epoch 1554 - lr: 0.05000 - Train loss: 1.85898 - Test loss: 2.10973\n",
      "Epoch 1555 - lr: 0.05000 - Train loss: 1.85898 - Test loss: 2.10976\n",
      "Epoch 1556 - lr: 0.05000 - Train loss: 1.85899 - Test loss: 2.10980\n",
      "Epoch 1557 - lr: 0.05000 - Train loss: 1.85899 - Test loss: 2.10983\n",
      "Epoch 1558 - lr: 0.05000 - Train loss: 1.85899 - Test loss: 2.10986\n",
      "Epoch 1559 - lr: 0.05000 - Train loss: 1.85900 - Test loss: 2.10989\n",
      "Epoch 1560 - lr: 0.05000 - Train loss: 1.85900 - Test loss: 2.10992\n",
      "Epoch 1561 - lr: 0.05000 - Train loss: 1.85900 - Test loss: 2.10995\n",
      "Epoch 1562 - lr: 0.05000 - Train loss: 1.85900 - Test loss: 2.10997\n",
      "Epoch 1563 - lr: 0.05000 - Train loss: 1.85900 - Test loss: 2.11000\n",
      "Epoch 1564 - lr: 0.05000 - Train loss: 1.85900 - Test loss: 2.11002\n",
      "Epoch 1565 - lr: 0.05000 - Train loss: 1.85900 - Test loss: 2.11005\n",
      "Epoch 1566 - lr: 0.05000 - Train loss: 1.85899 - Test loss: 2.11007\n",
      "Epoch 1567 - lr: 0.05000 - Train loss: 1.85899 - Test loss: 2.11009\n",
      "Epoch 1568 - lr: 0.05000 - Train loss: 1.85899 - Test loss: 2.11012\n",
      "Epoch 1569 - lr: 0.05000 - Train loss: 1.85899 - Test loss: 2.11014\n",
      "Epoch 1570 - lr: 0.05000 - Train loss: 1.85899 - Test loss: 2.11016\n",
      "Epoch 1571 - lr: 0.05000 - Train loss: 1.85898 - Test loss: 2.11018\n",
      "Epoch 1572 - lr: 0.05000 - Train loss: 1.85898 - Test loss: 2.11021\n",
      "Epoch 1573 - lr: 0.05000 - Train loss: 1.85898 - Test loss: 2.11023\n",
      "Epoch 1574 - lr: 0.05000 - Train loss: 1.85898 - Test loss: 2.11025\n",
      "Epoch 1575 - lr: 0.05000 - Train loss: 1.85898 - Test loss: 2.11027\n",
      "Epoch 1576 - lr: 0.05000 - Train loss: 1.85897 - Test loss: 2.11029\n",
      "Epoch 1577 - lr: 0.05000 - Train loss: 1.85897 - Test loss: 2.11031\n",
      "Epoch 1578 - lr: 0.05000 - Train loss: 1.85897 - Test loss: 2.11033\n",
      "Epoch 1579 - lr: 0.05000 - Train loss: 1.85896 - Test loss: 2.11035\n",
      "Epoch 1580 - lr: 0.05000 - Train loss: 1.85896 - Test loss: 2.11037\n",
      "Epoch 1581 - lr: 0.05000 - Train loss: 1.85896 - Test loss: 2.11039\n",
      "Epoch 1582 - lr: 0.05000 - Train loss: 1.85896 - Test loss: 2.11041\n",
      "Epoch 1583 - lr: 0.05000 - Train loss: 1.85895 - Test loss: 2.11043\n",
      "Epoch 1584 - lr: 0.05000 - Train loss: 1.85895 - Test loss: 2.11045\n",
      "Epoch 1585 - lr: 0.05000 - Train loss: 1.85895 - Test loss: 2.11047\n",
      "Epoch 1586 - lr: 0.05000 - Train loss: 1.85894 - Test loss: 2.11049\n",
      "Epoch 1587 - lr: 0.05000 - Train loss: 1.85894 - Test loss: 2.11051\n",
      "Epoch 1588 - lr: 0.05000 - Train loss: 1.85894 - Test loss: 2.11053\n",
      "Epoch 1589 - lr: 0.05000 - Train loss: 1.85894 - Test loss: 2.11055\n",
      "Epoch 1590 - lr: 0.05000 - Train loss: 1.85893 - Test loss: 2.11057\n",
      "Epoch 1591 - lr: 0.05000 - Train loss: 1.85893 - Test loss: 2.11059\n",
      "Epoch 1592 - lr: 0.05000 - Train loss: 1.85893 - Test loss: 2.11061\n",
      "Epoch 1593 - lr: 0.05000 - Train loss: 1.85892 - Test loss: 2.11062\n",
      "Epoch 1594 - lr: 0.05000 - Train loss: 1.85892 - Test loss: 2.11064\n",
      "Epoch 1595 - lr: 0.05000 - Train loss: 1.85892 - Test loss: 2.11066\n",
      "Epoch 1596 - lr: 0.05000 - Train loss: 1.85891 - Test loss: 2.11068\n",
      "Epoch 1597 - lr: 0.05000 - Train loss: 1.85891 - Test loss: 2.11070\n",
      "Epoch 1598 - lr: 0.05000 - Train loss: 1.85891 - Test loss: 2.11072\n",
      "Epoch 1599 - lr: 0.05000 - Train loss: 1.85891 - Test loss: 2.11074\n",
      "Epoch 1600 - lr: 0.05000 - Train loss: 1.85890 - Test loss: 2.11076\n",
      "Epoch 1601 - lr: 0.05000 - Train loss: 1.85890 - Test loss: 2.11077\n",
      "Epoch 1602 - lr: 0.05000 - Train loss: 1.85890 - Test loss: 2.11079\n",
      "Epoch 1603 - lr: 0.05000 - Train loss: 1.85889 - Test loss: 2.11081\n",
      "Epoch 1604 - lr: 0.05000 - Train loss: 1.85889 - Test loss: 2.11083\n",
      "Epoch 1605 - lr: 0.05000 - Train loss: 1.85889 - Test loss: 2.11085\n",
      "Epoch 1606 - lr: 0.05000 - Train loss: 1.85888 - Test loss: 2.11087\n",
      "Epoch 1607 - lr: 0.05000 - Train loss: 1.85888 - Test loss: 2.11089\n",
      "Epoch 1608 - lr: 0.05000 - Train loss: 1.85888 - Test loss: 2.11090\n",
      "Epoch 1609 - lr: 0.05000 - Train loss: 1.85888 - Test loss: 2.11092\n",
      "Epoch 1610 - lr: 0.05000 - Train loss: 1.85887 - Test loss: 2.11094\n",
      "Epoch 1611 - lr: 0.05000 - Train loss: 1.85887 - Test loss: 2.11096\n",
      "Epoch 1612 - lr: 0.05000 - Train loss: 1.85887 - Test loss: 2.11098\n",
      "Epoch 1613 - lr: 0.05000 - Train loss: 1.85886 - Test loss: 2.11100\n",
      "Epoch 1614 - lr: 0.05000 - Train loss: 1.85886 - Test loss: 2.11101\n",
      "Epoch 1615 - lr: 0.05000 - Train loss: 1.85886 - Test loss: 2.11103\n",
      "Epoch 1616 - lr: 0.05000 - Train loss: 1.85886 - Test loss: 2.11105\n",
      "Epoch 1617 - lr: 0.05000 - Train loss: 1.85885 - Test loss: 2.11107\n",
      "Epoch 1618 - lr: 0.05000 - Train loss: 1.85885 - Test loss: 2.11109\n",
      "Epoch 1619 - lr: 0.05000 - Train loss: 1.85885 - Test loss: 2.11110\n",
      "Epoch 1620 - lr: 0.05000 - Train loss: 1.85884 - Test loss: 2.11112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1621 - lr: 0.05000 - Train loss: 1.85884 - Test loss: 2.11114\n",
      "Epoch 1622 - lr: 0.05000 - Train loss: 1.85884 - Test loss: 2.11116\n",
      "Epoch 1623 - lr: 0.05000 - Train loss: 1.85883 - Test loss: 2.11118\n",
      "Epoch 1624 - lr: 0.05000 - Train loss: 1.85883 - Test loss: 2.11119\n",
      "Epoch 1625 - lr: 0.05000 - Train loss: 1.85883 - Test loss: 2.11121\n",
      "Epoch 1626 - lr: 0.05000 - Train loss: 1.85883 - Test loss: 2.11123\n",
      "Epoch 1627 - lr: 0.05000 - Train loss: 1.85882 - Test loss: 2.11125\n",
      "Epoch 1628 - lr: 0.05000 - Train loss: 1.85882 - Test loss: 2.11127\n",
      "Epoch 1629 - lr: 0.05000 - Train loss: 1.85882 - Test loss: 2.11128\n",
      "Epoch 1630 - lr: 0.05000 - Train loss: 1.85881 - Test loss: 2.11130\n",
      "Epoch 1631 - lr: 0.05000 - Train loss: 1.85881 - Test loss: 2.11132\n",
      "Epoch 1632 - lr: 0.05000 - Train loss: 1.85881 - Test loss: 2.11134\n",
      "Epoch 1633 - lr: 0.05000 - Train loss: 1.85881 - Test loss: 2.11135\n",
      "Epoch 1634 - lr: 0.05000 - Train loss: 1.85880 - Test loss: 2.11137\n",
      "Epoch 1635 - lr: 0.05000 - Train loss: 1.85880 - Test loss: 2.11139\n",
      "Epoch 1636 - lr: 0.05000 - Train loss: 1.85880 - Test loss: 2.11141\n",
      "Epoch 1637 - lr: 0.05000 - Train loss: 1.85879 - Test loss: 2.11142\n",
      "Epoch 1638 - lr: 0.05000 - Train loss: 1.85879 - Test loss: 2.11144\n",
      "Epoch 1639 - lr: 0.05000 - Train loss: 1.85879 - Test loss: 2.11146\n",
      "Epoch 1640 - lr: 0.05000 - Train loss: 1.85878 - Test loss: 2.11148\n",
      "Epoch 1641 - lr: 0.05000 - Train loss: 1.85878 - Test loss: 2.11149\n",
      "Epoch 1642 - lr: 0.05000 - Train loss: 1.85878 - Test loss: 2.11151\n",
      "Epoch 1643 - lr: 0.05000 - Train loss: 1.85878 - Test loss: 2.11153\n",
      "Epoch 1644 - lr: 0.05000 - Train loss: 1.85877 - Test loss: 2.11155\n",
      "Epoch 1645 - lr: 0.05000 - Train loss: 1.85877 - Test loss: 2.11156\n",
      "Epoch 1646 - lr: 0.05000 - Train loss: 1.85877 - Test loss: 2.11158\n",
      "Epoch 1647 - lr: 0.05000 - Train loss: 1.85876 - Test loss: 2.11160\n",
      "Epoch 1648 - lr: 0.05000 - Train loss: 1.85876 - Test loss: 2.11162\n",
      "Epoch 1649 - lr: 0.05000 - Train loss: 1.85876 - Test loss: 2.11163\n",
      "Epoch 1650 - lr: 0.05000 - Train loss: 1.85876 - Test loss: 2.11165\n",
      "Epoch 1651 - lr: 0.05000 - Train loss: 1.85875 - Test loss: 2.11167\n",
      "Epoch 1652 - lr: 0.05000 - Train loss: 1.85875 - Test loss: 2.11168\n",
      "Epoch 1653 - lr: 0.05000 - Train loss: 1.85875 - Test loss: 2.11170\n",
      "Epoch 1654 - lr: 0.05000 - Train loss: 1.85874 - Test loss: 2.11172\n",
      "Epoch 1655 - lr: 0.05000 - Train loss: 1.85874 - Test loss: 2.11174\n",
      "Epoch 1656 - lr: 0.05000 - Train loss: 1.85874 - Test loss: 2.11175\n",
      "Epoch 1657 - lr: 0.05000 - Train loss: 1.85874 - Test loss: 2.11177\n",
      "Epoch 1658 - lr: 0.05000 - Train loss: 1.85873 - Test loss: 2.11179\n",
      "Epoch 1659 - lr: 0.05000 - Train loss: 1.85873 - Test loss: 2.11180\n",
      "Epoch 1660 - lr: 0.05000 - Train loss: 1.85873 - Test loss: 2.11182\n",
      "Epoch 1661 - lr: 0.05000 - Train loss: 1.85872 - Test loss: 2.11184\n",
      "Epoch 1662 - lr: 0.05000 - Train loss: 1.85872 - Test loss: 2.11186\n",
      "Epoch 1663 - lr: 0.05000 - Train loss: 1.85872 - Test loss: 2.11187\n",
      "Epoch 1664 - lr: 0.05000 - Train loss: 1.85872 - Test loss: 2.11189\n",
      "Epoch 1665 - lr: 0.05000 - Train loss: 1.85871 - Test loss: 2.11191\n",
      "Epoch 1666 - lr: 0.05000 - Train loss: 1.85871 - Test loss: 2.11192\n",
      "Epoch 1667 - lr: 0.05000 - Train loss: 1.85871 - Test loss: 2.11194\n",
      "Epoch 1668 - lr: 0.05000 - Train loss: 1.85870 - Test loss: 2.11196\n",
      "Epoch 1669 - lr: 0.05000 - Train loss: 1.85870 - Test loss: 2.11197\n",
      "Epoch 1670 - lr: 0.05000 - Train loss: 1.85870 - Test loss: 2.11199\n",
      "Epoch 1671 - lr: 0.05000 - Train loss: 1.85870 - Test loss: 2.11201\n",
      "Epoch 1672 - lr: 0.05000 - Train loss: 1.85869 - Test loss: 2.11203\n",
      "Epoch 1673 - lr: 0.05000 - Train loss: 1.85869 - Test loss: 2.11204\n",
      "Epoch 1674 - lr: 0.05000 - Train loss: 1.85869 - Test loss: 2.11206\n",
      "Epoch 1675 - lr: 0.05000 - Train loss: 1.85868 - Test loss: 2.11208\n",
      "Epoch 1676 - lr: 0.05000 - Train loss: 1.85868 - Test loss: 2.11209\n",
      "Epoch 1677 - lr: 0.05000 - Train loss: 1.85868 - Test loss: 2.11211\n",
      "Epoch 1678 - lr: 0.05000 - Train loss: 1.85868 - Test loss: 2.11213\n",
      "Epoch 1679 - lr: 0.05000 - Train loss: 1.85867 - Test loss: 2.11214\n",
      "Epoch 1680 - lr: 0.05000 - Train loss: 1.85867 - Test loss: 2.11216\n",
      "Epoch 1681 - lr: 0.05000 - Train loss: 1.85867 - Test loss: 2.11218\n",
      "Epoch 1682 - lr: 0.05000 - Train loss: 1.85867 - Test loss: 2.11219\n",
      "Epoch 1683 - lr: 0.05000 - Train loss: 1.85866 - Test loss: 2.11221\n",
      "Epoch 1684 - lr: 0.05000 - Train loss: 1.85866 - Test loss: 2.11223\n",
      "Epoch 1685 - lr: 0.05000 - Train loss: 1.85866 - Test loss: 2.11224\n",
      "Epoch 1686 - lr: 0.05000 - Train loss: 1.85865 - Test loss: 2.11226\n",
      "Epoch 1687 - lr: 0.05000 - Train loss: 1.85865 - Test loss: 2.11228\n",
      "Epoch 1688 - lr: 0.05000 - Train loss: 1.85865 - Test loss: 2.11229\n",
      "Epoch 1689 - lr: 0.05000 - Train loss: 1.85865 - Test loss: 2.11231\n",
      "Epoch 1690 - lr: 0.05000 - Train loss: 1.85864 - Test loss: 2.11233\n",
      "Epoch 1691 - lr: 0.05000 - Train loss: 1.85864 - Test loss: 2.11234\n",
      "Epoch 1692 - lr: 0.05000 - Train loss: 1.85864 - Test loss: 2.11236\n",
      "Epoch 1693 - lr: 0.05000 - Train loss: 1.85863 - Test loss: 2.11238\n",
      "Epoch 1694 - lr: 0.05000 - Train loss: 1.85863 - Test loss: 2.11239\n",
      "Epoch 1695 - lr: 0.05000 - Train loss: 1.85863 - Test loss: 2.11241\n",
      "Epoch 1696 - lr: 0.05000 - Train loss: 1.85863 - Test loss: 2.11242\n",
      "Epoch 1697 - lr: 0.05000 - Train loss: 1.85862 - Test loss: 2.11244\n",
      "Epoch 1698 - lr: 0.05000 - Train loss: 1.85862 - Test loss: 2.11246\n",
      "Epoch 1699 - lr: 0.05000 - Train loss: 1.85862 - Test loss: 2.11247\n",
      "Epoch 1700 - lr: 0.05000 - Train loss: 1.85861 - Test loss: 2.11249\n",
      "Epoch 1701 - lr: 0.05000 - Train loss: 1.85861 - Test loss: 2.11251\n",
      "Epoch 1702 - lr: 0.05000 - Train loss: 1.85861 - Test loss: 2.11252\n",
      "Epoch 1703 - lr: 0.05000 - Train loss: 1.85861 - Test loss: 2.11254\n",
      "Epoch 1704 - lr: 0.05000 - Train loss: 1.85860 - Test loss: 2.11256\n",
      "Epoch 1705 - lr: 0.05000 - Train loss: 1.85860 - Test loss: 2.11257\n",
      "Epoch 1706 - lr: 0.05000 - Train loss: 1.85860 - Test loss: 2.11259\n",
      "Epoch 1707 - lr: 0.05000 - Train loss: 1.85860 - Test loss: 2.11260\n",
      "Epoch 1708 - lr: 0.05000 - Train loss: 1.85859 - Test loss: 2.11262\n",
      "Epoch 1709 - lr: 0.05000 - Train loss: 1.85859 - Test loss: 2.11264\n",
      "Epoch 1710 - lr: 0.05000 - Train loss: 1.85859 - Test loss: 2.11265\n",
      "Epoch 1711 - lr: 0.05000 - Train loss: 1.85858 - Test loss: 2.11267\n",
      "Epoch 1712 - lr: 0.05000 - Train loss: 1.85858 - Test loss: 2.11268\n",
      "Epoch 1713 - lr: 0.05000 - Train loss: 1.85858 - Test loss: 2.11270\n",
      "Epoch 1714 - lr: 0.05000 - Train loss: 1.85858 - Test loss: 2.11272\n",
      "Epoch 1715 - lr: 0.05000 - Train loss: 1.85857 - Test loss: 2.11273\n",
      "Epoch 1716 - lr: 0.05000 - Train loss: 1.85857 - Test loss: 2.11275\n",
      "Epoch 1717 - lr: 0.05000 - Train loss: 1.85857 - Test loss: 2.11277\n",
      "Epoch 1718 - lr: 0.05000 - Train loss: 1.85856 - Test loss: 2.11278\n",
      "Epoch 1719 - lr: 0.05000 - Train loss: 1.85856 - Test loss: 2.11280\n",
      "Epoch 1720 - lr: 0.05000 - Train loss: 1.85856 - Test loss: 2.11281\n",
      "Epoch 1721 - lr: 0.05000 - Train loss: 1.85856 - Test loss: 2.11283\n",
      "Epoch 1722 - lr: 0.05000 - Train loss: 1.85855 - Test loss: 2.11285\n",
      "Epoch 1723 - lr: 0.05000 - Train loss: 1.85855 - Test loss: 2.11286\n",
      "Epoch 1724 - lr: 0.05000 - Train loss: 1.85855 - Test loss: 2.11288\n",
      "Epoch 1725 - lr: 0.05000 - Train loss: 1.85855 - Test loss: 2.11289\n",
      "Epoch 1726 - lr: 0.05000 - Train loss: 1.85854 - Test loss: 2.11291\n",
      "Epoch 1727 - lr: 0.05000 - Train loss: 1.85854 - Test loss: 2.11293\n",
      "Epoch 1728 - lr: 0.05000 - Train loss: 1.85854 - Test loss: 2.11294\n",
      "Epoch 1729 - lr: 0.05000 - Train loss: 1.85853 - Test loss: 2.11296\n",
      "Epoch 1730 - lr: 0.05000 - Train loss: 1.85853 - Test loss: 2.11297\n",
      "Epoch 1731 - lr: 0.05000 - Train loss: 1.85853 - Test loss: 2.11299\n",
      "Epoch 1732 - lr: 0.05000 - Train loss: 1.85853 - Test loss: 2.11300\n",
      "Epoch 1733 - lr: 0.05000 - Train loss: 1.85852 - Test loss: 2.11302\n",
      "Epoch 1734 - lr: 0.05000 - Train loss: 1.85852 - Test loss: 2.11304\n",
      "Epoch 1735 - lr: 0.05000 - Train loss: 1.85852 - Test loss: 2.11305\n",
      "Epoch 1736 - lr: 0.05000 - Train loss: 1.85852 - Test loss: 2.11307\n",
      "Epoch 1737 - lr: 0.05000 - Train loss: 1.85851 - Test loss: 2.11308\n",
      "Epoch 1738 - lr: 0.05000 - Train loss: 1.85851 - Test loss: 2.11310\n",
      "Epoch 1739 - lr: 0.05000 - Train loss: 1.85851 - Test loss: 2.11312\n",
      "Epoch 1740 - lr: 0.05000 - Train loss: 1.85850 - Test loss: 2.11313\n",
      "Epoch 1741 - lr: 0.05000 - Train loss: 1.85850 - Test loss: 2.11315\n",
      "Epoch 1742 - lr: 0.05000 - Train loss: 1.85850 - Test loss: 2.11316\n",
      "Epoch 1743 - lr: 0.05000 - Train loss: 1.85850 - Test loss: 2.11318\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1744 - lr: 0.05000 - Train loss: 1.85849 - Test loss: 2.11319\n",
      "Epoch 1745 - lr: 0.05000 - Train loss: 1.85849 - Test loss: 2.11321\n",
      "Epoch 1746 - lr: 0.05000 - Train loss: 1.85849 - Test loss: 2.11323\n",
      "Epoch 1747 - lr: 0.05000 - Train loss: 1.85849 - Test loss: 2.11324\n",
      "Epoch 1748 - lr: 0.05000 - Train loss: 1.85848 - Test loss: 2.11326\n",
      "Epoch 1749 - lr: 0.05000 - Train loss: 1.85848 - Test loss: 2.11327\n",
      "Epoch 1750 - lr: 0.05000 - Train loss: 1.85848 - Test loss: 2.11329\n",
      "Epoch 1751 - lr: 0.05000 - Train loss: 1.85847 - Test loss: 2.11330\n",
      "Epoch 1752 - lr: 0.05000 - Train loss: 1.85847 - Test loss: 2.11332\n",
      "Epoch 1753 - lr: 0.05000 - Train loss: 1.85847 - Test loss: 2.11333\n",
      "Epoch 1754 - lr: 0.05000 - Train loss: 1.85847 - Test loss: 2.11335\n",
      "Epoch 1755 - lr: 0.05000 - Train loss: 1.85846 - Test loss: 2.11337\n",
      "Epoch 1756 - lr: 0.05000 - Train loss: 1.85846 - Test loss: 2.11338\n",
      "Epoch 1757 - lr: 0.05000 - Train loss: 1.85846 - Test loss: 2.11340\n",
      "Epoch 1758 - lr: 0.05000 - Train loss: 1.85846 - Test loss: 2.11341\n",
      "Epoch 1759 - lr: 0.05000 - Train loss: 1.85845 - Test loss: 2.11343\n",
      "Epoch 1760 - lr: 0.05000 - Train loss: 1.85845 - Test loss: 2.11344\n",
      "Epoch 1761 - lr: 0.05000 - Train loss: 1.85845 - Test loss: 2.11346\n",
      "Epoch 1762 - lr: 0.05000 - Train loss: 1.85844 - Test loss: 2.11347\n",
      "Epoch 1763 - lr: 0.05000 - Train loss: 1.85844 - Test loss: 2.11349\n",
      "Epoch 1764 - lr: 0.05000 - Train loss: 1.85844 - Test loss: 2.11350\n",
      "Epoch 1765 - lr: 0.05000 - Train loss: 1.85844 - Test loss: 2.11352\n",
      "Epoch 1766 - lr: 0.05000 - Train loss: 1.85843 - Test loss: 2.11353\n",
      "Epoch 1767 - lr: 0.05000 - Train loss: 1.85843 - Test loss: 2.11355\n",
      "Epoch 1768 - lr: 0.05000 - Train loss: 1.85843 - Test loss: 2.11357\n",
      "Epoch 1769 - lr: 0.05000 - Train loss: 1.85843 - Test loss: 2.11358\n",
      "Epoch 1770 - lr: 0.05000 - Train loss: 1.85842 - Test loss: 2.11360\n",
      "Epoch 1771 - lr: 0.05000 - Train loss: 1.85842 - Test loss: 2.11361\n",
      "Epoch 1772 - lr: 0.05000 - Train loss: 1.85842 - Test loss: 2.11363\n",
      "Epoch 1773 - lr: 0.05000 - Train loss: 1.85842 - Test loss: 2.11364\n",
      "Epoch 1774 - lr: 0.05000 - Train loss: 1.85841 - Test loss: 2.11366\n",
      "Epoch 1775 - lr: 0.05000 - Train loss: 1.85841 - Test loss: 2.11367\n",
      "Epoch 1776 - lr: 0.05000 - Train loss: 1.85841 - Test loss: 2.11369\n",
      "Epoch 1777 - lr: 0.05000 - Train loss: 1.85840 - Test loss: 2.11370\n",
      "Epoch 1778 - lr: 0.05000 - Train loss: 1.85840 - Test loss: 2.11372\n",
      "Epoch 1779 - lr: 0.05000 - Train loss: 1.85840 - Test loss: 2.11373\n",
      "Epoch 1780 - lr: 0.05000 - Train loss: 1.85840 - Test loss: 2.11375\n",
      "Epoch 1781 - lr: 0.05000 - Train loss: 1.85839 - Test loss: 2.11376\n",
      "Epoch 1782 - lr: 0.05000 - Train loss: 1.85839 - Test loss: 2.11378\n",
      "Epoch 1783 - lr: 0.05000 - Train loss: 1.85839 - Test loss: 2.11379\n",
      "Epoch 1784 - lr: 0.05000 - Train loss: 1.85839 - Test loss: 2.11381\n",
      "Epoch 1785 - lr: 0.05000 - Train loss: 1.85838 - Test loss: 2.11382\n",
      "Epoch 1786 - lr: 0.05000 - Train loss: 1.85838 - Test loss: 2.11384\n",
      "Epoch 1787 - lr: 0.05000 - Train loss: 1.85838 - Test loss: 2.11385\n",
      "Epoch 1788 - lr: 0.05000 - Train loss: 1.85837 - Test loss: 2.11387\n",
      "Epoch 1789 - lr: 0.05000 - Train loss: 1.85837 - Test loss: 2.11388\n",
      "Epoch 1790 - lr: 0.05000 - Train loss: 1.85837 - Test loss: 2.11390\n",
      "Epoch 1791 - lr: 0.05000 - Train loss: 1.85837 - Test loss: 2.11391\n",
      "Epoch 1792 - lr: 0.05000 - Train loss: 1.85836 - Test loss: 2.11393\n",
      "Epoch 1793 - lr: 0.05000 - Train loss: 1.85836 - Test loss: 2.11394\n",
      "Epoch 1794 - lr: 0.05000 - Train loss: 1.85836 - Test loss: 2.11396\n",
      "Epoch 1795 - lr: 0.05000 - Train loss: 1.85836 - Test loss: 2.11397\n",
      "Epoch 1796 - lr: 0.05000 - Train loss: 1.85835 - Test loss: 2.11399\n",
      "Epoch 1797 - lr: 0.05000 - Train loss: 1.85835 - Test loss: 2.11400\n",
      "Epoch 1798 - lr: 0.05000 - Train loss: 1.85835 - Test loss: 2.11402\n",
      "Epoch 1799 - lr: 0.05000 - Train loss: 1.85835 - Test loss: 2.11403\n",
      "Epoch 1800 - lr: 0.05000 - Train loss: 1.85834 - Test loss: 2.11405\n",
      "Epoch 1801 - lr: 0.05000 - Train loss: 1.85834 - Test loss: 2.11406\n",
      "Epoch 1802 - lr: 0.05000 - Train loss: 1.85834 - Test loss: 2.11408\n",
      "Epoch 1803 - lr: 0.05000 - Train loss: 1.85834 - Test loss: 2.11409\n",
      "Epoch 1804 - lr: 0.05000 - Train loss: 1.85833 - Test loss: 2.11411\n",
      "Epoch 1805 - lr: 0.05000 - Train loss: 1.85833 - Test loss: 2.11412\n",
      "Epoch 1806 - lr: 0.05000 - Train loss: 1.85833 - Test loss: 2.11414\n",
      "Epoch 1807 - lr: 0.05000 - Train loss: 1.85832 - Test loss: 2.11415\n",
      "Epoch 1808 - lr: 0.05000 - Train loss: 1.85832 - Test loss: 2.11417\n",
      "Epoch 1809 - lr: 0.05000 - Train loss: 1.85832 - Test loss: 2.11418\n",
      "Epoch 1810 - lr: 0.05000 - Train loss: 1.85832 - Test loss: 2.11420\n",
      "Epoch 1811 - lr: 0.05000 - Train loss: 1.85831 - Test loss: 2.11421\n",
      "Epoch 1812 - lr: 0.05000 - Train loss: 1.85831 - Test loss: 2.11423\n",
      "Epoch 1813 - lr: 0.05000 - Train loss: 1.85831 - Test loss: 2.11424\n",
      "Epoch 1814 - lr: 0.05000 - Train loss: 1.85831 - Test loss: 2.11426\n",
      "Epoch 1815 - lr: 0.05000 - Train loss: 1.85830 - Test loss: 2.11427\n",
      "Epoch 1816 - lr: 0.05000 - Train loss: 1.85830 - Test loss: 2.11428\n",
      "Epoch 1817 - lr: 0.05000 - Train loss: 1.85830 - Test loss: 2.11430\n",
      "Epoch 1818 - lr: 0.05000 - Train loss: 1.85830 - Test loss: 2.11431\n",
      "Epoch 1819 - lr: 0.05000 - Train loss: 1.85829 - Test loss: 2.11433\n",
      "Epoch 1820 - lr: 0.05000 - Train loss: 1.85829 - Test loss: 2.11434\n",
      "Epoch 1821 - lr: 0.05000 - Train loss: 1.85829 - Test loss: 2.11436\n",
      "Epoch 1822 - lr: 0.05000 - Train loss: 1.85828 - Test loss: 2.11437\n",
      "Epoch 1823 - lr: 0.05000 - Train loss: 1.85828 - Test loss: 2.11439\n",
      "Epoch 1824 - lr: 0.05000 - Train loss: 1.85828 - Test loss: 2.11440\n",
      "Epoch 1825 - lr: 0.05000 - Train loss: 1.85828 - Test loss: 2.11442\n",
      "Epoch 1826 - lr: 0.05000 - Train loss: 1.85827 - Test loss: 2.11443\n",
      "Epoch 1827 - lr: 0.05000 - Train loss: 1.85827 - Test loss: 2.11444\n",
      "Epoch 1828 - lr: 0.05000 - Train loss: 1.85827 - Test loss: 2.11446\n",
      "Epoch 1829 - lr: 0.05000 - Train loss: 1.85827 - Test loss: 2.11447\n",
      "Epoch 1830 - lr: 0.05000 - Train loss: 1.85826 - Test loss: 2.11449\n",
      "Epoch 1831 - lr: 0.05000 - Train loss: 1.85826 - Test loss: 2.11450\n",
      "Epoch 1832 - lr: 0.05000 - Train loss: 1.85826 - Test loss: 2.11452\n",
      "Epoch 1833 - lr: 0.05000 - Train loss: 1.85826 - Test loss: 2.11453\n",
      "Epoch 1834 - lr: 0.05000 - Train loss: 1.85825 - Test loss: 2.11455\n",
      "Epoch 1835 - lr: 0.05000 - Train loss: 1.85825 - Test loss: 2.11456\n",
      "Epoch 1836 - lr: 0.05000 - Train loss: 1.85825 - Test loss: 2.11457\n",
      "Epoch 1837 - lr: 0.05000 - Train loss: 1.85825 - Test loss: 2.11459\n",
      "Epoch 1838 - lr: 0.05000 - Train loss: 1.85824 - Test loss: 2.11460\n",
      "Epoch 1839 - lr: 0.05000 - Train loss: 1.85824 - Test loss: 2.11462\n",
      "Epoch 1840 - lr: 0.05000 - Train loss: 1.85824 - Test loss: 2.11463\n",
      "Epoch 1841 - lr: 0.05000 - Train loss: 1.85823 - Test loss: 2.11465\n",
      "Epoch 1842 - lr: 0.05000 - Train loss: 1.85823 - Test loss: 2.11466\n",
      "Epoch 1843 - lr: 0.05000 - Train loss: 1.85823 - Test loss: 2.11468\n",
      "Epoch 1844 - lr: 0.05000 - Train loss: 1.85823 - Test loss: 2.11469\n",
      "Epoch 1845 - lr: 0.05000 - Train loss: 1.85822 - Test loss: 2.11470\n",
      "Epoch 1846 - lr: 0.05000 - Train loss: 1.85822 - Test loss: 2.11472\n",
      "Epoch 1847 - lr: 0.05000 - Train loss: 1.85822 - Test loss: 2.11473\n",
      "Epoch 1848 - lr: 0.05000 - Train loss: 1.85822 - Test loss: 2.11475\n",
      "Epoch 1849 - lr: 0.05000 - Train loss: 1.85821 - Test loss: 2.11476\n",
      "Epoch 1850 - lr: 0.05000 - Train loss: 1.85821 - Test loss: 2.11478\n",
      "Epoch 1851 - lr: 0.05000 - Train loss: 1.85821 - Test loss: 2.11479\n",
      "Epoch 1852 - lr: 0.05000 - Train loss: 1.85821 - Test loss: 2.11480\n",
      "Epoch 1853 - lr: 0.05000 - Train loss: 1.85820 - Test loss: 2.11482\n",
      "Epoch 1854 - lr: 0.05000 - Train loss: 1.85820 - Test loss: 2.11483\n",
      "Epoch 1855 - lr: 0.05000 - Train loss: 1.85820 - Test loss: 2.11485\n",
      "Epoch 1856 - lr: 0.05000 - Train loss: 1.85820 - Test loss: 2.11486\n",
      "Epoch 1857 - lr: 0.05000 - Train loss: 1.85819 - Test loss: 2.11487\n",
      "Epoch 1858 - lr: 0.05000 - Train loss: 1.85819 - Test loss: 2.11489\n",
      "Epoch 1859 - lr: 0.05000 - Train loss: 1.85819 - Test loss: 2.11490\n",
      "Epoch 1860 - lr: 0.05000 - Train loss: 1.85819 - Test loss: 2.11492\n",
      "Epoch 1861 - lr: 0.05000 - Train loss: 1.85818 - Test loss: 2.11493\n",
      "Epoch 1862 - lr: 0.05000 - Train loss: 1.85818 - Test loss: 2.11494\n",
      "Epoch 1863 - lr: 0.05000 - Train loss: 1.85818 - Test loss: 2.11496\n",
      "Epoch 1864 - lr: 0.05000 - Train loss: 1.85817 - Test loss: 2.11497\n",
      "Epoch 1865 - lr: 0.05000 - Train loss: 1.85817 - Test loss: 2.11499\n",
      "Epoch 1866 - lr: 0.05000 - Train loss: 1.85817 - Test loss: 2.11500\n",
      "Epoch 1867 - lr: 0.05000 - Train loss: 1.85817 - Test loss: 2.11502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1868 - lr: 0.05000 - Train loss: 1.85816 - Test loss: 2.11503\n",
      "Epoch 1869 - lr: 0.05000 - Train loss: 1.85816 - Test loss: 2.11504\n",
      "Epoch 1870 - lr: 0.05000 - Train loss: 1.85816 - Test loss: 2.11506\n",
      "Epoch 1871 - lr: 0.05000 - Train loss: 1.85816 - Test loss: 2.11507\n",
      "Epoch 1872 - lr: 0.05000 - Train loss: 1.85815 - Test loss: 2.11509\n",
      "Epoch 1873 - lr: 0.05000 - Train loss: 1.85815 - Test loss: 2.11510\n",
      "Epoch 1874 - lr: 0.05000 - Train loss: 1.85815 - Test loss: 2.11511\n",
      "Epoch 1875 - lr: 0.05000 - Train loss: 1.85815 - Test loss: 2.11513\n",
      "Epoch 1876 - lr: 0.05000 - Train loss: 1.85814 - Test loss: 2.11514\n",
      "Epoch 1877 - lr: 0.05000 - Train loss: 1.85814 - Test loss: 2.11515\n",
      "Epoch 1878 - lr: 0.05000 - Train loss: 1.85814 - Test loss: 2.11517\n",
      "Epoch 1879 - lr: 0.05000 - Train loss: 1.85814 - Test loss: 2.11518\n",
      "Epoch 1880 - lr: 0.05000 - Train loss: 1.85813 - Test loss: 2.11520\n",
      "Epoch 1881 - lr: 0.05000 - Train loss: 1.85813 - Test loss: 2.11521\n",
      "Epoch 1882 - lr: 0.05000 - Train loss: 1.85813 - Test loss: 2.11522\n",
      "Epoch 1883 - lr: 0.05000 - Train loss: 1.85813 - Test loss: 2.11524\n",
      "Epoch 1884 - lr: 0.05000 - Train loss: 1.85812 - Test loss: 2.11525\n",
      "Epoch 1885 - lr: 0.05000 - Train loss: 1.85812 - Test loss: 2.11527\n",
      "Epoch 1886 - lr: 0.05000 - Train loss: 1.85812 - Test loss: 2.11528\n",
      "Epoch 1887 - lr: 0.05000 - Train loss: 1.85812 - Test loss: 2.11529\n",
      "Epoch 1888 - lr: 0.05000 - Train loss: 1.85811 - Test loss: 2.11531\n",
      "Epoch 1889 - lr: 0.05000 - Train loss: 1.85811 - Test loss: 2.11532\n",
      "Epoch 1890 - lr: 0.05000 - Train loss: 1.85811 - Test loss: 2.11533\n",
      "Epoch 1891 - lr: 0.05000 - Train loss: 1.85810 - Test loss: 2.11535\n",
      "Epoch 1892 - lr: 0.05000 - Train loss: 1.85810 - Test loss: 2.11536\n",
      "Epoch 1893 - lr: 0.05000 - Train loss: 1.85810 - Test loss: 2.11538\n",
      "Epoch 1894 - lr: 0.05000 - Train loss: 1.85810 - Test loss: 2.11539\n",
      "Epoch 1895 - lr: 0.05000 - Train loss: 1.85809 - Test loss: 2.11540\n",
      "Epoch 1896 - lr: 0.05000 - Train loss: 1.85809 - Test loss: 2.11542\n",
      "Epoch 1897 - lr: 0.05000 - Train loss: 1.85809 - Test loss: 2.11543\n",
      "Epoch 1898 - lr: 0.05000 - Train loss: 1.85809 - Test loss: 2.11544\n",
      "Epoch 1899 - lr: 0.05000 - Train loss: 1.85808 - Test loss: 2.11546\n",
      "Epoch 1900 - lr: 0.05000 - Train loss: 1.85808 - Test loss: 2.11547\n",
      "Epoch 1901 - lr: 0.05000 - Train loss: 1.85808 - Test loss: 2.11548\n",
      "Epoch 1902 - lr: 0.05000 - Train loss: 1.85808 - Test loss: 2.11550\n",
      "Epoch 1903 - lr: 0.05000 - Train loss: 1.85807 - Test loss: 2.11551\n",
      "Epoch 1904 - lr: 0.05000 - Train loss: 1.85807 - Test loss: 2.11553\n",
      "Epoch 1905 - lr: 0.05000 - Train loss: 1.85807 - Test loss: 2.11554\n",
      "Epoch 1906 - lr: 0.05000 - Train loss: 1.85807 - Test loss: 2.11555\n",
      "Epoch 1907 - lr: 0.05000 - Train loss: 1.85806 - Test loss: 2.11557\n",
      "Epoch 1908 - lr: 0.05000 - Train loss: 1.85806 - Test loss: 2.11558\n",
      "Epoch 1909 - lr: 0.05000 - Train loss: 1.85806 - Test loss: 2.11559\n",
      "Epoch 1910 - lr: 0.05000 - Train loss: 1.85806 - Test loss: 2.11561\n",
      "Epoch 1911 - lr: 0.05000 - Train loss: 1.85805 - Test loss: 2.11562\n",
      "Epoch 1912 - lr: 0.05000 - Train loss: 1.85805 - Test loss: 2.11563\n",
      "Epoch 1913 - lr: 0.05000 - Train loss: 1.85805 - Test loss: 2.11565\n",
      "Epoch 1914 - lr: 0.05000 - Train loss: 1.85805 - Test loss: 2.11566\n",
      "Epoch 1915 - lr: 0.05000 - Train loss: 1.85804 - Test loss: 2.11567\n",
      "Epoch 1916 - lr: 0.05000 - Train loss: 1.85804 - Test loss: 2.11569\n",
      "Epoch 1917 - lr: 0.05000 - Train loss: 1.85804 - Test loss: 2.11570\n",
      "Epoch 1918 - lr: 0.05000 - Train loss: 1.85804 - Test loss: 2.11571\n",
      "Epoch 1919 - lr: 0.05000 - Train loss: 1.85803 - Test loss: 2.11573\n",
      "Epoch 1920 - lr: 0.05000 - Train loss: 1.85803 - Test loss: 2.11574\n",
      "Epoch 1921 - lr: 0.05000 - Train loss: 1.85803 - Test loss: 2.11575\n",
      "Epoch 1922 - lr: 0.05000 - Train loss: 1.85803 - Test loss: 2.11577\n",
      "Epoch 1923 - lr: 0.05000 - Train loss: 1.85802 - Test loss: 2.11578\n",
      "Epoch 1924 - lr: 0.05000 - Train loss: 1.85802 - Test loss: 2.11579\n",
      "Epoch 1925 - lr: 0.05000 - Train loss: 1.85802 - Test loss: 2.11581\n",
      "Epoch 1926 - lr: 0.05000 - Train loss: 1.85801 - Test loss: 2.11582\n",
      "Epoch 1927 - lr: 0.05000 - Train loss: 1.85801 - Test loss: 2.11583\n",
      "Epoch 1928 - lr: 0.05000 - Train loss: 1.85801 - Test loss: 2.11585\n",
      "Epoch 1929 - lr: 0.05000 - Train loss: 1.85801 - Test loss: 2.11586\n",
      "Epoch 1930 - lr: 0.05000 - Train loss: 1.85800 - Test loss: 2.11587\n",
      "Epoch 1931 - lr: 0.05000 - Train loss: 1.85800 - Test loss: 2.11589\n",
      "Epoch 1932 - lr: 0.05000 - Train loss: 1.85800 - Test loss: 2.11590\n",
      "Epoch 1933 - lr: 0.05000 - Train loss: 1.85800 - Test loss: 2.11591\n",
      "Epoch 1934 - lr: 0.05000 - Train loss: 1.85799 - Test loss: 2.11593\n",
      "Epoch 1935 - lr: 0.05000 - Train loss: 1.85799 - Test loss: 2.11594\n",
      "Epoch 1936 - lr: 0.05000 - Train loss: 1.85799 - Test loss: 2.11595\n",
      "Epoch 1937 - lr: 0.05000 - Train loss: 1.85799 - Test loss: 2.11597\n",
      "Epoch 1938 - lr: 0.05000 - Train loss: 1.85798 - Test loss: 2.11598\n",
      "Epoch 1939 - lr: 0.05000 - Train loss: 1.85798 - Test loss: 2.11599\n",
      "Epoch 1940 - lr: 0.05000 - Train loss: 1.85798 - Test loss: 2.11601\n",
      "Epoch 1941 - lr: 0.05000 - Train loss: 1.85798 - Test loss: 2.11602\n",
      "Epoch 1942 - lr: 0.05000 - Train loss: 1.85797 - Test loss: 2.11603\n",
      "Epoch 1943 - lr: 0.05000 - Train loss: 1.85797 - Test loss: 2.11605\n",
      "Epoch 1944 - lr: 0.05000 - Train loss: 1.85797 - Test loss: 2.11606\n",
      "Epoch 1945 - lr: 0.05000 - Train loss: 1.85797 - Test loss: 2.11607\n",
      "Epoch 1946 - lr: 0.05000 - Train loss: 1.85796 - Test loss: 2.11608\n",
      "Epoch 1947 - lr: 0.05000 - Train loss: 1.85796 - Test loss: 2.11610\n",
      "Epoch 1948 - lr: 0.05000 - Train loss: 1.85796 - Test loss: 2.11611\n",
      "Epoch 1949 - lr: 0.05000 - Train loss: 1.85796 - Test loss: 2.11612\n",
      "Epoch 1950 - lr: 0.05000 - Train loss: 1.85795 - Test loss: 2.11614\n",
      "Epoch 1951 - lr: 0.05000 - Train loss: 1.85795 - Test loss: 2.11615\n",
      "Epoch 1952 - lr: 0.05000 - Train loss: 1.85795 - Test loss: 2.11616\n",
      "Epoch 1953 - lr: 0.05000 - Train loss: 1.85795 - Test loss: 2.11618\n",
      "Epoch 1954 - lr: 0.05000 - Train loss: 1.85794 - Test loss: 2.11619\n",
      "Epoch 1955 - lr: 0.05000 - Train loss: 1.85794 - Test loss: 2.11620\n",
      "Epoch 1956 - lr: 0.05000 - Train loss: 1.85794 - Test loss: 2.11621\n",
      "Epoch 1957 - lr: 0.05000 - Train loss: 1.85794 - Test loss: 2.11623\n",
      "Epoch 1958 - lr: 0.05000 - Train loss: 1.85793 - Test loss: 2.11624\n",
      "Epoch 1959 - lr: 0.05000 - Train loss: 1.85793 - Test loss: 2.11625\n",
      "Epoch 1960 - lr: 0.05000 - Train loss: 1.85793 - Test loss: 2.11627\n",
      "Epoch 1961 - lr: 0.05000 - Train loss: 1.85793 - Test loss: 2.11628\n",
      "Epoch 1962 - lr: 0.05000 - Train loss: 1.85792 - Test loss: 2.11629\n",
      "Epoch 1963 - lr: 0.05000 - Train loss: 1.85792 - Test loss: 2.11631\n",
      "Epoch 1964 - lr: 0.05000 - Train loss: 1.85792 - Test loss: 2.11632\n",
      "Epoch 1965 - lr: 0.05000 - Train loss: 1.85792 - Test loss: 2.11633\n",
      "Epoch 1966 - lr: 0.05000 - Train loss: 1.85791 - Test loss: 2.11634\n",
      "Epoch 1967 - lr: 0.05000 - Train loss: 1.85791 - Test loss: 2.11636\n",
      "Epoch 1968 - lr: 0.05000 - Train loss: 1.85791 - Test loss: 2.11637\n",
      "Epoch 1969 - lr: 0.05000 - Train loss: 1.85791 - Test loss: 2.11638\n",
      "Epoch 1970 - lr: 0.05000 - Train loss: 1.85790 - Test loss: 2.11640\n",
      "Epoch 1971 - lr: 0.05000 - Train loss: 1.85790 - Test loss: 2.11641\n",
      "Epoch 1972 - lr: 0.05000 - Train loss: 1.85790 - Test loss: 2.11642\n",
      "Epoch 1973 - lr: 0.05000 - Train loss: 1.85790 - Test loss: 2.11643\n",
      "Epoch 1974 - lr: 0.05000 - Train loss: 1.85789 - Test loss: 2.11645\n",
      "Epoch 1975 - lr: 0.05000 - Train loss: 1.85789 - Test loss: 2.11646\n",
      "Epoch 1976 - lr: 0.05000 - Train loss: 1.85789 - Test loss: 2.11647\n",
      "Epoch 1977 - lr: 0.05000 - Train loss: 1.85789 - Test loss: 2.11648\n",
      "Epoch 1978 - lr: 0.05000 - Train loss: 1.85788 - Test loss: 2.11650\n",
      "Epoch 1979 - lr: 0.05000 - Train loss: 1.85788 - Test loss: 2.11651\n",
      "Epoch 1980 - lr: 0.05000 - Train loss: 1.85788 - Test loss: 2.11652\n",
      "Epoch 1981 - lr: 0.05000 - Train loss: 1.85788 - Test loss: 2.11654\n",
      "Epoch 1982 - lr: 0.05000 - Train loss: 1.85787 - Test loss: 2.11655\n",
      "Epoch 1983 - lr: 0.05000 - Train loss: 1.85787 - Test loss: 2.11656\n",
      "Epoch 1984 - lr: 0.05000 - Train loss: 1.85787 - Test loss: 2.11657\n",
      "Epoch 1985 - lr: 0.05000 - Train loss: 1.85787 - Test loss: 2.11659\n",
      "Epoch 1986 - lr: 0.05000 - Train loss: 1.85786 - Test loss: 2.11660\n",
      "Epoch 1987 - lr: 0.05000 - Train loss: 1.85786 - Test loss: 2.11661\n",
      "Epoch 1988 - lr: 0.05000 - Train loss: 1.85786 - Test loss: 2.11662\n",
      "Epoch 1989 - lr: 0.05000 - Train loss: 1.85785 - Test loss: 2.11664\n",
      "Epoch 1990 - lr: 0.05000 - Train loss: 1.85785 - Test loss: 2.11665\n",
      "Epoch 1991 - lr: 0.05000 - Train loss: 1.85785 - Test loss: 2.11666\n",
      "Epoch 1992 - lr: 0.05000 - Train loss: 1.85785 - Test loss: 2.11667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1993 - lr: 0.05000 - Train loss: 1.85784 - Test loss: 2.11669\n",
      "Epoch 1994 - lr: 0.05000 - Train loss: 1.85784 - Test loss: 2.11670\n",
      "Epoch 1995 - lr: 0.05000 - Train loss: 1.85784 - Test loss: 2.11671\n",
      "Epoch 1996 - lr: 0.05000 - Train loss: 1.85784 - Test loss: 2.11672\n",
      "Epoch 1997 - lr: 0.05000 - Train loss: 1.85783 - Test loss: 2.11674\n",
      "Epoch 1998 - lr: 0.05000 - Train loss: 1.85783 - Test loss: 2.11675\n",
      "Epoch 1999 - lr: 0.05000 - Train loss: 1.85783 - Test loss: 2.11676\n",
      "Epoch 2000 - lr: 0.05000 - Train loss: 1.85783 - Test loss: 2.11677\n",
      "Epoch 2001 - lr: 0.05000 - Train loss: 1.85782 - Test loss: 2.11679\n",
      "Epoch 2002 - lr: 0.05000 - Train loss: 1.85782 - Test loss: 2.11680\n",
      "Epoch 2003 - lr: 0.05000 - Train loss: 1.85782 - Test loss: 2.11681\n",
      "Epoch 2004 - lr: 0.05000 - Train loss: 1.85782 - Test loss: 2.11682\n",
      "Epoch 2005 - lr: 0.05000 - Train loss: 1.85781 - Test loss: 2.11684\n",
      "Epoch 2006 - lr: 0.05000 - Train loss: 1.85781 - Test loss: 2.11685\n",
      "Epoch 2007 - lr: 0.05000 - Train loss: 1.85781 - Test loss: 2.11686\n",
      "Epoch 2008 - lr: 0.05000 - Train loss: 1.85781 - Test loss: 2.11687\n",
      "Epoch 2009 - lr: 0.05000 - Train loss: 1.85780 - Test loss: 2.11689\n",
      "Epoch 2010 - lr: 0.05000 - Train loss: 1.85780 - Test loss: 2.11690\n",
      "Epoch 2011 - lr: 0.05000 - Train loss: 1.85780 - Test loss: 2.11691\n",
      "Epoch 2012 - lr: 0.05000 - Train loss: 1.85780 - Test loss: 2.11692\n",
      "Epoch 2013 - lr: 0.05000 - Train loss: 1.85779 - Test loss: 2.11694\n",
      "Epoch 2014 - lr: 0.05000 - Train loss: 1.85779 - Test loss: 2.11695\n",
      "Epoch 2015 - lr: 0.05000 - Train loss: 1.85779 - Test loss: 2.11696\n",
      "Epoch 2016 - lr: 0.05000 - Train loss: 1.85779 - Test loss: 2.11697\n",
      "Epoch 2017 - lr: 0.05000 - Train loss: 1.85778 - Test loss: 2.11699\n",
      "Epoch 2018 - lr: 0.05000 - Train loss: 1.85778 - Test loss: 2.11700\n",
      "Epoch 2019 - lr: 0.05000 - Train loss: 1.85778 - Test loss: 2.11701\n",
      "Epoch 2020 - lr: 0.05000 - Train loss: 1.85778 - Test loss: 2.11702\n",
      "Epoch 2021 - lr: 0.05000 - Train loss: 1.85777 - Test loss: 2.11703\n",
      "Epoch 2022 - lr: 0.05000 - Train loss: 1.85777 - Test loss: 2.11705\n",
      "Epoch 2023 - lr: 0.05000 - Train loss: 1.85777 - Test loss: 2.11706\n",
      "Epoch 2024 - lr: 0.05000 - Train loss: 1.85777 - Test loss: 2.11707\n",
      "Epoch 2025 - lr: 0.05000 - Train loss: 1.85776 - Test loss: 2.11708\n",
      "Epoch 2026 - lr: 0.05000 - Train loss: 1.85776 - Test loss: 2.11710\n",
      "Epoch 2027 - lr: 0.05000 - Train loss: 1.85776 - Test loss: 2.11711\n",
      "Epoch 2028 - lr: 0.05000 - Train loss: 1.85776 - Test loss: 2.11712\n",
      "Epoch 2029 - lr: 0.05000 - Train loss: 1.85775 - Test loss: 2.11713\n",
      "Epoch 2030 - lr: 0.05000 - Train loss: 1.85775 - Test loss: 2.11714\n",
      "Epoch 2031 - lr: 0.05000 - Train loss: 1.85775 - Test loss: 2.11716\n",
      "Epoch 2032 - lr: 0.05000 - Train loss: 1.85775 - Test loss: 2.11717\n",
      "Epoch 2033 - lr: 0.05000 - Train loss: 1.85774 - Test loss: 2.11718\n",
      "Epoch 2034 - lr: 0.05000 - Train loss: 1.85774 - Test loss: 2.11719\n",
      "Epoch 2035 - lr: 0.05000 - Train loss: 1.85774 - Test loss: 2.11720\n",
      "Epoch 2036 - lr: 0.05000 - Train loss: 1.85774 - Test loss: 2.11722\n",
      "Epoch 2037 - lr: 0.05000 - Train loss: 1.85773 - Test loss: 2.11723\n",
      "Epoch 2038 - lr: 0.05000 - Train loss: 1.85773 - Test loss: 2.11724\n",
      "Epoch 2039 - lr: 0.05000 - Train loss: 1.85773 - Test loss: 2.11725\n",
      "Epoch 2040 - lr: 0.05000 - Train loss: 1.85773 - Test loss: 2.11726\n",
      "Epoch 2041 - lr: 0.05000 - Train loss: 1.85772 - Test loss: 2.11728\n",
      "Epoch 2042 - lr: 0.05000 - Train loss: 1.85772 - Test loss: 2.11729\n",
      "Epoch 2043 - lr: 0.05000 - Train loss: 1.85772 - Test loss: 2.11730\n",
      "Epoch 2044 - lr: 0.05000 - Train loss: 1.85772 - Test loss: 2.11731\n",
      "Epoch 2045 - lr: 0.05000 - Train loss: 1.85771 - Test loss: 2.11732\n",
      "Epoch 2046 - lr: 0.05000 - Train loss: 1.85771 - Test loss: 2.11734\n",
      "Epoch 2047 - lr: 0.05000 - Train loss: 1.85771 - Test loss: 2.11735\n",
      "Epoch 2048 - lr: 0.05000 - Train loss: 1.85771 - Test loss: 2.11736\n",
      "Epoch 2049 - lr: 0.05000 - Train loss: 1.85770 - Test loss: 2.11737\n",
      "Epoch 2050 - lr: 0.05000 - Train loss: 1.85770 - Test loss: 2.11738\n",
      "Epoch 2051 - lr: 0.05000 - Train loss: 1.85770 - Test loss: 2.11740\n",
      "Epoch 2052 - lr: 0.05000 - Train loss: 1.85770 - Test loss: 2.11741\n",
      "Epoch 2053 - lr: 0.05000 - Train loss: 1.85769 - Test loss: 2.11742\n",
      "Epoch 2054 - lr: 0.05000 - Train loss: 1.85769 - Test loss: 2.11743\n",
      "Epoch 2055 - lr: 0.05000 - Train loss: 1.85769 - Test loss: 2.11744\n",
      "Epoch 2056 - lr: 0.05000 - Train loss: 1.85769 - Test loss: 2.11746\n",
      "Epoch 2057 - lr: 0.05000 - Train loss: 1.85768 - Test loss: 2.11747\n",
      "Epoch 2058 - lr: 0.05000 - Train loss: 1.85768 - Test loss: 2.11748\n",
      "Epoch 2059 - lr: 0.05000 - Train loss: 1.85768 - Test loss: 2.11749\n",
      "Epoch 2060 - lr: 0.05000 - Train loss: 1.85768 - Test loss: 2.11750\n",
      "Epoch 2061 - lr: 0.05000 - Train loss: 1.85767 - Test loss: 2.11751\n",
      "Epoch 2062 - lr: 0.05000 - Train loss: 1.85767 - Test loss: 2.11753\n",
      "Epoch 2063 - lr: 0.05000 - Train loss: 1.85767 - Test loss: 2.11754\n",
      "Epoch 2064 - lr: 0.05000 - Train loss: 1.85767 - Test loss: 2.11755\n",
      "Epoch 2065 - lr: 0.05000 - Train loss: 1.85766 - Test loss: 2.11756\n",
      "Epoch 2066 - lr: 0.05000 - Train loss: 1.85766 - Test loss: 2.11757\n",
      "Epoch 2067 - lr: 0.05000 - Train loss: 1.85766 - Test loss: 2.11758\n",
      "Epoch 2068 - lr: 0.05000 - Train loss: 1.85766 - Test loss: 2.11760\n",
      "Epoch 2069 - lr: 0.05000 - Train loss: 1.85765 - Test loss: 2.11761\n",
      "Epoch 2070 - lr: 0.05000 - Train loss: 1.85765 - Test loss: 2.11762\n",
      "Epoch 2071 - lr: 0.05000 - Train loss: 1.85765 - Test loss: 2.11763\n",
      "Epoch 2072 - lr: 0.05000 - Train loss: 1.85765 - Test loss: 2.11764\n",
      "Epoch 2073 - lr: 0.05000 - Train loss: 1.85764 - Test loss: 2.11765\n",
      "Epoch 2074 - lr: 0.05000 - Train loss: 1.85764 - Test loss: 2.11767\n",
      "Epoch 2075 - lr: 0.05000 - Train loss: 1.85764 - Test loss: 2.11768\n",
      "Epoch 2076 - lr: 0.05000 - Train loss: 1.85764 - Test loss: 2.11769\n",
      "Epoch 2077 - lr: 0.05000 - Train loss: 1.85763 - Test loss: 2.11770\n",
      "Epoch 2078 - lr: 0.05000 - Train loss: 1.85763 - Test loss: 2.11771\n",
      "Epoch 2079 - lr: 0.05000 - Train loss: 1.85763 - Test loss: 2.11772\n",
      "Epoch 2080 - lr: 0.05000 - Train loss: 1.85763 - Test loss: 2.11773\n",
      "Epoch 2081 - lr: 0.05000 - Train loss: 1.85762 - Test loss: 2.11775\n",
      "Epoch 2082 - lr: 0.05000 - Train loss: 1.85762 - Test loss: 2.11776\n",
      "Epoch 2083 - lr: 0.05000 - Train loss: 1.85762 - Test loss: 2.11777\n",
      "Epoch 2084 - lr: 0.05000 - Train loss: 1.85762 - Test loss: 2.11778\n",
      "Epoch 2085 - lr: 0.05000 - Train loss: 1.85761 - Test loss: 2.11779\n",
      "Epoch 2086 - lr: 0.05000 - Train loss: 1.85761 - Test loss: 2.11780\n",
      "Epoch 2087 - lr: 0.05000 - Train loss: 1.85761 - Test loss: 2.11781\n",
      "Epoch 2088 - lr: 0.05000 - Train loss: 1.85761 - Test loss: 2.11783\n",
      "Epoch 2089 - lr: 0.05000 - Train loss: 1.85760 - Test loss: 2.11784\n",
      "Epoch 2090 - lr: 0.05000 - Train loss: 1.85760 - Test loss: 2.11785\n",
      "Epoch 2091 - lr: 0.05000 - Train loss: 1.85760 - Test loss: 2.11786\n",
      "Epoch 2092 - lr: 0.05000 - Train loss: 1.85760 - Test loss: 2.11787\n",
      "Epoch 2093 - lr: 0.05000 - Train loss: 1.85759 - Test loss: 2.11788\n",
      "Epoch 2094 - lr: 0.05000 - Train loss: 1.85759 - Test loss: 2.11789\n",
      "Epoch 2095 - lr: 0.05000 - Train loss: 1.85759 - Test loss: 2.11790\n",
      "Epoch 2096 - lr: 0.05000 - Train loss: 1.85759 - Test loss: 2.11792\n",
      "Epoch 2097 - lr: 0.05000 - Train loss: 1.85758 - Test loss: 2.11793\n",
      "Epoch 2098 - lr: 0.05000 - Train loss: 1.85758 - Test loss: 2.11794\n",
      "Epoch 2099 - lr: 0.05000 - Train loss: 1.85758 - Test loss: 2.11795\n",
      "Epoch 2100 - lr: 0.05000 - Train loss: 1.85757 - Test loss: 2.11796\n",
      "Epoch 2101 - lr: 0.05000 - Train loss: 1.85757 - Test loss: 2.11797\n",
      "Epoch 2102 - lr: 0.05000 - Train loss: 1.85757 - Test loss: 2.11798\n",
      "Epoch 2103 - lr: 0.05000 - Train loss: 1.85757 - Test loss: 2.11799\n",
      "Epoch 2104 - lr: 0.05000 - Train loss: 1.85756 - Test loss: 2.11800\n",
      "Epoch 2105 - lr: 0.05000 - Train loss: 1.85756 - Test loss: 2.11802\n",
      "Epoch 2106 - lr: 0.05000 - Train loss: 1.85756 - Test loss: 2.11803\n",
      "Epoch 2107 - lr: 0.05000 - Train loss: 1.85756 - Test loss: 2.11804\n",
      "Epoch 2108 - lr: 0.05000 - Train loss: 1.85755 - Test loss: 2.11805\n",
      "Epoch 2109 - lr: 0.05000 - Train loss: 1.85755 - Test loss: 2.11806\n",
      "Epoch 2110 - lr: 0.05000 - Train loss: 1.85755 - Test loss: 2.11807\n",
      "Epoch 2111 - lr: 0.05000 - Train loss: 1.85755 - Test loss: 2.11808\n",
      "Epoch 2112 - lr: 0.05000 - Train loss: 1.85754 - Test loss: 2.11809\n",
      "Epoch 2113 - lr: 0.05000 - Train loss: 1.85754 - Test loss: 2.11810\n",
      "Epoch 2114 - lr: 0.05000 - Train loss: 1.85754 - Test loss: 2.11811\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2115 - lr: 0.05000 - Train loss: 1.85754 - Test loss: 2.11812\n",
      "Epoch 2116 - lr: 0.05000 - Train loss: 1.85753 - Test loss: 2.11813\n",
      "Epoch 2117 - lr: 0.05000 - Train loss: 1.85753 - Test loss: 2.11814\n",
      "Epoch 2118 - lr: 0.05000 - Train loss: 1.85753 - Test loss: 2.11816\n",
      "Epoch 2119 - lr: 0.05000 - Train loss: 1.85752 - Test loss: 2.11817\n",
      "Epoch 2120 - lr: 0.05000 - Train loss: 1.85752 - Test loss: 2.11818\n",
      "Epoch 2121 - lr: 0.05000 - Train loss: 1.85752 - Test loss: 2.11819\n",
      "Epoch 2122 - lr: 0.05000 - Train loss: 1.85752 - Test loss: 2.11820\n",
      "Epoch 2123 - lr: 0.05000 - Train loss: 1.85751 - Test loss: 2.11821\n",
      "Epoch 2124 - lr: 0.05000 - Train loss: 1.85751 - Test loss: 2.11822\n",
      "Epoch 2125 - lr: 0.05000 - Train loss: 1.85751 - Test loss: 2.11823\n",
      "Epoch 2126 - lr: 0.05000 - Train loss: 1.85751 - Test loss: 2.11824\n",
      "Epoch 2127 - lr: 0.05000 - Train loss: 1.85750 - Test loss: 2.11825\n",
      "Epoch 2128 - lr: 0.05000 - Train loss: 1.85750 - Test loss: 2.11826\n",
      "Epoch 2129 - lr: 0.05000 - Train loss: 1.85750 - Test loss: 2.11827\n",
      "Epoch 2130 - lr: 0.05000 - Train loss: 1.85749 - Test loss: 2.11828\n",
      "Epoch 2131 - lr: 0.05000 - Train loss: 1.85749 - Test loss: 2.11829\n",
      "Epoch 2132 - lr: 0.05000 - Train loss: 1.85749 - Test loss: 2.11830\n",
      "Epoch 2133 - lr: 0.05000 - Train loss: 1.85749 - Test loss: 2.11831\n",
      "Epoch 2134 - lr: 0.05000 - Train loss: 1.85748 - Test loss: 2.11832\n",
      "Epoch 2135 - lr: 0.05000 - Train loss: 1.85748 - Test loss: 2.11833\n",
      "Epoch 2136 - lr: 0.05000 - Train loss: 1.85748 - Test loss: 2.11834\n",
      "Epoch 2137 - lr: 0.05000 - Train loss: 1.85747 - Test loss: 2.11835\n",
      "Epoch 2138 - lr: 0.05000 - Train loss: 1.85747 - Test loss: 2.11836\n",
      "Epoch 2139 - lr: 0.05000 - Train loss: 1.85747 - Test loss: 2.11836\n",
      "Epoch 2140 - lr: 0.05000 - Train loss: 1.85746 - Test loss: 2.11837\n",
      "Epoch 2141 - lr: 0.05000 - Train loss: 1.85746 - Test loss: 2.11838\n",
      "Epoch 2142 - lr: 0.05000 - Train loss: 1.85746 - Test loss: 2.11839\n",
      "Epoch 2143 - lr: 0.05000 - Train loss: 1.85745 - Test loss: 2.11840\n",
      "Epoch 2144 - lr: 0.05000 - Train loss: 1.85745 - Test loss: 2.11841\n",
      "Epoch 2145 - lr: 0.05000 - Train loss: 1.85745 - Test loss: 2.11842\n",
      "Epoch 2146 - lr: 0.05000 - Train loss: 1.85744 - Test loss: 2.11843\n",
      "Epoch 2147 - lr: 0.05000 - Train loss: 1.85744 - Test loss: 2.11843\n",
      "Epoch 2148 - lr: 0.05000 - Train loss: 1.85744 - Test loss: 2.11844\n",
      "Epoch 2149 - lr: 0.05000 - Train loss: 1.85743 - Test loss: 2.11845\n",
      "Epoch 2150 - lr: 0.05000 - Train loss: 1.85743 - Test loss: 2.11845\n",
      "Epoch 2151 - lr: 0.05000 - Train loss: 1.85743 - Test loss: 2.11846\n",
      "Epoch 2152 - lr: 0.05000 - Train loss: 1.85742 - Test loss: 2.11847\n",
      "Epoch 2153 - lr: 0.05000 - Train loss: 1.85742 - Test loss: 2.11847\n",
      "Epoch 2154 - lr: 0.05000 - Train loss: 1.85741 - Test loss: 2.11848\n",
      "Epoch 2155 - lr: 0.05000 - Train loss: 1.85741 - Test loss: 2.11848\n",
      "Epoch 2156 - lr: 0.05000 - Train loss: 1.85740 - Test loss: 2.11848\n",
      "Epoch 2157 - lr: 0.05000 - Train loss: 1.85740 - Test loss: 2.11849\n",
      "Epoch 2158 - lr: 0.05000 - Train loss: 1.85739 - Test loss: 2.11849\n",
      "Epoch 2159 - lr: 0.05000 - Train loss: 1.85738 - Test loss: 2.11849\n",
      "Epoch 2160 - lr: 0.05000 - Train loss: 1.85737 - Test loss: 2.11848\n",
      "Epoch 2161 - lr: 0.05000 - Train loss: 1.85736 - Test loss: 2.11848\n",
      "Epoch 2162 - lr: 0.05000 - Train loss: 1.85735 - Test loss: 2.11847\n",
      "Epoch 2163 - lr: 0.05000 - Train loss: 1.85734 - Test loss: 2.11845\n",
      "Epoch 2164 - lr: 0.05000 - Train loss: 1.85732 - Test loss: 2.11843\n",
      "Epoch 2165 - lr: 0.05000 - Train loss: 1.85729 - Test loss: 2.11839\n",
      "Epoch 2166 - lr: 0.05000 - Train loss: 1.85726 - Test loss: 2.11833\n",
      "Epoch 2167 - lr: 0.05000 - Train loss: 1.85720 - Test loss: 2.11821\n",
      "Epoch 2168 - lr: 0.05000 - Train loss: 1.85707 - Test loss: 2.11790\n",
      "Epoch 2169 - lr: 0.05000 - Train loss: 1.85657 - Test loss: 2.11586\n",
      "Epoch 2170 - lr: 0.05000 - Train loss: 1.78693 - Test loss: 2.10444\n",
      "Epoch 2171 - lr: 0.05000 - Train loss: 1.85693 - Test loss: 2.11788\n",
      "Epoch 2172 - lr: 0.05000 - Train loss: 1.85739 - Test loss: 2.11893\n",
      "Epoch 2173 - lr: 0.05000 - Train loss: 1.85743 - Test loss: 2.11902\n",
      "Epoch 2174 - lr: 0.05000 - Train loss: 1.85742 - Test loss: 2.11902\n",
      "Epoch 2175 - lr: 0.05000 - Train loss: 1.85742 - Test loss: 2.11903\n",
      "Epoch 2176 - lr: 0.05000 - Train loss: 1.85741 - Test loss: 2.11903\n",
      "Epoch 2177 - lr: 0.05000 - Train loss: 1.85740 - Test loss: 2.11904\n",
      "Epoch 2178 - lr: 0.05000 - Train loss: 1.85740 - Test loss: 2.11905\n",
      "Epoch 2179 - lr: 0.05000 - Train loss: 1.85739 - Test loss: 2.11905\n",
      "Epoch 2180 - lr: 0.05000 - Train loss: 1.85739 - Test loss: 2.11906\n",
      "Epoch 2181 - lr: 0.05000 - Train loss: 1.85739 - Test loss: 2.11907\n",
      "Epoch 2182 - lr: 0.05000 - Train loss: 1.85738 - Test loss: 2.11908\n",
      "Epoch 2183 - lr: 0.05000 - Train loss: 1.85738 - Test loss: 2.11909\n",
      "Epoch 2184 - lr: 0.05000 - Train loss: 1.85738 - Test loss: 2.11909\n",
      "Epoch 2185 - lr: 0.05000 - Train loss: 1.85737 - Test loss: 2.11910\n",
      "Epoch 2186 - lr: 0.05000 - Train loss: 1.85737 - Test loss: 2.11911\n",
      "Epoch 2187 - lr: 0.05000 - Train loss: 1.85737 - Test loss: 2.11912\n",
      "Epoch 2188 - lr: 0.05000 - Train loss: 1.85736 - Test loss: 2.11913\n",
      "Epoch 2189 - lr: 0.05000 - Train loss: 1.85736 - Test loss: 2.11914\n",
      "Epoch 2190 - lr: 0.05000 - Train loss: 1.85736 - Test loss: 2.11915\n",
      "Epoch 2191 - lr: 0.05000 - Train loss: 1.85735 - Test loss: 2.11916\n",
      "Epoch 2192 - lr: 0.05000 - Train loss: 1.85735 - Test loss: 2.11917\n",
      "Epoch 2193 - lr: 0.05000 - Train loss: 1.85735 - Test loss: 2.11918\n",
      "Epoch 2194 - lr: 0.05000 - Train loss: 1.85735 - Test loss: 2.11919\n",
      "Epoch 2195 - lr: 0.05000 - Train loss: 1.85734 - Test loss: 2.11920\n",
      "Epoch 2196 - lr: 0.05000 - Train loss: 1.85734 - Test loss: 2.11921\n",
      "Epoch 2197 - lr: 0.05000 - Train loss: 1.85734 - Test loss: 2.11923\n",
      "Epoch 2198 - lr: 0.05000 - Train loss: 1.85734 - Test loss: 2.11924\n",
      "Epoch 2199 - lr: 0.05000 - Train loss: 1.85733 - Test loss: 2.11925\n",
      "Epoch 2200 - lr: 0.05000 - Train loss: 1.85733 - Test loss: 2.11926\n",
      "Epoch 2201 - lr: 0.05000 - Train loss: 1.85733 - Test loss: 2.11927\n",
      "Epoch 2202 - lr: 0.05000 - Train loss: 1.85733 - Test loss: 2.11928\n",
      "Epoch 2203 - lr: 0.05000 - Train loss: 1.85732 - Test loss: 2.11929\n",
      "Epoch 2204 - lr: 0.05000 - Train loss: 1.85732 - Test loss: 2.11930\n",
      "Epoch 2205 - lr: 0.05000 - Train loss: 1.85732 - Test loss: 2.11931\n",
      "Epoch 2206 - lr: 0.05000 - Train loss: 1.85732 - Test loss: 2.11932\n",
      "Epoch 2207 - lr: 0.05000 - Train loss: 1.85732 - Test loss: 2.11933\n",
      "Epoch 2208 - lr: 0.05000 - Train loss: 1.85731 - Test loss: 2.11934\n",
      "Epoch 2209 - lr: 0.05000 - Train loss: 1.85731 - Test loss: 2.11935\n",
      "Epoch 2210 - lr: 0.05000 - Train loss: 1.85731 - Test loss: 2.11936\n",
      "Epoch 2211 - lr: 0.05000 - Train loss: 1.85731 - Test loss: 2.11937\n",
      "Epoch 2212 - lr: 0.05000 - Train loss: 1.85730 - Test loss: 2.11939\n",
      "Epoch 2213 - lr: 0.05000 - Train loss: 1.85730 - Test loss: 2.11940\n",
      "Epoch 2214 - lr: 0.05000 - Train loss: 1.85730 - Test loss: 2.11941\n",
      "Epoch 2215 - lr: 0.05000 - Train loss: 1.85730 - Test loss: 2.11942\n",
      "Epoch 2216 - lr: 0.05000 - Train loss: 1.85729 - Test loss: 2.11943\n",
      "Epoch 2217 - lr: 0.05000 - Train loss: 1.85729 - Test loss: 2.11944\n",
      "Epoch 2218 - lr: 0.05000 - Train loss: 1.85729 - Test loss: 2.11945\n",
      "Epoch 2219 - lr: 0.05000 - Train loss: 1.85729 - Test loss: 2.11946\n",
      "Epoch 2220 - lr: 0.05000 - Train loss: 1.85729 - Test loss: 2.11947\n",
      "Epoch 2221 - lr: 0.05000 - Train loss: 1.85728 - Test loss: 2.11948\n",
      "Epoch 2222 - lr: 0.05000 - Train loss: 1.85728 - Test loss: 2.11949\n",
      "Epoch 2223 - lr: 0.05000 - Train loss: 1.85728 - Test loss: 2.11950\n",
      "Epoch 2224 - lr: 0.05000 - Train loss: 1.85728 - Test loss: 2.11951\n",
      "Epoch 2225 - lr: 0.05000 - Train loss: 1.85727 - Test loss: 2.11953\n",
      "Epoch 2226 - lr: 0.05000 - Train loss: 1.85727 - Test loss: 2.11954\n",
      "Epoch 2227 - lr: 0.05000 - Train loss: 1.85727 - Test loss: 2.11955\n",
      "Epoch 2228 - lr: 0.05000 - Train loss: 1.85727 - Test loss: 2.11956\n",
      "Epoch 2229 - lr: 0.05000 - Train loss: 1.85726 - Test loss: 2.11957\n",
      "Epoch 2230 - lr: 0.05000 - Train loss: 1.85726 - Test loss: 2.11958\n",
      "Epoch 2231 - lr: 0.05000 - Train loss: 1.85726 - Test loss: 2.11959\n",
      "Epoch 2232 - lr: 0.05000 - Train loss: 1.85726 - Test loss: 2.11960\n",
      "Epoch 2233 - lr: 0.05000 - Train loss: 1.85726 - Test loss: 2.11961\n",
      "Epoch 2234 - lr: 0.05000 - Train loss: 1.85725 - Test loss: 2.11962\n",
      "Epoch 2235 - lr: 0.05000 - Train loss: 1.85725 - Test loss: 2.11963\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2236 - lr: 0.05000 - Train loss: 1.85725 - Test loss: 2.11964\n",
      "Epoch 2237 - lr: 0.05000 - Train loss: 1.85725 - Test loss: 2.11965\n",
      "Epoch 2238 - lr: 0.05000 - Train loss: 1.85724 - Test loss: 2.11966\n",
      "Epoch 2239 - lr: 0.05000 - Train loss: 1.85724 - Test loss: 2.11967\n",
      "Epoch 2240 - lr: 0.05000 - Train loss: 1.85724 - Test loss: 2.11968\n",
      "Epoch 2241 - lr: 0.05000 - Train loss: 1.85724 - Test loss: 2.11970\n",
      "Epoch 2242 - lr: 0.05000 - Train loss: 1.85723 - Test loss: 2.11971\n",
      "Epoch 2243 - lr: 0.05000 - Train loss: 1.85723 - Test loss: 2.11972\n",
      "Epoch 2244 - lr: 0.05000 - Train loss: 1.85723 - Test loss: 2.11973\n",
      "Epoch 2245 - lr: 0.05000 - Train loss: 1.85723 - Test loss: 2.11974\n",
      "Epoch 2246 - lr: 0.05000 - Train loss: 1.85723 - Test loss: 2.11975\n",
      "Epoch 2247 - lr: 0.05000 - Train loss: 1.85722 - Test loss: 2.11976\n",
      "Epoch 2248 - lr: 0.05000 - Train loss: 1.85722 - Test loss: 2.11977\n",
      "Epoch 2249 - lr: 0.05000 - Train loss: 1.85722 - Test loss: 2.11978\n",
      "Epoch 2250 - lr: 0.05000 - Train loss: 1.85722 - Test loss: 2.11979\n",
      "Epoch 2251 - lr: 0.05000 - Train loss: 1.85721 - Test loss: 2.11980\n",
      "Epoch 2252 - lr: 0.05000 - Train loss: 1.85721 - Test loss: 2.11981\n",
      "Epoch 2253 - lr: 0.05000 - Train loss: 1.85721 - Test loss: 2.11982\n",
      "Epoch 2254 - lr: 0.05000 - Train loss: 1.85721 - Test loss: 2.11983\n",
      "Epoch 2255 - lr: 0.05000 - Train loss: 1.85721 - Test loss: 2.11984\n",
      "Epoch 2256 - lr: 0.05000 - Train loss: 1.85720 - Test loss: 2.11985\n",
      "Epoch 2257 - lr: 0.05000 - Train loss: 1.85720 - Test loss: 2.11986\n",
      "Epoch 2258 - lr: 0.05000 - Train loss: 1.85720 - Test loss: 2.11987\n",
      "Epoch 2259 - lr: 0.05000 - Train loss: 1.85720 - Test loss: 2.11988\n",
      "Epoch 2260 - lr: 0.05000 - Train loss: 1.85719 - Test loss: 2.11990\n",
      "Epoch 2261 - lr: 0.05000 - Train loss: 1.85719 - Test loss: 2.11991\n",
      "Epoch 2262 - lr: 0.05000 - Train loss: 1.85719 - Test loss: 2.11992\n",
      "Epoch 2263 - lr: 0.05000 - Train loss: 1.85719 - Test loss: 2.11993\n",
      "Epoch 2264 - lr: 0.05000 - Train loss: 1.85718 - Test loss: 2.11994\n",
      "Epoch 2265 - lr: 0.05000 - Train loss: 1.85718 - Test loss: 2.11995\n",
      "Epoch 2266 - lr: 0.05000 - Train loss: 1.85718 - Test loss: 2.11996\n",
      "Epoch 2267 - lr: 0.05000 - Train loss: 1.85718 - Test loss: 2.11997\n",
      "Epoch 2268 - lr: 0.05000 - Train loss: 1.85718 - Test loss: 2.11998\n",
      "Epoch 2269 - lr: 0.05000 - Train loss: 1.85717 - Test loss: 2.11999\n",
      "Epoch 2270 - lr: 0.05000 - Train loss: 1.85717 - Test loss: 2.12000\n",
      "Epoch 2271 - lr: 0.05000 - Train loss: 1.85717 - Test loss: 2.12001\n",
      "Epoch 2272 - lr: 0.05000 - Train loss: 1.85717 - Test loss: 2.12002\n",
      "Epoch 2273 - lr: 0.05000 - Train loss: 1.85716 - Test loss: 2.12003\n",
      "Epoch 2274 - lr: 0.05000 - Train loss: 1.85716 - Test loss: 2.12004\n",
      "Epoch 2275 - lr: 0.05000 - Train loss: 1.85716 - Test loss: 2.12005\n",
      "Epoch 2276 - lr: 0.05000 - Train loss: 1.85716 - Test loss: 2.12006\n",
      "Epoch 2277 - lr: 0.05000 - Train loss: 1.85715 - Test loss: 2.12007\n",
      "Epoch 2278 - lr: 0.05000 - Train loss: 1.85715 - Test loss: 2.12008\n",
      "Epoch 2279 - lr: 0.05000 - Train loss: 1.85715 - Test loss: 2.12009\n",
      "Epoch 2280 - lr: 0.05000 - Train loss: 1.85715 - Test loss: 2.12010\n",
      "Epoch 2281 - lr: 0.05000 - Train loss: 1.85715 - Test loss: 2.12011\n",
      "Epoch 2282 - lr: 0.05000 - Train loss: 1.85714 - Test loss: 2.12012\n",
      "Epoch 2283 - lr: 0.05000 - Train loss: 1.85714 - Test loss: 2.12013\n",
      "Epoch 2284 - lr: 0.05000 - Train loss: 1.85714 - Test loss: 2.12014\n",
      "Epoch 2285 - lr: 0.05000 - Train loss: 1.85714 - Test loss: 2.12016\n",
      "Epoch 2286 - lr: 0.05000 - Train loss: 1.85713 - Test loss: 2.12017\n",
      "Epoch 2287 - lr: 0.05000 - Train loss: 1.85713 - Test loss: 2.12018\n",
      "Epoch 2288 - lr: 0.05000 - Train loss: 1.85713 - Test loss: 2.12019\n",
      "Epoch 2289 - lr: 0.05000 - Train loss: 1.85713 - Test loss: 2.12020\n",
      "Epoch 2290 - lr: 0.05000 - Train loss: 1.85713 - Test loss: 2.12021\n",
      "Epoch 2291 - lr: 0.05000 - Train loss: 1.85712 - Test loss: 2.12022\n",
      "Epoch 2292 - lr: 0.05000 - Train loss: 1.85712 - Test loss: 2.12023\n",
      "Epoch 2293 - lr: 0.05000 - Train loss: 1.85712 - Test loss: 2.12024\n",
      "Epoch 2294 - lr: 0.05000 - Train loss: 1.85712 - Test loss: 2.12025\n",
      "Epoch 2295 - lr: 0.05000 - Train loss: 1.85711 - Test loss: 2.12026\n",
      "Epoch 2296 - lr: 0.05000 - Train loss: 1.85711 - Test loss: 2.12027\n",
      "Epoch 2297 - lr: 0.05000 - Train loss: 1.85711 - Test loss: 2.12028\n",
      "Epoch 2298 - lr: 0.05000 - Train loss: 1.85711 - Test loss: 2.12029\n",
      "Epoch 2299 - lr: 0.05000 - Train loss: 1.85710 - Test loss: 2.12030\n",
      "Epoch 2300 - lr: 0.05000 - Train loss: 1.85710 - Test loss: 2.12031\n",
      "Epoch 2301 - lr: 0.05000 - Train loss: 1.85710 - Test loss: 2.12032\n",
      "Epoch 2302 - lr: 0.05000 - Train loss: 1.85710 - Test loss: 2.12033\n",
      "Epoch 2303 - lr: 0.05000 - Train loss: 1.85710 - Test loss: 2.12034\n",
      "Epoch 2304 - lr: 0.05000 - Train loss: 1.85709 - Test loss: 2.12035\n",
      "Epoch 2305 - lr: 0.05000 - Train loss: 1.85709 - Test loss: 2.12036\n",
      "Epoch 2306 - lr: 0.05000 - Train loss: 1.85709 - Test loss: 2.12037\n",
      "Epoch 2307 - lr: 0.05000 - Train loss: 1.85709 - Test loss: 2.12038\n",
      "Epoch 2308 - lr: 0.05000 - Train loss: 1.85708 - Test loss: 2.12039\n",
      "Epoch 2309 - lr: 0.05000 - Train loss: 1.85708 - Test loss: 2.12040\n",
      "Epoch 2310 - lr: 0.05000 - Train loss: 1.85708 - Test loss: 2.12041\n",
      "Epoch 2311 - lr: 0.05000 - Train loss: 1.85708 - Test loss: 2.12042\n",
      "Epoch 2312 - lr: 0.05000 - Train loss: 1.85707 - Test loss: 2.12043\n",
      "Epoch 2313 - lr: 0.05000 - Train loss: 1.85707 - Test loss: 2.12044\n",
      "Epoch 2314 - lr: 0.05000 - Train loss: 1.85707 - Test loss: 2.12045\n",
      "Epoch 2315 - lr: 0.05000 - Train loss: 1.85707 - Test loss: 2.12046\n",
      "Epoch 2316 - lr: 0.05000 - Train loss: 1.85707 - Test loss: 2.12047\n",
      "Epoch 2317 - lr: 0.05000 - Train loss: 1.85706 - Test loss: 2.12048\n",
      "Epoch 2318 - lr: 0.05000 - Train loss: 1.85706 - Test loss: 2.12049\n",
      "Epoch 2319 - lr: 0.05000 - Train loss: 1.85706 - Test loss: 2.12050\n",
      "Epoch 2320 - lr: 0.05000 - Train loss: 1.85706 - Test loss: 2.12051\n",
      "Epoch 2321 - lr: 0.05000 - Train loss: 1.85705 - Test loss: 2.12052\n",
      "Epoch 2322 - lr: 0.05000 - Train loss: 1.85705 - Test loss: 2.12053\n",
      "Epoch 2323 - lr: 0.05000 - Train loss: 1.85705 - Test loss: 2.12054\n",
      "Epoch 2324 - lr: 0.05000 - Train loss: 1.85705 - Test loss: 2.12055\n",
      "Epoch 2325 - lr: 0.05000 - Train loss: 1.85705 - Test loss: 2.12056\n",
      "Epoch 2326 - lr: 0.05000 - Train loss: 1.85704 - Test loss: 2.12057\n",
      "Epoch 2327 - lr: 0.05000 - Train loss: 1.85704 - Test loss: 2.12058\n",
      "Epoch 2328 - lr: 0.05000 - Train loss: 1.85704 - Test loss: 2.12059\n",
      "Epoch 2329 - lr: 0.05000 - Train loss: 1.85704 - Test loss: 2.12060\n",
      "Epoch 2330 - lr: 0.05000 - Train loss: 1.85703 - Test loss: 2.12061\n",
      "Epoch 2331 - lr: 0.05000 - Train loss: 1.85703 - Test loss: 2.12062\n",
      "Epoch 2332 - lr: 0.05000 - Train loss: 1.85703 - Test loss: 2.12063\n",
      "Epoch 2333 - lr: 0.05000 - Train loss: 1.85703 - Test loss: 2.12064\n",
      "Epoch 2334 - lr: 0.05000 - Train loss: 1.85702 - Test loss: 2.12065\n",
      "Epoch 2335 - lr: 0.05000 - Train loss: 1.85702 - Test loss: 2.12066\n",
      "Epoch 2336 - lr: 0.05000 - Train loss: 1.85702 - Test loss: 2.12067\n",
      "Epoch 2337 - lr: 0.05000 - Train loss: 1.85702 - Test loss: 2.12068\n",
      "Epoch 2338 - lr: 0.05000 - Train loss: 1.85702 - Test loss: 2.12069\n",
      "Epoch 2339 - lr: 0.05000 - Train loss: 1.85701 - Test loss: 2.12070\n",
      "Epoch 2340 - lr: 0.05000 - Train loss: 1.85701 - Test loss: 2.12071\n",
      "Epoch 2341 - lr: 0.05000 - Train loss: 1.85701 - Test loss: 2.12072\n",
      "Epoch 2342 - lr: 0.05000 - Train loss: 1.85701 - Test loss: 2.12073\n",
      "Epoch 2343 - lr: 0.05000 - Train loss: 1.85700 - Test loss: 2.12074\n",
      "Epoch 2344 - lr: 0.05000 - Train loss: 1.85700 - Test loss: 2.12075\n",
      "Epoch 2345 - lr: 0.05000 - Train loss: 1.85700 - Test loss: 2.12076\n",
      "Epoch 2346 - lr: 0.05000 - Train loss: 1.85700 - Test loss: 2.12077\n",
      "Epoch 2347 - lr: 0.05000 - Train loss: 1.85700 - Test loss: 2.12078\n",
      "Epoch 2348 - lr: 0.05000 - Train loss: 1.85699 - Test loss: 2.12079\n",
      "Epoch 2349 - lr: 0.05000 - Train loss: 1.85699 - Test loss: 2.12080\n",
      "Epoch 2350 - lr: 0.05000 - Train loss: 1.85699 - Test loss: 2.12081\n",
      "Epoch 2351 - lr: 0.05000 - Train loss: 1.85699 - Test loss: 2.12082\n",
      "Epoch 2352 - lr: 0.05000 - Train loss: 1.85698 - Test loss: 2.12083\n",
      "Epoch 2353 - lr: 0.05000 - Train loss: 1.85698 - Test loss: 2.12084\n",
      "Epoch 2354 - lr: 0.05000 - Train loss: 1.85698 - Test loss: 2.12085\n",
      "Epoch 2355 - lr: 0.05000 - Train loss: 1.85698 - Test loss: 2.12086\n",
      "Epoch 2356 - lr: 0.05000 - Train loss: 1.85697 - Test loss: 2.12087\n",
      "Epoch 2357 - lr: 0.05000 - Train loss: 1.85697 - Test loss: 2.12088\n",
      "Epoch 2358 - lr: 0.05000 - Train loss: 1.85697 - Test loss: 2.12089\n",
      "Epoch 2359 - lr: 0.05000 - Train loss: 1.85697 - Test loss: 2.12090\n",
      "Epoch 2360 - lr: 0.05000 - Train loss: 1.85697 - Test loss: 2.12091\n",
      "Epoch 2361 - lr: 0.05000 - Train loss: 1.85696 - Test loss: 2.12092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2362 - lr: 0.05000 - Train loss: 1.85696 - Test loss: 2.12093\n",
      "Epoch 2363 - lr: 0.05000 - Train loss: 1.85696 - Test loss: 2.12094\n",
      "Epoch 2364 - lr: 0.05000 - Train loss: 1.85696 - Test loss: 2.12095\n",
      "Epoch 2365 - lr: 0.05000 - Train loss: 1.85695 - Test loss: 2.12096\n",
      "Epoch 2366 - lr: 0.05000 - Train loss: 1.85695 - Test loss: 2.12097\n",
      "Epoch 2367 - lr: 0.05000 - Train loss: 1.85695 - Test loss: 2.12098\n",
      "Epoch 2368 - lr: 0.05000 - Train loss: 1.85695 - Test loss: 2.12099\n",
      "Epoch 2369 - lr: 0.05000 - Train loss: 1.85695 - Test loss: 2.12100\n",
      "Epoch 2370 - lr: 0.05000 - Train loss: 1.85694 - Test loss: 2.12101\n",
      "Epoch 2371 - lr: 0.05000 - Train loss: 1.85694 - Test loss: 2.12102\n",
      "Epoch 2372 - lr: 0.05000 - Train loss: 1.85694 - Test loss: 2.12103\n",
      "Epoch 2373 - lr: 0.05000 - Train loss: 1.85694 - Test loss: 2.12104\n",
      "Epoch 2374 - lr: 0.05000 - Train loss: 1.85693 - Test loss: 2.12105\n",
      "Epoch 2375 - lr: 0.05000 - Train loss: 1.85693 - Test loss: 2.12106\n",
      "Epoch 2376 - lr: 0.05000 - Train loss: 1.85693 - Test loss: 2.12107\n",
      "Epoch 2377 - lr: 0.05000 - Train loss: 1.85693 - Test loss: 2.12108\n",
      "Epoch 2378 - lr: 0.05000 - Train loss: 1.85692 - Test loss: 2.12109\n",
      "Epoch 2379 - lr: 0.05000 - Train loss: 1.85692 - Test loss: 2.12110\n",
      "Epoch 2380 - lr: 0.05000 - Train loss: 1.85692 - Test loss: 2.12111\n",
      "Epoch 2381 - lr: 0.05000 - Train loss: 1.85692 - Test loss: 2.12112\n",
      "Epoch 2382 - lr: 0.05000 - Train loss: 1.85692 - Test loss: 2.12113\n",
      "Epoch 2383 - lr: 0.05000 - Train loss: 1.85691 - Test loss: 2.12114\n",
      "Epoch 2384 - lr: 0.05000 - Train loss: 1.85691 - Test loss: 2.12115\n",
      "Epoch 2385 - lr: 0.05000 - Train loss: 1.85691 - Test loss: 2.12116\n",
      "Epoch 2386 - lr: 0.05000 - Train loss: 1.85691 - Test loss: 2.12117\n",
      "Epoch 2387 - lr: 0.05000 - Train loss: 1.85690 - Test loss: 2.12118\n",
      "Epoch 2388 - lr: 0.05000 - Train loss: 1.85690 - Test loss: 2.12119\n",
      "Epoch 2389 - lr: 0.05000 - Train loss: 1.85690 - Test loss: 2.12120\n",
      "Epoch 2390 - lr: 0.05000 - Train loss: 1.85690 - Test loss: 2.12121\n",
      "Epoch 2391 - lr: 0.05000 - Train loss: 1.85690 - Test loss: 2.12122\n",
      "Epoch 2392 - lr: 0.05000 - Train loss: 1.85689 - Test loss: 2.12123\n",
      "Epoch 2393 - lr: 0.05000 - Train loss: 1.85689 - Test loss: 2.12124\n",
      "Epoch 2394 - lr: 0.05000 - Train loss: 1.85689 - Test loss: 2.12125\n",
      "Epoch 2395 - lr: 0.05000 - Train loss: 1.85689 - Test loss: 2.12125\n",
      "Epoch 2396 - lr: 0.05000 - Train loss: 1.85688 - Test loss: 2.12126\n",
      "Epoch 2397 - lr: 0.05000 - Train loss: 1.85688 - Test loss: 2.12127\n",
      "Epoch 2398 - lr: 0.05000 - Train loss: 1.85688 - Test loss: 2.12128\n",
      "Epoch 2399 - lr: 0.05000 - Train loss: 1.85688 - Test loss: 2.12129\n",
      "Epoch 2400 - lr: 0.05000 - Train loss: 1.85688 - Test loss: 2.12130\n",
      "Epoch 2401 - lr: 0.05000 - Train loss: 1.85687 - Test loss: 2.12131\n",
      "Epoch 2402 - lr: 0.05000 - Train loss: 1.85687 - Test loss: 2.12132\n",
      "Epoch 2403 - lr: 0.05000 - Train loss: 1.85687 - Test loss: 2.12133\n",
      "Epoch 2404 - lr: 0.05000 - Train loss: 1.85687 - Test loss: 2.12134\n",
      "Epoch 2405 - lr: 0.05000 - Train loss: 1.85686 - Test loss: 2.12135\n",
      "Epoch 2406 - lr: 0.05000 - Train loss: 1.85686 - Test loss: 2.12136\n",
      "Epoch 2407 - lr: 0.05000 - Train loss: 1.85686 - Test loss: 2.12137\n",
      "Epoch 2408 - lr: 0.05000 - Train loss: 1.85686 - Test loss: 2.12138\n",
      "Epoch 2409 - lr: 0.05000 - Train loss: 1.85685 - Test loss: 2.12139\n",
      "Epoch 2410 - lr: 0.05000 - Train loss: 1.85685 - Test loss: 2.12140\n",
      "Epoch 2411 - lr: 0.05000 - Train loss: 1.85685 - Test loss: 2.12141\n",
      "Epoch 2412 - lr: 0.05000 - Train loss: 1.85685 - Test loss: 2.12142\n",
      "Epoch 2413 - lr: 0.05000 - Train loss: 1.85685 - Test loss: 2.12143\n",
      "Epoch 2414 - lr: 0.05000 - Train loss: 1.85684 - Test loss: 2.12144\n",
      "Epoch 2415 - lr: 0.05000 - Train loss: 1.85684 - Test loss: 2.12145\n",
      "Epoch 2416 - lr: 0.05000 - Train loss: 1.85684 - Test loss: 2.12146\n",
      "Epoch 2417 - lr: 0.05000 - Train loss: 1.85684 - Test loss: 2.12147\n",
      "Epoch 2418 - lr: 0.05000 - Train loss: 1.85683 - Test loss: 2.12148\n",
      "Epoch 2419 - lr: 0.05000 - Train loss: 1.85683 - Test loss: 2.12149\n",
      "Epoch 2420 - lr: 0.05000 - Train loss: 1.85683 - Test loss: 2.12150\n",
      "Epoch 2421 - lr: 0.05000 - Train loss: 1.85683 - Test loss: 2.12150\n",
      "Epoch 2422 - lr: 0.05000 - Train loss: 1.85683 - Test loss: 2.12151\n",
      "Epoch 2423 - lr: 0.05000 - Train loss: 1.85682 - Test loss: 2.12152\n",
      "Epoch 2424 - lr: 0.05000 - Train loss: 1.85682 - Test loss: 2.12153\n",
      "Epoch 2425 - lr: 0.05000 - Train loss: 1.85682 - Test loss: 2.12154\n",
      "Epoch 2426 - lr: 0.05000 - Train loss: 1.85682 - Test loss: 2.12155\n",
      "Epoch 2427 - lr: 0.05000 - Train loss: 1.85681 - Test loss: 2.12156\n",
      "Epoch 2428 - lr: 0.05000 - Train loss: 1.85681 - Test loss: 2.12157\n",
      "Epoch 2429 - lr: 0.05000 - Train loss: 1.85681 - Test loss: 2.12158\n",
      "Epoch 2430 - lr: 0.05000 - Train loss: 1.85681 - Test loss: 2.12159\n",
      "Epoch 2431 - lr: 0.05000 - Train loss: 1.85681 - Test loss: 2.12160\n",
      "Epoch 2432 - lr: 0.05000 - Train loss: 1.85680 - Test loss: 2.12161\n",
      "Epoch 2433 - lr: 0.05000 - Train loss: 1.85680 - Test loss: 2.12162\n",
      "Epoch 2434 - lr: 0.05000 - Train loss: 1.85680 - Test loss: 2.12163\n",
      "Epoch 2435 - lr: 0.05000 - Train loss: 1.85680 - Test loss: 2.12164\n",
      "Epoch 2436 - lr: 0.05000 - Train loss: 1.85679 - Test loss: 2.12165\n",
      "Epoch 2437 - lr: 0.05000 - Train loss: 1.85679 - Test loss: 2.12166\n",
      "Epoch 2438 - lr: 0.05000 - Train loss: 1.85679 - Test loss: 2.12167\n",
      "Epoch 2439 - lr: 0.05000 - Train loss: 1.85679 - Test loss: 2.12168\n",
      "Epoch 2440 - lr: 0.05000 - Train loss: 1.85679 - Test loss: 2.12168\n",
      "Epoch 2441 - lr: 0.05000 - Train loss: 1.85678 - Test loss: 2.12169\n",
      "Epoch 2442 - lr: 0.05000 - Train loss: 1.85678 - Test loss: 2.12170\n",
      "Epoch 2443 - lr: 0.05000 - Train loss: 1.85678 - Test loss: 2.12171\n",
      "Epoch 2444 - lr: 0.05000 - Train loss: 1.85678 - Test loss: 2.12172\n",
      "Epoch 2445 - lr: 0.05000 - Train loss: 1.85677 - Test loss: 2.12173\n",
      "Epoch 2446 - lr: 0.05000 - Train loss: 1.85677 - Test loss: 2.12174\n",
      "Epoch 2447 - lr: 0.05000 - Train loss: 1.85677 - Test loss: 2.12175\n",
      "Epoch 2448 - lr: 0.05000 - Train loss: 1.85677 - Test loss: 2.12176\n",
      "Epoch 2449 - lr: 0.05000 - Train loss: 1.85677 - Test loss: 2.12177\n",
      "Epoch 2450 - lr: 0.05000 - Train loss: 1.85676 - Test loss: 2.12178\n",
      "Epoch 2451 - lr: 0.05000 - Train loss: 1.85676 - Test loss: 2.12179\n",
      "Epoch 2452 - lr: 0.05000 - Train loss: 1.85676 - Test loss: 2.12180\n",
      "Epoch 2453 - lr: 0.05000 - Train loss: 1.85676 - Test loss: 2.12181\n",
      "Epoch 2454 - lr: 0.05000 - Train loss: 1.85675 - Test loss: 2.12182\n",
      "Epoch 2455 - lr: 0.05000 - Train loss: 1.85675 - Test loss: 2.12183\n",
      "Epoch 2456 - lr: 0.05000 - Train loss: 1.85675 - Test loss: 2.12183\n",
      "Epoch 2457 - lr: 0.05000 - Train loss: 1.85675 - Test loss: 2.12184\n",
      "Epoch 2458 - lr: 0.05000 - Train loss: 1.85674 - Test loss: 2.12185\n",
      "Epoch 2459 - lr: 0.05000 - Train loss: 1.85674 - Test loss: 2.12186\n",
      "Epoch 2460 - lr: 0.05000 - Train loss: 1.85674 - Test loss: 2.12187\n",
      "Epoch 2461 - lr: 0.05000 - Train loss: 1.85674 - Test loss: 2.12188\n",
      "Epoch 2462 - lr: 0.05000 - Train loss: 1.85674 - Test loss: 2.12189\n",
      "Epoch 2463 - lr: 0.05000 - Train loss: 1.85673 - Test loss: 2.12190\n",
      "Epoch 2464 - lr: 0.05000 - Train loss: 1.85673 - Test loss: 2.12191\n",
      "Epoch 2465 - lr: 0.05000 - Train loss: 1.85673 - Test loss: 2.12192\n",
      "Epoch 2466 - lr: 0.05000 - Train loss: 1.85673 - Test loss: 2.12193\n",
      "Epoch 2467 - lr: 0.05000 - Train loss: 1.85672 - Test loss: 2.12194\n",
      "Epoch 2468 - lr: 0.05000 - Train loss: 1.85672 - Test loss: 2.12195\n",
      "Epoch 2469 - lr: 0.05000 - Train loss: 1.85672 - Test loss: 2.12196\n",
      "Epoch 2470 - lr: 0.05000 - Train loss: 1.85672 - Test loss: 2.12196\n",
      "Epoch 2471 - lr: 0.05000 - Train loss: 1.85672 - Test loss: 2.12197\n",
      "Epoch 2472 - lr: 0.05000 - Train loss: 1.85671 - Test loss: 2.12198\n",
      "Epoch 2473 - lr: 0.05000 - Train loss: 1.85671 - Test loss: 2.12199\n",
      "Epoch 2474 - lr: 0.05000 - Train loss: 1.85671 - Test loss: 2.12200\n",
      "Epoch 2475 - lr: 0.05000 - Train loss: 1.85671 - Test loss: 2.12201\n",
      "Epoch 2476 - lr: 0.05000 - Train loss: 1.85670 - Test loss: 2.12202\n",
      "Epoch 2477 - lr: 0.05000 - Train loss: 1.85670 - Test loss: 2.12203\n",
      "Epoch 2478 - lr: 0.05000 - Train loss: 1.85670 - Test loss: 2.12204\n",
      "Epoch 2479 - lr: 0.05000 - Train loss: 1.85670 - Test loss: 2.12205\n",
      "Epoch 2480 - lr: 0.05000 - Train loss: 1.85670 - Test loss: 2.12206\n",
      "Epoch 2481 - lr: 0.05000 - Train loss: 1.85669 - Test loss: 2.12207\n",
      "Epoch 2482 - lr: 0.05000 - Train loss: 1.85669 - Test loss: 2.12207\n",
      "Epoch 2483 - lr: 0.05000 - Train loss: 1.85669 - Test loss: 2.12208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2484 - lr: 0.05000 - Train loss: 1.85669 - Test loss: 2.12209\n",
      "Epoch 2485 - lr: 0.05000 - Train loss: 1.85668 - Test loss: 2.12210\n",
      "Epoch 2486 - lr: 0.05000 - Train loss: 1.85668 - Test loss: 2.12211\n",
      "Epoch 2487 - lr: 0.05000 - Train loss: 1.85668 - Test loss: 2.12212\n",
      "Epoch 2488 - lr: 0.05000 - Train loss: 1.85668 - Test loss: 2.12213\n",
      "Epoch 2489 - lr: 0.05000 - Train loss: 1.85668 - Test loss: 2.12214\n",
      "Epoch 2490 - lr: 0.05000 - Train loss: 1.85667 - Test loss: 2.12215\n",
      "Epoch 2491 - lr: 0.05000 - Train loss: 1.85667 - Test loss: 2.12216\n",
      "Epoch 2492 - lr: 0.05000 - Train loss: 1.85667 - Test loss: 2.12217\n",
      "Epoch 2493 - lr: 0.05000 - Train loss: 1.85667 - Test loss: 2.12218\n",
      "Epoch 2494 - lr: 0.05000 - Train loss: 1.85666 - Test loss: 2.12218\n",
      "Epoch 2495 - lr: 0.05000 - Train loss: 1.85666 - Test loss: 2.12219\n",
      "Epoch 2496 - lr: 0.05000 - Train loss: 1.85666 - Test loss: 2.12220\n",
      "Epoch 2497 - lr: 0.05000 - Train loss: 1.85666 - Test loss: 2.12221\n",
      "Epoch 2498 - lr: 0.05000 - Train loss: 1.85666 - Test loss: 2.12222\n",
      "Epoch 2499 - lr: 0.05000 - Train loss: 1.85665 - Test loss: 2.12223\n",
      "Epoch 2500 - lr: 0.05000 - Train loss: 1.85665 - Test loss: 2.12224\n",
      "Epoch 2501 - lr: 0.05000 - Train loss: 1.85665 - Test loss: 2.12225\n",
      "Epoch 2502 - lr: 0.05000 - Train loss: 1.85665 - Test loss: 2.12226\n",
      "Epoch 2503 - lr: 0.05000 - Train loss: 1.85664 - Test loss: 2.12227\n",
      "Epoch 2504 - lr: 0.05000 - Train loss: 1.85664 - Test loss: 2.12228\n",
      "Epoch 2505 - lr: 0.05000 - Train loss: 1.85664 - Test loss: 2.12228\n",
      "Epoch 2506 - lr: 0.05000 - Train loss: 1.85664 - Test loss: 2.12229\n",
      "Epoch 2507 - lr: 0.05000 - Train loss: 1.85664 - Test loss: 2.12230\n",
      "Epoch 2508 - lr: 0.05000 - Train loss: 1.85663 - Test loss: 2.12231\n",
      "Epoch 2509 - lr: 0.05000 - Train loss: 1.85663 - Test loss: 2.12232\n",
      "Epoch 2510 - lr: 0.05000 - Train loss: 1.85663 - Test loss: 2.12233\n",
      "Epoch 2511 - lr: 0.05000 - Train loss: 1.85663 - Test loss: 2.12234\n",
      "Epoch 2512 - lr: 0.05000 - Train loss: 1.85662 - Test loss: 2.12235\n",
      "Epoch 2513 - lr: 0.05000 - Train loss: 1.85662 - Test loss: 2.12236\n",
      "Epoch 2514 - lr: 0.05000 - Train loss: 1.85662 - Test loss: 2.12237\n",
      "Epoch 2515 - lr: 0.05000 - Train loss: 1.85662 - Test loss: 2.12237\n",
      "Epoch 2516 - lr: 0.05000 - Train loss: 1.85662 - Test loss: 2.12238\n",
      "Epoch 2517 - lr: 0.05000 - Train loss: 1.85661 - Test loss: 2.12239\n",
      "Epoch 2518 - lr: 0.05000 - Train loss: 1.85661 - Test loss: 2.12240\n",
      "Epoch 2519 - lr: 0.05000 - Train loss: 1.85661 - Test loss: 2.12241\n",
      "Epoch 2520 - lr: 0.05000 - Train loss: 1.85661 - Test loss: 2.12242\n",
      "Epoch 2521 - lr: 0.05000 - Train loss: 1.85660 - Test loss: 2.12243\n",
      "Epoch 2522 - lr: 0.05000 - Train loss: 1.85660 - Test loss: 2.12244\n",
      "Epoch 2523 - lr: 0.05000 - Train loss: 1.85660 - Test loss: 2.12245\n",
      "Epoch 2524 - lr: 0.05000 - Train loss: 1.85660 - Test loss: 2.12246\n",
      "Epoch 2525 - lr: 0.05000 - Train loss: 1.85660 - Test loss: 2.12246\n",
      "Epoch 2526 - lr: 0.05000 - Train loss: 1.85659 - Test loss: 2.12247\n",
      "Epoch 2527 - lr: 0.05000 - Train loss: 1.85659 - Test loss: 2.12248\n",
      "Epoch 2528 - lr: 0.05000 - Train loss: 1.85659 - Test loss: 2.12249\n",
      "Epoch 2529 - lr: 0.05000 - Train loss: 1.85659 - Test loss: 2.12250\n",
      "Epoch 2530 - lr: 0.05000 - Train loss: 1.85658 - Test loss: 2.12251\n",
      "Epoch 2531 - lr: 0.05000 - Train loss: 1.85658 - Test loss: 2.12252\n",
      "Epoch 2532 - lr: 0.05000 - Train loss: 1.85658 - Test loss: 2.12253\n",
      "Epoch 2533 - lr: 0.05000 - Train loss: 1.85658 - Test loss: 2.12254\n",
      "Epoch 2534 - lr: 0.05000 - Train loss: 1.85658 - Test loss: 2.12254\n",
      "Epoch 2535 - lr: 0.05000 - Train loss: 1.85657 - Test loss: 2.12255\n",
      "Epoch 2536 - lr: 0.05000 - Train loss: 1.85657 - Test loss: 2.12256\n",
      "Epoch 2537 - lr: 0.05000 - Train loss: 1.85657 - Test loss: 2.12257\n",
      "Epoch 2538 - lr: 0.05000 - Train loss: 1.85657 - Test loss: 2.12258\n",
      "Epoch 2539 - lr: 0.05000 - Train loss: 1.85656 - Test loss: 2.12259\n",
      "Epoch 2540 - lr: 0.05000 - Train loss: 1.85656 - Test loss: 2.12260\n",
      "Epoch 2541 - lr: 0.05000 - Train loss: 1.85656 - Test loss: 2.12261\n",
      "Epoch 2542 - lr: 0.05000 - Train loss: 1.85656 - Test loss: 2.12261\n",
      "Epoch 2543 - lr: 0.05000 - Train loss: 1.85656 - Test loss: 2.12262\n",
      "Epoch 2544 - lr: 0.05000 - Train loss: 1.85655 - Test loss: 2.12263\n",
      "Epoch 2545 - lr: 0.05000 - Train loss: 1.85655 - Test loss: 2.12264\n",
      "Epoch 2546 - lr: 0.05000 - Train loss: 1.85655 - Test loss: 2.12265\n",
      "Epoch 2547 - lr: 0.05000 - Train loss: 1.85655 - Test loss: 2.12266\n",
      "Epoch 2548 - lr: 0.05000 - Train loss: 1.85654 - Test loss: 2.12267\n",
      "Epoch 2549 - lr: 0.05000 - Train loss: 1.85654 - Test loss: 2.12268\n",
      "Epoch 2550 - lr: 0.05000 - Train loss: 1.85654 - Test loss: 2.12269\n",
      "Epoch 2551 - lr: 0.05000 - Train loss: 1.85654 - Test loss: 2.12269\n",
      "Epoch 2552 - lr: 0.05000 - Train loss: 1.85654 - Test loss: 2.12270\n",
      "Epoch 2553 - lr: 0.05000 - Train loss: 1.85653 - Test loss: 2.12271\n",
      "Epoch 2554 - lr: 0.05000 - Train loss: 1.85653 - Test loss: 2.12272\n",
      "Epoch 2555 - lr: 0.05000 - Train loss: 1.85653 - Test loss: 2.12273\n",
      "Epoch 2556 - lr: 0.05000 - Train loss: 1.85653 - Test loss: 2.12274\n",
      "Epoch 2557 - lr: 0.05000 - Train loss: 1.85652 - Test loss: 2.12275\n",
      "Epoch 2558 - lr: 0.05000 - Train loss: 1.85652 - Test loss: 2.12276\n",
      "Epoch 2559 - lr: 0.05000 - Train loss: 1.85652 - Test loss: 2.12276\n",
      "Epoch 2560 - lr: 0.05000 - Train loss: 1.85652 - Test loss: 2.12277\n",
      "Epoch 2561 - lr: 0.05000 - Train loss: 1.85652 - Test loss: 2.12278\n",
      "Epoch 2562 - lr: 0.05000 - Train loss: 1.85651 - Test loss: 2.12279\n",
      "Epoch 2563 - lr: 0.05000 - Train loss: 1.85651 - Test loss: 2.12280\n",
      "Epoch 2564 - lr: 0.05000 - Train loss: 1.85651 - Test loss: 2.12281\n",
      "Epoch 2565 - lr: 0.05000 - Train loss: 1.85651 - Test loss: 2.12282\n",
      "Epoch 2566 - lr: 0.05000 - Train loss: 1.85650 - Test loss: 2.12282\n",
      "Epoch 2567 - lr: 0.05000 - Train loss: 1.85650 - Test loss: 2.12283\n",
      "Epoch 2568 - lr: 0.05000 - Train loss: 1.85650 - Test loss: 2.12284\n",
      "Epoch 2569 - lr: 0.05000 - Train loss: 1.85650 - Test loss: 2.12285\n",
      "Epoch 2570 - lr: 0.05000 - Train loss: 1.85650 - Test loss: 2.12286\n",
      "Epoch 2571 - lr: 0.05000 - Train loss: 1.85649 - Test loss: 2.12287\n",
      "Epoch 2572 - lr: 0.05000 - Train loss: 1.85649 - Test loss: 2.12288\n",
      "Epoch 2573 - lr: 0.05000 - Train loss: 1.85649 - Test loss: 2.12289\n",
      "Epoch 2574 - lr: 0.05000 - Train loss: 1.85649 - Test loss: 2.12289\n",
      "Epoch 2575 - lr: 0.05000 - Train loss: 1.85649 - Test loss: 2.12290\n",
      "Epoch 2576 - lr: 0.05000 - Train loss: 1.85648 - Test loss: 2.12291\n",
      "Epoch 2577 - lr: 0.05000 - Train loss: 1.85648 - Test loss: 2.12292\n",
      "Epoch 2578 - lr: 0.05000 - Train loss: 1.85648 - Test loss: 2.12293\n",
      "Epoch 2579 - lr: 0.05000 - Train loss: 1.85648 - Test loss: 2.12294\n",
      "Epoch 2580 - lr: 0.05000 - Train loss: 1.85647 - Test loss: 2.12295\n",
      "Epoch 2581 - lr: 0.05000 - Train loss: 1.85647 - Test loss: 2.12295\n",
      "Epoch 2582 - lr: 0.05000 - Train loss: 1.85647 - Test loss: 2.12296\n",
      "Epoch 2583 - lr: 0.05000 - Train loss: 1.85647 - Test loss: 2.12297\n",
      "Epoch 2584 - lr: 0.05000 - Train loss: 1.85647 - Test loss: 2.12298\n",
      "Epoch 2585 - lr: 0.05000 - Train loss: 1.85646 - Test loss: 2.12299\n",
      "Epoch 2586 - lr: 0.05000 - Train loss: 1.85646 - Test loss: 2.12300\n",
      "Epoch 2587 - lr: 0.05000 - Train loss: 1.85646 - Test loss: 2.12300\n",
      "Epoch 2588 - lr: 0.05000 - Train loss: 1.85646 - Test loss: 2.12301\n",
      "Epoch 2589 - lr: 0.05000 - Train loss: 1.85645 - Test loss: 2.12302\n",
      "Epoch 2590 - lr: 0.05000 - Train loss: 1.85645 - Test loss: 2.12303\n",
      "Epoch 2591 - lr: 0.05000 - Train loss: 1.85645 - Test loss: 2.12304\n",
      "Epoch 2592 - lr: 0.05000 - Train loss: 1.85645 - Test loss: 2.12305\n",
      "Epoch 2593 - lr: 0.05000 - Train loss: 1.85645 - Test loss: 2.12306\n",
      "Epoch 2594 - lr: 0.05000 - Train loss: 1.85644 - Test loss: 2.12306\n",
      "Epoch 2595 - lr: 0.05000 - Train loss: 1.85644 - Test loss: 2.12307\n",
      "Epoch 2596 - lr: 0.05000 - Train loss: 1.85644 - Test loss: 2.12308\n",
      "Epoch 2597 - lr: 0.05000 - Train loss: 1.85644 - Test loss: 2.12309\n",
      "Epoch 2598 - lr: 0.05000 - Train loss: 1.85643 - Test loss: 2.12310\n",
      "Epoch 2599 - lr: 0.05000 - Train loss: 1.85643 - Test loss: 2.12311\n",
      "Epoch 2600 - lr: 0.05000 - Train loss: 1.85643 - Test loss: 2.12311\n",
      "Epoch 2601 - lr: 0.05000 - Train loss: 1.85643 - Test loss: 2.12312\n",
      "Epoch 2602 - lr: 0.05000 - Train loss: 1.85643 - Test loss: 2.12313\n",
      "Epoch 2603 - lr: 0.05000 - Train loss: 1.85642 - Test loss: 2.12314\n",
      "Epoch 2604 - lr: 0.05000 - Train loss: 1.85642 - Test loss: 2.12315\n",
      "Epoch 2605 - lr: 0.05000 - Train loss: 1.85642 - Test loss: 2.12316\n",
      "Epoch 2606 - lr: 0.05000 - Train loss: 1.85642 - Test loss: 2.12316\n",
      "Epoch 2607 - lr: 0.05000 - Train loss: 1.85641 - Test loss: 2.12317\n",
      "Epoch 2608 - lr: 0.05000 - Train loss: 1.85641 - Test loss: 2.12318\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2609 - lr: 0.05000 - Train loss: 1.85641 - Test loss: 2.12319\n",
      "Epoch 2610 - lr: 0.05000 - Train loss: 1.85641 - Test loss: 2.12320\n",
      "Epoch 2611 - lr: 0.05000 - Train loss: 1.85641 - Test loss: 2.12321\n",
      "Epoch 2612 - lr: 0.05000 - Train loss: 1.85640 - Test loss: 2.12321\n",
      "Epoch 2613 - lr: 0.05000 - Train loss: 1.85640 - Test loss: 2.12322\n",
      "Epoch 2614 - lr: 0.05000 - Train loss: 1.85640 - Test loss: 2.12323\n",
      "Epoch 2615 - lr: 0.05000 - Train loss: 1.85640 - Test loss: 2.12324\n",
      "Epoch 2616 - lr: 0.05000 - Train loss: 1.85639 - Test loss: 2.12325\n",
      "Epoch 2617 - lr: 0.05000 - Train loss: 1.85639 - Test loss: 2.12326\n",
      "Epoch 2618 - lr: 0.05000 - Train loss: 1.85639 - Test loss: 2.12326\n",
      "Epoch 2619 - lr: 0.05000 - Train loss: 1.85639 - Test loss: 2.12327\n",
      "Epoch 2620 - lr: 0.05000 - Train loss: 1.85639 - Test loss: 2.12328\n",
      "Epoch 2621 - lr: 0.05000 - Train loss: 1.85638 - Test loss: 2.12329\n",
      "Epoch 2622 - lr: 0.05000 - Train loss: 1.85638 - Test loss: 2.12330\n",
      "Epoch 2623 - lr: 0.05000 - Train loss: 1.85638 - Test loss: 2.12331\n",
      "Epoch 2624 - lr: 0.05000 - Train loss: 1.85638 - Test loss: 2.12331\n",
      "Epoch 2625 - lr: 0.05000 - Train loss: 1.85637 - Test loss: 2.12332\n",
      "Epoch 2626 - lr: 0.05000 - Train loss: 1.85637 - Test loss: 2.12333\n",
      "Epoch 2627 - lr: 0.05000 - Train loss: 1.85637 - Test loss: 2.12334\n",
      "Epoch 2628 - lr: 0.05000 - Train loss: 1.85637 - Test loss: 2.12335\n",
      "Epoch 2629 - lr: 0.05000 - Train loss: 1.85636 - Test loss: 2.12335\n",
      "Epoch 2630 - lr: 0.05000 - Train loss: 1.85636 - Test loss: 2.12336\n",
      "Epoch 2631 - lr: 0.05000 - Train loss: 1.85636 - Test loss: 2.12337\n",
      "Epoch 2632 - lr: 0.05000 - Train loss: 1.85636 - Test loss: 2.12338\n",
      "Epoch 2633 - lr: 0.05000 - Train loss: 1.85636 - Test loss: 2.12339\n",
      "Epoch 2634 - lr: 0.05000 - Train loss: 1.85635 - Test loss: 2.12339\n",
      "Epoch 2635 - lr: 0.05000 - Train loss: 1.85635 - Test loss: 2.12340\n",
      "Epoch 2636 - lr: 0.05000 - Train loss: 1.85635 - Test loss: 2.12341\n",
      "Epoch 2637 - lr: 0.05000 - Train loss: 1.85635 - Test loss: 2.12342\n",
      "Epoch 2638 - lr: 0.05000 - Train loss: 1.85634 - Test loss: 2.12343\n",
      "Epoch 2639 - lr: 0.05000 - Train loss: 1.85634 - Test loss: 2.12343\n",
      "Epoch 2640 - lr: 0.05000 - Train loss: 1.85634 - Test loss: 2.12344\n",
      "Epoch 2641 - lr: 0.05000 - Train loss: 1.85634 - Test loss: 2.12345\n",
      "Epoch 2642 - lr: 0.05000 - Train loss: 1.85634 - Test loss: 2.12346\n",
      "Epoch 2643 - lr: 0.05000 - Train loss: 1.85633 - Test loss: 2.12347\n",
      "Epoch 2644 - lr: 0.05000 - Train loss: 1.85633 - Test loss: 2.12347\n",
      "Epoch 2645 - lr: 0.05000 - Train loss: 1.85633 - Test loss: 2.12348\n",
      "Epoch 2646 - lr: 0.05000 - Train loss: 1.85633 - Test loss: 2.12349\n",
      "Epoch 2647 - lr: 0.05000 - Train loss: 1.85632 - Test loss: 2.12350\n",
      "Epoch 2648 - lr: 0.05000 - Train loss: 1.85632 - Test loss: 2.12350\n",
      "Epoch 2649 - lr: 0.05000 - Train loss: 1.85632 - Test loss: 2.12351\n",
      "Epoch 2650 - lr: 0.05000 - Train loss: 1.85632 - Test loss: 2.12352\n",
      "Epoch 2651 - lr: 0.05000 - Train loss: 1.85631 - Test loss: 2.12353\n",
      "Epoch 2652 - lr: 0.05000 - Train loss: 1.85631 - Test loss: 2.12354\n",
      "Epoch 2653 - lr: 0.05000 - Train loss: 1.85631 - Test loss: 2.12354\n",
      "Epoch 2654 - lr: 0.05000 - Train loss: 1.85631 - Test loss: 2.12355\n",
      "Epoch 2655 - lr: 0.05000 - Train loss: 1.85630 - Test loss: 2.12356\n",
      "Epoch 2656 - lr: 0.05000 - Train loss: 1.85630 - Test loss: 2.12357\n",
      "Epoch 2657 - lr: 0.05000 - Train loss: 1.85630 - Test loss: 2.12357\n",
      "Epoch 2658 - lr: 0.05000 - Train loss: 1.85630 - Test loss: 2.12358\n",
      "Epoch 2659 - lr: 0.05000 - Train loss: 1.85630 - Test loss: 2.12359\n",
      "Epoch 2660 - lr: 0.05000 - Train loss: 1.85629 - Test loss: 2.12360\n",
      "Epoch 2661 - lr: 0.05000 - Train loss: 1.85629 - Test loss: 2.12360\n",
      "Epoch 2662 - lr: 0.05000 - Train loss: 1.85629 - Test loss: 2.12361\n",
      "Epoch 2663 - lr: 0.05000 - Train loss: 1.85629 - Test loss: 2.12362\n",
      "Epoch 2664 - lr: 0.05000 - Train loss: 1.85628 - Test loss: 2.12362\n",
      "Epoch 2665 - lr: 0.05000 - Train loss: 1.85628 - Test loss: 2.12363\n",
      "Epoch 2666 - lr: 0.05000 - Train loss: 1.85628 - Test loss: 2.12364\n",
      "Epoch 2667 - lr: 0.05000 - Train loss: 1.85628 - Test loss: 2.12365\n",
      "Epoch 2668 - lr: 0.05000 - Train loss: 1.85627 - Test loss: 2.12365\n",
      "Epoch 2669 - lr: 0.05000 - Train loss: 1.85627 - Test loss: 2.12366\n",
      "Epoch 2670 - lr: 0.05000 - Train loss: 1.85627 - Test loss: 2.12367\n",
      "Epoch 2671 - lr: 0.05000 - Train loss: 1.85626 - Test loss: 2.12367\n",
      "Epoch 2672 - lr: 0.05000 - Train loss: 1.85626 - Test loss: 2.12368\n",
      "Epoch 2673 - lr: 0.05000 - Train loss: 1.85626 - Test loss: 2.12369\n",
      "Epoch 2674 - lr: 0.05000 - Train loss: 1.85626 - Test loss: 2.12369\n",
      "Epoch 2675 - lr: 0.05000 - Train loss: 1.85625 - Test loss: 2.12370\n",
      "Epoch 2676 - lr: 0.05000 - Train loss: 1.85625 - Test loss: 2.12371\n",
      "Epoch 2677 - lr: 0.05000 - Train loss: 1.85625 - Test loss: 2.12371\n",
      "Epoch 2678 - lr: 0.05000 - Train loss: 1.85624 - Test loss: 2.12372\n",
      "Epoch 2679 - lr: 0.05000 - Train loss: 1.85624 - Test loss: 2.12372\n",
      "Epoch 2680 - lr: 0.05000 - Train loss: 1.85624 - Test loss: 2.12373\n",
      "Epoch 2681 - lr: 0.05000 - Train loss: 1.85624 - Test loss: 2.12373\n",
      "Epoch 2682 - lr: 0.05000 - Train loss: 1.85623 - Test loss: 2.12374\n",
      "Epoch 2683 - lr: 0.05000 - Train loss: 1.85623 - Test loss: 2.12375\n",
      "Epoch 2684 - lr: 0.05000 - Train loss: 1.85623 - Test loss: 2.12375\n",
      "Epoch 2685 - lr: 0.05000 - Train loss: 1.85622 - Test loss: 2.12375\n",
      "Epoch 2686 - lr: 0.05000 - Train loss: 1.85622 - Test loss: 2.12376\n",
      "Epoch 2687 - lr: 0.05000 - Train loss: 1.85621 - Test loss: 2.12376\n",
      "Epoch 2688 - lr: 0.05000 - Train loss: 1.85621 - Test loss: 2.12377\n",
      "Epoch 2689 - lr: 0.05000 - Train loss: 1.85621 - Test loss: 2.12377\n",
      "Epoch 2690 - lr: 0.05000 - Train loss: 1.85620 - Test loss: 2.12377\n",
      "Epoch 2691 - lr: 0.05000 - Train loss: 1.85620 - Test loss: 2.12377\n",
      "Epoch 2692 - lr: 0.05000 - Train loss: 1.85619 - Test loss: 2.12377\n",
      "Epoch 2693 - lr: 0.05000 - Train loss: 1.85619 - Test loss: 2.12377\n",
      "Epoch 2694 - lr: 0.05000 - Train loss: 1.85618 - Test loss: 2.12377\n",
      "Epoch 2695 - lr: 0.05000 - Train loss: 1.85617 - Test loss: 2.12377\n",
      "Epoch 2696 - lr: 0.05000 - Train loss: 1.85616 - Test loss: 2.12376\n",
      "Epoch 2697 - lr: 0.05000 - Train loss: 1.85615 - Test loss: 2.12375\n",
      "Epoch 2698 - lr: 0.05000 - Train loss: 1.85614 - Test loss: 2.12374\n",
      "Epoch 2699 - lr: 0.05000 - Train loss: 1.85613 - Test loss: 2.12372\n",
      "Epoch 2700 - lr: 0.05000 - Train loss: 1.85611 - Test loss: 2.12369\n",
      "Epoch 2701 - lr: 0.05000 - Train loss: 1.85608 - Test loss: 2.12364\n",
      "Epoch 2702 - lr: 0.05000 - Train loss: 1.85603 - Test loss: 2.12356\n",
      "Epoch 2703 - lr: 0.05000 - Train loss: 1.85595 - Test loss: 2.12338\n",
      "Epoch 2704 - lr: 0.05000 - Train loss: 1.85574 - Test loss: 2.12279\n",
      "Epoch 2705 - lr: 0.05000 - Train loss: 1.87722 - Test loss: 2.10511\n",
      "Epoch 2706 - lr: 0.05000 - Train loss: 1.85560 - Test loss: 2.12250\n",
      "Epoch 2707 - lr: 0.05000 - Train loss: 1.85620 - Test loss: 2.12404\n",
      "Epoch 2708 - lr: 0.05000 - Train loss: 1.85625 - Test loss: 2.12416\n",
      "Epoch 2709 - lr: 0.05000 - Train loss: 1.85625 - Test loss: 2.12417\n",
      "Epoch 2710 - lr: 0.05000 - Train loss: 1.85624 - Test loss: 2.12417\n",
      "Epoch 2711 - lr: 0.05000 - Train loss: 1.85624 - Test loss: 2.12417\n",
      "Epoch 2712 - lr: 0.05000 - Train loss: 1.85623 - Test loss: 2.12418\n",
      "Epoch 2713 - lr: 0.05000 - Train loss: 1.85622 - Test loss: 2.12418\n",
      "Epoch 2714 - lr: 0.05000 - Train loss: 1.85622 - Test loss: 2.12418\n",
      "Epoch 2715 - lr: 0.05000 - Train loss: 1.85621 - Test loss: 2.12419\n",
      "Epoch 2716 - lr: 0.05000 - Train loss: 1.85621 - Test loss: 2.12419\n",
      "Epoch 2717 - lr: 0.05000 - Train loss: 1.85621 - Test loss: 2.12420\n",
      "Epoch 2718 - lr: 0.05000 - Train loss: 1.85620 - Test loss: 2.12420\n",
      "Epoch 2719 - lr: 0.05000 - Train loss: 1.85620 - Test loss: 2.12421\n",
      "Epoch 2720 - lr: 0.05000 - Train loss: 1.85620 - Test loss: 2.12421\n",
      "Epoch 2721 - lr: 0.05000 - Train loss: 1.85619 - Test loss: 2.12422\n",
      "Epoch 2722 - lr: 0.05000 - Train loss: 1.85619 - Test loss: 2.12423\n",
      "Epoch 2723 - lr: 0.05000 - Train loss: 1.85619 - Test loss: 2.12423\n",
      "Epoch 2724 - lr: 0.05000 - Train loss: 1.85618 - Test loss: 2.12424\n",
      "Epoch 2725 - lr: 0.05000 - Train loss: 1.85618 - Test loss: 2.12425\n",
      "Epoch 2726 - lr: 0.05000 - Train loss: 1.85618 - Test loss: 2.12426\n",
      "Epoch 2727 - lr: 0.05000 - Train loss: 1.85618 - Test loss: 2.12426\n",
      "Epoch 2728 - lr: 0.05000 - Train loss: 1.85617 - Test loss: 2.12427\n",
      "Epoch 2729 - lr: 0.05000 - Train loss: 1.85617 - Test loss: 2.12428\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2730 - lr: 0.05000 - Train loss: 1.85617 - Test loss: 2.12429\n",
      "Epoch 2731 - lr: 0.05000 - Train loss: 1.85617 - Test loss: 2.12429\n",
      "Epoch 2732 - lr: 0.05000 - Train loss: 1.85616 - Test loss: 2.12430\n",
      "Epoch 2733 - lr: 0.05000 - Train loss: 1.85616 - Test loss: 2.12431\n",
      "Epoch 2734 - lr: 0.05000 - Train loss: 1.85616 - Test loss: 2.12432\n",
      "Epoch 2735 - lr: 0.05000 - Train loss: 1.85616 - Test loss: 2.12433\n",
      "Epoch 2736 - lr: 0.05000 - Train loss: 1.85615 - Test loss: 2.12433\n",
      "Epoch 2737 - lr: 0.05000 - Train loss: 1.85615 - Test loss: 2.12434\n",
      "Epoch 2738 - lr: 0.05000 - Train loss: 1.85615 - Test loss: 2.12435\n",
      "Epoch 2739 - lr: 0.05000 - Train loss: 1.85615 - Test loss: 2.12436\n",
      "Epoch 2740 - lr: 0.05000 - Train loss: 1.85615 - Test loss: 2.12437\n",
      "Epoch 2741 - lr: 0.05000 - Train loss: 1.85614 - Test loss: 2.12437\n",
      "Epoch 2742 - lr: 0.05000 - Train loss: 1.85614 - Test loss: 2.12438\n",
      "Epoch 2743 - lr: 0.05000 - Train loss: 1.85614 - Test loss: 2.12439\n",
      "Epoch 2744 - lr: 0.05000 - Train loss: 1.85614 - Test loss: 2.12440\n",
      "Epoch 2745 - lr: 0.05000 - Train loss: 1.85613 - Test loss: 2.12441\n",
      "Epoch 2746 - lr: 0.05000 - Train loss: 1.85613 - Test loss: 2.12441\n",
      "Epoch 2747 - lr: 0.05000 - Train loss: 1.85613 - Test loss: 2.12442\n",
      "Epoch 2748 - lr: 0.05000 - Train loss: 1.85613 - Test loss: 2.12443\n",
      "Epoch 2749 - lr: 0.05000 - Train loss: 1.85613 - Test loss: 2.12444\n",
      "Epoch 2750 - lr: 0.05000 - Train loss: 1.85612 - Test loss: 2.12445\n",
      "Epoch 2751 - lr: 0.05000 - Train loss: 1.85612 - Test loss: 2.12445\n",
      "Epoch 2752 - lr: 0.05000 - Train loss: 1.85612 - Test loss: 2.12446\n",
      "Epoch 2753 - lr: 0.05000 - Train loss: 1.85612 - Test loss: 2.12447\n",
      "Epoch 2754 - lr: 0.05000 - Train loss: 1.85612 - Test loss: 2.12448\n",
      "Epoch 2755 - lr: 0.05000 - Train loss: 1.85611 - Test loss: 2.12449\n",
      "Epoch 2756 - lr: 0.05000 - Train loss: 1.85611 - Test loss: 2.12450\n",
      "Epoch 2757 - lr: 0.05000 - Train loss: 1.85611 - Test loss: 2.12450\n",
      "Epoch 2758 - lr: 0.05000 - Train loss: 1.85611 - Test loss: 2.12451\n",
      "Epoch 2759 - lr: 0.05000 - Train loss: 1.85610 - Test loss: 2.12452\n",
      "Epoch 2760 - lr: 0.05000 - Train loss: 1.85610 - Test loss: 2.12453\n",
      "Epoch 2761 - lr: 0.05000 - Train loss: 1.85610 - Test loss: 2.12454\n",
      "Epoch 2762 - lr: 0.05000 - Train loss: 1.85610 - Test loss: 2.12454\n",
      "Epoch 2763 - lr: 0.05000 - Train loss: 1.85610 - Test loss: 2.12455\n",
      "Epoch 2764 - lr: 0.05000 - Train loss: 1.85609 - Test loss: 2.12456\n",
      "Epoch 2765 - lr: 0.05000 - Train loss: 1.85609 - Test loss: 2.12457\n",
      "Epoch 2766 - lr: 0.05000 - Train loss: 1.85609 - Test loss: 2.12458\n",
      "Epoch 2767 - lr: 0.05000 - Train loss: 1.85609 - Test loss: 2.12458\n",
      "Epoch 2768 - lr: 0.05000 - Train loss: 1.85609 - Test loss: 2.12459\n",
      "Epoch 2769 - lr: 0.05000 - Train loss: 1.85608 - Test loss: 2.12460\n",
      "Epoch 2770 - lr: 0.05000 - Train loss: 1.85608 - Test loss: 2.12461\n",
      "Epoch 2771 - lr: 0.05000 - Train loss: 1.85608 - Test loss: 2.12462\n",
      "Epoch 2772 - lr: 0.05000 - Train loss: 1.85608 - Test loss: 2.12462\n",
      "Epoch 2773 - lr: 0.05000 - Train loss: 1.85608 - Test loss: 2.12463\n",
      "Epoch 2774 - lr: 0.05000 - Train loss: 1.85607 - Test loss: 2.12464\n",
      "Epoch 2775 - lr: 0.05000 - Train loss: 1.85607 - Test loss: 2.12465\n",
      "Epoch 2776 - lr: 0.05000 - Train loss: 1.85607 - Test loss: 2.12466\n",
      "Epoch 2777 - lr: 0.05000 - Train loss: 1.85607 - Test loss: 2.12466\n",
      "Epoch 2778 - lr: 0.05000 - Train loss: 1.85606 - Test loss: 2.12467\n",
      "Epoch 2779 - lr: 0.05000 - Train loss: 1.85606 - Test loss: 2.12468\n",
      "Epoch 2780 - lr: 0.05000 - Train loss: 1.85606 - Test loss: 2.12469\n",
      "Epoch 2781 - lr: 0.05000 - Train loss: 1.85606 - Test loss: 2.12470\n",
      "Epoch 2782 - lr: 0.05000 - Train loss: 1.85606 - Test loss: 2.12470\n",
      "Epoch 2783 - lr: 0.05000 - Train loss: 1.85605 - Test loss: 2.12471\n",
      "Epoch 2784 - lr: 0.05000 - Train loss: 1.85605 - Test loss: 2.12472\n",
      "Epoch 2785 - lr: 0.05000 - Train loss: 1.85605 - Test loss: 2.12473\n",
      "Epoch 2786 - lr: 0.05000 - Train loss: 1.85605 - Test loss: 2.12474\n",
      "Epoch 2787 - lr: 0.05000 - Train loss: 1.85605 - Test loss: 2.12474\n",
      "Epoch 2788 - lr: 0.05000 - Train loss: 1.85604 - Test loss: 2.12475\n",
      "Epoch 2789 - lr: 0.05000 - Train loss: 1.85604 - Test loss: 2.12476\n",
      "Epoch 2790 - lr: 0.05000 - Train loss: 1.85604 - Test loss: 2.12477\n",
      "Epoch 2791 - lr: 0.05000 - Train loss: 1.85604 - Test loss: 2.12478\n",
      "Epoch 2792 - lr: 0.05000 - Train loss: 1.85604 - Test loss: 2.12478\n",
      "Epoch 2793 - lr: 0.05000 - Train loss: 1.85603 - Test loss: 2.12479\n",
      "Epoch 2794 - lr: 0.05000 - Train loss: 1.85603 - Test loss: 2.12480\n",
      "Epoch 2795 - lr: 0.05000 - Train loss: 1.85603 - Test loss: 2.12481\n",
      "Epoch 2796 - lr: 0.05000 - Train loss: 1.85603 - Test loss: 2.12482\n",
      "Epoch 2797 - lr: 0.05000 - Train loss: 1.85602 - Test loss: 2.12482\n",
      "Epoch 2798 - lr: 0.05000 - Train loss: 1.85602 - Test loss: 2.12483\n",
      "Epoch 2799 - lr: 0.05000 - Train loss: 1.85602 - Test loss: 2.12484\n",
      "Epoch 2800 - lr: 0.05000 - Train loss: 1.85602 - Test loss: 2.12485\n",
      "Epoch 2801 - lr: 0.05000 - Train loss: 1.85602 - Test loss: 2.12486\n",
      "Epoch 2802 - lr: 0.05000 - Train loss: 1.85601 - Test loss: 2.12486\n",
      "Epoch 2803 - lr: 0.05000 - Train loss: 1.85601 - Test loss: 2.12487\n",
      "Epoch 2804 - lr: 0.05000 - Train loss: 1.85601 - Test loss: 2.12488\n",
      "Epoch 2805 - lr: 0.05000 - Train loss: 1.85601 - Test loss: 2.12489\n",
      "Epoch 2806 - lr: 0.05000 - Train loss: 1.85601 - Test loss: 2.12490\n",
      "Epoch 2807 - lr: 0.05000 - Train loss: 1.85600 - Test loss: 2.12490\n",
      "Epoch 2808 - lr: 0.05000 - Train loss: 1.85600 - Test loss: 2.12491\n",
      "Epoch 2809 - lr: 0.05000 - Train loss: 1.85600 - Test loss: 2.12492\n",
      "Epoch 2810 - lr: 0.05000 - Train loss: 1.85600 - Test loss: 2.12493\n",
      "Epoch 2811 - lr: 0.05000 - Train loss: 1.85600 - Test loss: 2.12493\n",
      "Epoch 2812 - lr: 0.05000 - Train loss: 1.85599 - Test loss: 2.12494\n",
      "Epoch 2813 - lr: 0.05000 - Train loss: 1.85599 - Test loss: 2.12495\n",
      "Epoch 2814 - lr: 0.05000 - Train loss: 1.85599 - Test loss: 2.12496\n",
      "Epoch 2815 - lr: 0.05000 - Train loss: 1.85599 - Test loss: 2.12497\n",
      "Epoch 2816 - lr: 0.05000 - Train loss: 1.85599 - Test loss: 2.12497\n",
      "Epoch 2817 - lr: 0.05000 - Train loss: 1.85598 - Test loss: 2.12498\n",
      "Epoch 2818 - lr: 0.05000 - Train loss: 1.85598 - Test loss: 2.12499\n",
      "Epoch 2819 - lr: 0.05000 - Train loss: 1.85598 - Test loss: 2.12500\n",
      "Epoch 2820 - lr: 0.05000 - Train loss: 1.85598 - Test loss: 2.12501\n",
      "Epoch 2821 - lr: 0.05000 - Train loss: 1.85597 - Test loss: 2.12501\n",
      "Epoch 2822 - lr: 0.05000 - Train loss: 1.85597 - Test loss: 2.12502\n",
      "Epoch 2823 - lr: 0.05000 - Train loss: 1.85597 - Test loss: 2.12503\n",
      "Epoch 2824 - lr: 0.05000 - Train loss: 1.85597 - Test loss: 2.12504\n",
      "Epoch 2825 - lr: 0.05000 - Train loss: 1.85597 - Test loss: 2.12504\n",
      "Epoch 2826 - lr: 0.05000 - Train loss: 1.85596 - Test loss: 2.12505\n",
      "Epoch 2827 - lr: 0.05000 - Train loss: 1.85596 - Test loss: 2.12506\n",
      "Epoch 2828 - lr: 0.05000 - Train loss: 1.85596 - Test loss: 2.12507\n",
      "Epoch 2829 - lr: 0.05000 - Train loss: 1.85596 - Test loss: 2.12508\n",
      "Epoch 2830 - lr: 0.05000 - Train loss: 1.85596 - Test loss: 2.12508\n",
      "Epoch 2831 - lr: 0.05000 - Train loss: 1.85595 - Test loss: 2.12509\n",
      "Epoch 2832 - lr: 0.05000 - Train loss: 1.85595 - Test loss: 2.12510\n",
      "Epoch 2833 - lr: 0.05000 - Train loss: 1.85595 - Test loss: 2.12511\n",
      "Epoch 2834 - lr: 0.05000 - Train loss: 1.85595 - Test loss: 2.12511\n",
      "Epoch 2835 - lr: 0.05000 - Train loss: 1.85595 - Test loss: 2.12512\n",
      "Epoch 2836 - lr: 0.05000 - Train loss: 1.85594 - Test loss: 2.12513\n",
      "Epoch 2837 - lr: 0.05000 - Train loss: 1.85594 - Test loss: 2.12514\n",
      "Epoch 2838 - lr: 0.05000 - Train loss: 1.85594 - Test loss: 2.12515\n",
      "Epoch 2839 - lr: 0.05000 - Train loss: 1.85594 - Test loss: 2.12515\n",
      "Epoch 2840 - lr: 0.05000 - Train loss: 1.85594 - Test loss: 2.12516\n",
      "Epoch 2841 - lr: 0.05000 - Train loss: 1.85593 - Test loss: 2.12517\n",
      "Epoch 2842 - lr: 0.05000 - Train loss: 1.85593 - Test loss: 2.12518\n",
      "Epoch 2843 - lr: 0.05000 - Train loss: 1.85593 - Test loss: 2.12518\n",
      "Epoch 2844 - lr: 0.05000 - Train loss: 1.85593 - Test loss: 2.12519\n",
      "Epoch 2845 - lr: 0.05000 - Train loss: 1.85593 - Test loss: 2.12520\n",
      "Epoch 2846 - lr: 0.05000 - Train loss: 1.85592 - Test loss: 2.12521\n",
      "Epoch 2847 - lr: 0.05000 - Train loss: 1.85592 - Test loss: 2.12522\n",
      "Epoch 2848 - lr: 0.05000 - Train loss: 1.85592 - Test loss: 2.12522\n",
      "Epoch 2849 - lr: 0.05000 - Train loss: 1.85592 - Test loss: 2.12523\n",
      "Epoch 2850 - lr: 0.05000 - Train loss: 1.85592 - Test loss: 2.12524\n",
      "Epoch 2851 - lr: 0.05000 - Train loss: 1.85591 - Test loss: 2.12525\n",
      "Epoch 2852 - lr: 0.05000 - Train loss: 1.85591 - Test loss: 2.12525\n",
      "Epoch 2853 - lr: 0.05000 - Train loss: 1.85591 - Test loss: 2.12526\n",
      "Epoch 2854 - lr: 0.05000 - Train loss: 1.85591 - Test loss: 2.12527\n",
      "Epoch 2855 - lr: 0.05000 - Train loss: 1.85590 - Test loss: 2.12528\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2856 - lr: 0.05000 - Train loss: 1.85590 - Test loss: 2.12529\n",
      "Epoch 2857 - lr: 0.05000 - Train loss: 1.85590 - Test loss: 2.12529\n",
      "Epoch 2858 - lr: 0.05000 - Train loss: 1.85590 - Test loss: 2.12530\n",
      "Epoch 2859 - lr: 0.05000 - Train loss: 1.85590 - Test loss: 2.12531\n",
      "Epoch 2860 - lr: 0.05000 - Train loss: 1.85589 - Test loss: 2.12532\n",
      "Epoch 2861 - lr: 0.05000 - Train loss: 1.85589 - Test loss: 2.12532\n",
      "Epoch 2862 - lr: 0.05000 - Train loss: 1.85589 - Test loss: 2.12533\n",
      "Epoch 2863 - lr: 0.05000 - Train loss: 1.85589 - Test loss: 2.12534\n",
      "Epoch 2864 - lr: 0.05000 - Train loss: 1.85589 - Test loss: 2.12535\n",
      "Epoch 2865 - lr: 0.05000 - Train loss: 1.85588 - Test loss: 2.12535\n",
      "Epoch 2866 - lr: 0.05000 - Train loss: 1.85588 - Test loss: 2.12536\n",
      "Epoch 2867 - lr: 0.05000 - Train loss: 1.85588 - Test loss: 2.12537\n",
      "Epoch 2868 - lr: 0.05000 - Train loss: 1.85588 - Test loss: 2.12538\n",
      "Epoch 2869 - lr: 0.05000 - Train loss: 1.85588 - Test loss: 2.12539\n",
      "Epoch 2870 - lr: 0.05000 - Train loss: 1.85587 - Test loss: 2.12539\n",
      "Epoch 2871 - lr: 0.05000 - Train loss: 1.85587 - Test loss: 2.12540\n",
      "Epoch 2872 - lr: 0.05000 - Train loss: 1.85587 - Test loss: 2.12541\n",
      "Epoch 2873 - lr: 0.05000 - Train loss: 1.85587 - Test loss: 2.12542\n",
      "Epoch 2874 - lr: 0.05000 - Train loss: 1.85587 - Test loss: 2.12542\n",
      "Epoch 2875 - lr: 0.05000 - Train loss: 1.85586 - Test loss: 2.12543\n",
      "Epoch 2876 - lr: 0.05000 - Train loss: 1.85586 - Test loss: 2.12544\n",
      "Epoch 2877 - lr: 0.05000 - Train loss: 1.85586 - Test loss: 2.12545\n",
      "Epoch 2878 - lr: 0.05000 - Train loss: 1.85586 - Test loss: 2.12545\n",
      "Epoch 2879 - lr: 0.05000 - Train loss: 1.85586 - Test loss: 2.12546\n",
      "Epoch 2880 - lr: 0.05000 - Train loss: 1.85585 - Test loss: 2.12547\n",
      "Epoch 2881 - lr: 0.05000 - Train loss: 1.85585 - Test loss: 2.12548\n",
      "Epoch 2882 - lr: 0.05000 - Train loss: 1.85585 - Test loss: 2.12548\n",
      "Epoch 2883 - lr: 0.05000 - Train loss: 1.85585 - Test loss: 2.12549\n",
      "Epoch 2884 - lr: 0.05000 - Train loss: 1.85585 - Test loss: 2.12550\n",
      "Epoch 2885 - lr: 0.05000 - Train loss: 1.85584 - Test loss: 2.12551\n",
      "Epoch 2886 - lr: 0.05000 - Train loss: 1.85584 - Test loss: 2.12551\n",
      "Epoch 2887 - lr: 0.05000 - Train loss: 1.85584 - Test loss: 2.12552\n",
      "Epoch 2888 - lr: 0.05000 - Train loss: 1.85584 - Test loss: 2.12553\n",
      "Epoch 2889 - lr: 0.05000 - Train loss: 1.85584 - Test loss: 2.12554\n",
      "Epoch 2890 - lr: 0.05000 - Train loss: 1.85583 - Test loss: 2.12554\n",
      "Epoch 2891 - lr: 0.05000 - Train loss: 1.85583 - Test loss: 2.12555\n",
      "Epoch 2892 - lr: 0.05000 - Train loss: 1.85583 - Test loss: 2.12556\n",
      "Epoch 2893 - lr: 0.05000 - Train loss: 1.85583 - Test loss: 2.12557\n",
      "Epoch 2894 - lr: 0.05000 - Train loss: 1.85583 - Test loss: 2.12558\n",
      "Epoch 2895 - lr: 0.05000 - Train loss: 1.85582 - Test loss: 2.12558\n",
      "Epoch 2896 - lr: 0.05000 - Train loss: 1.85582 - Test loss: 2.12559\n",
      "Epoch 2897 - lr: 0.05000 - Train loss: 1.85582 - Test loss: 2.12560\n",
      "Epoch 2898 - lr: 0.05000 - Train loss: 1.85582 - Test loss: 2.12561\n",
      "Epoch 2899 - lr: 0.05000 - Train loss: 1.85582 - Test loss: 2.12561\n",
      "Epoch 2900 - lr: 0.05000 - Train loss: 1.85581 - Test loss: 2.12562\n",
      "Epoch 2901 - lr: 0.05000 - Train loss: 1.85581 - Test loss: 2.12563\n",
      "Epoch 2902 - lr: 0.05000 - Train loss: 1.85581 - Test loss: 2.12564\n",
      "Epoch 2903 - lr: 0.05000 - Train loss: 1.85581 - Test loss: 2.12564\n",
      "Epoch 2904 - lr: 0.05000 - Train loss: 1.85581 - Test loss: 2.12565\n",
      "Epoch 2905 - lr: 0.05000 - Train loss: 1.85580 - Test loss: 2.12566\n",
      "Epoch 2906 - lr: 0.05000 - Train loss: 1.85580 - Test loss: 2.12567\n",
      "Epoch 2907 - lr: 0.05000 - Train loss: 1.85580 - Test loss: 2.12567\n",
      "Epoch 2908 - lr: 0.05000 - Train loss: 1.85580 - Test loss: 2.12568\n",
      "Epoch 2909 - lr: 0.05000 - Train loss: 1.85580 - Test loss: 2.12569\n",
      "Epoch 2910 - lr: 0.05000 - Train loss: 1.85579 - Test loss: 2.12570\n",
      "Epoch 2911 - lr: 0.05000 - Train loss: 1.85579 - Test loss: 2.12570\n",
      "Epoch 2912 - lr: 0.05000 - Train loss: 1.85579 - Test loss: 2.12571\n",
      "Epoch 2913 - lr: 0.05000 - Train loss: 1.85579 - Test loss: 2.12572\n",
      "Epoch 2914 - lr: 0.05000 - Train loss: 1.85579 - Test loss: 2.12573\n",
      "Epoch 2915 - lr: 0.05000 - Train loss: 1.85578 - Test loss: 2.12573\n",
      "Epoch 2916 - lr: 0.05000 - Train loss: 1.85578 - Test loss: 2.12574\n",
      "Epoch 2917 - lr: 0.05000 - Train loss: 1.85578 - Test loss: 2.12575\n",
      "Epoch 2918 - lr: 0.05000 - Train loss: 1.85578 - Test loss: 2.12576\n",
      "Epoch 2919 - lr: 0.05000 - Train loss: 1.85578 - Test loss: 2.12576\n",
      "Epoch 2920 - lr: 0.05000 - Train loss: 1.85577 - Test loss: 2.12577\n",
      "Epoch 2921 - lr: 0.05000 - Train loss: 1.85577 - Test loss: 2.12578\n",
      "Epoch 2922 - lr: 0.05000 - Train loss: 1.85577 - Test loss: 2.12579\n",
      "Epoch 2923 - lr: 0.05000 - Train loss: 1.85577 - Test loss: 2.12579\n",
      "Epoch 2924 - lr: 0.05000 - Train loss: 1.85577 - Test loss: 2.12580\n",
      "Epoch 2925 - lr: 0.05000 - Train loss: 1.85576 - Test loss: 2.12581\n",
      "Epoch 2926 - lr: 0.05000 - Train loss: 1.85576 - Test loss: 2.12582\n",
      "Epoch 2927 - lr: 0.05000 - Train loss: 1.85576 - Test loss: 2.12582\n",
      "Epoch 2928 - lr: 0.05000 - Train loss: 1.85576 - Test loss: 2.12583\n",
      "Epoch 2929 - lr: 0.05000 - Train loss: 1.85576 - Test loss: 2.12584\n",
      "Epoch 2930 - lr: 0.05000 - Train loss: 1.85575 - Test loss: 2.12584\n",
      "Epoch 2931 - lr: 0.05000 - Train loss: 1.85575 - Test loss: 2.12585\n",
      "Epoch 2932 - lr: 0.05000 - Train loss: 1.85575 - Test loss: 2.12586\n",
      "Epoch 2933 - lr: 0.05000 - Train loss: 1.85575 - Test loss: 2.12587\n",
      "Epoch 2934 - lr: 0.05000 - Train loss: 1.85575 - Test loss: 2.12587\n",
      "Epoch 2935 - lr: 0.05000 - Train loss: 1.85574 - Test loss: 2.12588\n",
      "Epoch 2936 - lr: 0.05000 - Train loss: 1.85574 - Test loss: 2.12589\n",
      "Epoch 2937 - lr: 0.05000 - Train loss: 1.85574 - Test loss: 2.12590\n",
      "Epoch 2938 - lr: 0.05000 - Train loss: 1.85574 - Test loss: 2.12590\n",
      "Epoch 2939 - lr: 0.05000 - Train loss: 1.85574 - Test loss: 2.12591\n",
      "Epoch 2940 - lr: 0.05000 - Train loss: 1.85573 - Test loss: 2.12592\n",
      "Epoch 2941 - lr: 0.05000 - Train loss: 1.85573 - Test loss: 2.12593\n",
      "Epoch 2942 - lr: 0.05000 - Train loss: 1.85573 - Test loss: 2.12593\n",
      "Epoch 2943 - lr: 0.05000 - Train loss: 1.85573 - Test loss: 2.12594\n",
      "Epoch 2944 - lr: 0.05000 - Train loss: 1.85573 - Test loss: 2.12595\n",
      "Epoch 2945 - lr: 0.05000 - Train loss: 1.85572 - Test loss: 2.12596\n",
      "Epoch 2946 - lr: 0.05000 - Train loss: 1.85572 - Test loss: 2.12596\n",
      "Epoch 2947 - lr: 0.05000 - Train loss: 1.85572 - Test loss: 2.12597\n",
      "Epoch 2948 - lr: 0.05000 - Train loss: 1.85572 - Test loss: 2.12598\n",
      "Epoch 2949 - lr: 0.05000 - Train loss: 1.85572 - Test loss: 2.12599\n",
      "Epoch 2950 - lr: 0.05000 - Train loss: 1.85571 - Test loss: 2.12599\n",
      "Epoch 2951 - lr: 0.05000 - Train loss: 1.85571 - Test loss: 2.12600\n",
      "Epoch 2952 - lr: 0.05000 - Train loss: 1.85571 - Test loss: 2.12601\n",
      "Epoch 2953 - lr: 0.05000 - Train loss: 1.85571 - Test loss: 2.12601\n",
      "Epoch 2954 - lr: 0.05000 - Train loss: 1.85571 - Test loss: 2.12602\n",
      "Epoch 2955 - lr: 0.05000 - Train loss: 1.85570 - Test loss: 2.12603\n",
      "Epoch 2956 - lr: 0.05000 - Train loss: 1.85570 - Test loss: 2.12604\n",
      "Epoch 2957 - lr: 0.05000 - Train loss: 1.85570 - Test loss: 2.12604\n",
      "Epoch 2958 - lr: 0.05000 - Train loss: 1.85570 - Test loss: 2.12605\n",
      "Epoch 2959 - lr: 0.05000 - Train loss: 1.85570 - Test loss: 2.12606\n",
      "Epoch 2960 - lr: 0.05000 - Train loss: 1.85569 - Test loss: 2.12607\n",
      "Epoch 2961 - lr: 0.05000 - Train loss: 1.85569 - Test loss: 2.12607\n",
      "Epoch 2962 - lr: 0.05000 - Train loss: 1.85569 - Test loss: 2.12608\n",
      "Epoch 2963 - lr: 0.05000 - Train loss: 1.85569 - Test loss: 2.12609\n",
      "Epoch 2964 - lr: 0.05000 - Train loss: 1.85569 - Test loss: 2.12610\n",
      "Epoch 2965 - lr: 0.05000 - Train loss: 1.85568 - Test loss: 2.12610\n",
      "Epoch 2966 - lr: 0.05000 - Train loss: 1.85568 - Test loss: 2.12611\n",
      "Epoch 2967 - lr: 0.05000 - Train loss: 1.85568 - Test loss: 2.12612\n",
      "Epoch 2968 - lr: 0.05000 - Train loss: 1.85568 - Test loss: 2.12612\n",
      "Epoch 2969 - lr: 0.05000 - Train loss: 1.85568 - Test loss: 2.12613\n",
      "Epoch 2970 - lr: 0.05000 - Train loss: 1.85567 - Test loss: 2.12614\n",
      "Epoch 2971 - lr: 0.05000 - Train loss: 1.85567 - Test loss: 2.12615\n",
      "Epoch 2972 - lr: 0.05000 - Train loss: 1.85567 - Test loss: 2.12615\n",
      "Epoch 2973 - lr: 0.05000 - Train loss: 1.85567 - Test loss: 2.12616\n",
      "Epoch 2974 - lr: 0.05000 - Train loss: 1.85567 - Test loss: 2.12617\n",
      "Epoch 2975 - lr: 0.05000 - Train loss: 1.85566 - Test loss: 2.12618\n",
      "Epoch 2976 - lr: 0.05000 - Train loss: 1.85566 - Test loss: 2.12618\n",
      "Epoch 2977 - lr: 0.05000 - Train loss: 1.85566 - Test loss: 2.12619\n",
      "Epoch 2978 - lr: 0.05000 - Train loss: 1.85566 - Test loss: 2.12620\n",
      "Epoch 2979 - lr: 0.05000 - Train loss: 1.85566 - Test loss: 2.12620\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2980 - lr: 0.05000 - Train loss: 1.85565 - Test loss: 2.12621\n",
      "Epoch 2981 - lr: 0.05000 - Train loss: 1.85565 - Test loss: 2.12622\n",
      "Epoch 2982 - lr: 0.05000 - Train loss: 1.85565 - Test loss: 2.12623\n",
      "Epoch 2983 - lr: 0.05000 - Train loss: 1.85565 - Test loss: 2.12623\n",
      "Epoch 2984 - lr: 0.05000 - Train loss: 1.85565 - Test loss: 2.12624\n",
      "Epoch 2985 - lr: 0.05000 - Train loss: 1.85564 - Test loss: 2.12625\n",
      "Epoch 2986 - lr: 0.05000 - Train loss: 1.85564 - Test loss: 2.12626\n",
      "Epoch 2987 - lr: 0.05000 - Train loss: 1.85564 - Test loss: 2.12626\n",
      "Epoch 2988 - lr: 0.05000 - Train loss: 1.85564 - Test loss: 2.12627\n",
      "Epoch 2989 - lr: 0.05000 - Train loss: 1.85564 - Test loss: 2.12628\n",
      "Epoch 2990 - lr: 0.05000 - Train loss: 1.85563 - Test loss: 2.12628\n",
      "Epoch 2991 - lr: 0.05000 - Train loss: 1.85563 - Test loss: 2.12629\n",
      "Epoch 2992 - lr: 0.05000 - Train loss: 1.85563 - Test loss: 2.12630\n",
      "Epoch 2993 - lr: 0.05000 - Train loss: 1.85563 - Test loss: 2.12631\n",
      "Epoch 2994 - lr: 0.05000 - Train loss: 1.85563 - Test loss: 2.12631\n",
      "Epoch 2995 - lr: 0.05000 - Train loss: 1.85562 - Test loss: 2.12632\n",
      "Epoch 2996 - lr: 0.05000 - Train loss: 1.85562 - Test loss: 2.12633\n",
      "Epoch 2997 - lr: 0.05000 - Train loss: 1.85562 - Test loss: 2.12633\n",
      "Epoch 2998 - lr: 0.05000 - Train loss: 1.85562 - Test loss: 2.12634\n",
      "Epoch 2999 - lr: 0.05000 - Train loss: 1.85562 - Test loss: 2.12635\n",
      "Epoch 3000 - lr: 0.05000 - Train loss: 1.85561 - Test loss: 2.12636\n",
      "Epoch 3001 - lr: 0.05000 - Train loss: 1.85561 - Test loss: 2.12636\n",
      "Epoch 3002 - lr: 0.05000 - Train loss: 1.85561 - Test loss: 2.12637\n",
      "Epoch 3003 - lr: 0.05000 - Train loss: 1.85561 - Test loss: 2.12638\n",
      "Epoch 3004 - lr: 0.05000 - Train loss: 1.85561 - Test loss: 2.12639\n",
      "Epoch 3005 - lr: 0.05000 - Train loss: 1.85560 - Test loss: 2.12639\n",
      "Epoch 3006 - lr: 0.05000 - Train loss: 1.85560 - Test loss: 2.12640\n",
      "Epoch 3007 - lr: 0.05000 - Train loss: 1.85560 - Test loss: 2.12641\n",
      "Epoch 3008 - lr: 0.05000 - Train loss: 1.85560 - Test loss: 2.12641\n",
      "Epoch 3009 - lr: 0.05000 - Train loss: 1.85560 - Test loss: 2.12642\n",
      "Epoch 3010 - lr: 0.05000 - Train loss: 1.85559 - Test loss: 2.12643\n",
      "Epoch 3011 - lr: 0.05000 - Train loss: 1.85559 - Test loss: 2.12644\n",
      "Epoch 3012 - lr: 0.05000 - Train loss: 1.85559 - Test loss: 2.12644\n",
      "Epoch 3013 - lr: 0.05000 - Train loss: 1.85559 - Test loss: 2.12645\n",
      "Epoch 3014 - lr: 0.05000 - Train loss: 1.85559 - Test loss: 2.12646\n",
      "Epoch 3015 - lr: 0.05000 - Train loss: 1.85558 - Test loss: 2.12646\n",
      "Epoch 3016 - lr: 0.05000 - Train loss: 1.85558 - Test loss: 2.12647\n",
      "Epoch 3017 - lr: 0.05000 - Train loss: 1.85558 - Test loss: 2.12648\n",
      "Epoch 3018 - lr: 0.05000 - Train loss: 1.85558 - Test loss: 2.12649\n",
      "Epoch 3019 - lr: 0.05000 - Train loss: 1.85558 - Test loss: 2.12649\n",
      "Epoch 3020 - lr: 0.05000 - Train loss: 1.85558 - Test loss: 2.12650\n",
      "Epoch 3021 - lr: 0.05000 - Train loss: 1.85557 - Test loss: 2.12651\n",
      "Epoch 3022 - lr: 0.05000 - Train loss: 1.85557 - Test loss: 2.12651\n",
      "Epoch 3023 - lr: 0.05000 - Train loss: 1.85557 - Test loss: 2.12652\n",
      "Epoch 3024 - lr: 0.05000 - Train loss: 1.85557 - Test loss: 2.12653\n",
      "Epoch 3025 - lr: 0.05000 - Train loss: 1.85557 - Test loss: 2.12654\n",
      "Epoch 3026 - lr: 0.05000 - Train loss: 1.85556 - Test loss: 2.12654\n",
      "Epoch 3027 - lr: 0.05000 - Train loss: 1.85556 - Test loss: 2.12655\n",
      "Epoch 3028 - lr: 0.05000 - Train loss: 1.85556 - Test loss: 2.12656\n",
      "Epoch 3029 - lr: 0.05000 - Train loss: 1.85556 - Test loss: 2.12656\n",
      "Epoch 3030 - lr: 0.05000 - Train loss: 1.85556 - Test loss: 2.12657\n",
      "Epoch 3031 - lr: 0.05000 - Train loss: 1.85555 - Test loss: 2.12658\n",
      "Epoch 3032 - lr: 0.05000 - Train loss: 1.85555 - Test loss: 2.12658\n",
      "Epoch 3033 - lr: 0.05000 - Train loss: 1.85555 - Test loss: 2.12659\n",
      "Epoch 3034 - lr: 0.05000 - Train loss: 1.85555 - Test loss: 2.12660\n",
      "Epoch 3035 - lr: 0.05000 - Train loss: 1.85555 - Test loss: 2.12661\n",
      "Epoch 3036 - lr: 0.05000 - Train loss: 1.85554 - Test loss: 2.12661\n",
      "Epoch 3037 - lr: 0.05000 - Train loss: 1.85554 - Test loss: 2.12662\n",
      "Epoch 3038 - lr: 0.05000 - Train loss: 1.85554 - Test loss: 2.12663\n",
      "Epoch 3039 - lr: 0.05000 - Train loss: 1.85554 - Test loss: 2.12663\n",
      "Epoch 3040 - lr: 0.05000 - Train loss: 1.85554 - Test loss: 2.12664\n",
      "Epoch 3041 - lr: 0.05000 - Train loss: 1.85553 - Test loss: 2.12665\n",
      "Epoch 3042 - lr: 0.05000 - Train loss: 1.85553 - Test loss: 2.12666\n",
      "Epoch 3043 - lr: 0.05000 - Train loss: 1.85553 - Test loss: 2.12666\n",
      "Epoch 3044 - lr: 0.05000 - Train loss: 1.85553 - Test loss: 2.12667\n",
      "Epoch 3045 - lr: 0.05000 - Train loss: 1.85553 - Test loss: 2.12668\n",
      "Epoch 3046 - lr: 0.05000 - Train loss: 1.85552 - Test loss: 2.12668\n",
      "Epoch 3047 - lr: 0.05000 - Train loss: 1.85552 - Test loss: 2.12669\n",
      "Epoch 3048 - lr: 0.05000 - Train loss: 1.85552 - Test loss: 2.12670\n",
      "Epoch 3049 - lr: 0.05000 - Train loss: 1.85552 - Test loss: 2.12670\n",
      "Epoch 3050 - lr: 0.05000 - Train loss: 1.85552 - Test loss: 2.12671\n",
      "Epoch 3051 - lr: 0.05000 - Train loss: 1.85551 - Test loss: 2.12672\n",
      "Epoch 3052 - lr: 0.05000 - Train loss: 1.85551 - Test loss: 2.12673\n",
      "Epoch 3053 - lr: 0.05000 - Train loss: 1.85551 - Test loss: 2.12673\n",
      "Epoch 3054 - lr: 0.05000 - Train loss: 1.85551 - Test loss: 2.12674\n",
      "Epoch 3055 - lr: 0.05000 - Train loss: 1.85551 - Test loss: 2.12675\n",
      "Epoch 3056 - lr: 0.05000 - Train loss: 1.85551 - Test loss: 2.12675\n",
      "Epoch 3057 - lr: 0.05000 - Train loss: 1.85550 - Test loss: 2.12676\n",
      "Epoch 3058 - lr: 0.05000 - Train loss: 1.85550 - Test loss: 2.12677\n",
      "Epoch 3059 - lr: 0.05000 - Train loss: 1.85550 - Test loss: 2.12678\n",
      "Epoch 3060 - lr: 0.05000 - Train loss: 1.85550 - Test loss: 2.12678\n",
      "Epoch 3061 - lr: 0.05000 - Train loss: 1.85550 - Test loss: 2.12679\n",
      "Epoch 3062 - lr: 0.05000 - Train loss: 1.85549 - Test loss: 2.12680\n",
      "Epoch 3063 - lr: 0.05000 - Train loss: 1.85549 - Test loss: 2.12680\n",
      "Epoch 3064 - lr: 0.05000 - Train loss: 1.85549 - Test loss: 2.12681\n",
      "Epoch 3065 - lr: 0.05000 - Train loss: 1.85549 - Test loss: 2.12682\n",
      "Epoch 3066 - lr: 0.05000 - Train loss: 1.85549 - Test loss: 2.12682\n",
      "Epoch 3067 - lr: 0.05000 - Train loss: 1.85548 - Test loss: 2.12683\n",
      "Epoch 3068 - lr: 0.05000 - Train loss: 1.85548 - Test loss: 2.12684\n",
      "Epoch 3069 - lr: 0.05000 - Train loss: 1.85548 - Test loss: 2.12684\n",
      "Epoch 3070 - lr: 0.05000 - Train loss: 1.85548 - Test loss: 2.12685\n",
      "Epoch 3071 - lr: 0.05000 - Train loss: 1.85548 - Test loss: 2.12686\n",
      "Epoch 3072 - lr: 0.05000 - Train loss: 1.85547 - Test loss: 2.12687\n",
      "Epoch 3073 - lr: 0.05000 - Train loss: 1.85547 - Test loss: 2.12687\n",
      "Epoch 3074 - lr: 0.05000 - Train loss: 1.85547 - Test loss: 2.12688\n",
      "Epoch 3075 - lr: 0.05000 - Train loss: 1.85547 - Test loss: 2.12689\n",
      "Epoch 3076 - lr: 0.05000 - Train loss: 1.85547 - Test loss: 2.12689\n",
      "Epoch 3077 - lr: 0.05000 - Train loss: 1.85546 - Test loss: 2.12690\n",
      "Epoch 3078 - lr: 0.05000 - Train loss: 1.85546 - Test loss: 2.12691\n",
      "Epoch 3079 - lr: 0.05000 - Train loss: 1.85546 - Test loss: 2.12691\n",
      "Epoch 3080 - lr: 0.05000 - Train loss: 1.85546 - Test loss: 2.12692\n",
      "Epoch 3081 - lr: 0.05000 - Train loss: 1.85546 - Test loss: 2.12693\n",
      "Epoch 3082 - lr: 0.05000 - Train loss: 1.85546 - Test loss: 2.12694\n",
      "Epoch 3083 - lr: 0.05000 - Train loss: 1.85545 - Test loss: 2.12694\n",
      "Epoch 3084 - lr: 0.05000 - Train loss: 1.85545 - Test loss: 2.12695\n",
      "Epoch 3085 - lr: 0.05000 - Train loss: 1.85545 - Test loss: 2.12696\n",
      "Epoch 3086 - lr: 0.05000 - Train loss: 1.85545 - Test loss: 2.12696\n",
      "Epoch 3087 - lr: 0.05000 - Train loss: 1.85545 - Test loss: 2.12697\n",
      "Epoch 3088 - lr: 0.05000 - Train loss: 1.85544 - Test loss: 2.12698\n",
      "Epoch 3089 - lr: 0.05000 - Train loss: 1.85544 - Test loss: 2.12698\n",
      "Epoch 3090 - lr: 0.05000 - Train loss: 1.85544 - Test loss: 2.12699\n",
      "Epoch 3091 - lr: 0.05000 - Train loss: 1.85544 - Test loss: 2.12700\n",
      "Epoch 3092 - lr: 0.05000 - Train loss: 1.85544 - Test loss: 2.12700\n",
      "Epoch 3093 - lr: 0.05000 - Train loss: 1.85543 - Test loss: 2.12701\n",
      "Epoch 3094 - lr: 0.05000 - Train loss: 1.85543 - Test loss: 2.12702\n",
      "Epoch 3095 - lr: 0.05000 - Train loss: 1.85543 - Test loss: 2.12703\n",
      "Epoch 3096 - lr: 0.05000 - Train loss: 1.85543 - Test loss: 2.12703\n",
      "Epoch 3097 - lr: 0.05000 - Train loss: 1.85543 - Test loss: 2.12704\n",
      "Epoch 3098 - lr: 0.05000 - Train loss: 1.85542 - Test loss: 2.12705\n",
      "Epoch 3099 - lr: 0.05000 - Train loss: 1.85542 - Test loss: 2.12705\n",
      "Epoch 3100 - lr: 0.05000 - Train loss: 1.85542 - Test loss: 2.12706\n",
      "Epoch 3101 - lr: 0.05000 - Train loss: 1.85542 - Test loss: 2.12707\n",
      "Epoch 3102 - lr: 0.05000 - Train loss: 1.85542 - Test loss: 2.12707\n",
      "Epoch 3103 - lr: 0.05000 - Train loss: 1.85541 - Test loss: 2.12708\n",
      "Epoch 3104 - lr: 0.05000 - Train loss: 1.85541 - Test loss: 2.12709\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3105 - lr: 0.05000 - Train loss: 1.85541 - Test loss: 2.12709\n",
      "Epoch 3106 - lr: 0.05000 - Train loss: 1.85541 - Test loss: 2.12710\n",
      "Epoch 3107 - lr: 0.05000 - Train loss: 1.85541 - Test loss: 2.12711\n",
      "Epoch 3108 - lr: 0.05000 - Train loss: 1.85541 - Test loss: 2.12711\n",
      "Epoch 3109 - lr: 0.05000 - Train loss: 1.85540 - Test loss: 2.12712\n",
      "Epoch 3110 - lr: 0.05000 - Train loss: 1.85540 - Test loss: 2.12713\n",
      "Epoch 3111 - lr: 0.05000 - Train loss: 1.85540 - Test loss: 2.12714\n",
      "Epoch 3112 - lr: 0.05000 - Train loss: 1.85540 - Test loss: 2.12714\n",
      "Epoch 3113 - lr: 0.05000 - Train loss: 1.85540 - Test loss: 2.12715\n",
      "Epoch 3114 - lr: 0.05000 - Train loss: 1.85539 - Test loss: 2.12716\n",
      "Epoch 3115 - lr: 0.05000 - Train loss: 1.85539 - Test loss: 2.12716\n",
      "Epoch 3116 - lr: 0.05000 - Train loss: 1.85539 - Test loss: 2.12717\n",
      "Epoch 3117 - lr: 0.05000 - Train loss: 1.85539 - Test loss: 2.12718\n",
      "Epoch 3118 - lr: 0.05000 - Train loss: 1.85539 - Test loss: 2.12718\n",
      "Epoch 3119 - lr: 0.05000 - Train loss: 1.85538 - Test loss: 2.12719\n",
      "Epoch 3120 - lr: 0.05000 - Train loss: 1.85538 - Test loss: 2.12720\n",
      "Epoch 3121 - lr: 0.05000 - Train loss: 1.85538 - Test loss: 2.12720\n",
      "Epoch 3122 - lr: 0.05000 - Train loss: 1.85538 - Test loss: 2.12721\n",
      "Epoch 3123 - lr: 0.05000 - Train loss: 1.85538 - Test loss: 2.12722\n",
      "Epoch 3124 - lr: 0.05000 - Train loss: 1.85537 - Test loss: 2.12722\n",
      "Epoch 3125 - lr: 0.05000 - Train loss: 1.85537 - Test loss: 2.12723\n",
      "Epoch 3126 - lr: 0.05000 - Train loss: 1.85537 - Test loss: 2.12724\n",
      "Epoch 3127 - lr: 0.05000 - Train loss: 1.85537 - Test loss: 2.12724\n",
      "Epoch 3128 - lr: 0.05000 - Train loss: 1.85537 - Test loss: 2.12725\n",
      "Epoch 3129 - lr: 0.05000 - Train loss: 1.85537 - Test loss: 2.12726\n",
      "Epoch 3130 - lr: 0.05000 - Train loss: 1.85536 - Test loss: 2.12726\n",
      "Epoch 3131 - lr: 0.05000 - Train loss: 1.85536 - Test loss: 2.12727\n",
      "Epoch 3132 - lr: 0.05000 - Train loss: 1.85536 - Test loss: 2.12728\n",
      "Epoch 3133 - lr: 0.05000 - Train loss: 1.85536 - Test loss: 2.12728\n",
      "Epoch 3134 - lr: 0.05000 - Train loss: 1.85536 - Test loss: 2.12729\n",
      "Epoch 3135 - lr: 0.05000 - Train loss: 1.85535 - Test loss: 2.12730\n",
      "Epoch 3136 - lr: 0.05000 - Train loss: 1.85535 - Test loss: 2.12731\n",
      "Epoch 3137 - lr: 0.05000 - Train loss: 1.85535 - Test loss: 2.12731\n",
      "Epoch 3138 - lr: 0.05000 - Train loss: 1.85535 - Test loss: 2.12732\n",
      "Epoch 3139 - lr: 0.05000 - Train loss: 1.85535 - Test loss: 2.12733\n",
      "Epoch 3140 - lr: 0.05000 - Train loss: 1.85534 - Test loss: 2.12733\n",
      "Epoch 3141 - lr: 0.05000 - Train loss: 1.85534 - Test loss: 2.12734\n",
      "Epoch 3142 - lr: 0.05000 - Train loss: 1.85534 - Test loss: 2.12735\n",
      "Epoch 3143 - lr: 0.05000 - Train loss: 1.85534 - Test loss: 2.12735\n",
      "Epoch 3144 - lr: 0.05000 - Train loss: 1.85534 - Test loss: 2.12736\n",
      "Epoch 3145 - lr: 0.05000 - Train loss: 1.85534 - Test loss: 2.12737\n",
      "Epoch 3146 - lr: 0.05000 - Train loss: 1.85533 - Test loss: 2.12737\n",
      "Epoch 3147 - lr: 0.05000 - Train loss: 1.85533 - Test loss: 2.12738\n",
      "Epoch 3148 - lr: 0.05000 - Train loss: 1.85533 - Test loss: 2.12739\n",
      "Epoch 3149 - lr: 0.05000 - Train loss: 1.85533 - Test loss: 2.12739\n",
      "Epoch 3150 - lr: 0.05000 - Train loss: 1.85533 - Test loss: 2.12740\n",
      "Epoch 3151 - lr: 0.05000 - Train loss: 1.85532 - Test loss: 2.12741\n",
      "Epoch 3152 - lr: 0.05000 - Train loss: 1.85532 - Test loss: 2.12741\n",
      "Epoch 3153 - lr: 0.05000 - Train loss: 1.85532 - Test loss: 2.12742\n",
      "Epoch 3154 - lr: 0.05000 - Train loss: 1.85532 - Test loss: 2.12743\n",
      "Epoch 3155 - lr: 0.05000 - Train loss: 1.85532 - Test loss: 2.12743\n",
      "Epoch 3156 - lr: 0.05000 - Train loss: 1.85531 - Test loss: 2.12744\n",
      "Epoch 3157 - lr: 0.05000 - Train loss: 1.85531 - Test loss: 2.12745\n",
      "Epoch 3158 - lr: 0.05000 - Train loss: 1.85531 - Test loss: 2.12745\n",
      "Epoch 3159 - lr: 0.05000 - Train loss: 1.85531 - Test loss: 2.12746\n",
      "Epoch 3160 - lr: 0.05000 - Train loss: 1.85531 - Test loss: 2.12747\n",
      "Epoch 3161 - lr: 0.05000 - Train loss: 1.85530 - Test loss: 2.12747\n",
      "Epoch 3162 - lr: 0.05000 - Train loss: 1.85530 - Test loss: 2.12748\n",
      "Epoch 3163 - lr: 0.05000 - Train loss: 1.85530 - Test loss: 2.12749\n",
      "Epoch 3164 - lr: 0.05000 - Train loss: 1.85530 - Test loss: 2.12749\n",
      "Epoch 3165 - lr: 0.05000 - Train loss: 1.85530 - Test loss: 2.12750\n",
      "Epoch 3166 - lr: 0.05000 - Train loss: 1.85530 - Test loss: 2.12751\n",
      "Epoch 3167 - lr: 0.05000 - Train loss: 1.85529 - Test loss: 2.12751\n",
      "Epoch 3168 - lr: 0.05000 - Train loss: 1.85529 - Test loss: 2.12752\n",
      "Epoch 3169 - lr: 0.05000 - Train loss: 1.85529 - Test loss: 2.12753\n",
      "Epoch 3170 - lr: 0.05000 - Train loss: 1.85529 - Test loss: 2.12753\n",
      "Epoch 3171 - lr: 0.05000 - Train loss: 1.85529 - Test loss: 2.12754\n",
      "Epoch 3172 - lr: 0.05000 - Train loss: 1.85528 - Test loss: 2.12755\n",
      "Epoch 3173 - lr: 0.05000 - Train loss: 1.85528 - Test loss: 2.12755\n",
      "Epoch 3174 - lr: 0.05000 - Train loss: 1.85528 - Test loss: 2.12756\n",
      "Epoch 3175 - lr: 0.05000 - Train loss: 1.85528 - Test loss: 2.12757\n",
      "Epoch 3176 - lr: 0.05000 - Train loss: 1.85528 - Test loss: 2.12757\n",
      "Epoch 3177 - lr: 0.05000 - Train loss: 1.85527 - Test loss: 2.12758\n",
      "Epoch 3178 - lr: 0.05000 - Train loss: 1.85527 - Test loss: 2.12759\n",
      "Epoch 3179 - lr: 0.05000 - Train loss: 1.85527 - Test loss: 2.12759\n",
      "Epoch 3180 - lr: 0.05000 - Train loss: 1.85527 - Test loss: 2.12760\n",
      "Epoch 3181 - lr: 0.05000 - Train loss: 1.85527 - Test loss: 2.12761\n",
      "Epoch 3182 - lr: 0.05000 - Train loss: 1.85527 - Test loss: 2.12761\n",
      "Epoch 3183 - lr: 0.05000 - Train loss: 1.85526 - Test loss: 2.12762\n",
      "Epoch 3184 - lr: 0.05000 - Train loss: 1.85526 - Test loss: 2.12763\n",
      "Epoch 3185 - lr: 0.05000 - Train loss: 1.85526 - Test loss: 2.12763\n",
      "Epoch 3186 - lr: 0.05000 - Train loss: 1.85526 - Test loss: 2.12764\n",
      "Epoch 3187 - lr: 0.05000 - Train loss: 1.85526 - Test loss: 2.12765\n",
      "Epoch 3188 - lr: 0.05000 - Train loss: 1.85525 - Test loss: 2.12765\n",
      "Epoch 3189 - lr: 0.05000 - Train loss: 1.85525 - Test loss: 2.12766\n",
      "Epoch 3190 - lr: 0.05000 - Train loss: 1.85525 - Test loss: 2.12767\n",
      "Epoch 3191 - lr: 0.05000 - Train loss: 1.85525 - Test loss: 2.12767\n",
      "Epoch 3192 - lr: 0.05000 - Train loss: 1.85525 - Test loss: 2.12768\n",
      "Epoch 3193 - lr: 0.05000 - Train loss: 1.85524 - Test loss: 2.12769\n",
      "Epoch 3194 - lr: 0.05000 - Train loss: 1.85524 - Test loss: 2.12769\n",
      "Epoch 3195 - lr: 0.05000 - Train loss: 1.85524 - Test loss: 2.12770\n",
      "Epoch 3196 - lr: 0.05000 - Train loss: 1.85524 - Test loss: 2.12771\n",
      "Epoch 3197 - lr: 0.05000 - Train loss: 1.85524 - Test loss: 2.12771\n",
      "Epoch 3198 - lr: 0.05000 - Train loss: 1.85524 - Test loss: 2.12772\n",
      "Epoch 3199 - lr: 0.05000 - Train loss: 1.85523 - Test loss: 2.12773\n",
      "Epoch 3200 - lr: 0.05000 - Train loss: 1.85523 - Test loss: 2.12773\n",
      "Epoch 3201 - lr: 0.05000 - Train loss: 1.85523 - Test loss: 2.12774\n",
      "Epoch 3202 - lr: 0.05000 - Train loss: 1.85523 - Test loss: 2.12775\n",
      "Epoch 3203 - lr: 0.05000 - Train loss: 1.85523 - Test loss: 2.12775\n",
      "Epoch 3204 - lr: 0.05000 - Train loss: 1.85522 - Test loss: 2.12776\n",
      "Epoch 3205 - lr: 0.05000 - Train loss: 1.85522 - Test loss: 2.12777\n",
      "Epoch 3206 - lr: 0.05000 - Train loss: 1.85522 - Test loss: 2.12777\n",
      "Epoch 3207 - lr: 0.05000 - Train loss: 1.85522 - Test loss: 2.12778\n",
      "Epoch 3208 - lr: 0.05000 - Train loss: 1.85522 - Test loss: 2.12779\n",
      "Epoch 3209 - lr: 0.05000 - Train loss: 1.85522 - Test loss: 2.12779\n",
      "Epoch 3210 - lr: 0.05000 - Train loss: 1.85521 - Test loss: 2.12780\n",
      "Epoch 3211 - lr: 0.05000 - Train loss: 1.85521 - Test loss: 2.12781\n",
      "Epoch 3212 - lr: 0.05000 - Train loss: 1.85521 - Test loss: 2.12781\n",
      "Epoch 3213 - lr: 0.05000 - Train loss: 1.85521 - Test loss: 2.12782\n",
      "Epoch 3214 - lr: 0.05000 - Train loss: 1.85521 - Test loss: 2.12782\n",
      "Epoch 3215 - lr: 0.05000 - Train loss: 1.85520 - Test loss: 2.12783\n",
      "Epoch 3216 - lr: 0.05000 - Train loss: 1.85520 - Test loss: 2.12784\n",
      "Epoch 3217 - lr: 0.05000 - Train loss: 1.85520 - Test loss: 2.12784\n",
      "Epoch 3218 - lr: 0.05000 - Train loss: 1.85520 - Test loss: 2.12785\n",
      "Epoch 3219 - lr: 0.05000 - Train loss: 1.85520 - Test loss: 2.12786\n",
      "Epoch 3220 - lr: 0.05000 - Train loss: 1.85519 - Test loss: 2.12786\n",
      "Epoch 3221 - lr: 0.05000 - Train loss: 1.85519 - Test loss: 2.12787\n",
      "Epoch 3222 - lr: 0.05000 - Train loss: 1.85519 - Test loss: 2.12788\n",
      "Epoch 3223 - lr: 0.05000 - Train loss: 1.85519 - Test loss: 2.12788\n",
      "Epoch 3224 - lr: 0.05000 - Train loss: 1.85519 - Test loss: 2.12789\n",
      "Epoch 3225 - lr: 0.05000 - Train loss: 1.85519 - Test loss: 2.12790\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3226 - lr: 0.05000 - Train loss: 1.85518 - Test loss: 2.12790\n",
      "Epoch 3227 - lr: 0.05000 - Train loss: 1.85518 - Test loss: 2.12791\n",
      "Epoch 3228 - lr: 0.05000 - Train loss: 1.85518 - Test loss: 2.12792\n",
      "Epoch 3229 - lr: 0.05000 - Train loss: 1.85518 - Test loss: 2.12792\n",
      "Epoch 3230 - lr: 0.05000 - Train loss: 1.85518 - Test loss: 2.12793\n",
      "Epoch 3231 - lr: 0.05000 - Train loss: 1.85517 - Test loss: 2.12794\n",
      "Epoch 3232 - lr: 0.05000 - Train loss: 1.85517 - Test loss: 2.12794\n",
      "Epoch 3233 - lr: 0.05000 - Train loss: 1.85517 - Test loss: 2.12795\n",
      "Epoch 3234 - lr: 0.05000 - Train loss: 1.85517 - Test loss: 2.12796\n",
      "Epoch 3235 - lr: 0.05000 - Train loss: 1.85517 - Test loss: 2.12796\n",
      "Epoch 3236 - lr: 0.05000 - Train loss: 1.85517 - Test loss: 2.12797\n",
      "Epoch 3237 - lr: 0.05000 - Train loss: 1.85516 - Test loss: 2.12797\n",
      "Epoch 3238 - lr: 0.05000 - Train loss: 1.85516 - Test loss: 2.12798\n",
      "Epoch 3239 - lr: 0.05000 - Train loss: 1.85516 - Test loss: 2.12799\n",
      "Epoch 3240 - lr: 0.05000 - Train loss: 1.85516 - Test loss: 2.12799\n",
      "Epoch 3241 - lr: 0.05000 - Train loss: 1.85516 - Test loss: 2.12800\n",
      "Epoch 3242 - lr: 0.05000 - Train loss: 1.85515 - Test loss: 2.12801\n",
      "Epoch 3243 - lr: 0.05000 - Train loss: 1.85515 - Test loss: 2.12801\n",
      "Epoch 3244 - lr: 0.05000 - Train loss: 1.85515 - Test loss: 2.12802\n",
      "Epoch 3245 - lr: 0.05000 - Train loss: 1.85515 - Test loss: 2.12803\n",
      "Epoch 3246 - lr: 0.05000 - Train loss: 1.85515 - Test loss: 2.12803\n",
      "Epoch 3247 - lr: 0.05000 - Train loss: 1.85515 - Test loss: 2.12804\n",
      "Epoch 3248 - lr: 0.05000 - Train loss: 1.85514 - Test loss: 2.12805\n",
      "Epoch 3249 - lr: 0.05000 - Train loss: 1.85514 - Test loss: 2.12805\n",
      "Epoch 3250 - lr: 0.05000 - Train loss: 1.85514 - Test loss: 2.12806\n",
      "Epoch 3251 - lr: 0.05000 - Train loss: 1.85514 - Test loss: 2.12807\n",
      "Epoch 3252 - lr: 0.05000 - Train loss: 1.85514 - Test loss: 2.12807\n",
      "Epoch 3253 - lr: 0.05000 - Train loss: 1.85513 - Test loss: 2.12808\n",
      "Epoch 3254 - lr: 0.05000 - Train loss: 1.85513 - Test loss: 2.12808\n",
      "Epoch 3255 - lr: 0.05000 - Train loss: 1.85513 - Test loss: 2.12809\n",
      "Epoch 3256 - lr: 0.05000 - Train loss: 1.85513 - Test loss: 2.12810\n",
      "Epoch 3257 - lr: 0.05000 - Train loss: 1.85513 - Test loss: 2.12810\n",
      "Epoch 3258 - lr: 0.05000 - Train loss: 1.85512 - Test loss: 2.12811\n",
      "Epoch 3259 - lr: 0.05000 - Train loss: 1.85512 - Test loss: 2.12812\n",
      "Epoch 3260 - lr: 0.05000 - Train loss: 1.85512 - Test loss: 2.12812\n",
      "Epoch 3261 - lr: 0.05000 - Train loss: 1.85512 - Test loss: 2.12813\n",
      "Epoch 3262 - lr: 0.05000 - Train loss: 1.85512 - Test loss: 2.12814\n",
      "Epoch 3263 - lr: 0.05000 - Train loss: 1.85512 - Test loss: 2.12814\n",
      "Epoch 3264 - lr: 0.05000 - Train loss: 1.85511 - Test loss: 2.12815\n",
      "Epoch 3265 - lr: 0.05000 - Train loss: 1.85511 - Test loss: 2.12816\n",
      "Epoch 3266 - lr: 0.05000 - Train loss: 1.85511 - Test loss: 2.12816\n",
      "Epoch 3267 - lr: 0.05000 - Train loss: 1.85511 - Test loss: 2.12817\n",
      "Epoch 3268 - lr: 0.05000 - Train loss: 1.85511 - Test loss: 2.12817\n",
      "Epoch 3269 - lr: 0.05000 - Train loss: 1.85510 - Test loss: 2.12818\n",
      "Epoch 3270 - lr: 0.05000 - Train loss: 1.85510 - Test loss: 2.12819\n",
      "Epoch 3271 - lr: 0.05000 - Train loss: 1.85510 - Test loss: 2.12819\n",
      "Epoch 3272 - lr: 0.05000 - Train loss: 1.85510 - Test loss: 2.12820\n",
      "Epoch 3273 - lr: 0.05000 - Train loss: 1.85510 - Test loss: 2.12821\n",
      "Epoch 3274 - lr: 0.05000 - Train loss: 1.85510 - Test loss: 2.12821\n",
      "Epoch 3275 - lr: 0.05000 - Train loss: 1.85509 - Test loss: 2.12822\n",
      "Epoch 3276 - lr: 0.05000 - Train loss: 1.85509 - Test loss: 2.12823\n",
      "Epoch 3277 - lr: 0.05000 - Train loss: 1.85509 - Test loss: 2.12823\n",
      "Epoch 3278 - lr: 0.05000 - Train loss: 1.85509 - Test loss: 2.12824\n",
      "Epoch 3279 - lr: 0.05000 - Train loss: 1.85509 - Test loss: 2.12825\n",
      "Epoch 3280 - lr: 0.05000 - Train loss: 1.85508 - Test loss: 2.12825\n",
      "Epoch 3281 - lr: 0.05000 - Train loss: 1.85508 - Test loss: 2.12826\n",
      "Epoch 3282 - lr: 0.05000 - Train loss: 1.85508 - Test loss: 2.12826\n",
      "Epoch 3283 - lr: 0.05000 - Train loss: 1.85508 - Test loss: 2.12827\n",
      "Epoch 3284 - lr: 0.05000 - Train loss: 1.85508 - Test loss: 2.12828\n",
      "Epoch 3285 - lr: 0.05000 - Train loss: 1.85508 - Test loss: 2.12828\n",
      "Epoch 3286 - lr: 0.05000 - Train loss: 1.85507 - Test loss: 2.12829\n",
      "Epoch 3287 - lr: 0.05000 - Train loss: 1.85507 - Test loss: 2.12830\n",
      "Epoch 3288 - lr: 0.05000 - Train loss: 1.85507 - Test loss: 2.12830\n",
      "Epoch 3289 - lr: 0.05000 - Train loss: 1.85507 - Test loss: 2.12831\n",
      "Epoch 3290 - lr: 0.05000 - Train loss: 1.85507 - Test loss: 2.12832\n",
      "Epoch 3291 - lr: 0.05000 - Train loss: 1.85506 - Test loss: 2.12832\n",
      "Epoch 3292 - lr: 0.05000 - Train loss: 1.85506 - Test loss: 2.12833\n",
      "Epoch 3293 - lr: 0.05000 - Train loss: 1.85506 - Test loss: 2.12833\n",
      "Epoch 3294 - lr: 0.05000 - Train loss: 1.85506 - Test loss: 2.12834\n",
      "Epoch 3295 - lr: 0.05000 - Train loss: 1.85506 - Test loss: 2.12835\n",
      "Epoch 3296 - lr: 0.05000 - Train loss: 1.85506 - Test loss: 2.12835\n",
      "Epoch 3297 - lr: 0.05000 - Train loss: 1.85505 - Test loss: 2.12836\n",
      "Epoch 3298 - lr: 0.05000 - Train loss: 1.85505 - Test loss: 2.12837\n",
      "Epoch 3299 - lr: 0.05000 - Train loss: 1.85505 - Test loss: 2.12837\n",
      "Epoch 3300 - lr: 0.05000 - Train loss: 1.85505 - Test loss: 2.12838\n",
      "Epoch 3301 - lr: 0.05000 - Train loss: 1.85505 - Test loss: 2.12838\n",
      "Epoch 3302 - lr: 0.05000 - Train loss: 1.85504 - Test loss: 2.12839\n",
      "Epoch 3303 - lr: 0.05000 - Train loss: 1.85504 - Test loss: 2.12840\n",
      "Epoch 3304 - lr: 0.05000 - Train loss: 1.85504 - Test loss: 2.12840\n",
      "Epoch 3305 - lr: 0.05000 - Train loss: 1.85504 - Test loss: 2.12841\n",
      "Epoch 3306 - lr: 0.05000 - Train loss: 1.85504 - Test loss: 2.12842\n",
      "Epoch 3307 - lr: 0.05000 - Train loss: 1.85504 - Test loss: 2.12842\n",
      "Epoch 3308 - lr: 0.05000 - Train loss: 1.85503 - Test loss: 2.12843\n",
      "Epoch 3309 - lr: 0.05000 - Train loss: 1.85503 - Test loss: 2.12844\n",
      "Epoch 3310 - lr: 0.05000 - Train loss: 1.85503 - Test loss: 2.12844\n",
      "Epoch 3311 - lr: 0.05000 - Train loss: 1.85503 - Test loss: 2.12845\n",
      "Epoch 3312 - lr: 0.05000 - Train loss: 1.85503 - Test loss: 2.12845\n",
      "Epoch 3313 - lr: 0.05000 - Train loss: 1.85502 - Test loss: 2.12846\n",
      "Epoch 3314 - lr: 0.05000 - Train loss: 1.85502 - Test loss: 2.12847\n",
      "Epoch 3315 - lr: 0.05000 - Train loss: 1.85502 - Test loss: 2.12847\n",
      "Epoch 3316 - lr: 0.05000 - Train loss: 1.85502 - Test loss: 2.12848\n",
      "Epoch 3317 - lr: 0.05000 - Train loss: 1.85502 - Test loss: 2.12849\n",
      "Epoch 3318 - lr: 0.05000 - Train loss: 1.85502 - Test loss: 2.12849\n",
      "Epoch 3319 - lr: 0.05000 - Train loss: 1.85501 - Test loss: 2.12850\n",
      "Epoch 3320 - lr: 0.05000 - Train loss: 1.85501 - Test loss: 2.12850\n",
      "Epoch 3321 - lr: 0.05000 - Train loss: 1.85501 - Test loss: 2.12851\n",
      "Epoch 3322 - lr: 0.05000 - Train loss: 1.85501 - Test loss: 2.12852\n",
      "Epoch 3323 - lr: 0.05000 - Train loss: 1.85501 - Test loss: 2.12852\n",
      "Epoch 3324 - lr: 0.05000 - Train loss: 1.85501 - Test loss: 2.12853\n",
      "Epoch 3325 - lr: 0.05000 - Train loss: 1.85500 - Test loss: 2.12854\n",
      "Epoch 3326 - lr: 0.05000 - Train loss: 1.85500 - Test loss: 2.12854\n",
      "Epoch 3327 - lr: 0.05000 - Train loss: 1.85500 - Test loss: 2.12855\n",
      "Epoch 3328 - lr: 0.05000 - Train loss: 1.85500 - Test loss: 2.12855\n",
      "Epoch 3329 - lr: 0.05000 - Train loss: 1.85500 - Test loss: 2.12856\n",
      "Epoch 3330 - lr: 0.05000 - Train loss: 1.85499 - Test loss: 2.12857\n",
      "Epoch 3331 - lr: 0.05000 - Train loss: 1.85499 - Test loss: 2.12857\n",
      "Epoch 3332 - lr: 0.05000 - Train loss: 1.85499 - Test loss: 2.12858\n",
      "Epoch 3333 - lr: 0.05000 - Train loss: 1.85499 - Test loss: 2.12859\n",
      "Epoch 3334 - lr: 0.05000 - Train loss: 1.85499 - Test loss: 2.12859\n",
      "Epoch 3335 - lr: 0.05000 - Train loss: 1.85499 - Test loss: 2.12860\n",
      "Epoch 3336 - lr: 0.05000 - Train loss: 1.85498 - Test loss: 2.12860\n",
      "Epoch 3337 - lr: 0.05000 - Train loss: 1.85498 - Test loss: 2.12861\n",
      "Epoch 3338 - lr: 0.05000 - Train loss: 1.85498 - Test loss: 2.12862\n",
      "Epoch 3339 - lr: 0.05000 - Train loss: 1.85498 - Test loss: 2.12862\n",
      "Epoch 3340 - lr: 0.05000 - Train loss: 1.85498 - Test loss: 2.12863\n",
      "Epoch 3341 - lr: 0.05000 - Train loss: 1.85497 - Test loss: 2.12864\n",
      "Epoch 3342 - lr: 0.05000 - Train loss: 1.85497 - Test loss: 2.12864\n",
      "Epoch 3343 - lr: 0.05000 - Train loss: 1.85497 - Test loss: 2.12865\n",
      "Epoch 3344 - lr: 0.05000 - Train loss: 1.85497 - Test loss: 2.12865\n",
      "Epoch 3345 - lr: 0.05000 - Train loss: 1.85497 - Test loss: 2.12866\n",
      "Epoch 3346 - lr: 0.05000 - Train loss: 1.85497 - Test loss: 2.12867\n",
      "Epoch 3347 - lr: 0.05000 - Train loss: 1.85496 - Test loss: 2.12867\n",
      "Epoch 3348 - lr: 0.05000 - Train loss: 1.85496 - Test loss: 2.12868\n",
      "Epoch 3349 - lr: 0.05000 - Train loss: 1.85496 - Test loss: 2.12869\n",
      "Epoch 3350 - lr: 0.05000 - Train loss: 1.85496 - Test loss: 2.12869\n",
      "Epoch 3351 - lr: 0.05000 - Train loss: 1.85496 - Test loss: 2.12870\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3352 - lr: 0.05000 - Train loss: 1.85495 - Test loss: 2.12870\n",
      "Epoch 3353 - lr: 0.05000 - Train loss: 1.85495 - Test loss: 2.12871\n",
      "Epoch 3354 - lr: 0.05000 - Train loss: 1.85495 - Test loss: 2.12872\n",
      "Epoch 3355 - lr: 0.05000 - Train loss: 1.85495 - Test loss: 2.12872\n",
      "Epoch 3356 - lr: 0.05000 - Train loss: 1.85495 - Test loss: 2.12873\n",
      "Epoch 3357 - lr: 0.05000 - Train loss: 1.85495 - Test loss: 2.12874\n",
      "Epoch 3358 - lr: 0.05000 - Train loss: 1.85494 - Test loss: 2.12874\n",
      "Epoch 3359 - lr: 0.05000 - Train loss: 1.85494 - Test loss: 2.12875\n",
      "Epoch 3360 - lr: 0.05000 - Train loss: 1.85494 - Test loss: 2.12875\n",
      "Epoch 3361 - lr: 0.05000 - Train loss: 1.85494 - Test loss: 2.12876\n",
      "Epoch 3362 - lr: 0.05000 - Train loss: 1.85494 - Test loss: 2.12877\n",
      "Epoch 3363 - lr: 0.05000 - Train loss: 1.85494 - Test loss: 2.12877\n",
      "Epoch 3364 - lr: 0.05000 - Train loss: 1.85493 - Test loss: 2.12878\n",
      "Epoch 3365 - lr: 0.05000 - Train loss: 1.85493 - Test loss: 2.12878\n",
      "Epoch 3366 - lr: 0.05000 - Train loss: 1.85493 - Test loss: 2.12879\n",
      "Epoch 3367 - lr: 0.05000 - Train loss: 1.85493 - Test loss: 2.12880\n",
      "Epoch 3368 - lr: 0.05000 - Train loss: 1.85493 - Test loss: 2.12880\n",
      "Epoch 3369 - lr: 0.05000 - Train loss: 1.85492 - Test loss: 2.12881\n",
      "Epoch 3370 - lr: 0.05000 - Train loss: 1.85492 - Test loss: 2.12882\n",
      "Epoch 3371 - lr: 0.05000 - Train loss: 1.85492 - Test loss: 2.12882\n",
      "Epoch 3372 - lr: 0.05000 - Train loss: 1.85492 - Test loss: 2.12883\n",
      "Epoch 3373 - lr: 0.05000 - Train loss: 1.85492 - Test loss: 2.12883\n",
      "Epoch 3374 - lr: 0.05000 - Train loss: 1.85492 - Test loss: 2.12884\n",
      "Epoch 3375 - lr: 0.05000 - Train loss: 1.85491 - Test loss: 2.12885\n",
      "Epoch 3376 - lr: 0.05000 - Train loss: 1.85491 - Test loss: 2.12885\n",
      "Epoch 3377 - lr: 0.05000 - Train loss: 1.85491 - Test loss: 2.12886\n",
      "Epoch 3378 - lr: 0.05000 - Train loss: 1.85491 - Test loss: 2.12886\n",
      "Epoch 3379 - lr: 0.05000 - Train loss: 1.85491 - Test loss: 2.12887\n",
      "Epoch 3380 - lr: 0.05000 - Train loss: 1.85491 - Test loss: 2.12888\n",
      "Epoch 3381 - lr: 0.05000 - Train loss: 1.85490 - Test loss: 2.12888\n",
      "Epoch 3382 - lr: 0.05000 - Train loss: 1.85490 - Test loss: 2.12889\n",
      "Epoch 3383 - lr: 0.05000 - Train loss: 1.85490 - Test loss: 2.12890\n",
      "Epoch 3384 - lr: 0.05000 - Train loss: 1.85490 - Test loss: 2.12890\n",
      "Epoch 3385 - lr: 0.05000 - Train loss: 1.85490 - Test loss: 2.12891\n",
      "Epoch 3386 - lr: 0.05000 - Train loss: 1.85489 - Test loss: 2.12891\n",
      "Epoch 3387 - lr: 0.05000 - Train loss: 1.85489 - Test loss: 2.12892\n",
      "Epoch 3388 - lr: 0.05000 - Train loss: 1.85489 - Test loss: 2.12893\n",
      "Epoch 3389 - lr: 0.05000 - Train loss: 1.85489 - Test loss: 2.12893\n",
      "Epoch 3390 - lr: 0.05000 - Train loss: 1.85489 - Test loss: 2.12894\n",
      "Epoch 3391 - lr: 0.05000 - Train loss: 1.85489 - Test loss: 2.12894\n",
      "Epoch 3392 - lr: 0.05000 - Train loss: 1.85488 - Test loss: 2.12895\n",
      "Epoch 3393 - lr: 0.05000 - Train loss: 1.85488 - Test loss: 2.12896\n",
      "Epoch 3394 - lr: 0.05000 - Train loss: 1.85488 - Test loss: 2.12896\n",
      "Epoch 3395 - lr: 0.05000 - Train loss: 1.85488 - Test loss: 2.12897\n",
      "Epoch 3396 - lr: 0.05000 - Train loss: 1.85488 - Test loss: 2.12897\n",
      "Epoch 3397 - lr: 0.05000 - Train loss: 1.85487 - Test loss: 2.12898\n",
      "Epoch 3398 - lr: 0.05000 - Train loss: 1.85487 - Test loss: 2.12899\n",
      "Epoch 3399 - lr: 0.05000 - Train loss: 1.85487 - Test loss: 2.12899\n",
      "Epoch 3400 - lr: 0.05000 - Train loss: 1.85487 - Test loss: 2.12900\n",
      "Epoch 3401 - lr: 0.05000 - Train loss: 1.85487 - Test loss: 2.12901\n",
      "Epoch 3402 - lr: 0.05000 - Train loss: 1.85487 - Test loss: 2.12901\n",
      "Epoch 3403 - lr: 0.05000 - Train loss: 1.85486 - Test loss: 2.12902\n",
      "Epoch 3404 - lr: 0.05000 - Train loss: 1.85486 - Test loss: 2.12902\n",
      "Epoch 3405 - lr: 0.05000 - Train loss: 1.85486 - Test loss: 2.12903\n",
      "Epoch 3406 - lr: 0.05000 - Train loss: 1.85486 - Test loss: 2.12904\n",
      "Epoch 3407 - lr: 0.05000 - Train loss: 1.85486 - Test loss: 2.12904\n",
      "Epoch 3408 - lr: 0.05000 - Train loss: 1.85486 - Test loss: 2.12905\n",
      "Epoch 3409 - lr: 0.05000 - Train loss: 1.85485 - Test loss: 2.12905\n",
      "Epoch 3410 - lr: 0.05000 - Train loss: 1.85485 - Test loss: 2.12906\n",
      "Epoch 3411 - lr: 0.05000 - Train loss: 1.85485 - Test loss: 2.12907\n",
      "Epoch 3412 - lr: 0.05000 - Train loss: 1.85485 - Test loss: 2.12907\n",
      "Epoch 3413 - lr: 0.05000 - Train loss: 1.85485 - Test loss: 2.12908\n",
      "Epoch 3414 - lr: 0.05000 - Train loss: 1.85485 - Test loss: 2.12908\n",
      "Epoch 3415 - lr: 0.05000 - Train loss: 1.85484 - Test loss: 2.12909\n",
      "Epoch 3416 - lr: 0.05000 - Train loss: 1.85484 - Test loss: 2.12910\n",
      "Epoch 3417 - lr: 0.05000 - Train loss: 1.85484 - Test loss: 2.12910\n",
      "Epoch 3418 - lr: 0.05000 - Train loss: 1.85484 - Test loss: 2.12911\n",
      "Epoch 3419 - lr: 0.05000 - Train loss: 1.85484 - Test loss: 2.12911\n",
      "Epoch 3420 - lr: 0.05000 - Train loss: 1.85483 - Test loss: 2.12912\n",
      "Epoch 3421 - lr: 0.05000 - Train loss: 1.85483 - Test loss: 2.12913\n",
      "Epoch 3422 - lr: 0.05000 - Train loss: 1.85483 - Test loss: 2.12913\n",
      "Epoch 3423 - lr: 0.05000 - Train loss: 1.85483 - Test loss: 2.12914\n",
      "Epoch 3424 - lr: 0.05000 - Train loss: 1.85483 - Test loss: 2.12914\n",
      "Epoch 3425 - lr: 0.05000 - Train loss: 1.85483 - Test loss: 2.12915\n",
      "Epoch 3426 - lr: 0.05000 - Train loss: 1.85482 - Test loss: 2.12916\n",
      "Epoch 3427 - lr: 0.05000 - Train loss: 1.85482 - Test loss: 2.12916\n",
      "Epoch 3428 - lr: 0.05000 - Train loss: 1.85482 - Test loss: 2.12917\n",
      "Epoch 3429 - lr: 0.05000 - Train loss: 1.85482 - Test loss: 2.12917\n",
      "Epoch 3430 - lr: 0.05000 - Train loss: 1.85482 - Test loss: 2.12918\n",
      "Epoch 3431 - lr: 0.05000 - Train loss: 1.85482 - Test loss: 2.12919\n",
      "Epoch 3432 - lr: 0.05000 - Train loss: 1.85481 - Test loss: 2.12919\n",
      "Epoch 3433 - lr: 0.05000 - Train loss: 1.85481 - Test loss: 2.12920\n",
      "Epoch 3434 - lr: 0.05000 - Train loss: 1.85481 - Test loss: 2.12920\n",
      "Epoch 3435 - lr: 0.05000 - Train loss: 1.85481 - Test loss: 2.12921\n",
      "Epoch 3436 - lr: 0.05000 - Train loss: 1.85481 - Test loss: 2.12922\n",
      "Epoch 3437 - lr: 0.05000 - Train loss: 1.85480 - Test loss: 2.12922\n",
      "Epoch 3438 - lr: 0.05000 - Train loss: 1.85480 - Test loss: 2.12923\n",
      "Epoch 3439 - lr: 0.05000 - Train loss: 1.85480 - Test loss: 2.12923\n",
      "Epoch 3440 - lr: 0.05000 - Train loss: 1.85480 - Test loss: 2.12924\n",
      "Epoch 3441 - lr: 0.05000 - Train loss: 1.85480 - Test loss: 2.12925\n",
      "Epoch 3442 - lr: 0.05000 - Train loss: 1.85480 - Test loss: 2.12925\n",
      "Epoch 3443 - lr: 0.05000 - Train loss: 1.85479 - Test loss: 2.12926\n",
      "Epoch 3444 - lr: 0.05000 - Train loss: 1.85479 - Test loss: 2.12926\n",
      "Epoch 3445 - lr: 0.05000 - Train loss: 1.85479 - Test loss: 2.12927\n",
      "Epoch 3446 - lr: 0.05000 - Train loss: 1.85479 - Test loss: 2.12928\n",
      "Epoch 3447 - lr: 0.05000 - Train loss: 1.85479 - Test loss: 2.12928\n",
      "Epoch 3448 - lr: 0.05000 - Train loss: 1.85479 - Test loss: 2.12929\n",
      "Epoch 3449 - lr: 0.05000 - Train loss: 1.85478 - Test loss: 2.12929\n",
      "Epoch 3450 - lr: 0.05000 - Train loss: 1.85478 - Test loss: 2.12930\n",
      "Epoch 3451 - lr: 0.05000 - Train loss: 1.85478 - Test loss: 2.12931\n",
      "Epoch 3452 - lr: 0.05000 - Train loss: 1.85478 - Test loss: 2.12931\n",
      "Epoch 3453 - lr: 0.05000 - Train loss: 1.85478 - Test loss: 2.12932\n",
      "Epoch 3454 - lr: 0.05000 - Train loss: 1.85478 - Test loss: 2.12932\n",
      "Epoch 3455 - lr: 0.05000 - Train loss: 1.85477 - Test loss: 2.12933\n",
      "Epoch 3456 - lr: 0.05000 - Train loss: 1.85477 - Test loss: 2.12934\n",
      "Epoch 3457 - lr: 0.05000 - Train loss: 1.85477 - Test loss: 2.12934\n",
      "Epoch 3458 - lr: 0.05000 - Train loss: 1.85477 - Test loss: 2.12935\n",
      "Epoch 3459 - lr: 0.05000 - Train loss: 1.85477 - Test loss: 2.12935\n",
      "Epoch 3460 - lr: 0.05000 - Train loss: 1.85476 - Test loss: 2.12936\n",
      "Epoch 3461 - lr: 0.05000 - Train loss: 1.85476 - Test loss: 2.12937\n",
      "Epoch 3462 - lr: 0.05000 - Train loss: 1.85476 - Test loss: 2.12937\n",
      "Epoch 3463 - lr: 0.05000 - Train loss: 1.85476 - Test loss: 2.12938\n",
      "Epoch 3464 - lr: 0.05000 - Train loss: 1.85476 - Test loss: 2.12938\n",
      "Epoch 3465 - lr: 0.05000 - Train loss: 1.85476 - Test loss: 2.12939\n",
      "Epoch 3466 - lr: 0.05000 - Train loss: 1.85475 - Test loss: 2.12940\n",
      "Epoch 3467 - lr: 0.05000 - Train loss: 1.85475 - Test loss: 2.12940\n",
      "Epoch 3468 - lr: 0.05000 - Train loss: 1.85475 - Test loss: 2.12941\n",
      "Epoch 3469 - lr: 0.05000 - Train loss: 1.85475 - Test loss: 2.12941\n",
      "Epoch 3470 - lr: 0.05000 - Train loss: 1.85475 - Test loss: 2.12942\n",
      "Epoch 3471 - lr: 0.05000 - Train loss: 1.85475 - Test loss: 2.12943\n",
      "Epoch 3472 - lr: 0.05000 - Train loss: 1.85474 - Test loss: 2.12943\n",
      "Epoch 3473 - lr: 0.05000 - Train loss: 1.85474 - Test loss: 2.12944\n",
      "Epoch 3474 - lr: 0.05000 - Train loss: 1.85474 - Test loss: 2.12944\n",
      "Epoch 3475 - lr: 0.05000 - Train loss: 1.85474 - Test loss: 2.12945\n",
      "Epoch 3476 - lr: 0.05000 - Train loss: 1.85474 - Test loss: 2.12946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3477 - lr: 0.05000 - Train loss: 1.85474 - Test loss: 2.12946\n",
      "Epoch 3478 - lr: 0.05000 - Train loss: 1.85473 - Test loss: 2.12947\n",
      "Epoch 3479 - lr: 0.05000 - Train loss: 1.85473 - Test loss: 2.12947\n",
      "Epoch 3480 - lr: 0.05000 - Train loss: 1.85473 - Test loss: 2.12948\n",
      "Epoch 3481 - lr: 0.05000 - Train loss: 1.85473 - Test loss: 2.12948\n",
      "Epoch 3482 - lr: 0.05000 - Train loss: 1.85473 - Test loss: 2.12949\n",
      "Epoch 3483 - lr: 0.05000 - Train loss: 1.85472 - Test loss: 2.12950\n",
      "Epoch 3484 - lr: 0.05000 - Train loss: 1.85472 - Test loss: 2.12950\n",
      "Epoch 3485 - lr: 0.05000 - Train loss: 1.85472 - Test loss: 2.12951\n",
      "Epoch 3486 - lr: 0.05000 - Train loss: 1.85472 - Test loss: 2.12951\n",
      "Epoch 3487 - lr: 0.05000 - Train loss: 1.85472 - Test loss: 2.12952\n",
      "Epoch 3488 - lr: 0.05000 - Train loss: 1.85472 - Test loss: 2.12953\n",
      "Epoch 3489 - lr: 0.05000 - Train loss: 1.85471 - Test loss: 2.12953\n",
      "Epoch 3490 - lr: 0.05000 - Train loss: 1.85471 - Test loss: 2.12954\n",
      "Epoch 3491 - lr: 0.05000 - Train loss: 1.85471 - Test loss: 2.12954\n",
      "Epoch 3492 - lr: 0.05000 - Train loss: 1.85471 - Test loss: 2.12955\n",
      "Epoch 3493 - lr: 0.05000 - Train loss: 1.85471 - Test loss: 2.12956\n",
      "Epoch 3494 - lr: 0.05000 - Train loss: 1.85471 - Test loss: 2.12956\n",
      "Epoch 3495 - lr: 0.05000 - Train loss: 1.85470 - Test loss: 2.12957\n",
      "Epoch 3496 - lr: 0.05000 - Train loss: 1.85470 - Test loss: 2.12957\n",
      "Epoch 3497 - lr: 0.05000 - Train loss: 1.85470 - Test loss: 2.12958\n",
      "Epoch 3498 - lr: 0.05000 - Train loss: 1.85470 - Test loss: 2.12958\n",
      "Epoch 3499 - lr: 0.05000 - Train loss: 1.85470 - Test loss: 2.12959\n",
      "Epoch 3500 - lr: 0.05000 - Train loss: 1.85470 - Test loss: 2.12960\n",
      "Epoch 3501 - lr: 0.05000 - Train loss: 1.85469 - Test loss: 2.12960\n",
      "Epoch 3502 - lr: 0.05000 - Train loss: 1.85469 - Test loss: 2.12961\n",
      "Epoch 3503 - lr: 0.05000 - Train loss: 1.85469 - Test loss: 2.12961\n",
      "Epoch 3504 - lr: 0.05000 - Train loss: 1.85469 - Test loss: 2.12962\n",
      "Epoch 3505 - lr: 0.05000 - Train loss: 1.85469 - Test loss: 2.12963\n",
      "Epoch 3506 - lr: 0.05000 - Train loss: 1.85469 - Test loss: 2.12963\n",
      "Epoch 3507 - lr: 0.05000 - Train loss: 1.85468 - Test loss: 2.12964\n",
      "Epoch 3508 - lr: 0.05000 - Train loss: 1.85468 - Test loss: 2.12964\n",
      "Epoch 3509 - lr: 0.05000 - Train loss: 1.85468 - Test loss: 2.12965\n",
      "Epoch 3510 - lr: 0.05000 - Train loss: 1.85468 - Test loss: 2.12965\n",
      "Epoch 3511 - lr: 0.05000 - Train loss: 1.85468 - Test loss: 2.12966\n",
      "Epoch 3512 - lr: 0.05000 - Train loss: 1.85468 - Test loss: 2.12967\n",
      "Epoch 3513 - lr: 0.05000 - Train loss: 1.85467 - Test loss: 2.12967\n",
      "Epoch 3514 - lr: 0.05000 - Train loss: 1.85467 - Test loss: 2.12968\n",
      "Epoch 3515 - lr: 0.05000 - Train loss: 1.85467 - Test loss: 2.12968\n",
      "Epoch 3516 - lr: 0.05000 - Train loss: 1.85467 - Test loss: 2.12969\n",
      "Epoch 3517 - lr: 0.05000 - Train loss: 1.85467 - Test loss: 2.12970\n",
      "Epoch 3518 - lr: 0.05000 - Train loss: 1.85466 - Test loss: 2.12970\n",
      "Epoch 3519 - lr: 0.05000 - Train loss: 1.85466 - Test loss: 2.12971\n",
      "Epoch 3520 - lr: 0.05000 - Train loss: 1.85466 - Test loss: 2.12971\n",
      "Epoch 3521 - lr: 0.05000 - Train loss: 1.85466 - Test loss: 2.12972\n",
      "Epoch 3522 - lr: 0.05000 - Train loss: 1.85466 - Test loss: 2.12972\n",
      "Epoch 3523 - lr: 0.05000 - Train loss: 1.85466 - Test loss: 2.12973\n",
      "Epoch 3524 - lr: 0.05000 - Train loss: 1.85465 - Test loss: 2.12974\n",
      "Epoch 3525 - lr: 0.05000 - Train loss: 1.85465 - Test loss: 2.12974\n",
      "Epoch 3526 - lr: 0.05000 - Train loss: 1.85465 - Test loss: 2.12975\n",
      "Epoch 3527 - lr: 0.05000 - Train loss: 1.85465 - Test loss: 2.12975\n",
      "Epoch 3528 - lr: 0.05000 - Train loss: 1.85465 - Test loss: 2.12976\n",
      "Epoch 3529 - lr: 0.05000 - Train loss: 1.85465 - Test loss: 2.12977\n",
      "Epoch 3530 - lr: 0.05000 - Train loss: 1.85464 - Test loss: 2.12977\n",
      "Epoch 3531 - lr: 0.05000 - Train loss: 1.85464 - Test loss: 2.12978\n",
      "Epoch 3532 - lr: 0.05000 - Train loss: 1.85464 - Test loss: 2.12978\n",
      "Epoch 3533 - lr: 0.05000 - Train loss: 1.85464 - Test loss: 2.12979\n",
      "Epoch 3534 - lr: 0.05000 - Train loss: 1.85464 - Test loss: 2.12979\n",
      "Epoch 3535 - lr: 0.05000 - Train loss: 1.85464 - Test loss: 2.12980\n",
      "Epoch 3536 - lr: 0.05000 - Train loss: 1.85463 - Test loss: 2.12981\n",
      "Epoch 3537 - lr: 0.05000 - Train loss: 1.85463 - Test loss: 2.12981\n",
      "Epoch 3538 - lr: 0.05000 - Train loss: 1.85463 - Test loss: 2.12982\n",
      "Epoch 3539 - lr: 0.05000 - Train loss: 1.85463 - Test loss: 2.12982\n",
      "Epoch 3540 - lr: 0.05000 - Train loss: 1.85463 - Test loss: 2.12983\n",
      "Epoch 3541 - lr: 0.05000 - Train loss: 1.85463 - Test loss: 2.12983\n",
      "Epoch 3542 - lr: 0.05000 - Train loss: 1.85462 - Test loss: 2.12984\n",
      "Epoch 3543 - lr: 0.05000 - Train loss: 1.85462 - Test loss: 2.12985\n",
      "Epoch 3544 - lr: 0.05000 - Train loss: 1.85462 - Test loss: 2.12985\n",
      "Epoch 3545 - lr: 0.05000 - Train loss: 1.85462 - Test loss: 2.12986\n",
      "Epoch 3546 - lr: 0.05000 - Train loss: 1.85462 - Test loss: 2.12986\n",
      "Epoch 3547 - lr: 0.05000 - Train loss: 1.85462 - Test loss: 2.12987\n",
      "Epoch 3548 - lr: 0.05000 - Train loss: 1.85461 - Test loss: 2.12988\n",
      "Epoch 3549 - lr: 0.05000 - Train loss: 1.85461 - Test loss: 2.12988\n",
      "Epoch 3550 - lr: 0.05000 - Train loss: 1.85461 - Test loss: 2.12989\n",
      "Epoch 3551 - lr: 0.05000 - Train loss: 1.85461 - Test loss: 2.12989\n",
      "Epoch 3552 - lr: 0.05000 - Train loss: 1.85461 - Test loss: 2.12990\n",
      "Epoch 3553 - lr: 0.05000 - Train loss: 1.85461 - Test loss: 2.12990\n",
      "Epoch 3554 - lr: 0.05000 - Train loss: 1.85460 - Test loss: 2.12991\n",
      "Epoch 3555 - lr: 0.05000 - Train loss: 1.85460 - Test loss: 2.12992\n",
      "Epoch 3556 - lr: 0.05000 - Train loss: 1.85460 - Test loss: 2.12992\n",
      "Epoch 3557 - lr: 0.05000 - Train loss: 1.85460 - Test loss: 2.12993\n",
      "Epoch 3558 - lr: 0.05000 - Train loss: 1.85460 - Test loss: 2.12993\n",
      "Epoch 3559 - lr: 0.05000 - Train loss: 1.85460 - Test loss: 2.12994\n",
      "Epoch 3560 - lr: 0.05000 - Train loss: 1.85459 - Test loss: 2.12994\n",
      "Epoch 3561 - lr: 0.05000 - Train loss: 1.85459 - Test loss: 2.12995\n",
      "Epoch 3562 - lr: 0.05000 - Train loss: 1.85459 - Test loss: 2.12996\n",
      "Epoch 3563 - lr: 0.05000 - Train loss: 1.85459 - Test loss: 2.12996\n",
      "Epoch 3564 - lr: 0.05000 - Train loss: 1.85459 - Test loss: 2.12997\n",
      "Epoch 3565 - lr: 0.05000 - Train loss: 1.85459 - Test loss: 2.12997\n",
      "Epoch 3566 - lr: 0.05000 - Train loss: 1.85458 - Test loss: 2.12998\n",
      "Epoch 3567 - lr: 0.05000 - Train loss: 1.85458 - Test loss: 2.12998\n",
      "Epoch 3568 - lr: 0.05000 - Train loss: 1.85458 - Test loss: 2.12999\n",
      "Epoch 3569 - lr: 0.05000 - Train loss: 1.85458 - Test loss: 2.13000\n",
      "Epoch 3570 - lr: 0.05000 - Train loss: 1.85458 - Test loss: 2.13000\n",
      "Epoch 3571 - lr: 0.05000 - Train loss: 1.85458 - Test loss: 2.13001\n",
      "Epoch 3572 - lr: 0.05000 - Train loss: 1.85457 - Test loss: 2.13001\n",
      "Epoch 3573 - lr: 0.05000 - Train loss: 1.85457 - Test loss: 2.13002\n",
      "Epoch 3574 - lr: 0.05000 - Train loss: 1.85457 - Test loss: 2.13002\n",
      "Epoch 3575 - lr: 0.05000 - Train loss: 1.85457 - Test loss: 2.13003\n",
      "Epoch 3576 - lr: 0.05000 - Train loss: 1.85457 - Test loss: 2.13004\n",
      "Epoch 3577 - lr: 0.05000 - Train loss: 1.85456 - Test loss: 2.13004\n",
      "Epoch 3578 - lr: 0.05000 - Train loss: 1.85456 - Test loss: 2.13005\n",
      "Epoch 3579 - lr: 0.05000 - Train loss: 1.85456 - Test loss: 2.13005\n",
      "Epoch 3580 - lr: 0.05000 - Train loss: 1.85456 - Test loss: 2.13006\n",
      "Epoch 3581 - lr: 0.05000 - Train loss: 1.85456 - Test loss: 2.13006\n",
      "Epoch 3582 - lr: 0.05000 - Train loss: 1.85456 - Test loss: 2.13007\n",
      "Epoch 3583 - lr: 0.05000 - Train loss: 1.85455 - Test loss: 2.13008\n",
      "Epoch 3584 - lr: 0.05000 - Train loss: 1.85455 - Test loss: 2.13008\n",
      "Epoch 3585 - lr: 0.05000 - Train loss: 1.85455 - Test loss: 2.13009\n",
      "Epoch 3586 - lr: 0.05000 - Train loss: 1.85455 - Test loss: 2.13009\n",
      "Epoch 3587 - lr: 0.05000 - Train loss: 1.85455 - Test loss: 2.13010\n",
      "Epoch 3588 - lr: 0.05000 - Train loss: 1.85455 - Test loss: 2.13010\n",
      "Epoch 3589 - lr: 0.05000 - Train loss: 1.85454 - Test loss: 2.13011\n",
      "Epoch 3590 - lr: 0.05000 - Train loss: 1.85454 - Test loss: 2.13012\n",
      "Epoch 3591 - lr: 0.05000 - Train loss: 1.85454 - Test loss: 2.13012\n",
      "Epoch 3592 - lr: 0.05000 - Train loss: 1.85454 - Test loss: 2.13013\n",
      "Epoch 3593 - lr: 0.05000 - Train loss: 1.85454 - Test loss: 2.13013\n",
      "Epoch 3594 - lr: 0.05000 - Train loss: 1.85454 - Test loss: 2.13014\n",
      "Epoch 3595 - lr: 0.05000 - Train loss: 1.85453 - Test loss: 2.13014\n",
      "Epoch 3596 - lr: 0.05000 - Train loss: 1.85453 - Test loss: 2.13015\n",
      "Epoch 3597 - lr: 0.05000 - Train loss: 1.85453 - Test loss: 2.13016\n",
      "Epoch 3598 - lr: 0.05000 - Train loss: 1.85453 - Test loss: 2.13016\n",
      "Epoch 3599 - lr: 0.05000 - Train loss: 1.85453 - Test loss: 2.13017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3600 - lr: 0.05000 - Train loss: 1.85453 - Test loss: 2.13017\n",
      "Epoch 3601 - lr: 0.05000 - Train loss: 1.85452 - Test loss: 2.13018\n",
      "Epoch 3602 - lr: 0.05000 - Train loss: 1.85452 - Test loss: 2.13018\n",
      "Epoch 3603 - lr: 0.05000 - Train loss: 1.85452 - Test loss: 2.13019\n",
      "Epoch 3604 - lr: 0.05000 - Train loss: 1.85452 - Test loss: 2.13019\n",
      "Epoch 3605 - lr: 0.05000 - Train loss: 1.85452 - Test loss: 2.13020\n",
      "Epoch 3606 - lr: 0.05000 - Train loss: 1.85452 - Test loss: 2.13021\n",
      "Epoch 3607 - lr: 0.05000 - Train loss: 1.85451 - Test loss: 2.13021\n",
      "Epoch 3608 - lr: 0.05000 - Train loss: 1.85451 - Test loss: 2.13022\n",
      "Epoch 3609 - lr: 0.05000 - Train loss: 1.85451 - Test loss: 2.13022\n",
      "Epoch 3610 - lr: 0.05000 - Train loss: 1.85451 - Test loss: 2.13023\n",
      "Epoch 3611 - lr: 0.05000 - Train loss: 1.85451 - Test loss: 2.13023\n",
      "Epoch 3612 - lr: 0.05000 - Train loss: 1.85451 - Test loss: 2.13024\n",
      "Epoch 3613 - lr: 0.05000 - Train loss: 1.85450 - Test loss: 2.13025\n",
      "Epoch 3614 - lr: 0.05000 - Train loss: 1.85450 - Test loss: 2.13025\n",
      "Epoch 3615 - lr: 0.05000 - Train loss: 1.85450 - Test loss: 2.13026\n",
      "Epoch 3616 - lr: 0.05000 - Train loss: 1.85450 - Test loss: 2.13026\n",
      "Epoch 3617 - lr: 0.05000 - Train loss: 1.85450 - Test loss: 2.13027\n",
      "Epoch 3618 - lr: 0.05000 - Train loss: 1.85450 - Test loss: 2.13027\n",
      "Epoch 3619 - lr: 0.05000 - Train loss: 1.85449 - Test loss: 2.13028\n",
      "Epoch 3620 - lr: 0.05000 - Train loss: 1.85449 - Test loss: 2.13028\n",
      "Epoch 3621 - lr: 0.05000 - Train loss: 1.85449 - Test loss: 2.13029\n",
      "Epoch 3622 - lr: 0.05000 - Train loss: 1.85449 - Test loss: 2.13030\n",
      "Epoch 3623 - lr: 0.05000 - Train loss: 1.85449 - Test loss: 2.13030\n",
      "Epoch 3624 - lr: 0.05000 - Train loss: 1.85449 - Test loss: 2.13031\n",
      "Epoch 3625 - lr: 0.05000 - Train loss: 1.85448 - Test loss: 2.13031\n",
      "Epoch 3626 - lr: 0.05000 - Train loss: 1.85448 - Test loss: 2.13032\n",
      "Epoch 3627 - lr: 0.05000 - Train loss: 1.85448 - Test loss: 2.13032\n",
      "Epoch 3628 - lr: 0.05000 - Train loss: 1.85448 - Test loss: 2.13033\n",
      "Epoch 3629 - lr: 0.05000 - Train loss: 1.85448 - Test loss: 2.13034\n",
      "Epoch 3630 - lr: 0.05000 - Train loss: 1.85448 - Test loss: 2.13034\n",
      "Epoch 3631 - lr: 0.05000 - Train loss: 1.85447 - Test loss: 2.13035\n",
      "Epoch 3632 - lr: 0.05000 - Train loss: 1.85447 - Test loss: 2.13035\n",
      "Epoch 3633 - lr: 0.05000 - Train loss: 1.85447 - Test loss: 2.13036\n",
      "Epoch 3634 - lr: 0.05000 - Train loss: 1.85447 - Test loss: 2.13036\n",
      "Epoch 3635 - lr: 0.05000 - Train loss: 1.85447 - Test loss: 2.13037\n",
      "Epoch 3636 - lr: 0.05000 - Train loss: 1.85447 - Test loss: 2.13037\n",
      "Epoch 3637 - lr: 0.05000 - Train loss: 1.85447 - Test loss: 2.13038\n",
      "Epoch 3638 - lr: 0.05000 - Train loss: 1.85446 - Test loss: 2.13039\n",
      "Epoch 3639 - lr: 0.05000 - Train loss: 1.85446 - Test loss: 2.13039\n",
      "Epoch 3640 - lr: 0.05000 - Train loss: 1.85446 - Test loss: 2.13040\n",
      "Epoch 3641 - lr: 0.05000 - Train loss: 1.85446 - Test loss: 2.13040\n",
      "Epoch 3642 - lr: 0.05000 - Train loss: 1.85446 - Test loss: 2.13041\n",
      "Epoch 3643 - lr: 0.05000 - Train loss: 1.85446 - Test loss: 2.13041\n",
      "Epoch 3644 - lr: 0.05000 - Train loss: 1.85445 - Test loss: 2.13042\n",
      "Epoch 3645 - lr: 0.05000 - Train loss: 1.85445 - Test loss: 2.13042\n",
      "Epoch 3646 - lr: 0.05000 - Train loss: 1.85445 - Test loss: 2.13043\n",
      "Epoch 3647 - lr: 0.05000 - Train loss: 1.85445 - Test loss: 2.13044\n",
      "Epoch 3648 - lr: 0.05000 - Train loss: 1.85445 - Test loss: 2.13044\n",
      "Epoch 3649 - lr: 0.05000 - Train loss: 1.85445 - Test loss: 2.13045\n",
      "Epoch 3650 - lr: 0.05000 - Train loss: 1.85444 - Test loss: 2.13045\n",
      "Epoch 3651 - lr: 0.05000 - Train loss: 1.85444 - Test loss: 2.13046\n",
      "Epoch 3652 - lr: 0.05000 - Train loss: 1.85444 - Test loss: 2.13046\n",
      "Epoch 3653 - lr: 0.05000 - Train loss: 1.85444 - Test loss: 2.13047\n",
      "Epoch 3654 - lr: 0.05000 - Train loss: 1.85444 - Test loss: 2.13047\n",
      "Epoch 3655 - lr: 0.05000 - Train loss: 1.85444 - Test loss: 2.13048\n",
      "Epoch 3656 - lr: 0.05000 - Train loss: 1.85443 - Test loss: 2.13049\n",
      "Epoch 3657 - lr: 0.05000 - Train loss: 1.85443 - Test loss: 2.13049\n",
      "Epoch 3658 - lr: 0.05000 - Train loss: 1.85443 - Test loss: 2.13050\n",
      "Epoch 3659 - lr: 0.05000 - Train loss: 1.85443 - Test loss: 2.13050\n",
      "Epoch 3660 - lr: 0.05000 - Train loss: 1.85443 - Test loss: 2.13051\n",
      "Epoch 3661 - lr: 0.05000 - Train loss: 1.85443 - Test loss: 2.13051\n",
      "Epoch 3662 - lr: 0.05000 - Train loss: 1.85442 - Test loss: 2.13052\n",
      "Epoch 3663 - lr: 0.05000 - Train loss: 1.85442 - Test loss: 2.13052\n",
      "Epoch 3664 - lr: 0.05000 - Train loss: 1.85442 - Test loss: 2.13053\n",
      "Epoch 3665 - lr: 0.05000 - Train loss: 1.85442 - Test loss: 2.13054\n",
      "Epoch 3666 - lr: 0.05000 - Train loss: 1.85442 - Test loss: 2.13054\n",
      "Epoch 3667 - lr: 0.05000 - Train loss: 1.85442 - Test loss: 2.13055\n",
      "Epoch 3668 - lr: 0.05000 - Train loss: 1.85441 - Test loss: 2.13055\n",
      "Epoch 3669 - lr: 0.05000 - Train loss: 1.85441 - Test loss: 2.13056\n",
      "Epoch 3670 - lr: 0.05000 - Train loss: 1.85441 - Test loss: 2.13056\n",
      "Epoch 3671 - lr: 0.05000 - Train loss: 1.85441 - Test loss: 2.13057\n",
      "Epoch 3672 - lr: 0.05000 - Train loss: 1.85441 - Test loss: 2.13057\n",
      "Epoch 3673 - lr: 0.05000 - Train loss: 1.85441 - Test loss: 2.13058\n",
      "Epoch 3674 - lr: 0.05000 - Train loss: 1.85440 - Test loss: 2.13058\n",
      "Epoch 3675 - lr: 0.05000 - Train loss: 1.85440 - Test loss: 2.13059\n",
      "Epoch 3676 - lr: 0.05000 - Train loss: 1.85440 - Test loss: 2.13060\n",
      "Epoch 3677 - lr: 0.05000 - Train loss: 1.85440 - Test loss: 2.13060\n",
      "Epoch 3678 - lr: 0.05000 - Train loss: 1.85440 - Test loss: 2.13061\n",
      "Epoch 3679 - lr: 0.05000 - Train loss: 1.85440 - Test loss: 2.13061\n",
      "Epoch 3680 - lr: 0.05000 - Train loss: 1.85439 - Test loss: 2.13062\n",
      "Epoch 3681 - lr: 0.05000 - Train loss: 1.85439 - Test loss: 2.13062\n",
      "Epoch 3682 - lr: 0.05000 - Train loss: 1.85439 - Test loss: 2.13063\n",
      "Epoch 3683 - lr: 0.05000 - Train loss: 1.85439 - Test loss: 2.13063\n",
      "Epoch 3684 - lr: 0.05000 - Train loss: 1.85439 - Test loss: 2.13064\n",
      "Epoch 3685 - lr: 0.05000 - Train loss: 1.85439 - Test loss: 2.13065\n",
      "Epoch 3686 - lr: 0.05000 - Train loss: 1.85438 - Test loss: 2.13065\n",
      "Epoch 3687 - lr: 0.05000 - Train loss: 1.85438 - Test loss: 2.13066\n",
      "Epoch 3688 - lr: 0.05000 - Train loss: 1.85438 - Test loss: 2.13066\n",
      "Epoch 3689 - lr: 0.05000 - Train loss: 1.85438 - Test loss: 2.13067\n",
      "Epoch 3690 - lr: 0.05000 - Train loss: 1.85438 - Test loss: 2.13067\n",
      "Epoch 3691 - lr: 0.05000 - Train loss: 1.85438 - Test loss: 2.13068\n",
      "Epoch 3692 - lr: 0.05000 - Train loss: 1.85437 - Test loss: 2.13068\n",
      "Epoch 3693 - lr: 0.05000 - Train loss: 1.85437 - Test loss: 2.13069\n",
      "Epoch 3694 - lr: 0.05000 - Train loss: 1.85437 - Test loss: 2.13069\n",
      "Epoch 3695 - lr: 0.05000 - Train loss: 1.85437 - Test loss: 2.13070\n",
      "Epoch 3696 - lr: 0.05000 - Train loss: 1.85437 - Test loss: 2.13071\n",
      "Epoch 3697 - lr: 0.05000 - Train loss: 1.85437 - Test loss: 2.13071\n",
      "Epoch 3698 - lr: 0.05000 - Train loss: 1.85437 - Test loss: 2.13072\n",
      "Epoch 3699 - lr: 0.05000 - Train loss: 1.85436 - Test loss: 2.13072\n",
      "Epoch 3700 - lr: 0.05000 - Train loss: 1.85436 - Test loss: 2.13073\n",
      "Epoch 3701 - lr: 0.05000 - Train loss: 1.85436 - Test loss: 2.13073\n",
      "Epoch 3702 - lr: 0.05000 - Train loss: 1.85436 - Test loss: 2.13074\n",
      "Epoch 3703 - lr: 0.05000 - Train loss: 1.85436 - Test loss: 2.13074\n",
      "Epoch 3704 - lr: 0.05000 - Train loss: 1.85436 - Test loss: 2.13075\n",
      "Epoch 3705 - lr: 0.05000 - Train loss: 1.85435 - Test loss: 2.13075\n",
      "Epoch 3706 - lr: 0.05000 - Train loss: 1.85435 - Test loss: 2.13076\n",
      "Epoch 3707 - lr: 0.05000 - Train loss: 1.85435 - Test loss: 2.13077\n",
      "Epoch 3708 - lr: 0.05000 - Train loss: 1.85435 - Test loss: 2.13077\n",
      "Epoch 3709 - lr: 0.05000 - Train loss: 1.85435 - Test loss: 2.13078\n",
      "Epoch 3710 - lr: 0.05000 - Train loss: 1.85435 - Test loss: 2.13078\n",
      "Epoch 3711 - lr: 0.05000 - Train loss: 1.85434 - Test loss: 2.13079\n",
      "Epoch 3712 - lr: 0.05000 - Train loss: 1.85434 - Test loss: 2.13079\n",
      "Epoch 3713 - lr: 0.05000 - Train loss: 1.85434 - Test loss: 2.13080\n",
      "Epoch 3714 - lr: 0.05000 - Train loss: 1.85434 - Test loss: 2.13080\n",
      "Epoch 3715 - lr: 0.05000 - Train loss: 1.85434 - Test loss: 2.13081\n",
      "Epoch 3716 - lr: 0.05000 - Train loss: 1.85434 - Test loss: 2.13081\n",
      "Epoch 3717 - lr: 0.05000 - Train loss: 1.85433 - Test loss: 2.13082\n",
      "Epoch 3718 - lr: 0.05000 - Train loss: 1.85433 - Test loss: 2.13083\n",
      "Epoch 3719 - lr: 0.05000 - Train loss: 1.85433 - Test loss: 2.13083\n",
      "Epoch 3720 - lr: 0.05000 - Train loss: 1.85433 - Test loss: 2.13084\n",
      "Epoch 3721 - lr: 0.05000 - Train loss: 1.85433 - Test loss: 2.13084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3722 - lr: 0.05000 - Train loss: 1.85433 - Test loss: 2.13085\n",
      "Epoch 3723 - lr: 0.05000 - Train loss: 1.85432 - Test loss: 2.13085\n",
      "Epoch 3724 - lr: 0.05000 - Train loss: 1.85432 - Test loss: 2.13086\n",
      "Epoch 3725 - lr: 0.05000 - Train loss: 1.85432 - Test loss: 2.13086\n",
      "Epoch 3726 - lr: 0.05000 - Train loss: 1.85432 - Test loss: 2.13087\n",
      "Epoch 3727 - lr: 0.05000 - Train loss: 1.85432 - Test loss: 2.13087\n",
      "Epoch 3728 - lr: 0.05000 - Train loss: 1.85432 - Test loss: 2.13088\n",
      "Epoch 3729 - lr: 0.05000 - Train loss: 1.85431 - Test loss: 2.13089\n",
      "Epoch 3730 - lr: 0.05000 - Train loss: 1.85431 - Test loss: 2.13089\n",
      "Epoch 3731 - lr: 0.05000 - Train loss: 1.85431 - Test loss: 2.13090\n",
      "Epoch 3732 - lr: 0.05000 - Train loss: 1.85431 - Test loss: 2.13090\n",
      "Epoch 3733 - lr: 0.05000 - Train loss: 1.85431 - Test loss: 2.13091\n",
      "Epoch 3734 - lr: 0.05000 - Train loss: 1.85431 - Test loss: 2.13091\n",
      "Epoch 3735 - lr: 0.05000 - Train loss: 1.85431 - Test loss: 2.13092\n",
      "Epoch 3736 - lr: 0.05000 - Train loss: 1.85430 - Test loss: 2.13092\n",
      "Epoch 3737 - lr: 0.05000 - Train loss: 1.85430 - Test loss: 2.13093\n",
      "Epoch 3738 - lr: 0.05000 - Train loss: 1.85430 - Test loss: 2.13093\n",
      "Epoch 3739 - lr: 0.05000 - Train loss: 1.85430 - Test loss: 2.13094\n",
      "Epoch 3740 - lr: 0.05000 - Train loss: 1.85430 - Test loss: 2.13094\n",
      "Epoch 3741 - lr: 0.05000 - Train loss: 1.85430 - Test loss: 2.13095\n",
      "Epoch 3742 - lr: 0.05000 - Train loss: 1.85429 - Test loss: 2.13096\n",
      "Epoch 3743 - lr: 0.05000 - Train loss: 1.85429 - Test loss: 2.13096\n",
      "Epoch 3744 - lr: 0.05000 - Train loss: 1.85429 - Test loss: 2.13097\n",
      "Epoch 3745 - lr: 0.05000 - Train loss: 1.85429 - Test loss: 2.13097\n",
      "Epoch 3746 - lr: 0.05000 - Train loss: 1.85429 - Test loss: 2.13098\n",
      "Epoch 3747 - lr: 0.05000 - Train loss: 1.85429 - Test loss: 2.13098\n",
      "Epoch 3748 - lr: 0.05000 - Train loss: 1.85428 - Test loss: 2.13099\n",
      "Epoch 3749 - lr: 0.05000 - Train loss: 1.85428 - Test loss: 2.13099\n",
      "Epoch 3750 - lr: 0.05000 - Train loss: 1.85428 - Test loss: 2.13100\n",
      "Epoch 3751 - lr: 0.05000 - Train loss: 1.85428 - Test loss: 2.13100\n",
      "Epoch 3752 - lr: 0.05000 - Train loss: 1.85428 - Test loss: 2.13101\n",
      "Epoch 3753 - lr: 0.05000 - Train loss: 1.85428 - Test loss: 2.13101\n",
      "Epoch 3754 - lr: 0.05000 - Train loss: 1.85427 - Test loss: 2.13102\n",
      "Epoch 3755 - lr: 0.05000 - Train loss: 1.85427 - Test loss: 2.13103\n",
      "Epoch 3756 - lr: 0.05000 - Train loss: 1.85427 - Test loss: 2.13103\n",
      "Epoch 3757 - lr: 0.05000 - Train loss: 1.85427 - Test loss: 2.13104\n",
      "Epoch 3758 - lr: 0.05000 - Train loss: 1.85427 - Test loss: 2.13104\n",
      "Epoch 3759 - lr: 0.05000 - Train loss: 1.85427 - Test loss: 2.13105\n",
      "Epoch 3760 - lr: 0.05000 - Train loss: 1.85427 - Test loss: 2.13105\n",
      "Epoch 3761 - lr: 0.05000 - Train loss: 1.85426 - Test loss: 2.13106\n",
      "Epoch 3762 - lr: 0.05000 - Train loss: 1.85426 - Test loss: 2.13106\n",
      "Epoch 3763 - lr: 0.05000 - Train loss: 1.85426 - Test loss: 2.13107\n",
      "Epoch 3764 - lr: 0.05000 - Train loss: 1.85426 - Test loss: 2.13107\n",
      "Epoch 3765 - lr: 0.05000 - Train loss: 1.85426 - Test loss: 2.13108\n",
      "Epoch 3766 - lr: 0.05000 - Train loss: 1.85426 - Test loss: 2.13108\n",
      "Epoch 3767 - lr: 0.05000 - Train loss: 1.85425 - Test loss: 2.13109\n",
      "Epoch 3768 - lr: 0.05000 - Train loss: 1.85425 - Test loss: 2.13109\n",
      "Epoch 3769 - lr: 0.05000 - Train loss: 1.85425 - Test loss: 2.13110\n",
      "Epoch 3770 - lr: 0.05000 - Train loss: 1.85425 - Test loss: 2.13111\n",
      "Epoch 3771 - lr: 0.05000 - Train loss: 1.85425 - Test loss: 2.13111\n",
      "Epoch 3772 - lr: 0.05000 - Train loss: 1.85425 - Test loss: 2.13112\n",
      "Epoch 3773 - lr: 0.05000 - Train loss: 1.85424 - Test loss: 2.13112\n",
      "Epoch 3774 - lr: 0.05000 - Train loss: 1.85424 - Test loss: 2.13113\n",
      "Epoch 3775 - lr: 0.05000 - Train loss: 1.85424 - Test loss: 2.13113\n",
      "Epoch 3776 - lr: 0.05000 - Train loss: 1.85424 - Test loss: 2.13114\n",
      "Epoch 3777 - lr: 0.05000 - Train loss: 1.85424 - Test loss: 2.13114\n",
      "Epoch 3778 - lr: 0.05000 - Train loss: 1.85424 - Test loss: 2.13115\n",
      "Epoch 3779 - lr: 0.05000 - Train loss: 1.85423 - Test loss: 2.13115\n",
      "Epoch 3780 - lr: 0.05000 - Train loss: 1.85423 - Test loss: 2.13116\n",
      "Epoch 3781 - lr: 0.05000 - Train loss: 1.85423 - Test loss: 2.13116\n",
      "Epoch 3782 - lr: 0.05000 - Train loss: 1.85423 - Test loss: 2.13117\n",
      "Epoch 3783 - lr: 0.05000 - Train loss: 1.85423 - Test loss: 2.13117\n",
      "Epoch 3784 - lr: 0.05000 - Train loss: 1.85423 - Test loss: 2.13118\n",
      "Epoch 3785 - lr: 0.05000 - Train loss: 1.85423 - Test loss: 2.13119\n",
      "Epoch 3786 - lr: 0.05000 - Train loss: 1.85422 - Test loss: 2.13119\n",
      "Epoch 3787 - lr: 0.05000 - Train loss: 1.85422 - Test loss: 2.13120\n",
      "Epoch 3788 - lr: 0.05000 - Train loss: 1.85422 - Test loss: 2.13120\n",
      "Epoch 3789 - lr: 0.05000 - Train loss: 1.85422 - Test loss: 2.13121\n",
      "Epoch 3790 - lr: 0.05000 - Train loss: 1.85422 - Test loss: 2.13121\n",
      "Epoch 3791 - lr: 0.05000 - Train loss: 1.85422 - Test loss: 2.13122\n",
      "Epoch 3792 - lr: 0.05000 - Train loss: 1.85421 - Test loss: 2.13122\n",
      "Epoch 3793 - lr: 0.05000 - Train loss: 1.85421 - Test loss: 2.13123\n",
      "Epoch 3794 - lr: 0.05000 - Train loss: 1.85421 - Test loss: 2.13123\n",
      "Epoch 3795 - lr: 0.05000 - Train loss: 1.85421 - Test loss: 2.13124\n",
      "Epoch 3796 - lr: 0.05000 - Train loss: 1.85421 - Test loss: 2.13124\n",
      "Epoch 3797 - lr: 0.05000 - Train loss: 1.85421 - Test loss: 2.13125\n",
      "Epoch 3798 - lr: 0.05000 - Train loss: 1.85420 - Test loss: 2.13125\n",
      "Epoch 3799 - lr: 0.05000 - Train loss: 1.85420 - Test loss: 2.13126\n",
      "Epoch 3800 - lr: 0.05000 - Train loss: 1.85420 - Test loss: 2.13126\n",
      "Epoch 3801 - lr: 0.05000 - Train loss: 1.85420 - Test loss: 2.13127\n",
      "Epoch 3802 - lr: 0.05000 - Train loss: 1.85420 - Test loss: 2.13128\n",
      "Epoch 3803 - lr: 0.05000 - Train loss: 1.85420 - Test loss: 2.13128\n",
      "Epoch 3804 - lr: 0.05000 - Train loss: 1.85420 - Test loss: 2.13129\n",
      "Epoch 3805 - lr: 0.05000 - Train loss: 1.85419 - Test loss: 2.13129\n",
      "Epoch 3806 - lr: 0.05000 - Train loss: 1.85419 - Test loss: 2.13130\n",
      "Epoch 3807 - lr: 0.05000 - Train loss: 1.85419 - Test loss: 2.13130\n",
      "Epoch 3808 - lr: 0.05000 - Train loss: 1.85419 - Test loss: 2.13131\n",
      "Epoch 3809 - lr: 0.05000 - Train loss: 1.85419 - Test loss: 2.13131\n",
      "Epoch 3810 - lr: 0.05000 - Train loss: 1.85419 - Test loss: 2.13132\n",
      "Epoch 3811 - lr: 0.05000 - Train loss: 1.85418 - Test loss: 2.13132\n",
      "Epoch 3812 - lr: 0.05000 - Train loss: 1.85418 - Test loss: 2.13133\n",
      "Epoch 3813 - lr: 0.05000 - Train loss: 1.85418 - Test loss: 2.13133\n",
      "Epoch 3814 - lr: 0.05000 - Train loss: 1.85418 - Test loss: 2.13134\n",
      "Epoch 3815 - lr: 0.05000 - Train loss: 1.85418 - Test loss: 2.13134\n",
      "Epoch 3816 - lr: 0.05000 - Train loss: 1.85418 - Test loss: 2.13135\n",
      "Epoch 3817 - lr: 0.05000 - Train loss: 1.85417 - Test loss: 2.13135\n",
      "Epoch 3818 - lr: 0.05000 - Train loss: 1.85417 - Test loss: 2.13136\n",
      "Epoch 3819 - lr: 0.05000 - Train loss: 1.85417 - Test loss: 2.13136\n",
      "Epoch 3820 - lr: 0.05000 - Train loss: 1.85417 - Test loss: 2.13137\n",
      "Epoch 3821 - lr: 0.05000 - Train loss: 1.85417 - Test loss: 2.13138\n",
      "Epoch 3822 - lr: 0.05000 - Train loss: 1.85417 - Test loss: 2.13138\n",
      "Epoch 3823 - lr: 0.05000 - Train loss: 1.85417 - Test loss: 2.13139\n",
      "Epoch 3824 - lr: 0.05000 - Train loss: 1.85416 - Test loss: 2.13139\n",
      "Epoch 3825 - lr: 0.05000 - Train loss: 1.85416 - Test loss: 2.13140\n",
      "Epoch 3826 - lr: 0.05000 - Train loss: 1.85416 - Test loss: 2.13140\n",
      "Epoch 3827 - lr: 0.05000 - Train loss: 1.85416 - Test loss: 2.13141\n",
      "Epoch 3828 - lr: 0.05000 - Train loss: 1.85416 - Test loss: 2.13141\n",
      "Epoch 3829 - lr: 0.05000 - Train loss: 1.85416 - Test loss: 2.13142\n",
      "Epoch 3830 - lr: 0.05000 - Train loss: 1.85415 - Test loss: 2.13142\n",
      "Epoch 3831 - lr: 0.05000 - Train loss: 1.85415 - Test loss: 2.13143\n",
      "Epoch 3832 - lr: 0.05000 - Train loss: 1.85415 - Test loss: 2.13143\n",
      "Epoch 3833 - lr: 0.05000 - Train loss: 1.85415 - Test loss: 2.13144\n",
      "Epoch 3834 - lr: 0.05000 - Train loss: 1.85415 - Test loss: 2.13144\n",
      "Epoch 3835 - lr: 0.05000 - Train loss: 1.85415 - Test loss: 2.13145\n",
      "Epoch 3836 - lr: 0.05000 - Train loss: 1.85415 - Test loss: 2.13145\n",
      "Epoch 3837 - lr: 0.05000 - Train loss: 1.85414 - Test loss: 2.13146\n",
      "Epoch 3838 - lr: 0.05000 - Train loss: 1.85414 - Test loss: 2.13146\n",
      "Epoch 3839 - lr: 0.05000 - Train loss: 1.85414 - Test loss: 2.13147\n",
      "Epoch 3840 - lr: 0.05000 - Train loss: 1.85414 - Test loss: 2.13147\n",
      "Epoch 3841 - lr: 0.05000 - Train loss: 1.85414 - Test loss: 2.13148\n",
      "Epoch 3842 - lr: 0.05000 - Train loss: 1.85414 - Test loss: 2.13149\n",
      "Epoch 3843 - lr: 0.05000 - Train loss: 1.85413 - Test loss: 2.13149\n",
      "Epoch 3844 - lr: 0.05000 - Train loss: 1.85413 - Test loss: 2.13150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3845 - lr: 0.05000 - Train loss: 1.85413 - Test loss: 2.13150\n",
      "Epoch 3846 - lr: 0.05000 - Train loss: 1.85413 - Test loss: 2.13151\n",
      "Epoch 3847 - lr: 0.05000 - Train loss: 1.85413 - Test loss: 2.13151\n",
      "Epoch 3848 - lr: 0.05000 - Train loss: 1.85413 - Test loss: 2.13152\n",
      "Epoch 3849 - lr: 0.05000 - Train loss: 1.85412 - Test loss: 2.13152\n",
      "Epoch 3850 - lr: 0.05000 - Train loss: 1.85412 - Test loss: 2.13153\n",
      "Epoch 3851 - lr: 0.05000 - Train loss: 1.85412 - Test loss: 2.13153\n",
      "Epoch 3852 - lr: 0.05000 - Train loss: 1.85412 - Test loss: 2.13154\n",
      "Epoch 3853 - lr: 0.05000 - Train loss: 1.85412 - Test loss: 2.13154\n",
      "Epoch 3854 - lr: 0.05000 - Train loss: 1.85412 - Test loss: 2.13155\n",
      "Epoch 3855 - lr: 0.05000 - Train loss: 1.85412 - Test loss: 2.13155\n",
      "Epoch 3856 - lr: 0.05000 - Train loss: 1.85411 - Test loss: 2.13156\n",
      "Epoch 3857 - lr: 0.05000 - Train loss: 1.85411 - Test loss: 2.13156\n",
      "Epoch 3858 - lr: 0.05000 - Train loss: 1.85411 - Test loss: 2.13157\n",
      "Epoch 3859 - lr: 0.05000 - Train loss: 1.85411 - Test loss: 2.13157\n",
      "Epoch 3860 - lr: 0.05000 - Train loss: 1.85411 - Test loss: 2.13158\n",
      "Epoch 3861 - lr: 0.05000 - Train loss: 1.85411 - Test loss: 2.13158\n",
      "Epoch 3862 - lr: 0.05000 - Train loss: 1.85410 - Test loss: 2.13159\n",
      "Epoch 3863 - lr: 0.05000 - Train loss: 1.85410 - Test loss: 2.13159\n",
      "Epoch 3864 - lr: 0.05000 - Train loss: 1.85410 - Test loss: 2.13160\n",
      "Epoch 3865 - lr: 0.05000 - Train loss: 1.85410 - Test loss: 2.13160\n",
      "Epoch 3866 - lr: 0.05000 - Train loss: 1.85410 - Test loss: 2.13161\n",
      "Epoch 3867 - lr: 0.05000 - Train loss: 1.85410 - Test loss: 2.13161\n",
      "Epoch 3868 - lr: 0.05000 - Train loss: 1.85410 - Test loss: 2.13162\n",
      "Epoch 3869 - lr: 0.05000 - Train loss: 1.85409 - Test loss: 2.13163\n",
      "Epoch 3870 - lr: 0.05000 - Train loss: 1.85409 - Test loss: 2.13163\n",
      "Epoch 3871 - lr: 0.05000 - Train loss: 1.85409 - Test loss: 2.13164\n",
      "Epoch 3872 - lr: 0.05000 - Train loss: 1.85409 - Test loss: 2.13164\n",
      "Epoch 3873 - lr: 0.05000 - Train loss: 1.85409 - Test loss: 2.13165\n",
      "Epoch 3874 - lr: 0.05000 - Train loss: 1.85409 - Test loss: 2.13165\n",
      "Epoch 3875 - lr: 0.05000 - Train loss: 1.85408 - Test loss: 2.13166\n",
      "Epoch 3876 - lr: 0.05000 - Train loss: 1.85408 - Test loss: 2.13166\n",
      "Epoch 3877 - lr: 0.05000 - Train loss: 1.85408 - Test loss: 2.13167\n",
      "Epoch 3878 - lr: 0.05000 - Train loss: 1.85408 - Test loss: 2.13167\n",
      "Epoch 3879 - lr: 0.05000 - Train loss: 1.85408 - Test loss: 2.13168\n",
      "Epoch 3880 - lr: 0.05000 - Train loss: 1.85408 - Test loss: 2.13168\n",
      "Epoch 3881 - lr: 0.05000 - Train loss: 1.85408 - Test loss: 2.13169\n",
      "Epoch 3882 - lr: 0.05000 - Train loss: 1.85407 - Test loss: 2.13169\n",
      "Epoch 3883 - lr: 0.05000 - Train loss: 1.85407 - Test loss: 2.13170\n",
      "Epoch 3884 - lr: 0.05000 - Train loss: 1.85407 - Test loss: 2.13170\n",
      "Epoch 3885 - lr: 0.05000 - Train loss: 1.85407 - Test loss: 2.13171\n",
      "Epoch 3886 - lr: 0.05000 - Train loss: 1.85407 - Test loss: 2.13171\n",
      "Epoch 3887 - lr: 0.05000 - Train loss: 1.85407 - Test loss: 2.13172\n",
      "Epoch 3888 - lr: 0.05000 - Train loss: 1.85406 - Test loss: 2.13172\n",
      "Epoch 3889 - lr: 0.05000 - Train loss: 1.85406 - Test loss: 2.13173\n",
      "Epoch 3890 - lr: 0.05000 - Train loss: 1.85406 - Test loss: 2.13173\n",
      "Epoch 3891 - lr: 0.05000 - Train loss: 1.85406 - Test loss: 2.13174\n",
      "Epoch 3892 - lr: 0.05000 - Train loss: 1.85406 - Test loss: 2.13174\n",
      "Epoch 3893 - lr: 0.05000 - Train loss: 1.85406 - Test loss: 2.13175\n",
      "Epoch 3894 - lr: 0.05000 - Train loss: 1.85406 - Test loss: 2.13175\n",
      "Epoch 3895 - lr: 0.05000 - Train loss: 1.85405 - Test loss: 2.13176\n",
      "Epoch 3896 - lr: 0.05000 - Train loss: 1.85405 - Test loss: 2.13176\n",
      "Epoch 3897 - lr: 0.05000 - Train loss: 1.85405 - Test loss: 2.13177\n",
      "Epoch 3898 - lr: 0.05000 - Train loss: 1.85405 - Test loss: 2.13177\n",
      "Epoch 3899 - lr: 0.05000 - Train loss: 1.85405 - Test loss: 2.13178\n",
      "Epoch 3900 - lr: 0.05000 - Train loss: 1.85405 - Test loss: 2.13178\n",
      "Epoch 3901 - lr: 0.05000 - Train loss: 1.85404 - Test loss: 2.13179\n",
      "Epoch 3902 - lr: 0.05000 - Train loss: 1.85404 - Test loss: 2.13179\n",
      "Epoch 3903 - lr: 0.05000 - Train loss: 1.85404 - Test loss: 2.13180\n",
      "Epoch 3904 - lr: 0.05000 - Train loss: 1.85404 - Test loss: 2.13181\n",
      "Epoch 3905 - lr: 0.05000 - Train loss: 1.85404 - Test loss: 2.13181\n",
      "Epoch 3906 - lr: 0.05000 - Train loss: 1.85404 - Test loss: 2.13182\n",
      "Epoch 3907 - lr: 0.05000 - Train loss: 1.85404 - Test loss: 2.13182\n",
      "Epoch 3908 - lr: 0.05000 - Train loss: 1.85403 - Test loss: 2.13183\n",
      "Epoch 3909 - lr: 0.05000 - Train loss: 1.85403 - Test loss: 2.13183\n",
      "Epoch 3910 - lr: 0.05000 - Train loss: 1.85403 - Test loss: 2.13184\n",
      "Epoch 3911 - lr: 0.05000 - Train loss: 1.85403 - Test loss: 2.13184\n",
      "Epoch 3912 - lr: 0.05000 - Train loss: 1.85403 - Test loss: 2.13185\n",
      "Epoch 3913 - lr: 0.05000 - Train loss: 1.85403 - Test loss: 2.13185\n",
      "Epoch 3914 - lr: 0.05000 - Train loss: 1.85402 - Test loss: 2.13186\n",
      "Epoch 3915 - lr: 0.05000 - Train loss: 1.85402 - Test loss: 2.13186\n",
      "Epoch 3916 - lr: 0.05000 - Train loss: 1.85402 - Test loss: 2.13187\n",
      "Epoch 3917 - lr: 0.05000 - Train loss: 1.85402 - Test loss: 2.13187\n",
      "Epoch 3918 - lr: 0.05000 - Train loss: 1.85402 - Test loss: 2.13188\n",
      "Epoch 3919 - lr: 0.05000 - Train loss: 1.85402 - Test loss: 2.13188\n",
      "Epoch 3920 - lr: 0.05000 - Train loss: 1.85402 - Test loss: 2.13189\n",
      "Epoch 3921 - lr: 0.05000 - Train loss: 1.85401 - Test loss: 2.13189\n",
      "Epoch 3922 - lr: 0.05000 - Train loss: 1.85401 - Test loss: 2.13190\n",
      "Epoch 3923 - lr: 0.05000 - Train loss: 1.85401 - Test loss: 2.13190\n",
      "Epoch 3924 - lr: 0.05000 - Train loss: 1.85401 - Test loss: 2.13191\n",
      "Epoch 3925 - lr: 0.05000 - Train loss: 1.85401 - Test loss: 2.13191\n",
      "Epoch 3926 - lr: 0.05000 - Train loss: 1.85401 - Test loss: 2.13192\n",
      "Epoch 3927 - lr: 0.05000 - Train loss: 1.85400 - Test loss: 2.13192\n",
      "Epoch 3928 - lr: 0.05000 - Train loss: 1.85400 - Test loss: 2.13193\n",
      "Epoch 3929 - lr: 0.05000 - Train loss: 1.85400 - Test loss: 2.13193\n",
      "Epoch 3930 - lr: 0.05000 - Train loss: 1.85400 - Test loss: 2.13194\n",
      "Epoch 3931 - lr: 0.05000 - Train loss: 1.85400 - Test loss: 2.13194\n",
      "Epoch 3932 - lr: 0.05000 - Train loss: 1.85400 - Test loss: 2.13195\n",
      "Epoch 3933 - lr: 0.05000 - Train loss: 1.85400 - Test loss: 2.13195\n",
      "Epoch 3934 - lr: 0.05000 - Train loss: 1.85399 - Test loss: 2.13196\n",
      "Epoch 3935 - lr: 0.05000 - Train loss: 1.85399 - Test loss: 2.13196\n",
      "Epoch 3936 - lr: 0.05000 - Train loss: 1.85399 - Test loss: 2.13197\n",
      "Epoch 3937 - lr: 0.05000 - Train loss: 1.85399 - Test loss: 2.13197\n",
      "Epoch 3938 - lr: 0.05000 - Train loss: 1.85399 - Test loss: 2.13198\n",
      "Epoch 3939 - lr: 0.05000 - Train loss: 1.85399 - Test loss: 2.13198\n",
      "Epoch 3940 - lr: 0.05000 - Train loss: 1.85398 - Test loss: 2.13199\n",
      "Epoch 3941 - lr: 0.05000 - Train loss: 1.85398 - Test loss: 2.13199\n",
      "Epoch 3942 - lr: 0.05000 - Train loss: 1.85398 - Test loss: 2.13200\n",
      "Epoch 3943 - lr: 0.05000 - Train loss: 1.85398 - Test loss: 2.13200\n",
      "Epoch 3944 - lr: 0.05000 - Train loss: 1.85398 - Test loss: 2.13201\n",
      "Epoch 3945 - lr: 0.05000 - Train loss: 1.85398 - Test loss: 2.13201\n",
      "Epoch 3946 - lr: 0.05000 - Train loss: 1.85398 - Test loss: 2.13202\n",
      "Epoch 3947 - lr: 0.05000 - Train loss: 1.85397 - Test loss: 2.13202\n",
      "Epoch 3948 - lr: 0.05000 - Train loss: 1.85397 - Test loss: 2.13203\n",
      "Epoch 3949 - lr: 0.05000 - Train loss: 1.85397 - Test loss: 2.13203\n",
      "Epoch 3950 - lr: 0.05000 - Train loss: 1.85397 - Test loss: 2.13204\n",
      "Epoch 3951 - lr: 0.05000 - Train loss: 1.85397 - Test loss: 2.13204\n",
      "Epoch 3952 - lr: 0.05000 - Train loss: 1.85397 - Test loss: 2.13205\n",
      "Epoch 3953 - lr: 0.05000 - Train loss: 1.85396 - Test loss: 2.13205\n",
      "Epoch 3954 - lr: 0.05000 - Train loss: 1.85396 - Test loss: 2.13206\n",
      "Epoch 3955 - lr: 0.05000 - Train loss: 1.85396 - Test loss: 2.13206\n",
      "Epoch 3956 - lr: 0.05000 - Train loss: 1.85396 - Test loss: 2.13207\n",
      "Epoch 3957 - lr: 0.05000 - Train loss: 1.85396 - Test loss: 2.13207\n",
      "Epoch 3958 - lr: 0.05000 - Train loss: 1.85396 - Test loss: 2.13208\n",
      "Epoch 3959 - lr: 0.05000 - Train loss: 1.85396 - Test loss: 2.13208\n",
      "Epoch 3960 - lr: 0.05000 - Train loss: 1.85395 - Test loss: 2.13209\n",
      "Epoch 3961 - lr: 0.05000 - Train loss: 1.85395 - Test loss: 2.13209\n",
      "Epoch 3962 - lr: 0.05000 - Train loss: 1.85395 - Test loss: 2.13210\n",
      "Epoch 3963 - lr: 0.05000 - Train loss: 1.85395 - Test loss: 2.13210\n",
      "Epoch 3964 - lr: 0.05000 - Train loss: 1.85395 - Test loss: 2.13211\n",
      "Epoch 3965 - lr: 0.05000 - Train loss: 1.85395 - Test loss: 2.13211\n",
      "Epoch 3966 - lr: 0.05000 - Train loss: 1.85395 - Test loss: 2.13212\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3967 - lr: 0.05000 - Train loss: 1.85394 - Test loss: 2.13212\n",
      "Epoch 3968 - lr: 0.05000 - Train loss: 1.85394 - Test loss: 2.13213\n",
      "Epoch 3969 - lr: 0.05000 - Train loss: 1.85394 - Test loss: 2.13213\n",
      "Epoch 3970 - lr: 0.05000 - Train loss: 1.85394 - Test loss: 2.13214\n",
      "Epoch 3971 - lr: 0.05000 - Train loss: 1.85394 - Test loss: 2.13214\n",
      "Epoch 3972 - lr: 0.05000 - Train loss: 1.85394 - Test loss: 2.13215\n",
      "Epoch 3973 - lr: 0.05000 - Train loss: 1.85393 - Test loss: 2.13215\n",
      "Epoch 3974 - lr: 0.05000 - Train loss: 1.85393 - Test loss: 2.13216\n",
      "Epoch 3975 - lr: 0.05000 - Train loss: 1.85393 - Test loss: 2.13216\n",
      "Epoch 3976 - lr: 0.05000 - Train loss: 1.85393 - Test loss: 2.13217\n",
      "Epoch 3977 - lr: 0.05000 - Train loss: 1.85393 - Test loss: 2.13217\n",
      "Epoch 3978 - lr: 0.05000 - Train loss: 1.85393 - Test loss: 2.13218\n",
      "Epoch 3979 - lr: 0.05000 - Train loss: 1.85393 - Test loss: 2.13218\n",
      "Epoch 3980 - lr: 0.05000 - Train loss: 1.85392 - Test loss: 2.13219\n",
      "Epoch 3981 - lr: 0.05000 - Train loss: 1.85392 - Test loss: 2.13219\n",
      "Epoch 3982 - lr: 0.05000 - Train loss: 1.85392 - Test loss: 2.13220\n",
      "Epoch 3983 - lr: 0.05000 - Train loss: 1.85392 - Test loss: 2.13220\n",
      "Epoch 3984 - lr: 0.05000 - Train loss: 1.85392 - Test loss: 2.13221\n",
      "Epoch 3985 - lr: 0.05000 - Train loss: 1.85392 - Test loss: 2.13221\n",
      "Epoch 3986 - lr: 0.05000 - Train loss: 1.85392 - Test loss: 2.13222\n",
      "Epoch 3987 - lr: 0.05000 - Train loss: 1.85391 - Test loss: 2.13222\n",
      "Epoch 3988 - lr: 0.05000 - Train loss: 1.85391 - Test loss: 2.13223\n",
      "Epoch 3989 - lr: 0.05000 - Train loss: 1.85391 - Test loss: 2.13223\n",
      "Epoch 3990 - lr: 0.05000 - Train loss: 1.85391 - Test loss: 2.13224\n",
      "Epoch 3991 - lr: 0.05000 - Train loss: 1.85391 - Test loss: 2.13224\n",
      "Epoch 3992 - lr: 0.05000 - Train loss: 1.85391 - Test loss: 2.13225\n",
      "Epoch 3993 - lr: 0.05000 - Train loss: 1.85390 - Test loss: 2.13225\n",
      "Epoch 3994 - lr: 0.05000 - Train loss: 1.85390 - Test loss: 2.13226\n",
      "Epoch 3995 - lr: 0.05000 - Train loss: 1.85390 - Test loss: 2.13226\n",
      "Epoch 3996 - lr: 0.05000 - Train loss: 1.85390 - Test loss: 2.13227\n",
      "Epoch 3997 - lr: 0.05000 - Train loss: 1.85390 - Test loss: 2.13227\n",
      "Epoch 3998 - lr: 0.05000 - Train loss: 1.85390 - Test loss: 2.13228\n",
      "Epoch 3999 - lr: 0.05000 - Train loss: 1.85390 - Test loss: 2.13228\n",
      "Epoch 4000 - lr: 0.05000 - Train loss: 1.85389 - Test loss: 2.13229\n",
      "Epoch 4001 - lr: 0.05000 - Train loss: 1.85389 - Test loss: 2.13229\n",
      "Epoch 4002 - lr: 0.05000 - Train loss: 1.85389 - Test loss: 2.13230\n",
      "Epoch 4003 - lr: 0.05000 - Train loss: 1.85389 - Test loss: 2.13230\n",
      "Epoch 4004 - lr: 0.05000 - Train loss: 1.85389 - Test loss: 2.13231\n",
      "Epoch 4005 - lr: 0.05000 - Train loss: 1.85389 - Test loss: 2.13231\n",
      "Epoch 4006 - lr: 0.05000 - Train loss: 1.85389 - Test loss: 2.13232\n",
      "Epoch 4007 - lr: 0.05000 - Train loss: 1.85388 - Test loss: 2.13232\n",
      "Epoch 4008 - lr: 0.05000 - Train loss: 1.85388 - Test loss: 2.13233\n",
      "Epoch 4009 - lr: 0.05000 - Train loss: 1.85388 - Test loss: 2.13233\n",
      "Epoch 4010 - lr: 0.05000 - Train loss: 1.85388 - Test loss: 2.13234\n",
      "Epoch 4011 - lr: 0.05000 - Train loss: 1.85388 - Test loss: 2.13234\n",
      "Epoch 4012 - lr: 0.05000 - Train loss: 1.85388 - Test loss: 2.13235\n",
      "Epoch 4013 - lr: 0.05000 - Train loss: 1.85387 - Test loss: 2.13235\n",
      "Epoch 4014 - lr: 0.05000 - Train loss: 1.85387 - Test loss: 2.13236\n",
      "Epoch 4015 - lr: 0.05000 - Train loss: 1.85387 - Test loss: 2.13236\n",
      "Epoch 4016 - lr: 0.05000 - Train loss: 1.85387 - Test loss: 2.13237\n",
      "Epoch 4017 - lr: 0.05000 - Train loss: 1.85387 - Test loss: 2.13237\n",
      "Epoch 4018 - lr: 0.05000 - Train loss: 1.85387 - Test loss: 2.13238\n",
      "Epoch 4019 - lr: 0.05000 - Train loss: 1.85387 - Test loss: 2.13238\n",
      "Epoch 4020 - lr: 0.05000 - Train loss: 1.85386 - Test loss: 2.13239\n",
      "Epoch 4021 - lr: 0.05000 - Train loss: 1.85386 - Test loss: 2.13239\n",
      "Epoch 4022 - lr: 0.05000 - Train loss: 1.85386 - Test loss: 2.13240\n",
      "Epoch 4023 - lr: 0.05000 - Train loss: 1.85386 - Test loss: 2.13240\n",
      "Epoch 4024 - lr: 0.05000 - Train loss: 1.85386 - Test loss: 2.13241\n",
      "Epoch 4025 - lr: 0.05000 - Train loss: 1.85386 - Test loss: 2.13241\n",
      "Epoch 4026 - lr: 0.05000 - Train loss: 1.85386 - Test loss: 2.13242\n",
      "Epoch 4027 - lr: 0.05000 - Train loss: 1.85385 - Test loss: 2.13242\n",
      "Epoch 4028 - lr: 0.05000 - Train loss: 1.85385 - Test loss: 2.13243\n",
      "Epoch 4029 - lr: 0.05000 - Train loss: 1.85385 - Test loss: 2.13243\n",
      "Epoch 4030 - lr: 0.05000 - Train loss: 1.85385 - Test loss: 2.13244\n",
      "Epoch 4031 - lr: 0.05000 - Train loss: 1.85385 - Test loss: 2.13244\n",
      "Epoch 4032 - lr: 0.05000 - Train loss: 1.85385 - Test loss: 2.13245\n",
      "Epoch 4033 - lr: 0.05000 - Train loss: 1.85385 - Test loss: 2.13245\n",
      "Epoch 4034 - lr: 0.05000 - Train loss: 1.85384 - Test loss: 2.13246\n",
      "Epoch 4035 - lr: 0.05000 - Train loss: 1.85384 - Test loss: 2.13246\n",
      "Epoch 4036 - lr: 0.05000 - Train loss: 1.85384 - Test loss: 2.13247\n",
      "Epoch 4037 - lr: 0.05000 - Train loss: 1.85384 - Test loss: 2.13247\n",
      "Epoch 4038 - lr: 0.05000 - Train loss: 1.85384 - Test loss: 2.13248\n",
      "Epoch 4039 - lr: 0.05000 - Train loss: 1.85384 - Test loss: 2.13248\n",
      "Epoch 4040 - lr: 0.05000 - Train loss: 1.85383 - Test loss: 2.13249\n",
      "Epoch 4041 - lr: 0.05000 - Train loss: 1.85383 - Test loss: 2.13249\n",
      "Epoch 4042 - lr: 0.05000 - Train loss: 1.85383 - Test loss: 2.13250\n",
      "Epoch 4043 - lr: 0.05000 - Train loss: 1.85383 - Test loss: 2.13250\n",
      "Epoch 4044 - lr: 0.05000 - Train loss: 1.85383 - Test loss: 2.13251\n",
      "Epoch 4045 - lr: 0.05000 - Train loss: 1.85383 - Test loss: 2.13251\n",
      "Epoch 4046 - lr: 0.05000 - Train loss: 1.85383 - Test loss: 2.13252\n",
      "Epoch 4047 - lr: 0.05000 - Train loss: 1.85382 - Test loss: 2.13252\n",
      "Epoch 4048 - lr: 0.05000 - Train loss: 1.85382 - Test loss: 2.13253\n",
      "Epoch 4049 - lr: 0.05000 - Train loss: 1.85382 - Test loss: 2.13253\n",
      "Epoch 4050 - lr: 0.05000 - Train loss: 1.85382 - Test loss: 2.13253\n",
      "Epoch 4051 - lr: 0.05000 - Train loss: 1.85382 - Test loss: 2.13254\n",
      "Epoch 4052 - lr: 0.05000 - Train loss: 1.85382 - Test loss: 2.13254\n",
      "Epoch 4053 - lr: 0.05000 - Train loss: 1.85382 - Test loss: 2.13255\n",
      "Epoch 4054 - lr: 0.05000 - Train loss: 1.85381 - Test loss: 2.13255\n",
      "Epoch 4055 - lr: 0.05000 - Train loss: 1.85381 - Test loss: 2.13256\n",
      "Epoch 4056 - lr: 0.05000 - Train loss: 1.85381 - Test loss: 2.13256\n",
      "Epoch 4057 - lr: 0.05000 - Train loss: 1.85381 - Test loss: 2.13257\n",
      "Epoch 4058 - lr: 0.05000 - Train loss: 1.85381 - Test loss: 2.13257\n",
      "Epoch 4059 - lr: 0.05000 - Train loss: 1.85381 - Test loss: 2.13258\n",
      "Epoch 4060 - lr: 0.05000 - Train loss: 1.85381 - Test loss: 2.13258\n",
      "Epoch 4061 - lr: 0.05000 - Train loss: 1.85380 - Test loss: 2.13259\n",
      "Epoch 4062 - lr: 0.05000 - Train loss: 1.85380 - Test loss: 2.13259\n",
      "Epoch 4063 - lr: 0.05000 - Train loss: 1.85380 - Test loss: 2.13260\n",
      "Epoch 4064 - lr: 0.05000 - Train loss: 1.85380 - Test loss: 2.13260\n",
      "Epoch 4065 - lr: 0.05000 - Train loss: 1.85380 - Test loss: 2.13261\n",
      "Epoch 4066 - lr: 0.05000 - Train loss: 1.85380 - Test loss: 2.13261\n",
      "Epoch 4067 - lr: 0.05000 - Train loss: 1.85379 - Test loss: 2.13262\n",
      "Epoch 4068 - lr: 0.05000 - Train loss: 1.85379 - Test loss: 2.13262\n",
      "Epoch 4069 - lr: 0.05000 - Train loss: 1.85379 - Test loss: 2.13263\n",
      "Epoch 4070 - lr: 0.05000 - Train loss: 1.85379 - Test loss: 2.13263\n",
      "Epoch 4071 - lr: 0.05000 - Train loss: 1.85379 - Test loss: 2.13264\n",
      "Epoch 4072 - lr: 0.05000 - Train loss: 1.85379 - Test loss: 2.13264\n",
      "Epoch 4073 - lr: 0.05000 - Train loss: 1.85379 - Test loss: 2.13265\n",
      "Epoch 4074 - lr: 0.05000 - Train loss: 1.85378 - Test loss: 2.13265\n",
      "Epoch 4075 - lr: 0.05000 - Train loss: 1.85378 - Test loss: 2.13266\n",
      "Epoch 4076 - lr: 0.05000 - Train loss: 1.85378 - Test loss: 2.13266\n",
      "Epoch 4077 - lr: 0.05000 - Train loss: 1.85378 - Test loss: 2.13267\n",
      "Epoch 4078 - lr: 0.05000 - Train loss: 1.85378 - Test loss: 2.13267\n",
      "Epoch 4079 - lr: 0.05000 - Train loss: 1.85378 - Test loss: 2.13268\n",
      "Epoch 4080 - lr: 0.05000 - Train loss: 1.85378 - Test loss: 2.13268\n",
      "Epoch 4081 - lr: 0.05000 - Train loss: 1.85377 - Test loss: 2.13269\n",
      "Epoch 4082 - lr: 0.05000 - Train loss: 1.85377 - Test loss: 2.13269\n",
      "Epoch 4083 - lr: 0.05000 - Train loss: 1.85377 - Test loss: 2.13270\n",
      "Epoch 4084 - lr: 0.05000 - Train loss: 1.85377 - Test loss: 2.13270\n",
      "Epoch 4085 - lr: 0.05000 - Train loss: 1.85377 - Test loss: 2.13271\n",
      "Epoch 4086 - lr: 0.05000 - Train loss: 1.85377 - Test loss: 2.13271\n",
      "Epoch 4087 - lr: 0.05000 - Train loss: 1.85377 - Test loss: 2.13271\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4088 - lr: 0.05000 - Train loss: 1.85376 - Test loss: 2.13272\n",
      "Epoch 4089 - lr: 0.05000 - Train loss: 1.85376 - Test loss: 2.13272\n",
      "Epoch 4090 - lr: 0.05000 - Train loss: 1.85376 - Test loss: 2.13273\n",
      "Epoch 4091 - lr: 0.05000 - Train loss: 1.85376 - Test loss: 2.13273\n",
      "Epoch 4092 - lr: 0.05000 - Train loss: 1.85376 - Test loss: 2.13274\n",
      "Epoch 4093 - lr: 0.05000 - Train loss: 1.85376 - Test loss: 2.13274\n",
      "Epoch 4094 - lr: 0.05000 - Train loss: 1.85376 - Test loss: 2.13275\n",
      "Epoch 4095 - lr: 0.05000 - Train loss: 1.85375 - Test loss: 2.13275\n",
      "Epoch 4096 - lr: 0.05000 - Train loss: 1.85375 - Test loss: 2.13276\n",
      "Epoch 4097 - lr: 0.05000 - Train loss: 1.85375 - Test loss: 2.13276\n",
      "Epoch 4098 - lr: 0.05000 - Train loss: 1.85375 - Test loss: 2.13277\n",
      "Epoch 4099 - lr: 0.05000 - Train loss: 1.85375 - Test loss: 2.13277\n",
      "Epoch 4100 - lr: 0.05000 - Train loss: 1.85375 - Test loss: 2.13278\n",
      "Epoch 4101 - lr: 0.05000 - Train loss: 1.85375 - Test loss: 2.13278\n",
      "Epoch 4102 - lr: 0.05000 - Train loss: 1.85374 - Test loss: 2.13279\n",
      "Epoch 4103 - lr: 0.05000 - Train loss: 1.85374 - Test loss: 2.13279\n",
      "Epoch 4104 - lr: 0.05000 - Train loss: 1.85374 - Test loss: 2.13280\n",
      "Epoch 4105 - lr: 0.05000 - Train loss: 1.85374 - Test loss: 2.13280\n",
      "Epoch 4106 - lr: 0.05000 - Train loss: 1.85374 - Test loss: 2.13281\n",
      "Epoch 4107 - lr: 0.05000 - Train loss: 1.85374 - Test loss: 2.13281\n",
      "Epoch 4108 - lr: 0.05000 - Train loss: 1.85373 - Test loss: 2.13282\n",
      "Epoch 4109 - lr: 0.05000 - Train loss: 1.85373 - Test loss: 2.13282\n",
      "Epoch 4110 - lr: 0.05000 - Train loss: 1.85373 - Test loss: 2.13283\n",
      "Epoch 4111 - lr: 0.05000 - Train loss: 1.85373 - Test loss: 2.13283\n",
      "Epoch 4112 - lr: 0.05000 - Train loss: 1.85373 - Test loss: 2.13284\n",
      "Epoch 4113 - lr: 0.05000 - Train loss: 1.85373 - Test loss: 2.13284\n",
      "Epoch 4114 - lr: 0.05000 - Train loss: 1.85373 - Test loss: 2.13285\n",
      "Epoch 4115 - lr: 0.05000 - Train loss: 1.85372 - Test loss: 2.13285\n",
      "Epoch 4116 - lr: 0.05000 - Train loss: 1.85372 - Test loss: 2.13285\n",
      "Epoch 4117 - lr: 0.05000 - Train loss: 1.85372 - Test loss: 2.13286\n",
      "Epoch 4118 - lr: 0.05000 - Train loss: 1.85372 - Test loss: 2.13286\n",
      "Epoch 4119 - lr: 0.05000 - Train loss: 1.85372 - Test loss: 2.13287\n",
      "Epoch 4120 - lr: 0.05000 - Train loss: 1.85372 - Test loss: 2.13287\n",
      "Epoch 4121 - lr: 0.05000 - Train loss: 1.85372 - Test loss: 2.13288\n",
      "Epoch 4122 - lr: 0.05000 - Train loss: 1.85371 - Test loss: 2.13288\n",
      "Epoch 4123 - lr: 0.05000 - Train loss: 1.85371 - Test loss: 2.13289\n",
      "Epoch 4124 - lr: 0.05000 - Train loss: 1.85371 - Test loss: 2.13289\n",
      "Epoch 4125 - lr: 0.05000 - Train loss: 1.85371 - Test loss: 2.13290\n",
      "Epoch 4126 - lr: 0.05000 - Train loss: 1.85371 - Test loss: 2.13290\n",
      "Epoch 4127 - lr: 0.05000 - Train loss: 1.85371 - Test loss: 2.13291\n",
      "Epoch 4128 - lr: 0.05000 - Train loss: 1.85371 - Test loss: 2.13291\n",
      "Epoch 4129 - lr: 0.05000 - Train loss: 1.85370 - Test loss: 2.13292\n",
      "Epoch 4130 - lr: 0.05000 - Train loss: 1.85370 - Test loss: 2.13292\n",
      "Epoch 4131 - lr: 0.05000 - Train loss: 1.85370 - Test loss: 2.13293\n",
      "Epoch 4132 - lr: 0.05000 - Train loss: 1.85370 - Test loss: 2.13293\n",
      "Epoch 4133 - lr: 0.05000 - Train loss: 1.85370 - Test loss: 2.13294\n",
      "Epoch 4134 - lr: 0.05000 - Train loss: 1.85370 - Test loss: 2.13294\n",
      "Epoch 4135 - lr: 0.05000 - Train loss: 1.85370 - Test loss: 2.13295\n",
      "Epoch 4136 - lr: 0.05000 - Train loss: 1.85369 - Test loss: 2.13295\n",
      "Epoch 4137 - lr: 0.05000 - Train loss: 1.85369 - Test loss: 2.13296\n",
      "Epoch 4138 - lr: 0.05000 - Train loss: 1.85369 - Test loss: 2.13296\n",
      "Epoch 4139 - lr: 0.05000 - Train loss: 1.85369 - Test loss: 2.13296\n",
      "Epoch 4140 - lr: 0.05000 - Train loss: 1.85369 - Test loss: 2.13297\n",
      "Epoch 4141 - lr: 0.05000 - Train loss: 1.85369 - Test loss: 2.13297\n",
      "Epoch 4142 - lr: 0.05000 - Train loss: 1.85369 - Test loss: 2.13298\n",
      "Epoch 4143 - lr: 0.05000 - Train loss: 1.85368 - Test loss: 2.13298\n",
      "Epoch 4144 - lr: 0.05000 - Train loss: 1.85368 - Test loss: 2.13299\n",
      "Epoch 4145 - lr: 0.05000 - Train loss: 1.85368 - Test loss: 2.13299\n",
      "Epoch 4146 - lr: 0.05000 - Train loss: 1.85368 - Test loss: 2.13300\n",
      "Epoch 4147 - lr: 0.05000 - Train loss: 1.85368 - Test loss: 2.13300\n",
      "Epoch 4148 - lr: 0.05000 - Train loss: 1.85368 - Test loss: 2.13301\n",
      "Epoch 4149 - lr: 0.05000 - Train loss: 1.85368 - Test loss: 2.13301\n",
      "Epoch 4150 - lr: 0.05000 - Train loss: 1.85367 - Test loss: 2.13302\n",
      "Epoch 4151 - lr: 0.05000 - Train loss: 1.85367 - Test loss: 2.13302\n",
      "Epoch 4152 - lr: 0.05000 - Train loss: 1.85367 - Test loss: 2.13303\n",
      "Epoch 4153 - lr: 0.05000 - Train loss: 1.85367 - Test loss: 2.13303\n",
      "Epoch 4154 - lr: 0.05000 - Train loss: 1.85367 - Test loss: 2.13304\n",
      "Epoch 4155 - lr: 0.05000 - Train loss: 1.85367 - Test loss: 2.13304\n",
      "Epoch 4156 - lr: 0.05000 - Train loss: 1.85367 - Test loss: 2.13305\n",
      "Epoch 4157 - lr: 0.05000 - Train loss: 1.85366 - Test loss: 2.13305\n",
      "Epoch 4158 - lr: 0.05000 - Train loss: 1.85366 - Test loss: 2.13305\n",
      "Epoch 4159 - lr: 0.05000 - Train loss: 1.85366 - Test loss: 2.13306\n",
      "Epoch 4160 - lr: 0.05000 - Train loss: 1.85366 - Test loss: 2.13306\n",
      "Epoch 4161 - lr: 0.05000 - Train loss: 1.85366 - Test loss: 2.13307\n",
      "Epoch 4162 - lr: 0.05000 - Train loss: 1.85366 - Test loss: 2.13307\n",
      "Epoch 4163 - lr: 0.05000 - Train loss: 1.85366 - Test loss: 2.13308\n",
      "Epoch 4164 - lr: 0.05000 - Train loss: 1.85365 - Test loss: 2.13308\n",
      "Epoch 4165 - lr: 0.05000 - Train loss: 1.85365 - Test loss: 2.13309\n",
      "Epoch 4166 - lr: 0.05000 - Train loss: 1.85365 - Test loss: 2.13309\n",
      "Epoch 4167 - lr: 0.05000 - Train loss: 1.85365 - Test loss: 2.13310\n",
      "Epoch 4168 - lr: 0.05000 - Train loss: 1.85365 - Test loss: 2.13310\n",
      "Epoch 4169 - lr: 0.05000 - Train loss: 1.85365 - Test loss: 2.13311\n",
      "Epoch 4170 - lr: 0.05000 - Train loss: 1.85365 - Test loss: 2.13311\n",
      "Epoch 4171 - lr: 0.05000 - Train loss: 1.85364 - Test loss: 2.13312\n",
      "Epoch 4172 - lr: 0.05000 - Train loss: 1.85364 - Test loss: 2.13312\n",
      "Epoch 4173 - lr: 0.05000 - Train loss: 1.85364 - Test loss: 2.13313\n",
      "Epoch 4174 - lr: 0.05000 - Train loss: 1.85364 - Test loss: 2.13313\n",
      "Epoch 4175 - lr: 0.05000 - Train loss: 1.85364 - Test loss: 2.13314\n",
      "Epoch 4176 - lr: 0.05000 - Train loss: 1.85364 - Test loss: 2.13314\n",
      "Epoch 4177 - lr: 0.05000 - Train loss: 1.85364 - Test loss: 2.13314\n",
      "Epoch 4178 - lr: 0.05000 - Train loss: 1.85363 - Test loss: 2.13315\n",
      "Epoch 4179 - lr: 0.05000 - Train loss: 1.85363 - Test loss: 2.13315\n",
      "Epoch 4180 - lr: 0.05000 - Train loss: 1.85363 - Test loss: 2.13316\n",
      "Epoch 4181 - lr: 0.05000 - Train loss: 1.85363 - Test loss: 2.13316\n",
      "Epoch 4182 - lr: 0.05000 - Train loss: 1.85363 - Test loss: 2.13317\n",
      "Epoch 4183 - lr: 0.05000 - Train loss: 1.85363 - Test loss: 2.13317\n",
      "Epoch 4184 - lr: 0.05000 - Train loss: 1.85363 - Test loss: 2.13318\n",
      "Epoch 4185 - lr: 0.05000 - Train loss: 1.85362 - Test loss: 2.13318\n",
      "Epoch 4186 - lr: 0.05000 - Train loss: 1.85362 - Test loss: 2.13319\n",
      "Epoch 4187 - lr: 0.05000 - Train loss: 1.85362 - Test loss: 2.13319\n",
      "Epoch 4188 - lr: 0.05000 - Train loss: 1.85362 - Test loss: 2.13320\n",
      "Epoch 4189 - lr: 0.05000 - Train loss: 1.85362 - Test loss: 2.13320\n",
      "Epoch 4190 - lr: 0.05000 - Train loss: 1.85362 - Test loss: 2.13321\n",
      "Epoch 4191 - lr: 0.05000 - Train loss: 1.85362 - Test loss: 2.13321\n",
      "Epoch 4192 - lr: 0.05000 - Train loss: 1.85361 - Test loss: 2.13322\n",
      "Epoch 4193 - lr: 0.05000 - Train loss: 1.85361 - Test loss: 2.13322\n",
      "Epoch 4194 - lr: 0.05000 - Train loss: 1.85361 - Test loss: 2.13322\n",
      "Epoch 4195 - lr: 0.05000 - Train loss: 1.85361 - Test loss: 2.13323\n",
      "Epoch 4196 - lr: 0.05000 - Train loss: 1.85361 - Test loss: 2.13323\n",
      "Epoch 4197 - lr: 0.05000 - Train loss: 1.85361 - Test loss: 2.13324\n",
      "Epoch 4198 - lr: 0.05000 - Train loss: 1.85361 - Test loss: 2.13324\n",
      "Epoch 4199 - lr: 0.05000 - Train loss: 1.85360 - Test loss: 2.13325\n",
      "Epoch 4200 - lr: 0.05000 - Train loss: 1.85360 - Test loss: 2.13325\n",
      "Epoch 4201 - lr: 0.05000 - Train loss: 1.85360 - Test loss: 2.13326\n",
      "Epoch 4202 - lr: 0.05000 - Train loss: 1.85360 - Test loss: 2.13326\n",
      "Epoch 4203 - lr: 0.05000 - Train loss: 1.85360 - Test loss: 2.13327\n",
      "Epoch 4204 - lr: 0.05000 - Train loss: 1.85360 - Test loss: 2.13327\n",
      "Epoch 4205 - lr: 0.05000 - Train loss: 1.85360 - Test loss: 2.13328\n",
      "Epoch 4206 - lr: 0.05000 - Train loss: 1.85359 - Test loss: 2.13328\n",
      "Epoch 4207 - lr: 0.05000 - Train loss: 1.85359 - Test loss: 2.13329\n",
      "Epoch 4208 - lr: 0.05000 - Train loss: 1.85359 - Test loss: 2.13329\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4209 - lr: 0.05000 - Train loss: 1.85359 - Test loss: 2.13330\n",
      "Epoch 4210 - lr: 0.05000 - Train loss: 1.85359 - Test loss: 2.13330\n",
      "Epoch 4211 - lr: 0.05000 - Train loss: 1.85359 - Test loss: 2.13330\n",
      "Epoch 4212 - lr: 0.05000 - Train loss: 1.85359 - Test loss: 2.13331\n",
      "Epoch 4213 - lr: 0.05000 - Train loss: 1.85358 - Test loss: 2.13331\n",
      "Epoch 4214 - lr: 0.05000 - Train loss: 1.85358 - Test loss: 2.13332\n",
      "Epoch 4215 - lr: 0.05000 - Train loss: 1.85358 - Test loss: 2.13332\n",
      "Epoch 4216 - lr: 0.05000 - Train loss: 1.85358 - Test loss: 2.13333\n",
      "Epoch 4217 - lr: 0.05000 - Train loss: 1.85358 - Test loss: 2.13333\n",
      "Epoch 4218 - lr: 0.05000 - Train loss: 1.85358 - Test loss: 2.13334\n",
      "Epoch 4219 - lr: 0.05000 - Train loss: 1.85358 - Test loss: 2.13334\n",
      "Epoch 4220 - lr: 0.05000 - Train loss: 1.85357 - Test loss: 2.13335\n",
      "Epoch 4221 - lr: 0.05000 - Train loss: 1.85357 - Test loss: 2.13335\n",
      "Epoch 4222 - lr: 0.05000 - Train loss: 1.85357 - Test loss: 2.13336\n",
      "Epoch 4223 - lr: 0.05000 - Train loss: 1.85357 - Test loss: 2.13336\n",
      "Epoch 4224 - lr: 0.05000 - Train loss: 1.85357 - Test loss: 2.13337\n",
      "Epoch 4225 - lr: 0.05000 - Train loss: 1.85357 - Test loss: 2.13337\n",
      "Epoch 4226 - lr: 0.05000 - Train loss: 1.85357 - Test loss: 2.13337\n",
      "Epoch 4227 - lr: 0.05000 - Train loss: 1.85356 - Test loss: 2.13338\n",
      "Epoch 4228 - lr: 0.05000 - Train loss: 1.85356 - Test loss: 2.13338\n",
      "Epoch 4229 - lr: 0.05000 - Train loss: 1.85356 - Test loss: 2.13339\n",
      "Epoch 4230 - lr: 0.05000 - Train loss: 1.85356 - Test loss: 2.13339\n",
      "Epoch 4231 - lr: 0.05000 - Train loss: 1.85356 - Test loss: 2.13340\n",
      "Epoch 4232 - lr: 0.05000 - Train loss: 1.85356 - Test loss: 2.13340\n",
      "Epoch 4233 - lr: 0.05000 - Train loss: 1.85356 - Test loss: 2.13341\n",
      "Epoch 4234 - lr: 0.05000 - Train loss: 1.85355 - Test loss: 2.13341\n",
      "Epoch 4235 - lr: 0.05000 - Train loss: 1.85355 - Test loss: 2.13342\n",
      "Epoch 4236 - lr: 0.05000 - Train loss: 1.85355 - Test loss: 2.13342\n",
      "Epoch 4237 - lr: 0.05000 - Train loss: 1.85355 - Test loss: 2.13343\n",
      "Epoch 4238 - lr: 0.05000 - Train loss: 1.85355 - Test loss: 2.13343\n",
      "Epoch 4239 - lr: 0.05000 - Train loss: 1.85355 - Test loss: 2.13343\n",
      "Epoch 4240 - lr: 0.05000 - Train loss: 1.85355 - Test loss: 2.13344\n",
      "Epoch 4241 - lr: 0.05000 - Train loss: 1.85355 - Test loss: 2.13344\n",
      "Epoch 4242 - lr: 0.05000 - Train loss: 1.85354 - Test loss: 2.13345\n",
      "Epoch 4243 - lr: 0.05000 - Train loss: 1.85354 - Test loss: 2.13345\n",
      "Epoch 4244 - lr: 0.05000 - Train loss: 1.85354 - Test loss: 2.13346\n",
      "Epoch 4245 - lr: 0.05000 - Train loss: 1.85354 - Test loss: 2.13346\n",
      "Epoch 4246 - lr: 0.05000 - Train loss: 1.85354 - Test loss: 2.13347\n",
      "Epoch 4247 - lr: 0.05000 - Train loss: 1.85354 - Test loss: 2.13347\n",
      "Epoch 4248 - lr: 0.05000 - Train loss: 1.85354 - Test loss: 2.13348\n",
      "Epoch 4249 - lr: 0.05000 - Train loss: 1.85353 - Test loss: 2.13348\n",
      "Epoch 4250 - lr: 0.05000 - Train loss: 1.85353 - Test loss: 2.13349\n",
      "Epoch 4251 - lr: 0.05000 - Train loss: 1.85353 - Test loss: 2.13349\n",
      "Epoch 4252 - lr: 0.05000 - Train loss: 1.85353 - Test loss: 2.13350\n",
      "Epoch 4253 - lr: 0.05000 - Train loss: 1.85353 - Test loss: 2.13350\n",
      "Epoch 4254 - lr: 0.05000 - Train loss: 1.85353 - Test loss: 2.13350\n",
      "Epoch 4255 - lr: 0.05000 - Train loss: 1.85353 - Test loss: 2.13351\n",
      "Epoch 4256 - lr: 0.05000 - Train loss: 1.85352 - Test loss: 2.13351\n",
      "Epoch 4257 - lr: 0.05000 - Train loss: 1.85352 - Test loss: 2.13352\n",
      "Epoch 4258 - lr: 0.05000 - Train loss: 1.85352 - Test loss: 2.13352\n",
      "Epoch 4259 - lr: 0.05000 - Train loss: 1.85352 - Test loss: 2.13353\n",
      "Epoch 4260 - lr: 0.05000 - Train loss: 1.85352 - Test loss: 2.13353\n",
      "Epoch 4261 - lr: 0.05000 - Train loss: 1.85352 - Test loss: 2.13354\n",
      "Epoch 4262 - lr: 0.05000 - Train loss: 1.85352 - Test loss: 2.13354\n",
      "Epoch 4263 - lr: 0.05000 - Train loss: 1.85351 - Test loss: 2.13355\n",
      "Epoch 4264 - lr: 0.05000 - Train loss: 1.85351 - Test loss: 2.13355\n",
      "Epoch 4265 - lr: 0.05000 - Train loss: 1.85351 - Test loss: 2.13356\n",
      "Epoch 4266 - lr: 0.05000 - Train loss: 1.85351 - Test loss: 2.13356\n",
      "Epoch 4267 - lr: 0.05000 - Train loss: 1.85351 - Test loss: 2.13356\n",
      "Epoch 4268 - lr: 0.05000 - Train loss: 1.85351 - Test loss: 2.13357\n",
      "Epoch 4269 - lr: 0.05000 - Train loss: 1.85351 - Test loss: 2.13357\n",
      "Epoch 4270 - lr: 0.05000 - Train loss: 1.85350 - Test loss: 2.13358\n",
      "Epoch 4271 - lr: 0.05000 - Train loss: 1.85350 - Test loss: 2.13358\n",
      "Epoch 4272 - lr: 0.05000 - Train loss: 1.85350 - Test loss: 2.13359\n",
      "Epoch 4273 - lr: 0.05000 - Train loss: 1.85350 - Test loss: 2.13359\n",
      "Epoch 4274 - lr: 0.05000 - Train loss: 1.85350 - Test loss: 2.13360\n",
      "Epoch 4275 - lr: 0.05000 - Train loss: 1.85350 - Test loss: 2.13360\n",
      "Epoch 4276 - lr: 0.05000 - Train loss: 1.85350 - Test loss: 2.13361\n",
      "Epoch 4277 - lr: 0.05000 - Train loss: 1.85349 - Test loss: 2.13361\n",
      "Epoch 4278 - lr: 0.05000 - Train loss: 1.85349 - Test loss: 2.13361\n",
      "Epoch 4279 - lr: 0.05000 - Train loss: 1.85349 - Test loss: 2.13362\n",
      "Epoch 4280 - lr: 0.05000 - Train loss: 1.85349 - Test loss: 2.13362\n",
      "Epoch 4281 - lr: 0.05000 - Train loss: 1.85349 - Test loss: 2.13363\n",
      "Epoch 4282 - lr: 0.05000 - Train loss: 1.85349 - Test loss: 2.13363\n",
      "Epoch 4283 - lr: 0.05000 - Train loss: 1.85349 - Test loss: 2.13364\n",
      "Epoch 4284 - lr: 0.05000 - Train loss: 1.85349 - Test loss: 2.13364\n",
      "Epoch 4285 - lr: 0.05000 - Train loss: 1.85348 - Test loss: 2.13365\n",
      "Epoch 4286 - lr: 0.05000 - Train loss: 1.85348 - Test loss: 2.13365\n",
      "Epoch 4287 - lr: 0.05000 - Train loss: 1.85348 - Test loss: 2.13366\n",
      "Epoch 4288 - lr: 0.05000 - Train loss: 1.85348 - Test loss: 2.13366\n",
      "Epoch 4289 - lr: 0.05000 - Train loss: 1.85348 - Test loss: 2.13367\n",
      "Epoch 4290 - lr: 0.05000 - Train loss: 1.85348 - Test loss: 2.13367\n",
      "Epoch 4291 - lr: 0.05000 - Train loss: 1.85348 - Test loss: 2.13367\n",
      "Epoch 4292 - lr: 0.05000 - Train loss: 1.85347 - Test loss: 2.13368\n",
      "Epoch 4293 - lr: 0.05000 - Train loss: 1.85347 - Test loss: 2.13368\n",
      "Epoch 4294 - lr: 0.05000 - Train loss: 1.85347 - Test loss: 2.13369\n",
      "Epoch 4295 - lr: 0.05000 - Train loss: 1.85347 - Test loss: 2.13369\n",
      "Epoch 4296 - lr: 0.05000 - Train loss: 1.85347 - Test loss: 2.13370\n",
      "Epoch 4297 - lr: 0.05000 - Train loss: 1.85347 - Test loss: 2.13370\n",
      "Epoch 4298 - lr: 0.05000 - Train loss: 1.85347 - Test loss: 2.13371\n",
      "Epoch 4299 - lr: 0.05000 - Train loss: 1.85346 - Test loss: 2.13371\n",
      "Epoch 4300 - lr: 0.05000 - Train loss: 1.85346 - Test loss: 2.13372\n",
      "Epoch 4301 - lr: 0.05000 - Train loss: 1.85346 - Test loss: 2.13372\n",
      "Epoch 4302 - lr: 0.05000 - Train loss: 1.85346 - Test loss: 2.13372\n",
      "Epoch 4303 - lr: 0.05000 - Train loss: 1.85346 - Test loss: 2.13373\n",
      "Epoch 4304 - lr: 0.05000 - Train loss: 1.85346 - Test loss: 2.13373\n",
      "Epoch 4305 - lr: 0.05000 - Train loss: 1.85346 - Test loss: 2.13374\n",
      "Epoch 4306 - lr: 0.05000 - Train loss: 1.85345 - Test loss: 2.13374\n",
      "Epoch 4307 - lr: 0.05000 - Train loss: 1.85345 - Test loss: 2.13375\n",
      "Epoch 4308 - lr: 0.05000 - Train loss: 1.85345 - Test loss: 2.13375\n",
      "Epoch 4309 - lr: 0.05000 - Train loss: 1.85345 - Test loss: 2.13376\n",
      "Epoch 4310 - lr: 0.05000 - Train loss: 1.85345 - Test loss: 2.13376\n",
      "Epoch 4311 - lr: 0.05000 - Train loss: 1.85345 - Test loss: 2.13377\n",
      "Epoch 4312 - lr: 0.05000 - Train loss: 1.85345 - Test loss: 2.13377\n",
      "Epoch 4313 - lr: 0.05000 - Train loss: 1.85345 - Test loss: 2.13377\n",
      "Epoch 4314 - lr: 0.05000 - Train loss: 1.85344 - Test loss: 2.13378\n",
      "Epoch 4315 - lr: 0.05000 - Train loss: 1.85344 - Test loss: 2.13378\n",
      "Epoch 4316 - lr: 0.05000 - Train loss: 1.85344 - Test loss: 2.13379\n",
      "Epoch 4317 - lr: 0.05000 - Train loss: 1.85344 - Test loss: 2.13379\n",
      "Epoch 4318 - lr: 0.05000 - Train loss: 1.85344 - Test loss: 2.13380\n",
      "Epoch 4319 - lr: 0.05000 - Train loss: 1.85344 - Test loss: 2.13380\n",
      "Epoch 4320 - lr: 0.05000 - Train loss: 1.85344 - Test loss: 2.13381\n",
      "Epoch 4321 - lr: 0.05000 - Train loss: 1.85343 - Test loss: 2.13381\n",
      "Epoch 4322 - lr: 0.05000 - Train loss: 1.85343 - Test loss: 2.13382\n",
      "Epoch 4323 - lr: 0.05000 - Train loss: 1.85343 - Test loss: 2.13382\n",
      "Epoch 4324 - lr: 0.05000 - Train loss: 1.85343 - Test loss: 2.13382\n",
      "Epoch 4325 - lr: 0.05000 - Train loss: 1.85343 - Test loss: 2.13383\n",
      "Epoch 4326 - lr: 0.05000 - Train loss: 1.85343 - Test loss: 2.13383\n",
      "Epoch 4327 - lr: 0.05000 - Train loss: 1.85343 - Test loss: 2.13384\n",
      "Epoch 4328 - lr: 0.05000 - Train loss: 1.85342 - Test loss: 2.13384\n",
      "Epoch 4329 - lr: 0.05000 - Train loss: 1.85342 - Test loss: 2.13385\n",
      "Epoch 4330 - lr: 0.05000 - Train loss: 1.85342 - Test loss: 2.13385\n",
      "Epoch 4331 - lr: 0.05000 - Train loss: 1.85342 - Test loss: 2.13386\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4332 - lr: 0.05000 - Train loss: 1.85342 - Test loss: 2.13386\n",
      "Epoch 4333 - lr: 0.05000 - Train loss: 1.85342 - Test loss: 2.13387\n",
      "Epoch 4334 - lr: 0.05000 - Train loss: 1.85342 - Test loss: 2.13387\n",
      "Epoch 4335 - lr: 0.05000 - Train loss: 1.85341 - Test loss: 2.13387\n",
      "Epoch 4336 - lr: 0.05000 - Train loss: 1.85341 - Test loss: 2.13388\n",
      "Epoch 4337 - lr: 0.05000 - Train loss: 1.85341 - Test loss: 2.13388\n",
      "Epoch 4338 - lr: 0.05000 - Train loss: 1.85341 - Test loss: 2.13389\n",
      "Epoch 4339 - lr: 0.05000 - Train loss: 1.85341 - Test loss: 2.13389\n",
      "Epoch 4340 - lr: 0.05000 - Train loss: 1.85341 - Test loss: 2.13390\n",
      "Epoch 4341 - lr: 0.05000 - Train loss: 1.85341 - Test loss: 2.13390\n",
      "Epoch 4342 - lr: 0.05000 - Train loss: 1.85341 - Test loss: 2.13391\n",
      "Epoch 4343 - lr: 0.05000 - Train loss: 1.85340 - Test loss: 2.13391\n",
      "Epoch 4344 - lr: 0.05000 - Train loss: 1.85340 - Test loss: 2.13392\n",
      "Epoch 4345 - lr: 0.05000 - Train loss: 1.85340 - Test loss: 2.13392\n",
      "Epoch 4346 - lr: 0.05000 - Train loss: 1.85340 - Test loss: 2.13392\n",
      "Epoch 4347 - lr: 0.05000 - Train loss: 1.85340 - Test loss: 2.13393\n",
      "Epoch 4348 - lr: 0.05000 - Train loss: 1.85340 - Test loss: 2.13393\n",
      "Epoch 4349 - lr: 0.05000 - Train loss: 1.85340 - Test loss: 2.13394\n",
      "Epoch 4350 - lr: 0.05000 - Train loss: 1.85339 - Test loss: 2.13394\n",
      "Epoch 4351 - lr: 0.05000 - Train loss: 1.85339 - Test loss: 2.13395\n",
      "Epoch 4352 - lr: 0.05000 - Train loss: 1.85339 - Test loss: 2.13395\n",
      "Epoch 4353 - lr: 0.05000 - Train loss: 1.85339 - Test loss: 2.13396\n",
      "Epoch 4354 - lr: 0.05000 - Train loss: 1.85339 - Test loss: 2.13396\n",
      "Epoch 4355 - lr: 0.05000 - Train loss: 1.85339 - Test loss: 2.13396\n",
      "Epoch 4356 - lr: 0.05000 - Train loss: 1.85339 - Test loss: 2.13397\n",
      "Epoch 4357 - lr: 0.05000 - Train loss: 1.85338 - Test loss: 2.13397\n",
      "Epoch 4358 - lr: 0.05000 - Train loss: 1.85338 - Test loss: 2.13398\n",
      "Epoch 4359 - lr: 0.05000 - Train loss: 1.85338 - Test loss: 2.13398\n",
      "Epoch 4360 - lr: 0.05000 - Train loss: 1.85338 - Test loss: 2.13399\n",
      "Epoch 4361 - lr: 0.05000 - Train loss: 1.85338 - Test loss: 2.13399\n",
      "Epoch 4362 - lr: 0.05000 - Train loss: 1.85338 - Test loss: 2.13400\n",
      "Epoch 4363 - lr: 0.05000 - Train loss: 1.85338 - Test loss: 2.13400\n",
      "Epoch 4364 - lr: 0.05000 - Train loss: 1.85338 - Test loss: 2.13401\n",
      "Epoch 4365 - lr: 0.05000 - Train loss: 1.85337 - Test loss: 2.13401\n",
      "Epoch 4366 - lr: 0.05000 - Train loss: 1.85337 - Test loss: 2.13401\n",
      "Epoch 4367 - lr: 0.05000 - Train loss: 1.85337 - Test loss: 2.13402\n",
      "Epoch 4368 - lr: 0.05000 - Train loss: 1.85337 - Test loss: 2.13402\n",
      "Epoch 4369 - lr: 0.05000 - Train loss: 1.85337 - Test loss: 2.13403\n",
      "Epoch 4370 - lr: 0.05000 - Train loss: 1.85337 - Test loss: 2.13403\n",
      "Epoch 4371 - lr: 0.05000 - Train loss: 1.85337 - Test loss: 2.13404\n",
      "Epoch 4372 - lr: 0.05000 - Train loss: 1.85336 - Test loss: 2.13404\n",
      "Epoch 4373 - lr: 0.05000 - Train loss: 1.85336 - Test loss: 2.13405\n",
      "Epoch 4374 - lr: 0.05000 - Train loss: 1.85336 - Test loss: 2.13405\n",
      "Epoch 4375 - lr: 0.05000 - Train loss: 1.85336 - Test loss: 2.13405\n",
      "Epoch 4376 - lr: 0.05000 - Train loss: 1.85336 - Test loss: 2.13406\n",
      "Epoch 4377 - lr: 0.05000 - Train loss: 1.85336 - Test loss: 2.13406\n",
      "Epoch 4378 - lr: 0.05000 - Train loss: 1.85336 - Test loss: 2.13407\n",
      "Epoch 4379 - lr: 0.05000 - Train loss: 1.85335 - Test loss: 2.13407\n",
      "Epoch 4380 - lr: 0.05000 - Train loss: 1.85335 - Test loss: 2.13408\n",
      "Epoch 4381 - lr: 0.05000 - Train loss: 1.85335 - Test loss: 2.13408\n",
      "Epoch 4382 - lr: 0.05000 - Train loss: 1.85335 - Test loss: 2.13409\n",
      "Epoch 4383 - lr: 0.05000 - Train loss: 1.85335 - Test loss: 2.13409\n",
      "Epoch 4384 - lr: 0.05000 - Train loss: 1.85335 - Test loss: 2.13409\n",
      "Epoch 4385 - lr: 0.05000 - Train loss: 1.85335 - Test loss: 2.13410\n",
      "Epoch 4386 - lr: 0.05000 - Train loss: 1.85335 - Test loss: 2.13410\n",
      "Epoch 4387 - lr: 0.05000 - Train loss: 1.85334 - Test loss: 2.13411\n",
      "Epoch 4388 - lr: 0.05000 - Train loss: 1.85334 - Test loss: 2.13411\n",
      "Epoch 4389 - lr: 0.05000 - Train loss: 1.85334 - Test loss: 2.13412\n",
      "Epoch 4390 - lr: 0.05000 - Train loss: 1.85334 - Test loss: 2.13412\n",
      "Epoch 4391 - lr: 0.05000 - Train loss: 1.85334 - Test loss: 2.13413\n",
      "Epoch 4392 - lr: 0.05000 - Train loss: 1.85334 - Test loss: 2.13413\n",
      "Epoch 4393 - lr: 0.05000 - Train loss: 1.85334 - Test loss: 2.13413\n",
      "Epoch 4394 - lr: 0.05000 - Train loss: 1.85333 - Test loss: 2.13414\n",
      "Epoch 4395 - lr: 0.05000 - Train loss: 1.85333 - Test loss: 2.13414\n",
      "Epoch 4396 - lr: 0.05000 - Train loss: 1.85333 - Test loss: 2.13415\n",
      "Epoch 4397 - lr: 0.05000 - Train loss: 1.85333 - Test loss: 2.13415\n",
      "Epoch 4398 - lr: 0.05000 - Train loss: 1.85333 - Test loss: 2.13416\n",
      "Epoch 4399 - lr: 0.05000 - Train loss: 1.85333 - Test loss: 2.13416\n",
      "Epoch 4400 - lr: 0.05000 - Train loss: 1.85333 - Test loss: 2.13417\n",
      "Epoch 4401 - lr: 0.05000 - Train loss: 1.85333 - Test loss: 2.13417\n",
      "Epoch 4402 - lr: 0.05000 - Train loss: 1.85332 - Test loss: 2.13417\n",
      "Epoch 4403 - lr: 0.05000 - Train loss: 1.85332 - Test loss: 2.13418\n",
      "Epoch 4404 - lr: 0.05000 - Train loss: 1.85332 - Test loss: 2.13418\n",
      "Epoch 4405 - lr: 0.05000 - Train loss: 1.85332 - Test loss: 2.13419\n",
      "Epoch 4406 - lr: 0.05000 - Train loss: 1.85332 - Test loss: 2.13419\n",
      "Epoch 4407 - lr: 0.05000 - Train loss: 1.85332 - Test loss: 2.13420\n",
      "Epoch 4408 - lr: 0.05000 - Train loss: 1.85332 - Test loss: 2.13420\n",
      "Epoch 4409 - lr: 0.05000 - Train loss: 1.85331 - Test loss: 2.13421\n",
      "Epoch 4410 - lr: 0.05000 - Train loss: 1.85331 - Test loss: 2.13421\n",
      "Epoch 4411 - lr: 0.05000 - Train loss: 1.85331 - Test loss: 2.13421\n",
      "Epoch 4412 - lr: 0.05000 - Train loss: 1.85331 - Test loss: 2.13422\n",
      "Epoch 4413 - lr: 0.05000 - Train loss: 1.85331 - Test loss: 2.13422\n",
      "Epoch 4414 - lr: 0.05000 - Train loss: 1.85331 - Test loss: 2.13423\n",
      "Epoch 4415 - lr: 0.05000 - Train loss: 1.85331 - Test loss: 2.13423\n",
      "Epoch 4416 - lr: 0.05000 - Train loss: 1.85330 - Test loss: 2.13424\n",
      "Epoch 4417 - lr: 0.05000 - Train loss: 1.85330 - Test loss: 2.13424\n",
      "Epoch 4418 - lr: 0.05000 - Train loss: 1.85330 - Test loss: 2.13425\n",
      "Epoch 4419 - lr: 0.05000 - Train loss: 1.85330 - Test loss: 2.13425\n",
      "Epoch 4420 - lr: 0.05000 - Train loss: 1.85330 - Test loss: 2.13425\n",
      "Epoch 4421 - lr: 0.05000 - Train loss: 1.85330 - Test loss: 2.13426\n",
      "Epoch 4422 - lr: 0.05000 - Train loss: 1.85330 - Test loss: 2.13426\n",
      "Epoch 4423 - lr: 0.05000 - Train loss: 1.85330 - Test loss: 2.13427\n",
      "Epoch 4424 - lr: 0.05000 - Train loss: 1.85329 - Test loss: 2.13427\n",
      "Epoch 4425 - lr: 0.05000 - Train loss: 1.85329 - Test loss: 2.13428\n",
      "Epoch 4426 - lr: 0.05000 - Train loss: 1.85329 - Test loss: 2.13428\n",
      "Epoch 4427 - lr: 0.05000 - Train loss: 1.85329 - Test loss: 2.13429\n",
      "Epoch 4428 - lr: 0.05000 - Train loss: 1.85329 - Test loss: 2.13429\n",
      "Epoch 4429 - lr: 0.05000 - Train loss: 1.85329 - Test loss: 2.13429\n",
      "Epoch 4430 - lr: 0.05000 - Train loss: 1.85329 - Test loss: 2.13430\n",
      "Epoch 4431 - lr: 0.05000 - Train loss: 1.85328 - Test loss: 2.13430\n",
      "Epoch 4432 - lr: 0.05000 - Train loss: 1.85328 - Test loss: 2.13431\n",
      "Epoch 4433 - lr: 0.05000 - Train loss: 1.85328 - Test loss: 2.13431\n",
      "Epoch 4434 - lr: 0.05000 - Train loss: 1.85328 - Test loss: 2.13432\n",
      "Epoch 4435 - lr: 0.05000 - Train loss: 1.85328 - Test loss: 2.13432\n",
      "Epoch 4436 - lr: 0.05000 - Train loss: 1.85328 - Test loss: 2.13433\n",
      "Epoch 4437 - lr: 0.05000 - Train loss: 1.85328 - Test loss: 2.13433\n",
      "Epoch 4438 - lr: 0.05000 - Train loss: 1.85328 - Test loss: 2.13433\n",
      "Epoch 4439 - lr: 0.05000 - Train loss: 1.85327 - Test loss: 2.13434\n",
      "Epoch 4440 - lr: 0.05000 - Train loss: 1.85327 - Test loss: 2.13434\n",
      "Epoch 4441 - lr: 0.05000 - Train loss: 1.85327 - Test loss: 2.13435\n",
      "Epoch 4442 - lr: 0.05000 - Train loss: 1.85327 - Test loss: 2.13435\n",
      "Epoch 4443 - lr: 0.05000 - Train loss: 1.85327 - Test loss: 2.13436\n",
      "Epoch 4444 - lr: 0.05000 - Train loss: 1.85327 - Test loss: 2.13436\n",
      "Epoch 4445 - lr: 0.05000 - Train loss: 1.85327 - Test loss: 2.13436\n",
      "Epoch 4446 - lr: 0.05000 - Train loss: 1.85326 - Test loss: 2.13437\n",
      "Epoch 4447 - lr: 0.05000 - Train loss: 1.85326 - Test loss: 2.13437\n",
      "Epoch 4448 - lr: 0.05000 - Train loss: 1.85326 - Test loss: 2.13438\n",
      "Epoch 4449 - lr: 0.05000 - Train loss: 1.85326 - Test loss: 2.13438\n",
      "Epoch 4450 - lr: 0.05000 - Train loss: 1.85326 - Test loss: 2.13439\n",
      "Epoch 4451 - lr: 0.05000 - Train loss: 1.85326 - Test loss: 2.13439\n",
      "Epoch 4452 - lr: 0.05000 - Train loss: 1.85326 - Test loss: 2.13440\n",
      "Epoch 4453 - lr: 0.05000 - Train loss: 1.85326 - Test loss: 2.13440\n",
      "Epoch 4454 - lr: 0.05000 - Train loss: 1.85325 - Test loss: 2.13440\n",
      "Epoch 4455 - lr: 0.05000 - Train loss: 1.85325 - Test loss: 2.13441\n",
      "Epoch 4456 - lr: 0.05000 - Train loss: 1.85325 - Test loss: 2.13441\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4457 - lr: 0.05000 - Train loss: 1.85325 - Test loss: 2.13442\n",
      "Epoch 4458 - lr: 0.05000 - Train loss: 1.85325 - Test loss: 2.13442\n",
      "Epoch 4459 - lr: 0.05000 - Train loss: 1.85325 - Test loss: 2.13443\n",
      "Epoch 4460 - lr: 0.05000 - Train loss: 1.85325 - Test loss: 2.13443\n",
      "Epoch 4461 - lr: 0.05000 - Train loss: 1.85324 - Test loss: 2.13444\n",
      "Epoch 4462 - lr: 0.05000 - Train loss: 1.85324 - Test loss: 2.13444\n",
      "Epoch 4463 - lr: 0.05000 - Train loss: 1.85324 - Test loss: 2.13444\n",
      "Epoch 4464 - lr: 0.05000 - Train loss: 1.85324 - Test loss: 2.13445\n",
      "Epoch 4465 - lr: 0.05000 - Train loss: 1.85324 - Test loss: 2.13445\n",
      "Epoch 4466 - lr: 0.05000 - Train loss: 1.85324 - Test loss: 2.13446\n",
      "Epoch 4467 - lr: 0.05000 - Train loss: 1.85324 - Test loss: 2.13446\n",
      "Epoch 4468 - lr: 0.05000 - Train loss: 1.85324 - Test loss: 2.13447\n",
      "Epoch 4469 - lr: 0.05000 - Train loss: 1.85323 - Test loss: 2.13447\n",
      "Epoch 4470 - lr: 0.05000 - Train loss: 1.85323 - Test loss: 2.13447\n",
      "Epoch 4471 - lr: 0.05000 - Train loss: 1.85323 - Test loss: 2.13448\n",
      "Epoch 4472 - lr: 0.05000 - Train loss: 1.85323 - Test loss: 2.13448\n",
      "Epoch 4473 - lr: 0.05000 - Train loss: 1.85323 - Test loss: 2.13449\n",
      "Epoch 4474 - lr: 0.05000 - Train loss: 1.85323 - Test loss: 2.13449\n",
      "Epoch 4475 - lr: 0.05000 - Train loss: 1.85323 - Test loss: 2.13450\n",
      "Epoch 4476 - lr: 0.05000 - Train loss: 1.85323 - Test loss: 2.13450\n",
      "Epoch 4477 - lr: 0.05000 - Train loss: 1.85322 - Test loss: 2.13450\n",
      "Epoch 4478 - lr: 0.05000 - Train loss: 1.85322 - Test loss: 2.13451\n",
      "Epoch 4479 - lr: 0.05000 - Train loss: 1.85322 - Test loss: 2.13451\n",
      "Epoch 4480 - lr: 0.05000 - Train loss: 1.85322 - Test loss: 2.13452\n",
      "Epoch 4481 - lr: 0.05000 - Train loss: 1.85322 - Test loss: 2.13452\n",
      "Epoch 4482 - lr: 0.05000 - Train loss: 1.85322 - Test loss: 2.13453\n",
      "Epoch 4483 - lr: 0.05000 - Train loss: 1.85322 - Test loss: 2.13453\n",
      "Epoch 4484 - lr: 0.05000 - Train loss: 1.85321 - Test loss: 2.13454\n",
      "Epoch 4485 - lr: 0.05000 - Train loss: 1.85321 - Test loss: 2.13454\n",
      "Epoch 4486 - lr: 0.05000 - Train loss: 1.85321 - Test loss: 2.13454\n",
      "Epoch 4487 - lr: 0.05000 - Train loss: 1.85321 - Test loss: 2.13455\n",
      "Epoch 4488 - lr: 0.05000 - Train loss: 1.85321 - Test loss: 2.13455\n",
      "Epoch 4489 - lr: 0.05000 - Train loss: 1.85321 - Test loss: 2.13456\n",
      "Epoch 4490 - lr: 0.05000 - Train loss: 1.85321 - Test loss: 2.13456\n",
      "Epoch 4491 - lr: 0.05000 - Train loss: 1.85321 - Test loss: 2.13457\n",
      "Epoch 4492 - lr: 0.05000 - Train loss: 1.85320 - Test loss: 2.13457\n",
      "Epoch 4493 - lr: 0.05000 - Train loss: 1.85320 - Test loss: 2.13457\n",
      "Epoch 4494 - lr: 0.05000 - Train loss: 1.85320 - Test loss: 2.13458\n",
      "Epoch 4495 - lr: 0.05000 - Train loss: 1.85320 - Test loss: 2.13458\n",
      "Epoch 4496 - lr: 0.05000 - Train loss: 1.85320 - Test loss: 2.13459\n",
      "Epoch 4497 - lr: 0.05000 - Train loss: 1.85320 - Test loss: 2.13459\n",
      "Epoch 4498 - lr: 0.05000 - Train loss: 1.85320 - Test loss: 2.13460\n",
      "Epoch 4499 - lr: 0.05000 - Train loss: 1.85319 - Test loss: 2.13460\n",
      "Epoch 4500 - lr: 0.05000 - Train loss: 1.85319 - Test loss: 2.13460\n",
      "Epoch 4501 - lr: 0.05000 - Train loss: 1.85319 - Test loss: 2.13461\n",
      "Epoch 4502 - lr: 0.05000 - Train loss: 1.85319 - Test loss: 2.13461\n",
      "Epoch 4503 - lr: 0.05000 - Train loss: 1.85319 - Test loss: 2.13462\n",
      "Epoch 4504 - lr: 0.05000 - Train loss: 1.85319 - Test loss: 2.13462\n",
      "Epoch 4505 - lr: 0.05000 - Train loss: 1.85319 - Test loss: 2.13463\n",
      "Epoch 4506 - lr: 0.05000 - Train loss: 1.85319 - Test loss: 2.13463\n",
      "Epoch 4507 - lr: 0.05000 - Train loss: 1.85318 - Test loss: 2.13464\n",
      "Epoch 4508 - lr: 0.05000 - Train loss: 1.85318 - Test loss: 2.13464\n",
      "Epoch 4509 - lr: 0.05000 - Train loss: 1.85318 - Test loss: 2.13464\n",
      "Epoch 4510 - lr: 0.05000 - Train loss: 1.85318 - Test loss: 2.13465\n",
      "Epoch 4511 - lr: 0.05000 - Train loss: 1.85318 - Test loss: 2.13465\n",
      "Epoch 4512 - lr: 0.05000 - Train loss: 1.85318 - Test loss: 2.13466\n",
      "Epoch 4513 - lr: 0.05000 - Train loss: 1.85318 - Test loss: 2.13466\n",
      "Epoch 4514 - lr: 0.05000 - Train loss: 1.85318 - Test loss: 2.13467\n",
      "Epoch 4515 - lr: 0.05000 - Train loss: 1.85317 - Test loss: 2.13467\n",
      "Epoch 4516 - lr: 0.05000 - Train loss: 1.85317 - Test loss: 2.13467\n",
      "Epoch 4517 - lr: 0.05000 - Train loss: 1.85317 - Test loss: 2.13468\n",
      "Epoch 4518 - lr: 0.05000 - Train loss: 1.85317 - Test loss: 2.13468\n",
      "Epoch 4519 - lr: 0.05000 - Train loss: 1.85317 - Test loss: 2.13469\n",
      "Epoch 4520 - lr: 0.05000 - Train loss: 1.85317 - Test loss: 2.13469\n",
      "Epoch 4521 - lr: 0.05000 - Train loss: 1.85317 - Test loss: 2.13470\n",
      "Epoch 4522 - lr: 0.05000 - Train loss: 1.85316 - Test loss: 2.13470\n",
      "Epoch 4523 - lr: 0.05000 - Train loss: 1.85316 - Test loss: 2.13470\n",
      "Epoch 4524 - lr: 0.05000 - Train loss: 1.85316 - Test loss: 2.13471\n",
      "Epoch 4525 - lr: 0.05000 - Train loss: 1.85316 - Test loss: 2.13471\n",
      "Epoch 4526 - lr: 0.05000 - Train loss: 1.85316 - Test loss: 2.13472\n",
      "Epoch 4527 - lr: 0.05000 - Train loss: 1.85316 - Test loss: 2.13472\n",
      "Epoch 4528 - lr: 0.05000 - Train loss: 1.85316 - Test loss: 2.13473\n",
      "Epoch 4529 - lr: 0.05000 - Train loss: 1.85316 - Test loss: 2.13473\n",
      "Epoch 4530 - lr: 0.05000 - Train loss: 1.85315 - Test loss: 2.13473\n",
      "Epoch 4531 - lr: 0.05000 - Train loss: 1.85315 - Test loss: 2.13474\n",
      "Epoch 4532 - lr: 0.05000 - Train loss: 1.85315 - Test loss: 2.13474\n",
      "Epoch 4533 - lr: 0.05000 - Train loss: 1.85315 - Test loss: 2.13475\n",
      "Epoch 4534 - lr: 0.05000 - Train loss: 1.85315 - Test loss: 2.13475\n",
      "Epoch 4535 - lr: 0.05000 - Train loss: 1.85315 - Test loss: 2.13476\n",
      "Epoch 4536 - lr: 0.05000 - Train loss: 1.85315 - Test loss: 2.13476\n",
      "Epoch 4537 - lr: 0.05000 - Train loss: 1.85315 - Test loss: 2.13476\n",
      "Epoch 4538 - lr: 0.05000 - Train loss: 1.85314 - Test loss: 2.13477\n",
      "Epoch 4539 - lr: 0.05000 - Train loss: 1.85314 - Test loss: 2.13477\n",
      "Epoch 4540 - lr: 0.05000 - Train loss: 1.85314 - Test loss: 2.13478\n",
      "Epoch 4541 - lr: 0.05000 - Train loss: 1.85314 - Test loss: 2.13478\n",
      "Epoch 4542 - lr: 0.05000 - Train loss: 1.85314 - Test loss: 2.13479\n",
      "Epoch 4543 - lr: 0.05000 - Train loss: 1.85314 - Test loss: 2.13479\n",
      "Epoch 4544 - lr: 0.05000 - Train loss: 1.85314 - Test loss: 2.13479\n",
      "Epoch 4545 - lr: 0.05000 - Train loss: 1.85313 - Test loss: 2.13480\n",
      "Epoch 4546 - lr: 0.05000 - Train loss: 1.85313 - Test loss: 2.13480\n",
      "Epoch 4547 - lr: 0.05000 - Train loss: 1.85313 - Test loss: 2.13481\n",
      "Epoch 4548 - lr: 0.05000 - Train loss: 1.85313 - Test loss: 2.13481\n",
      "Epoch 4549 - lr: 0.05000 - Train loss: 1.85313 - Test loss: 2.13482\n",
      "Epoch 4550 - lr: 0.05000 - Train loss: 1.85313 - Test loss: 2.13482\n",
      "Epoch 4551 - lr: 0.05000 - Train loss: 1.85313 - Test loss: 2.13482\n",
      "Epoch 4552 - lr: 0.05000 - Train loss: 1.85313 - Test loss: 2.13483\n",
      "Epoch 4553 - lr: 0.05000 - Train loss: 1.85312 - Test loss: 2.13483\n",
      "Epoch 4554 - lr: 0.05000 - Train loss: 1.85312 - Test loss: 2.13484\n",
      "Epoch 4555 - lr: 0.05000 - Train loss: 1.85312 - Test loss: 2.13484\n",
      "Epoch 4556 - lr: 0.05000 - Train loss: 1.85312 - Test loss: 2.13485\n",
      "Epoch 4557 - lr: 0.05000 - Train loss: 1.85312 - Test loss: 2.13485\n",
      "Epoch 4558 - lr: 0.05000 - Train loss: 1.85312 - Test loss: 2.13485\n",
      "Epoch 4559 - lr: 0.05000 - Train loss: 1.85312 - Test loss: 2.13486\n",
      "Epoch 4560 - lr: 0.05000 - Train loss: 1.85312 - Test loss: 2.13486\n",
      "Epoch 4561 - lr: 0.05000 - Train loss: 1.85311 - Test loss: 2.13487\n",
      "Epoch 4562 - lr: 0.05000 - Train loss: 1.85311 - Test loss: 2.13487\n",
      "Epoch 4563 - lr: 0.05000 - Train loss: 1.85311 - Test loss: 2.13488\n",
      "Epoch 4564 - lr: 0.05000 - Train loss: 1.85311 - Test loss: 2.13488\n",
      "Epoch 4565 - lr: 0.05000 - Train loss: 1.85311 - Test loss: 2.13488\n",
      "Epoch 4566 - lr: 0.05000 - Train loss: 1.85311 - Test loss: 2.13489\n",
      "Epoch 4567 - lr: 0.05000 - Train loss: 1.85311 - Test loss: 2.13489\n",
      "Epoch 4568 - lr: 0.05000 - Train loss: 1.85310 - Test loss: 2.13490\n",
      "Epoch 4569 - lr: 0.05000 - Train loss: 1.85310 - Test loss: 2.13490\n",
      "Epoch 4570 - lr: 0.05000 - Train loss: 1.85310 - Test loss: 2.13491\n",
      "Epoch 4571 - lr: 0.05000 - Train loss: 1.85310 - Test loss: 2.13491\n",
      "Epoch 4572 - lr: 0.05000 - Train loss: 1.85310 - Test loss: 2.13491\n",
      "Epoch 4573 - lr: 0.05000 - Train loss: 1.85310 - Test loss: 2.13492\n",
      "Epoch 4574 - lr: 0.05000 - Train loss: 1.85310 - Test loss: 2.13492\n",
      "Epoch 4575 - lr: 0.05000 - Train loss: 1.85310 - Test loss: 2.13493\n",
      "Epoch 4576 - lr: 0.05000 - Train loss: 1.85309 - Test loss: 2.13493\n",
      "Epoch 4577 - lr: 0.05000 - Train loss: 1.85309 - Test loss: 2.13494\n",
      "Epoch 4578 - lr: 0.05000 - Train loss: 1.85309 - Test loss: 2.13494\n",
      "Epoch 4579 - lr: 0.05000 - Train loss: 1.85309 - Test loss: 2.13494\n",
      "Epoch 4580 - lr: 0.05000 - Train loss: 1.85309 - Test loss: 2.13495\n",
      "Epoch 4581 - lr: 0.05000 - Train loss: 1.85309 - Test loss: 2.13495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4582 - lr: 0.05000 - Train loss: 1.85309 - Test loss: 2.13496\n",
      "Epoch 4583 - lr: 0.05000 - Train loss: 1.85309 - Test loss: 2.13496\n",
      "Epoch 4584 - lr: 0.05000 - Train loss: 1.85308 - Test loss: 2.13497\n",
      "Epoch 4585 - lr: 0.05000 - Train loss: 1.85308 - Test loss: 2.13497\n",
      "Epoch 4586 - lr: 0.05000 - Train loss: 1.85308 - Test loss: 2.13497\n",
      "Epoch 4587 - lr: 0.05000 - Train loss: 1.85308 - Test loss: 2.13498\n",
      "Epoch 4588 - lr: 0.05000 - Train loss: 1.85308 - Test loss: 2.13498\n",
      "Epoch 4589 - lr: 0.05000 - Train loss: 1.85308 - Test loss: 2.13499\n",
      "Epoch 4590 - lr: 0.05000 - Train loss: 1.85308 - Test loss: 2.13499\n",
      "Epoch 4591 - lr: 0.05000 - Train loss: 1.85308 - Test loss: 2.13500\n",
      "Epoch 4592 - lr: 0.05000 - Train loss: 1.85307 - Test loss: 2.13500\n",
      "Epoch 4593 - lr: 0.05000 - Train loss: 1.85307 - Test loss: 2.13500\n",
      "Epoch 4594 - lr: 0.05000 - Train loss: 1.85307 - Test loss: 2.13501\n",
      "Epoch 4595 - lr: 0.05000 - Train loss: 1.85307 - Test loss: 2.13501\n",
      "Epoch 4596 - lr: 0.05000 - Train loss: 1.85307 - Test loss: 2.13502\n",
      "Epoch 4597 - lr: 0.05000 - Train loss: 1.85307 - Test loss: 2.13502\n",
      "Epoch 4598 - lr: 0.05000 - Train loss: 1.85307 - Test loss: 2.13502\n",
      "Epoch 4599 - lr: 0.05000 - Train loss: 1.85306 - Test loss: 2.13503\n",
      "Epoch 4600 - lr: 0.05000 - Train loss: 1.85306 - Test loss: 2.13503\n",
      "Epoch 4601 - lr: 0.05000 - Train loss: 1.85306 - Test loss: 2.13504\n",
      "Epoch 4602 - lr: 0.05000 - Train loss: 1.85306 - Test loss: 2.13504\n",
      "Epoch 4603 - lr: 0.05000 - Train loss: 1.85306 - Test loss: 2.13505\n",
      "Epoch 4604 - lr: 0.05000 - Train loss: 1.85306 - Test loss: 2.13505\n",
      "Epoch 4605 - lr: 0.05000 - Train loss: 1.85306 - Test loss: 2.13505\n",
      "Epoch 4606 - lr: 0.05000 - Train loss: 1.85306 - Test loss: 2.13506\n",
      "Epoch 4607 - lr: 0.05000 - Train loss: 1.85305 - Test loss: 2.13506\n",
      "Epoch 4608 - lr: 0.05000 - Train loss: 1.85305 - Test loss: 2.13507\n",
      "Epoch 4609 - lr: 0.05000 - Train loss: 1.85305 - Test loss: 2.13507\n",
      "Epoch 4610 - lr: 0.05000 - Train loss: 1.85305 - Test loss: 2.13508\n",
      "Epoch 4611 - lr: 0.05000 - Train loss: 1.85305 - Test loss: 2.13508\n",
      "Epoch 4612 - lr: 0.05000 - Train loss: 1.85305 - Test loss: 2.13508\n",
      "Epoch 4613 - lr: 0.05000 - Train loss: 1.85305 - Test loss: 2.13509\n",
      "Epoch 4614 - lr: 0.05000 - Train loss: 1.85305 - Test loss: 2.13509\n",
      "Epoch 4615 - lr: 0.05000 - Train loss: 1.85304 - Test loss: 2.13510\n",
      "Epoch 4616 - lr: 0.05000 - Train loss: 1.85304 - Test loss: 2.13510\n",
      "Epoch 4617 - lr: 0.05000 - Train loss: 1.85304 - Test loss: 2.13510\n",
      "Epoch 4618 - lr: 0.05000 - Train loss: 1.85304 - Test loss: 2.13511\n",
      "Epoch 4619 - lr: 0.05000 - Train loss: 1.85304 - Test loss: 2.13511\n",
      "Epoch 4620 - lr: 0.05000 - Train loss: 1.85304 - Test loss: 2.13512\n",
      "Epoch 4621 - lr: 0.05000 - Train loss: 1.85304 - Test loss: 2.13512\n",
      "Epoch 4622 - lr: 0.05000 - Train loss: 1.85304 - Test loss: 2.13513\n",
      "Epoch 4623 - lr: 0.05000 - Train loss: 1.85303 - Test loss: 2.13513\n",
      "Epoch 4624 - lr: 0.05000 - Train loss: 1.85303 - Test loss: 2.13513\n",
      "Epoch 4625 - lr: 0.05000 - Train loss: 1.85303 - Test loss: 2.13514\n",
      "Epoch 4626 - lr: 0.05000 - Train loss: 1.85303 - Test loss: 2.13514\n",
      "Epoch 4627 - lr: 0.05000 - Train loss: 1.85303 - Test loss: 2.13515\n",
      "Epoch 4628 - lr: 0.05000 - Train loss: 1.85303 - Test loss: 2.13515\n",
      "Epoch 4629 - lr: 0.05000 - Train loss: 1.85303 - Test loss: 2.13516\n",
      "Epoch 4630 - lr: 0.05000 - Train loss: 1.85303 - Test loss: 2.13516\n",
      "Epoch 4631 - lr: 0.05000 - Train loss: 1.85302 - Test loss: 2.13516\n",
      "Epoch 4632 - lr: 0.05000 - Train loss: 1.85302 - Test loss: 2.13517\n",
      "Epoch 4633 - lr: 0.05000 - Train loss: 1.85302 - Test loss: 2.13517\n",
      "Epoch 4634 - lr: 0.05000 - Train loss: 1.85302 - Test loss: 2.13518\n",
      "Epoch 4635 - lr: 0.05000 - Train loss: 1.85302 - Test loss: 2.13518\n",
      "Epoch 4636 - lr: 0.05000 - Train loss: 1.85302 - Test loss: 2.13518\n",
      "Epoch 4637 - lr: 0.05000 - Train loss: 1.85302 - Test loss: 2.13519\n",
      "Epoch 4638 - lr: 0.05000 - Train loss: 1.85302 - Test loss: 2.13519\n",
      "Epoch 4639 - lr: 0.05000 - Train loss: 1.85301 - Test loss: 2.13520\n",
      "Epoch 4640 - lr: 0.05000 - Train loss: 1.85301 - Test loss: 2.13520\n",
      "Epoch 4641 - lr: 0.05000 - Train loss: 1.85301 - Test loss: 2.13521\n",
      "Epoch 4642 - lr: 0.05000 - Train loss: 1.85301 - Test loss: 2.13521\n",
      "Epoch 4643 - lr: 0.05000 - Train loss: 1.85301 - Test loss: 2.13521\n",
      "Epoch 4644 - lr: 0.05000 - Train loss: 1.85301 - Test loss: 2.13522\n",
      "Epoch 4645 - lr: 0.05000 - Train loss: 1.85301 - Test loss: 2.13522\n",
      "Epoch 4646 - lr: 0.05000 - Train loss: 1.85300 - Test loss: 2.13523\n",
      "Epoch 4647 - lr: 0.05000 - Train loss: 1.85300 - Test loss: 2.13523\n",
      "Epoch 4648 - lr: 0.05000 - Train loss: 1.85300 - Test loss: 2.13524\n",
      "Epoch 4649 - lr: 0.05000 - Train loss: 1.85300 - Test loss: 2.13524\n",
      "Epoch 4650 - lr: 0.05000 - Train loss: 1.85300 - Test loss: 2.13524\n",
      "Epoch 4651 - lr: 0.05000 - Train loss: 1.85300 - Test loss: 2.13525\n",
      "Epoch 4652 - lr: 0.05000 - Train loss: 1.85300 - Test loss: 2.13525\n",
      "Epoch 4653 - lr: 0.05000 - Train loss: 1.85300 - Test loss: 2.13526\n",
      "Epoch 4654 - lr: 0.05000 - Train loss: 1.85299 - Test loss: 2.13526\n",
      "Epoch 4655 - lr: 0.05000 - Train loss: 1.85299 - Test loss: 2.13526\n",
      "Epoch 4656 - lr: 0.05000 - Train loss: 1.85299 - Test loss: 2.13527\n",
      "Epoch 4657 - lr: 0.05000 - Train loss: 1.85299 - Test loss: 2.13527\n",
      "Epoch 4658 - lr: 0.05000 - Train loss: 1.85299 - Test loss: 2.13528\n",
      "Epoch 4659 - lr: 0.05000 - Train loss: 1.85299 - Test loss: 2.13528\n",
      "Epoch 4660 - lr: 0.05000 - Train loss: 1.85299 - Test loss: 2.13529\n",
      "Epoch 4661 - lr: 0.05000 - Train loss: 1.85299 - Test loss: 2.13529\n",
      "Epoch 4662 - lr: 0.05000 - Train loss: 1.85298 - Test loss: 2.13529\n",
      "Epoch 4663 - lr: 0.05000 - Train loss: 1.85298 - Test loss: 2.13530\n",
      "Epoch 4664 - lr: 0.05000 - Train loss: 1.85298 - Test loss: 2.13530\n",
      "Epoch 4665 - lr: 0.05000 - Train loss: 1.85298 - Test loss: 2.13531\n",
      "Epoch 4666 - lr: 0.05000 - Train loss: 1.85298 - Test loss: 2.13531\n",
      "Epoch 4667 - lr: 0.05000 - Train loss: 1.85298 - Test loss: 2.13531\n",
      "Epoch 4668 - lr: 0.05000 - Train loss: 1.85298 - Test loss: 2.13532\n",
      "Epoch 4669 - lr: 0.05000 - Train loss: 1.85298 - Test loss: 2.13532\n",
      "Epoch 4670 - lr: 0.05000 - Train loss: 1.85297 - Test loss: 2.13533\n",
      "Epoch 4671 - lr: 0.05000 - Train loss: 1.85297 - Test loss: 2.13533\n",
      "Epoch 4672 - lr: 0.05000 - Train loss: 1.85297 - Test loss: 2.13534\n",
      "Epoch 4673 - lr: 0.05000 - Train loss: 1.85297 - Test loss: 2.13534\n",
      "Epoch 4674 - lr: 0.05000 - Train loss: 1.85297 - Test loss: 2.13534\n",
      "Epoch 4675 - lr: 0.05000 - Train loss: 1.85297 - Test loss: 2.13535\n",
      "Epoch 4676 - lr: 0.05000 - Train loss: 1.85297 - Test loss: 2.13535\n",
      "Epoch 4677 - lr: 0.05000 - Train loss: 1.85297 - Test loss: 2.13536\n",
      "Epoch 4678 - lr: 0.05000 - Train loss: 1.85296 - Test loss: 2.13536\n",
      "Epoch 4679 - lr: 0.05000 - Train loss: 1.85296 - Test loss: 2.13536\n",
      "Epoch 4680 - lr: 0.05000 - Train loss: 1.85296 - Test loss: 2.13537\n",
      "Epoch 4681 - lr: 0.05000 - Train loss: 1.85296 - Test loss: 2.13537\n",
      "Epoch 4682 - lr: 0.05000 - Train loss: 1.85296 - Test loss: 2.13538\n",
      "Epoch 4683 - lr: 0.05000 - Train loss: 1.85296 - Test loss: 2.13538\n",
      "Epoch 4684 - lr: 0.05000 - Train loss: 1.85296 - Test loss: 2.13539\n",
      "Epoch 4685 - lr: 0.05000 - Train loss: 1.85296 - Test loss: 2.13539\n",
      "Epoch 4686 - lr: 0.05000 - Train loss: 1.85295 - Test loss: 2.13539\n",
      "Epoch 4687 - lr: 0.05000 - Train loss: 1.85295 - Test loss: 2.13540\n",
      "Epoch 4688 - lr: 0.05000 - Train loss: 1.85295 - Test loss: 2.13540\n",
      "Epoch 4689 - lr: 0.05000 - Train loss: 1.85295 - Test loss: 2.13541\n",
      "Epoch 4690 - lr: 0.05000 - Train loss: 1.85295 - Test loss: 2.13541\n",
      "Epoch 4691 - lr: 0.05000 - Train loss: 1.85295 - Test loss: 2.13541\n",
      "Epoch 4692 - lr: 0.05000 - Train loss: 1.85295 - Test loss: 2.13542\n",
      "Epoch 4693 - lr: 0.05000 - Train loss: 1.85295 - Test loss: 2.13542\n",
      "Epoch 4694 - lr: 0.05000 - Train loss: 1.85294 - Test loss: 2.13543\n",
      "Epoch 4695 - lr: 0.05000 - Train loss: 1.85294 - Test loss: 2.13543\n",
      "Epoch 4696 - lr: 0.05000 - Train loss: 1.85294 - Test loss: 2.13543\n",
      "Epoch 4697 - lr: 0.05000 - Train loss: 1.85294 - Test loss: 2.13544\n",
      "Epoch 4698 - lr: 0.05000 - Train loss: 1.85294 - Test loss: 2.13544\n",
      "Epoch 4699 - lr: 0.05000 - Train loss: 1.85294 - Test loss: 2.13545\n",
      "Epoch 4700 - lr: 0.05000 - Train loss: 1.85294 - Test loss: 2.13545\n",
      "Epoch 4701 - lr: 0.05000 - Train loss: 1.85294 - Test loss: 2.13546\n",
      "Epoch 4702 - lr: 0.05000 - Train loss: 1.85293 - Test loss: 2.13546\n",
      "Epoch 4703 - lr: 0.05000 - Train loss: 1.85293 - Test loss: 2.13546\n",
      "Epoch 4704 - lr: 0.05000 - Train loss: 1.85293 - Test loss: 2.13547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4705 - lr: 0.05000 - Train loss: 1.85293 - Test loss: 2.13547\n",
      "Epoch 4706 - lr: 0.05000 - Train loss: 1.85293 - Test loss: 2.13548\n",
      "Epoch 4707 - lr: 0.05000 - Train loss: 1.85293 - Test loss: 2.13548\n",
      "Epoch 4708 - lr: 0.05000 - Train loss: 1.85293 - Test loss: 2.13548\n",
      "Epoch 4709 - lr: 0.05000 - Train loss: 1.85293 - Test loss: 2.13549\n",
      "Epoch 4710 - lr: 0.05000 - Train loss: 1.85292 - Test loss: 2.13549\n",
      "Epoch 4711 - lr: 0.05000 - Train loss: 1.85292 - Test loss: 2.13550\n",
      "Epoch 4712 - lr: 0.05000 - Train loss: 1.85292 - Test loss: 2.13550\n",
      "Epoch 4713 - lr: 0.05000 - Train loss: 1.85292 - Test loss: 2.13550\n",
      "Epoch 4714 - lr: 0.05000 - Train loss: 1.85292 - Test loss: 2.13551\n",
      "Epoch 4715 - lr: 0.05000 - Train loss: 1.85292 - Test loss: 2.13551\n",
      "Epoch 4716 - lr: 0.05000 - Train loss: 1.85292 - Test loss: 2.13552\n",
      "Epoch 4717 - lr: 0.05000 - Train loss: 1.85292 - Test loss: 2.13552\n",
      "Epoch 4718 - lr: 0.05000 - Train loss: 1.85291 - Test loss: 2.13553\n",
      "Epoch 4719 - lr: 0.05000 - Train loss: 1.85291 - Test loss: 2.13553\n",
      "Epoch 4720 - lr: 0.05000 - Train loss: 1.85291 - Test loss: 2.13553\n",
      "Epoch 4721 - lr: 0.05000 - Train loss: 1.85291 - Test loss: 2.13554\n",
      "Epoch 4722 - lr: 0.05000 - Train loss: 1.85291 - Test loss: 2.13554\n",
      "Epoch 4723 - lr: 0.05000 - Train loss: 1.85291 - Test loss: 2.13555\n",
      "Epoch 4724 - lr: 0.05000 - Train loss: 1.85291 - Test loss: 2.13555\n",
      "Epoch 4725 - lr: 0.05000 - Train loss: 1.85291 - Test loss: 2.13555\n",
      "Epoch 4726 - lr: 0.05000 - Train loss: 1.85290 - Test loss: 2.13556\n",
      "Epoch 4727 - lr: 0.05000 - Train loss: 1.85290 - Test loss: 2.13556\n",
      "Epoch 4728 - lr: 0.05000 - Train loss: 1.85290 - Test loss: 2.13557\n",
      "Epoch 4729 - lr: 0.05000 - Train loss: 1.85290 - Test loss: 2.13557\n",
      "Epoch 4730 - lr: 0.05000 - Train loss: 1.85290 - Test loss: 2.13557\n",
      "Epoch 4731 - lr: 0.05000 - Train loss: 1.85290 - Test loss: 2.13558\n",
      "Epoch 4732 - lr: 0.05000 - Train loss: 1.85290 - Test loss: 2.13558\n",
      "Epoch 4733 - lr: 0.05000 - Train loss: 1.85290 - Test loss: 2.13559\n",
      "Epoch 4734 - lr: 0.05000 - Train loss: 1.85289 - Test loss: 2.13559\n",
      "Epoch 4735 - lr: 0.05000 - Train loss: 1.85289 - Test loss: 2.13560\n",
      "Epoch 4736 - lr: 0.05000 - Train loss: 1.85289 - Test loss: 2.13560\n",
      "Epoch 4737 - lr: 0.05000 - Train loss: 1.85289 - Test loss: 2.13560\n",
      "Epoch 4738 - lr: 0.05000 - Train loss: 1.85289 - Test loss: 2.13561\n",
      "Epoch 4739 - lr: 0.05000 - Train loss: 1.85289 - Test loss: 2.13561\n",
      "Epoch 4740 - lr: 0.05000 - Train loss: 1.85289 - Test loss: 2.13562\n",
      "Epoch 4741 - lr: 0.05000 - Train loss: 1.85289 - Test loss: 2.13562\n",
      "Epoch 4742 - lr: 0.05000 - Train loss: 1.85288 - Test loss: 2.13562\n",
      "Epoch 4743 - lr: 0.05000 - Train loss: 1.85288 - Test loss: 2.13563\n",
      "Epoch 4744 - lr: 0.05000 - Train loss: 1.85288 - Test loss: 2.13563\n",
      "Epoch 4745 - lr: 0.05000 - Train loss: 1.85288 - Test loss: 2.13564\n",
      "Epoch 4746 - lr: 0.05000 - Train loss: 1.85288 - Test loss: 2.13564\n",
      "Epoch 4747 - lr: 0.05000 - Train loss: 1.85288 - Test loss: 2.13564\n",
      "Epoch 4748 - lr: 0.05000 - Train loss: 1.85288 - Test loss: 2.13565\n",
      "Epoch 4749 - lr: 0.05000 - Train loss: 1.85288 - Test loss: 2.13565\n",
      "Epoch 4750 - lr: 0.05000 - Train loss: 1.85287 - Test loss: 2.13566\n",
      "Epoch 4751 - lr: 0.05000 - Train loss: 1.85287 - Test loss: 2.13566\n",
      "Epoch 4752 - lr: 0.05000 - Train loss: 1.85287 - Test loss: 2.13566\n",
      "Epoch 4753 - lr: 0.05000 - Train loss: 1.85287 - Test loss: 2.13567\n",
      "Epoch 4754 - lr: 0.05000 - Train loss: 1.85287 - Test loss: 2.13567\n",
      "Epoch 4755 - lr: 0.05000 - Train loss: 1.85287 - Test loss: 2.13568\n",
      "Epoch 4756 - lr: 0.05000 - Train loss: 1.85287 - Test loss: 2.13568\n",
      "Epoch 4757 - lr: 0.05000 - Train loss: 1.85287 - Test loss: 2.13569\n",
      "Epoch 4758 - lr: 0.05000 - Train loss: 1.85286 - Test loss: 2.13569\n",
      "Epoch 4759 - lr: 0.05000 - Train loss: 1.85286 - Test loss: 2.13569\n",
      "Epoch 4760 - lr: 0.05000 - Train loss: 1.85286 - Test loss: 2.13570\n",
      "Epoch 4761 - lr: 0.05000 - Train loss: 1.85286 - Test loss: 2.13570\n",
      "Epoch 4762 - lr: 0.05000 - Train loss: 1.85286 - Test loss: 2.13571\n",
      "Epoch 4763 - lr: 0.05000 - Train loss: 1.85286 - Test loss: 2.13571\n",
      "Epoch 4764 - lr: 0.05000 - Train loss: 1.85286 - Test loss: 2.13571\n",
      "Epoch 4765 - lr: 0.05000 - Train loss: 1.85286 - Test loss: 2.13572\n",
      "Epoch 4766 - lr: 0.05000 - Train loss: 1.85285 - Test loss: 2.13572\n",
      "Epoch 4767 - lr: 0.05000 - Train loss: 1.85285 - Test loss: 2.13573\n",
      "Epoch 4768 - lr: 0.05000 - Train loss: 1.85285 - Test loss: 2.13573\n",
      "Epoch 4769 - lr: 0.05000 - Train loss: 1.85285 - Test loss: 2.13573\n",
      "Epoch 4770 - lr: 0.05000 - Train loss: 1.85285 - Test loss: 2.13574\n",
      "Epoch 4771 - lr: 0.05000 - Train loss: 1.85285 - Test loss: 2.13574\n",
      "Epoch 4772 - lr: 0.05000 - Train loss: 1.85285 - Test loss: 2.13575\n",
      "Epoch 4773 - lr: 0.05000 - Train loss: 1.85285 - Test loss: 2.13575\n",
      "Epoch 4774 - lr: 0.05000 - Train loss: 1.85285 - Test loss: 2.13575\n",
      "Epoch 4775 - lr: 0.05000 - Train loss: 1.85284 - Test loss: 2.13576\n",
      "Epoch 4776 - lr: 0.05000 - Train loss: 1.85284 - Test loss: 2.13576\n",
      "Epoch 4777 - lr: 0.05000 - Train loss: 1.85284 - Test loss: 2.13577\n",
      "Epoch 4778 - lr: 0.05000 - Train loss: 1.85284 - Test loss: 2.13577\n",
      "Epoch 4779 - lr: 0.05000 - Train loss: 1.85284 - Test loss: 2.13577\n",
      "Epoch 4780 - lr: 0.05000 - Train loss: 1.85284 - Test loss: 2.13578\n",
      "Epoch 4781 - lr: 0.05000 - Train loss: 1.85284 - Test loss: 2.13578\n",
      "Epoch 4782 - lr: 0.05000 - Train loss: 1.85284 - Test loss: 2.13579\n",
      "Epoch 4783 - lr: 0.05000 - Train loss: 1.85283 - Test loss: 2.13579\n",
      "Epoch 4784 - lr: 0.05000 - Train loss: 1.85283 - Test loss: 2.13579\n",
      "Epoch 4785 - lr: 0.05000 - Train loss: 1.85283 - Test loss: 2.13580\n",
      "Epoch 4786 - lr: 0.05000 - Train loss: 1.85283 - Test loss: 2.13580\n",
      "Epoch 4787 - lr: 0.05000 - Train loss: 1.85283 - Test loss: 2.13581\n",
      "Epoch 4788 - lr: 0.05000 - Train loss: 1.85283 - Test loss: 2.13581\n",
      "Epoch 4789 - lr: 0.05000 - Train loss: 1.85283 - Test loss: 2.13582\n",
      "Epoch 4790 - lr: 0.05000 - Train loss: 1.85283 - Test loss: 2.13582\n",
      "Epoch 4791 - lr: 0.05000 - Train loss: 1.85282 - Test loss: 2.13582\n",
      "Epoch 4792 - lr: 0.05000 - Train loss: 1.85282 - Test loss: 2.13583\n",
      "Epoch 4793 - lr: 0.05000 - Train loss: 1.85282 - Test loss: 2.13583\n",
      "Epoch 4794 - lr: 0.05000 - Train loss: 1.85282 - Test loss: 2.13584\n",
      "Epoch 4795 - lr: 0.05000 - Train loss: 1.85282 - Test loss: 2.13584\n",
      "Epoch 4796 - lr: 0.05000 - Train loss: 1.85282 - Test loss: 2.13584\n",
      "Epoch 4797 - lr: 0.05000 - Train loss: 1.85282 - Test loss: 2.13585\n",
      "Epoch 4798 - lr: 0.05000 - Train loss: 1.85282 - Test loss: 2.13585\n",
      "Epoch 4799 - lr: 0.05000 - Train loss: 1.85281 - Test loss: 2.13586\n",
      "Epoch 4800 - lr: 0.05000 - Train loss: 1.85281 - Test loss: 2.13586\n",
      "Epoch 4801 - lr: 0.05000 - Train loss: 1.85281 - Test loss: 2.13586\n",
      "Epoch 4802 - lr: 0.05000 - Train loss: 1.85281 - Test loss: 2.13587\n",
      "Epoch 4803 - lr: 0.05000 - Train loss: 1.85281 - Test loss: 2.13587\n",
      "Epoch 4804 - lr: 0.05000 - Train loss: 1.85281 - Test loss: 2.13588\n",
      "Epoch 4805 - lr: 0.05000 - Train loss: 1.85281 - Test loss: 2.13588\n",
      "Epoch 4806 - lr: 0.05000 - Train loss: 1.85281 - Test loss: 2.13588\n",
      "Epoch 4807 - lr: 0.05000 - Train loss: 1.85280 - Test loss: 2.13589\n",
      "Epoch 4808 - lr: 0.05000 - Train loss: 1.85280 - Test loss: 2.13589\n",
      "Epoch 4809 - lr: 0.05000 - Train loss: 1.85280 - Test loss: 2.13590\n",
      "Epoch 4810 - lr: 0.05000 - Train loss: 1.85280 - Test loss: 2.13590\n",
      "Epoch 4811 - lr: 0.05000 - Train loss: 1.85280 - Test loss: 2.13590\n",
      "Epoch 4812 - lr: 0.05000 - Train loss: 1.85280 - Test loss: 2.13591\n",
      "Epoch 4813 - lr: 0.05000 - Train loss: 1.85280 - Test loss: 2.13591\n",
      "Epoch 4814 - lr: 0.05000 - Train loss: 1.85280 - Test loss: 2.13592\n",
      "Epoch 4815 - lr: 0.05000 - Train loss: 1.85280 - Test loss: 2.13592\n",
      "Epoch 4816 - lr: 0.05000 - Train loss: 1.85279 - Test loss: 2.13592\n",
      "Epoch 4817 - lr: 0.05000 - Train loss: 1.85279 - Test loss: 2.13593\n",
      "Epoch 4818 - lr: 0.05000 - Train loss: 1.85279 - Test loss: 2.13593\n",
      "Epoch 4819 - lr: 0.05000 - Train loss: 1.85279 - Test loss: 2.13594\n",
      "Epoch 4820 - lr: 0.05000 - Train loss: 1.85279 - Test loss: 2.13594\n",
      "Epoch 4821 - lr: 0.05000 - Train loss: 1.85279 - Test loss: 2.13594\n",
      "Epoch 4822 - lr: 0.05000 - Train loss: 1.85279 - Test loss: 2.13595\n",
      "Epoch 4823 - lr: 0.05000 - Train loss: 1.85279 - Test loss: 2.13595\n",
      "Epoch 4824 - lr: 0.05000 - Train loss: 1.85278 - Test loss: 2.13596\n",
      "Epoch 4825 - lr: 0.05000 - Train loss: 1.85278 - Test loss: 2.13596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4826 - lr: 0.05000 - Train loss: 1.85278 - Test loss: 2.13596\n",
      "Epoch 4827 - lr: 0.05000 - Train loss: 1.85278 - Test loss: 2.13597\n",
      "Epoch 4828 - lr: 0.05000 - Train loss: 1.85278 - Test loss: 2.13597\n",
      "Epoch 4829 - lr: 0.05000 - Train loss: 1.85278 - Test loss: 2.13598\n",
      "Epoch 4830 - lr: 0.05000 - Train loss: 1.85278 - Test loss: 2.13598\n",
      "Epoch 4831 - lr: 0.05000 - Train loss: 1.85278 - Test loss: 2.13598\n",
      "Epoch 4832 - lr: 0.05000 - Train loss: 1.85277 - Test loss: 2.13599\n",
      "Epoch 4833 - lr: 0.05000 - Train loss: 1.85277 - Test loss: 2.13599\n",
      "Epoch 4834 - lr: 0.05000 - Train loss: 1.85277 - Test loss: 2.13600\n",
      "Epoch 4835 - lr: 0.05000 - Train loss: 1.85277 - Test loss: 2.13600\n",
      "Epoch 4836 - lr: 0.05000 - Train loss: 1.85277 - Test loss: 2.13600\n",
      "Epoch 4837 - lr: 0.05000 - Train loss: 1.85277 - Test loss: 2.13601\n",
      "Epoch 4838 - lr: 0.05000 - Train loss: 1.85277 - Test loss: 2.13601\n",
      "Epoch 4839 - lr: 0.05000 - Train loss: 1.85277 - Test loss: 2.13602\n",
      "Epoch 4840 - lr: 0.05000 - Train loss: 1.85276 - Test loss: 2.13602\n",
      "Epoch 4841 - lr: 0.05000 - Train loss: 1.85276 - Test loss: 2.13602\n",
      "Epoch 4842 - lr: 0.05000 - Train loss: 1.85276 - Test loss: 2.13603\n",
      "Epoch 4843 - lr: 0.05000 - Train loss: 1.85276 - Test loss: 2.13603\n",
      "Epoch 4844 - lr: 0.05000 - Train loss: 1.85276 - Test loss: 2.13604\n",
      "Epoch 4845 - lr: 0.05000 - Train loss: 1.85276 - Test loss: 2.13604\n",
      "Epoch 4846 - lr: 0.05000 - Train loss: 1.85276 - Test loss: 2.13604\n",
      "Epoch 4847 - lr: 0.05000 - Train loss: 1.85276 - Test loss: 2.13605\n",
      "Epoch 4848 - lr: 0.05000 - Train loss: 1.85276 - Test loss: 2.13605\n",
      "Epoch 4849 - lr: 0.05000 - Train loss: 1.85275 - Test loss: 2.13606\n",
      "Epoch 4850 - lr: 0.05000 - Train loss: 1.85275 - Test loss: 2.13606\n",
      "Epoch 4851 - lr: 0.05000 - Train loss: 1.85275 - Test loss: 2.13606\n",
      "Epoch 4852 - lr: 0.05000 - Train loss: 1.85275 - Test loss: 2.13607\n",
      "Epoch 4853 - lr: 0.05000 - Train loss: 1.85275 - Test loss: 2.13607\n",
      "Epoch 4854 - lr: 0.05000 - Train loss: 1.85275 - Test loss: 2.13608\n",
      "Epoch 4855 - lr: 0.05000 - Train loss: 1.85275 - Test loss: 2.13608\n",
      "Epoch 4856 - lr: 0.05000 - Train loss: 1.85275 - Test loss: 2.13608\n",
      "Epoch 4857 - lr: 0.05000 - Train loss: 1.85274 - Test loss: 2.13609\n",
      "Epoch 4858 - lr: 0.05000 - Train loss: 1.85274 - Test loss: 2.13609\n",
      "Epoch 4859 - lr: 0.05000 - Train loss: 1.85274 - Test loss: 2.13610\n",
      "Epoch 4860 - lr: 0.05000 - Train loss: 1.85274 - Test loss: 2.13610\n",
      "Epoch 4861 - lr: 0.05000 - Train loss: 1.85274 - Test loss: 2.13610\n",
      "Epoch 4862 - lr: 0.05000 - Train loss: 1.85274 - Test loss: 2.13611\n",
      "Epoch 4863 - lr: 0.05000 - Train loss: 1.85274 - Test loss: 2.13611\n",
      "Epoch 4864 - lr: 0.05000 - Train loss: 1.85274 - Test loss: 2.13612\n",
      "Epoch 4865 - lr: 0.05000 - Train loss: 1.85273 - Test loss: 2.13612\n",
      "Epoch 4866 - lr: 0.05000 - Train loss: 1.85273 - Test loss: 2.13612\n",
      "Epoch 4867 - lr: 0.05000 - Train loss: 1.85273 - Test loss: 2.13613\n",
      "Epoch 4868 - lr: 0.05000 - Train loss: 1.85273 - Test loss: 2.13613\n",
      "Epoch 4869 - lr: 0.05000 - Train loss: 1.85273 - Test loss: 2.13614\n",
      "Epoch 4870 - lr: 0.05000 - Train loss: 1.85273 - Test loss: 2.13614\n",
      "Epoch 4871 - lr: 0.05000 - Train loss: 1.85273 - Test loss: 2.13614\n",
      "Epoch 4872 - lr: 0.05000 - Train loss: 1.85273 - Test loss: 2.13615\n",
      "Epoch 4873 - lr: 0.05000 - Train loss: 1.85272 - Test loss: 2.13615\n",
      "Epoch 4874 - lr: 0.05000 - Train loss: 1.85272 - Test loss: 2.13616\n",
      "Epoch 4875 - lr: 0.05000 - Train loss: 1.85272 - Test loss: 2.13616\n",
      "Epoch 4876 - lr: 0.05000 - Train loss: 1.85272 - Test loss: 2.13616\n",
      "Epoch 4877 - lr: 0.05000 - Train loss: 1.85272 - Test loss: 2.13617\n",
      "Epoch 4878 - lr: 0.05000 - Train loss: 1.85272 - Test loss: 2.13617\n",
      "Epoch 4879 - lr: 0.05000 - Train loss: 1.85272 - Test loss: 2.13618\n",
      "Epoch 4880 - lr: 0.05000 - Train loss: 1.85272 - Test loss: 2.13618\n",
      "Epoch 4881 - lr: 0.05000 - Train loss: 1.85272 - Test loss: 2.13618\n",
      "Epoch 4882 - lr: 0.05000 - Train loss: 1.85271 - Test loss: 2.13619\n",
      "Epoch 4883 - lr: 0.05000 - Train loss: 1.85271 - Test loss: 2.13619\n",
      "Epoch 4884 - lr: 0.05000 - Train loss: 1.85271 - Test loss: 2.13620\n",
      "Epoch 4885 - lr: 0.05000 - Train loss: 1.85271 - Test loss: 2.13620\n",
      "Epoch 4886 - lr: 0.05000 - Train loss: 1.85271 - Test loss: 2.13620\n",
      "Epoch 4887 - lr: 0.05000 - Train loss: 1.85271 - Test loss: 2.13621\n",
      "Epoch 4888 - lr: 0.05000 - Train loss: 1.85271 - Test loss: 2.13621\n",
      "Epoch 4889 - lr: 0.05000 - Train loss: 1.85271 - Test loss: 2.13622\n",
      "Epoch 4890 - lr: 0.05000 - Train loss: 1.85270 - Test loss: 2.13622\n",
      "Epoch 4891 - lr: 0.05000 - Train loss: 1.85270 - Test loss: 2.13622\n",
      "Epoch 4892 - lr: 0.05000 - Train loss: 1.85270 - Test loss: 2.13623\n",
      "Epoch 4893 - lr: 0.05000 - Train loss: 1.85270 - Test loss: 2.13623\n",
      "Epoch 4894 - lr: 0.05000 - Train loss: 1.85270 - Test loss: 2.13623\n",
      "Epoch 4895 - lr: 0.05000 - Train loss: 1.85270 - Test loss: 2.13624\n",
      "Epoch 4896 - lr: 0.05000 - Train loss: 1.85270 - Test loss: 2.13624\n",
      "Epoch 4897 - lr: 0.05000 - Train loss: 1.85270 - Test loss: 2.13625\n",
      "Epoch 4898 - lr: 0.05000 - Train loss: 1.85270 - Test loss: 2.13625\n",
      "Epoch 4899 - lr: 0.05000 - Train loss: 1.85269 - Test loss: 2.13625\n",
      "Epoch 4900 - lr: 0.05000 - Train loss: 1.85269 - Test loss: 2.13626\n",
      "Epoch 4901 - lr: 0.05000 - Train loss: 1.85269 - Test loss: 2.13626\n",
      "Epoch 4902 - lr: 0.05000 - Train loss: 1.85269 - Test loss: 2.13627\n",
      "Epoch 4903 - lr: 0.05000 - Train loss: 1.85269 - Test loss: 2.13627\n",
      "Epoch 4904 - lr: 0.05000 - Train loss: 1.85269 - Test loss: 2.13627\n",
      "Epoch 4905 - lr: 0.05000 - Train loss: 1.85269 - Test loss: 2.13628\n",
      "Epoch 4906 - lr: 0.05000 - Train loss: 1.85269 - Test loss: 2.13628\n",
      "Epoch 4907 - lr: 0.05000 - Train loss: 1.85268 - Test loss: 2.13629\n",
      "Epoch 4908 - lr: 0.05000 - Train loss: 1.85268 - Test loss: 2.13629\n",
      "Epoch 4909 - lr: 0.05000 - Train loss: 1.85268 - Test loss: 2.13629\n",
      "Epoch 4910 - lr: 0.05000 - Train loss: 1.85268 - Test loss: 2.13630\n",
      "Epoch 4911 - lr: 0.05000 - Train loss: 1.85268 - Test loss: 2.13630\n",
      "Epoch 4912 - lr: 0.05000 - Train loss: 1.85268 - Test loss: 2.13631\n",
      "Epoch 4913 - lr: 0.05000 - Train loss: 1.85268 - Test loss: 2.13631\n",
      "Epoch 4914 - lr: 0.05000 - Train loss: 1.85268 - Test loss: 2.13631\n",
      "Epoch 4915 - lr: 0.05000 - Train loss: 1.85267 - Test loss: 2.13632\n",
      "Epoch 4916 - lr: 0.05000 - Train loss: 1.85267 - Test loss: 2.13632\n",
      "Epoch 4917 - lr: 0.05000 - Train loss: 1.85267 - Test loss: 2.13633\n",
      "Epoch 4918 - lr: 0.05000 - Train loss: 1.85267 - Test loss: 2.13633\n",
      "Epoch 4919 - lr: 0.05000 - Train loss: 1.85267 - Test loss: 2.13633\n",
      "Epoch 4920 - lr: 0.05000 - Train loss: 1.85267 - Test loss: 2.13634\n",
      "Epoch 4921 - lr: 0.05000 - Train loss: 1.85267 - Test loss: 2.13634\n",
      "Epoch 4922 - lr: 0.05000 - Train loss: 1.85267 - Test loss: 2.13635\n",
      "Epoch 4923 - lr: 0.05000 - Train loss: 1.85267 - Test loss: 2.13635\n",
      "Epoch 4924 - lr: 0.05000 - Train loss: 1.85266 - Test loss: 2.13635\n",
      "Epoch 4925 - lr: 0.05000 - Train loss: 1.85266 - Test loss: 2.13636\n",
      "Epoch 4926 - lr: 0.05000 - Train loss: 1.85266 - Test loss: 2.13636\n",
      "Epoch 4927 - lr: 0.05000 - Train loss: 1.85266 - Test loss: 2.13636\n",
      "Epoch 4928 - lr: 0.05000 - Train loss: 1.85266 - Test loss: 2.13637\n",
      "Epoch 4929 - lr: 0.05000 - Train loss: 1.85266 - Test loss: 2.13637\n",
      "Epoch 4930 - lr: 0.05000 - Train loss: 1.85266 - Test loss: 2.13638\n",
      "Epoch 4931 - lr: 0.05000 - Train loss: 1.85266 - Test loss: 2.13638\n",
      "Epoch 4932 - lr: 0.05000 - Train loss: 1.85265 - Test loss: 2.13638\n",
      "Epoch 4933 - lr: 0.05000 - Train loss: 1.85265 - Test loss: 2.13639\n",
      "Epoch 4934 - lr: 0.05000 - Train loss: 1.85265 - Test loss: 2.13639\n",
      "Epoch 4935 - lr: 0.05000 - Train loss: 1.85265 - Test loss: 2.13640\n",
      "Epoch 4936 - lr: 0.05000 - Train loss: 1.85265 - Test loss: 2.13640\n",
      "Epoch 4937 - lr: 0.05000 - Train loss: 1.85265 - Test loss: 2.13640\n",
      "Epoch 4938 - lr: 0.05000 - Train loss: 1.85265 - Test loss: 2.13641\n",
      "Epoch 4939 - lr: 0.05000 - Train loss: 1.85265 - Test loss: 2.13641\n",
      "Epoch 4940 - lr: 0.05000 - Train loss: 1.85265 - Test loss: 2.13642\n",
      "Epoch 4941 - lr: 0.05000 - Train loss: 1.85264 - Test loss: 2.13642\n",
      "Epoch 4942 - lr: 0.05000 - Train loss: 1.85264 - Test loss: 2.13642\n",
      "Epoch 4943 - lr: 0.05000 - Train loss: 1.85264 - Test loss: 2.13643\n",
      "Epoch 4944 - lr: 0.05000 - Train loss: 1.85264 - Test loss: 2.13643\n",
      "Epoch 4945 - lr: 0.05000 - Train loss: 1.85264 - Test loss: 2.13644\n",
      "Epoch 4946 - lr: 0.05000 - Train loss: 1.85264 - Test loss: 2.13644\n",
      "Epoch 4947 - lr: 0.05000 - Train loss: 1.85264 - Test loss: 2.13644\n",
      "Epoch 4948 - lr: 0.05000 - Train loss: 1.85264 - Test loss: 2.13645\n",
      "Epoch 4949 - lr: 0.05000 - Train loss: 1.85263 - Test loss: 2.13645\n",
      "Epoch 4950 - lr: 0.05000 - Train loss: 1.85263 - Test loss: 2.13645\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4951 - lr: 0.05000 - Train loss: 1.85263 - Test loss: 2.13646\n",
      "Epoch 4952 - lr: 0.05000 - Train loss: 1.85263 - Test loss: 2.13646\n",
      "Epoch 4953 - lr: 0.05000 - Train loss: 1.85263 - Test loss: 2.13647\n",
      "Epoch 4954 - lr: 0.05000 - Train loss: 1.85263 - Test loss: 2.13647\n",
      "Epoch 4955 - lr: 0.05000 - Train loss: 1.85263 - Test loss: 2.13647\n",
      "Epoch 4956 - lr: 0.05000 - Train loss: 1.85263 - Test loss: 2.13648\n",
      "Epoch 4957 - lr: 0.05000 - Train loss: 1.85263 - Test loss: 2.13648\n",
      "Epoch 4958 - lr: 0.05000 - Train loss: 1.85262 - Test loss: 2.13649\n",
      "Epoch 4959 - lr: 0.05000 - Train loss: 1.85262 - Test loss: 2.13649\n",
      "Epoch 4960 - lr: 0.05000 - Train loss: 1.85262 - Test loss: 2.13649\n",
      "Epoch 4961 - lr: 0.05000 - Train loss: 1.85262 - Test loss: 2.13650\n",
      "Epoch 4962 - lr: 0.05000 - Train loss: 1.85262 - Test loss: 2.13650\n",
      "Epoch 4963 - lr: 0.05000 - Train loss: 1.85262 - Test loss: 2.13651\n",
      "Epoch 4964 - lr: 0.05000 - Train loss: 1.85262 - Test loss: 2.13651\n",
      "Epoch 4965 - lr: 0.05000 - Train loss: 1.85262 - Test loss: 2.13651\n",
      "Epoch 4966 - lr: 0.05000 - Train loss: 1.85261 - Test loss: 2.13652\n",
      "Epoch 4967 - lr: 0.05000 - Train loss: 1.85261 - Test loss: 2.13652\n",
      "Epoch 4968 - lr: 0.05000 - Train loss: 1.85261 - Test loss: 2.13653\n",
      "Epoch 4969 - lr: 0.05000 - Train loss: 1.85261 - Test loss: 2.13653\n",
      "Epoch 4970 - lr: 0.05000 - Train loss: 1.85261 - Test loss: 2.13653\n",
      "Epoch 4971 - lr: 0.05000 - Train loss: 1.85261 - Test loss: 2.13654\n",
      "Epoch 4972 - lr: 0.05000 - Train loss: 1.85261 - Test loss: 2.13654\n",
      "Epoch 4973 - lr: 0.05000 - Train loss: 1.85261 - Test loss: 2.13654\n",
      "Epoch 4974 - lr: 0.05000 - Train loss: 1.85261 - Test loss: 2.13655\n",
      "Epoch 4975 - lr: 0.05000 - Train loss: 1.85260 - Test loss: 2.13655\n",
      "Epoch 4976 - lr: 0.05000 - Train loss: 1.85260 - Test loss: 2.13656\n",
      "Epoch 4977 - lr: 0.05000 - Train loss: 1.85260 - Test loss: 2.13656\n",
      "Epoch 4978 - lr: 0.05000 - Train loss: 1.85260 - Test loss: 2.13656\n",
      "Epoch 4979 - lr: 0.05000 - Train loss: 1.85260 - Test loss: 2.13657\n",
      "Epoch 4980 - lr: 0.05000 - Train loss: 1.85260 - Test loss: 2.13657\n",
      "Epoch 4981 - lr: 0.05000 - Train loss: 1.85260 - Test loss: 2.13658\n",
      "Epoch 4982 - lr: 0.05000 - Train loss: 1.85260 - Test loss: 2.13658\n",
      "Epoch 4983 - lr: 0.05000 - Train loss: 1.85259 - Test loss: 2.13658\n",
      "Epoch 4984 - lr: 0.05000 - Train loss: 1.85259 - Test loss: 2.13659\n",
      "Epoch 4985 - lr: 0.05000 - Train loss: 1.85259 - Test loss: 2.13659\n",
      "Epoch 4986 - lr: 0.05000 - Train loss: 1.85259 - Test loss: 2.13659\n",
      "Epoch 4987 - lr: 0.05000 - Train loss: 1.85259 - Test loss: 2.13660\n",
      "Epoch 4988 - lr: 0.05000 - Train loss: 1.85259 - Test loss: 2.13660\n",
      "Epoch 4989 - lr: 0.05000 - Train loss: 1.85259 - Test loss: 2.13661\n",
      "Epoch 4990 - lr: 0.05000 - Train loss: 1.85259 - Test loss: 2.13661\n",
      "Epoch 4991 - lr: 0.05000 - Train loss: 1.85259 - Test loss: 2.13661\n",
      "Epoch 4992 - lr: 0.05000 - Train loss: 1.85258 - Test loss: 2.13662\n",
      "Epoch 4993 - lr: 0.05000 - Train loss: 1.85258 - Test loss: 2.13662\n",
      "Epoch 4994 - lr: 0.05000 - Train loss: 1.85258 - Test loss: 2.13663\n",
      "Epoch 4995 - lr: 0.05000 - Train loss: 1.85258 - Test loss: 2.13663\n",
      "Epoch 4996 - lr: 0.05000 - Train loss: 1.85258 - Test loss: 2.13663\n",
      "Epoch 4997 - lr: 0.05000 - Train loss: 1.85258 - Test loss: 2.13664\n",
      "Epoch 4998 - lr: 0.05000 - Train loss: 1.85258 - Test loss: 2.13664\n",
      "Epoch 4999 - lr: 0.05000 - Train loss: 1.85258 - Test loss: 2.13665\n",
      "Epoch 5000 - lr: 0.05000 - Train loss: 1.85257 - Test loss: 2.13665\n",
      "number of hidden neurons in first layer is: 360 number of hidden neurons in second layer is: 240\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of hidden neurons in first layer is: 480 number of hidden neurons in second layer is: 120\n",
      "Epoch 1 - lr: 0.05000 - Train loss: 17.57723 - Test loss: 115.82187\n",
      "Epoch 2 - lr: 0.05000 - Train loss: 8.44619 - Test loss: 116.17688\n",
      "Epoch 3 - lr: 0.05000 - Train loss: 8.45344 - Test loss: 116.17346\n",
      "Epoch 4 - lr: 0.05000 - Train loss: 8.45358 - Test loss: 116.17132\n",
      "Epoch 5 - lr: 0.05000 - Train loss: 8.45364 - Test loss: 116.16978\n",
      "Epoch 6 - lr: 0.05000 - Train loss: 8.45369 - Test loss: 116.16856\n",
      "Epoch 7 - lr: 0.05000 - Train loss: 8.45371 - Test loss: 116.16754\n",
      "Epoch 8 - lr: 0.05000 - Train loss: 8.45373 - Test loss: 116.16666\n",
      "Epoch 9 - lr: 0.05000 - Train loss: 8.45374 - Test loss: 116.16586\n",
      "Epoch 10 - lr: 0.05000 - Train loss: 8.45374 - Test loss: 116.16512\n",
      "Epoch 11 - lr: 0.05000 - Train loss: 8.45375 - Test loss: 116.16443\n",
      "Epoch 12 - lr: 0.05000 - Train loss: 8.45374 - Test loss: 116.16377\n",
      "Epoch 13 - lr: 0.05000 - Train loss: 8.45374 - Test loss: 116.16312\n",
      "Epoch 14 - lr: 0.05000 - Train loss: 8.45373 - Test loss: 116.16249\n",
      "Epoch 15 - lr: 0.05000 - Train loss: 8.45373 - Test loss: 116.16185\n",
      "Epoch 16 - lr: 0.05000 - Train loss: 8.45371 - Test loss: 116.16121\n",
      "Epoch 17 - lr: 0.05000 - Train loss: 8.45370 - Test loss: 116.16055\n",
      "Epoch 18 - lr: 0.05000 - Train loss: 8.45369 - Test loss: 116.15988\n",
      "Epoch 19 - lr: 0.05000 - Train loss: 8.45367 - Test loss: 116.15917\n",
      "Epoch 20 - lr: 0.05000 - Train loss: 8.45365 - Test loss: 116.15842\n",
      "Epoch 21 - lr: 0.05000 - Train loss: 8.45363 - Test loss: 116.15763\n",
      "Epoch 22 - lr: 0.05000 - Train loss: 8.45360 - Test loss: 116.15677\n",
      "Epoch 23 - lr: 0.05000 - Train loss: 8.45357 - Test loss: 116.15584\n",
      "Epoch 24 - lr: 0.05000 - Train loss: 8.45353 - Test loss: 116.15481\n",
      "Epoch 25 - lr: 0.05000 - Train loss: 8.45349 - Test loss: 116.15366\n",
      "Epoch 26 - lr: 0.05000 - Train loss: 8.45345 - Test loss: 116.15236\n",
      "Epoch 27 - lr: 0.05000 - Train loss: 8.45339 - Test loss: 116.15086\n",
      "Epoch 28 - lr: 0.05000 - Train loss: 8.45333 - Test loss: 116.14912\n",
      "Epoch 29 - lr: 0.05000 - Train loss: 8.45325 - Test loss: 116.14706\n",
      "Epoch 30 - lr: 0.05000 - Train loss: 8.45316 - Test loss: 116.14458\n",
      "Epoch 31 - lr: 0.05000 - Train loss: 8.45304 - Test loss: 116.14155\n",
      "Epoch 32 - lr: 0.05000 - Train loss: 8.45290 - Test loss: 116.13777\n",
      "Epoch 33 - lr: 0.05000 - Train loss: 8.45273 - Test loss: 116.13300\n",
      "Epoch 34 - lr: 0.05000 - Train loss: 8.45252 - Test loss: 116.12696\n",
      "Epoch 35 - lr: 0.05000 - Train loss: 8.45226 - Test loss: 116.11935\n",
      "Epoch 36 - lr: 0.05000 - Train loss: 8.45196 - Test loss: 116.11008\n",
      "Epoch 37 - lr: 0.05000 - Train loss: 8.45163 - Test loss: 116.09949\n",
      "Epoch 38 - lr: 0.05000 - Train loss: 8.45130 - Test loss: 116.08843\n",
      "Epoch 39 - lr: 0.05000 - Train loss: 8.45100 - Test loss: 116.07765\n",
      "Epoch 40 - lr: 0.05000 - Train loss: 8.45072 - Test loss: 116.06707\n",
      "Epoch 41 - lr: 0.05000 - Train loss: 8.45044 - Test loss: 116.05599\n",
      "Epoch 42 - lr: 0.05000 - Train loss: 8.45014 - Test loss: 116.04395\n",
      "Epoch 43 - lr: 0.05000 - Train loss: 8.44983 - Test loss: 116.03130\n",
      "Epoch 44 - lr: 0.05000 - Train loss: 8.44954 - Test loss: 116.01922\n",
      "Epoch 45 - lr: 0.05000 - Train loss: 8.44930 - Test loss: 116.00880\n",
      "Epoch 46 - lr: 0.05000 - Train loss: 8.44911 - Test loss: 115.99997\n",
      "Epoch 47 - lr: 0.05000 - Train loss: 8.44892 - Test loss: 115.99185\n",
      "Epoch 48 - lr: 0.05000 - Train loss: 8.44867 - Test loss: 115.98339\n",
      "Epoch 49 - lr: 0.05000 - Train loss: 8.44819 - Test loss: 115.97148\n",
      "Epoch 50 - lr: 0.05000 - Train loss: 8.44899 - Test loss: 115.95801\n",
      "Epoch 51 - lr: 0.05000 - Train loss: 8.44885 - Test loss: 115.94647\n",
      "Epoch 52 - lr: 0.05000 - Train loss: 8.45056 - Test loss: 115.88068\n",
      "Epoch 53 - lr: 0.05000 - Train loss: 8.50120 - Test loss: 118.68739\n",
      "Epoch 54 - lr: 0.05000 - Train loss: 8.07332 - Test loss: 118.50095\n",
      "Epoch 55 - lr: 0.05000 - Train loss: 8.08363 - Test loss: 114.29050\n",
      "Epoch 56 - lr: 0.05000 - Train loss: 8.16270 - Test loss: 117.57422\n",
      "Epoch 57 - lr: 0.05000 - Train loss: 8.25351 - Test loss: 116.06398\n",
      "Epoch 58 - lr: 0.05000 - Train loss: 8.47190 - Test loss: 115.81440\n",
      "Epoch 59 - lr: 0.05000 - Train loss: 8.34367 - Test loss: 116.11179\n",
      "Epoch 60 - lr: 0.05000 - Train loss: 8.45204 - Test loss: 116.09725\n",
      "Epoch 61 - lr: 0.05000 - Train loss: 8.45181 - Test loss: 116.08981\n",
      "Epoch 62 - lr: 0.05000 - Train loss: 8.45173 - Test loss: 116.08531\n",
      "Epoch 63 - lr: 0.05000 - Train loss: 8.45169 - Test loss: 116.08076\n",
      "Epoch 64 - lr: 0.05000 - Train loss: 8.45161 - Test loss: 116.07712\n",
      "Epoch 65 - lr: 0.05000 - Train loss: 8.45151 - Test loss: 116.07274\n",
      "Epoch 66 - lr: 0.05000 - Train loss: 8.45134 - Test loss: 116.06839\n",
      "Epoch 67 - lr: 0.05000 - Train loss: 8.45096 - Test loss: 116.06021\n",
      "Epoch 68 - lr: 0.05000 - Train loss: 8.45124 - Test loss: 116.04714\n",
      "Epoch 69 - lr: 0.05000 - Train loss: 8.45483 - Test loss: 115.96360\n",
      "Epoch 70 - lr: 0.05000 - Train loss: 8.44921 - Test loss: 116.09228\n",
      "Epoch 71 - lr: 0.05000 - Train loss: 8.46284 - Test loss: 115.95364\n",
      "Epoch 72 - lr: 0.05000 - Train loss: 8.44901 - Test loss: 116.09406\n",
      "Epoch 73 - lr: 0.05000 - Train loss: 8.51887 - Test loss: 115.49555\n",
      "Epoch 74 - lr: 0.05000 - Train loss: 8.44136 - Test loss: 116.16683\n",
      "Epoch 75 - lr: 0.05000 - Train loss: 8.45410 - Test loss: 116.16647\n",
      "Epoch 76 - lr: 0.05000 - Train loss: 8.45409 - Test loss: 116.16611\n",
      "Epoch 77 - lr: 0.05000 - Train loss: 8.45409 - Test loss: 116.16575\n",
      "Epoch 78 - lr: 0.05000 - Train loss: 8.45408 - Test loss: 116.16538\n",
      "Epoch 79 - lr: 0.05000 - Train loss: 8.45407 - Test loss: 116.16502\n",
      "Epoch 80 - lr: 0.05000 - Train loss: 8.45406 - Test loss: 116.16466\n",
      "Epoch 81 - lr: 0.05000 - Train loss: 8.45406 - Test loss: 116.16429\n",
      "Epoch 82 - lr: 0.05000 - Train loss: 8.45405 - Test loss: 116.16391\n",
      "Epoch 83 - lr: 0.05000 - Train loss: 8.45404 - Test loss: 116.16353\n",
      "Epoch 84 - lr: 0.05000 - Train loss: 8.45403 - Test loss: 116.16314\n",
      "Epoch 85 - lr: 0.05000 - Train loss: 8.45402 - Test loss: 116.16273\n",
      "Epoch 86 - lr: 0.05000 - Train loss: 8.45401 - Test loss: 116.16232\n",
      "Epoch 87 - lr: 0.05000 - Train loss: 8.45400 - Test loss: 116.16189\n",
      "Epoch 88 - lr: 0.05000 - Train loss: 8.45399 - Test loss: 116.16143\n",
      "Epoch 89 - lr: 0.05000 - Train loss: 8.45397 - Test loss: 116.16095\n",
      "Epoch 90 - lr: 0.05000 - Train loss: 8.45396 - Test loss: 116.16043\n",
      "Epoch 91 - lr: 0.05000 - Train loss: 8.45394 - Test loss: 116.15987\n",
      "Epoch 92 - lr: 0.05000 - Train loss: 8.45392 - Test loss: 116.15925\n",
      "Epoch 93 - lr: 0.05000 - Train loss: 8.45390 - Test loss: 116.15855\n",
      "Epoch 94 - lr: 0.05000 - Train loss: 8.45388 - Test loss: 116.15774\n",
      "Epoch 95 - lr: 0.05000 - Train loss: 8.45385 - Test loss: 116.15677\n",
      "Epoch 96 - lr: 0.05000 - Train loss: 8.45381 - Test loss: 116.15555\n",
      "Epoch 97 - lr: 0.05000 - Train loss: 8.45375 - Test loss: 116.15393\n",
      "Epoch 98 - lr: 0.05000 - Train loss: 8.45368 - Test loss: 116.15159\n",
      "Epoch 99 - lr: 0.05000 - Train loss: 8.45356 - Test loss: 116.14778\n",
      "Epoch 100 - lr: 0.05000 - Train loss: 8.45335 - Test loss: 116.14030\n",
      "Epoch 101 - lr: 0.05000 - Train loss: 8.45288 - Test loss: 116.11924\n",
      "Epoch 102 - lr: 0.05000 - Train loss: 8.45195 - Test loss: 116.04453\n",
      "Epoch 103 - lr: 0.05000 - Train loss: 8.47069 - Test loss: 116.85064\n",
      "Epoch 104 - lr: 0.05000 - Train loss: 8.31123 - Test loss: 116.16044\n",
      "Epoch 105 - lr: 0.05000 - Train loss: 8.45402 - Test loss: 116.16013\n",
      "Epoch 106 - lr: 0.05000 - Train loss: 8.45402 - Test loss: 116.15981\n",
      "Epoch 107 - lr: 0.05000 - Train loss: 8.45401 - Test loss: 116.15948\n",
      "Epoch 108 - lr: 0.05000 - Train loss: 8.45401 - Test loss: 116.15916\n",
      "Epoch 109 - lr: 0.05000 - Train loss: 8.45400 - Test loss: 116.15883\n",
      "Epoch 110 - lr: 0.05000 - Train loss: 8.45400 - Test loss: 116.15849\n",
      "Epoch 111 - lr: 0.05000 - Train loss: 8.45399 - Test loss: 116.15816\n",
      "Epoch 112 - lr: 0.05000 - Train loss: 8.45399 - Test loss: 116.15781\n",
      "Epoch 113 - lr: 0.05000 - Train loss: 8.45399 - Test loss: 116.15746\n",
      "Epoch 114 - lr: 0.05000 - Train loss: 8.45398 - Test loss: 116.15710\n",
      "Epoch 115 - lr: 0.05000 - Train loss: 8.45398 - Test loss: 116.15673\n",
      "Epoch 116 - lr: 0.05000 - Train loss: 8.45397 - Test loss: 116.15634\n",
      "Epoch 117 - lr: 0.05000 - Train loss: 8.45397 - Test loss: 116.15595\n",
      "Epoch 118 - lr: 0.05000 - Train loss: 8.45397 - Test loss: 116.15554\n",
      "Epoch 119 - lr: 0.05000 - Train loss: 8.45396 - Test loss: 116.15510\n",
      "Epoch 120 - lr: 0.05000 - Train loss: 8.45396 - Test loss: 116.15465\n",
      "Epoch 121 - lr: 0.05000 - Train loss: 8.45396 - Test loss: 116.15416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 122 - lr: 0.05000 - Train loss: 8.45396 - Test loss: 116.15363\n",
      "Epoch 123 - lr: 0.05000 - Train loss: 8.45396 - Test loss: 116.15306\n",
      "Epoch 124 - lr: 0.05000 - Train loss: 8.45396 - Test loss: 116.15241\n",
      "Epoch 125 - lr: 0.05000 - Train loss: 8.45396 - Test loss: 116.15168\n",
      "Epoch 126 - lr: 0.05000 - Train loss: 8.45396 - Test loss: 116.15082\n",
      "Epoch 127 - lr: 0.05000 - Train loss: 8.45397 - Test loss: 116.14976\n",
      "Epoch 128 - lr: 0.05000 - Train loss: 8.45398 - Test loss: 116.14842\n",
      "Epoch 129 - lr: 0.05000 - Train loss: 8.45400 - Test loss: 116.14658\n",
      "Epoch 130 - lr: 0.05000 - Train loss: 8.45402 - Test loss: 116.14382\n",
      "Epoch 131 - lr: 0.05000 - Train loss: 8.45408 - Test loss: 116.13902\n",
      "Epoch 132 - lr: 0.05000 - Train loss: 8.45420 - Test loss: 116.12829\n",
      "Epoch 133 - lr: 0.05000 - Train loss: 8.45484 - Test loss: 116.08640\n",
      "Epoch 134 - lr: 0.05000 - Train loss: 8.69385 - Test loss: 115.96467\n",
      "Epoch 135 - lr: 0.05000 - Train loss: 8.80232 - Test loss: 113.59547\n",
      "Epoch 136 - lr: 0.05000 - Train loss: 9.06914 - Test loss: 114.02693\n",
      "Epoch 137 - lr: 0.05000 - Train loss: 8.96570 - Test loss: 113.66233\n",
      "Epoch 138 - lr: 0.05000 - Train loss: 9.05025 - Test loss: 113.66160\n",
      "Epoch 139 - lr: 0.05000 - Train loss: 9.05022 - Test loss: 113.66087\n",
      "Epoch 140 - lr: 0.05000 - Train loss: 9.05019 - Test loss: 113.66014\n",
      "Epoch 141 - lr: 0.05000 - Train loss: 9.05015 - Test loss: 113.65941\n",
      "Epoch 142 - lr: 0.05000 - Train loss: 9.05012 - Test loss: 113.65869\n",
      "Epoch 143 - lr: 0.05000 - Train loss: 9.05009 - Test loss: 113.65796\n",
      "Epoch 144 - lr: 0.05000 - Train loss: 9.05005 - Test loss: 113.65725\n",
      "Epoch 145 - lr: 0.05000 - Train loss: 9.05002 - Test loss: 113.65655\n",
      "Epoch 146 - lr: 0.05000 - Train loss: 9.04999 - Test loss: 113.65585\n",
      "Epoch 147 - lr: 0.05000 - Train loss: 9.04996 - Test loss: 113.65517\n",
      "Epoch 148 - lr: 0.05000 - Train loss: 9.04992 - Test loss: 113.65449\n",
      "Epoch 149 - lr: 0.05000 - Train loss: 9.04989 - Test loss: 113.65384\n",
      "Epoch 150 - lr: 0.05000 - Train loss: 9.04986 - Test loss: 113.65319\n",
      "Epoch 151 - lr: 0.05000 - Train loss: 9.04983 - Test loss: 113.65256\n",
      "Epoch 152 - lr: 0.05000 - Train loss: 9.04980 - Test loss: 113.65194\n",
      "Epoch 153 - lr: 0.05000 - Train loss: 9.04977 - Test loss: 113.65134\n",
      "Epoch 154 - lr: 0.05000 - Train loss: 9.04974 - Test loss: 113.65075\n",
      "Epoch 155 - lr: 0.05000 - Train loss: 9.04971 - Test loss: 113.65018\n",
      "Epoch 156 - lr: 0.05000 - Train loss: 9.04968 - Test loss: 113.64962\n",
      "Epoch 157 - lr: 0.05000 - Train loss: 9.04965 - Test loss: 113.64906\n",
      "Epoch 158 - lr: 0.05000 - Train loss: 9.04962 - Test loss: 113.64852\n",
      "Epoch 159 - lr: 0.05000 - Train loss: 9.04960 - Test loss: 113.64799\n",
      "Epoch 160 - lr: 0.05000 - Train loss: 9.04957 - Test loss: 113.64746\n",
      "Epoch 161 - lr: 0.05000 - Train loss: 9.04954 - Test loss: 113.64694\n",
      "Epoch 162 - lr: 0.05000 - Train loss: 9.04951 - Test loss: 113.64643\n",
      "Epoch 163 - lr: 0.05000 - Train loss: 9.04949 - Test loss: 113.64591\n",
      "Epoch 164 - lr: 0.05000 - Train loss: 9.04946 - Test loss: 113.64540\n",
      "Epoch 165 - lr: 0.05000 - Train loss: 9.04943 - Test loss: 113.64488\n",
      "Epoch 166 - lr: 0.05000 - Train loss: 9.04940 - Test loss: 113.64436\n",
      "Epoch 167 - lr: 0.05000 - Train loss: 9.04937 - Test loss: 113.64382\n",
      "Epoch 168 - lr: 0.05000 - Train loss: 9.04935 - Test loss: 113.64327\n",
      "Epoch 169 - lr: 0.05000 - Train loss: 9.04931 - Test loss: 113.64269\n",
      "Epoch 170 - lr: 0.05000 - Train loss: 9.04928 - Test loss: 113.64208\n",
      "Epoch 171 - lr: 0.05000 - Train loss: 9.04925 - Test loss: 113.64143\n",
      "Epoch 172 - lr: 0.05000 - Train loss: 9.04921 - Test loss: 113.64070\n",
      "Epoch 173 - lr: 0.05000 - Train loss: 9.04917 - Test loss: 113.63986\n",
      "Epoch 174 - lr: 0.05000 - Train loss: 9.04912 - Test loss: 113.63885\n",
      "Epoch 175 - lr: 0.05000 - Train loss: 9.04905 - Test loss: 113.63756\n",
      "Epoch 176 - lr: 0.05000 - Train loss: 9.04897 - Test loss: 113.63574\n",
      "Epoch 177 - lr: 0.05000 - Train loss: 9.04884 - Test loss: 113.63279\n",
      "Epoch 178 - lr: 0.05000 - Train loss: 9.04862 - Test loss: 113.62671\n",
      "Epoch 179 - lr: 0.05000 - Train loss: 9.04806 - Test loss: 113.60575\n",
      "Epoch 180 - lr: 0.05000 - Train loss: 9.05926 - Test loss: 113.27385\n",
      "Epoch 181 - lr: 0.05000 - Train loss: 9.03725 - Test loss: 113.13958\n",
      "Epoch 182 - lr: 0.05000 - Train loss: 8.88438 - Test loss: 113.64234\n",
      "Epoch 183 - lr: 0.05000 - Train loss: 9.04924 - Test loss: 113.64201\n",
      "Epoch 184 - lr: 0.05000 - Train loss: 9.04923 - Test loss: 113.64168\n",
      "Epoch 185 - lr: 0.05000 - Train loss: 9.04922 - Test loss: 113.64136\n",
      "Epoch 186 - lr: 0.05000 - Train loss: 9.04920 - Test loss: 113.64104\n",
      "Epoch 187 - lr: 0.05000 - Train loss: 9.04919 - Test loss: 113.64072\n",
      "Epoch 188 - lr: 0.05000 - Train loss: 9.04918 - Test loss: 113.64041\n",
      "Epoch 189 - lr: 0.05000 - Train loss: 9.04917 - Test loss: 113.64009\n",
      "Epoch 190 - lr: 0.05000 - Train loss: 9.04916 - Test loss: 113.63977\n",
      "Epoch 191 - lr: 0.05000 - Train loss: 9.04915 - Test loss: 113.63945\n",
      "Epoch 192 - lr: 0.05000 - Train loss: 9.04915 - Test loss: 113.63912\n",
      "Epoch 193 - lr: 0.05000 - Train loss: 9.04914 - Test loss: 113.63879\n",
      "Epoch 194 - lr: 0.05000 - Train loss: 9.04913 - Test loss: 113.63844\n",
      "Epoch 195 - lr: 0.05000 - Train loss: 9.04912 - Test loss: 113.63809\n",
      "Epoch 196 - lr: 0.05000 - Train loss: 9.04911 - Test loss: 113.63772\n",
      "Epoch 197 - lr: 0.05000 - Train loss: 9.04911 - Test loss: 113.63732\n",
      "Epoch 198 - lr: 0.05000 - Train loss: 9.04910 - Test loss: 113.63691\n",
      "Epoch 199 - lr: 0.05000 - Train loss: 9.04909 - Test loss: 113.63646\n",
      "Epoch 200 - lr: 0.05000 - Train loss: 9.04909 - Test loss: 113.63598\n",
      "Epoch 201 - lr: 0.05000 - Train loss: 9.04908 - Test loss: 113.63544\n",
      "Epoch 202 - lr: 0.05000 - Train loss: 9.04908 - Test loss: 113.63485\n",
      "Epoch 203 - lr: 0.05000 - Train loss: 9.04907 - Test loss: 113.63418\n",
      "Epoch 204 - lr: 0.05000 - Train loss: 9.04907 - Test loss: 113.63340\n",
      "Epoch 205 - lr: 0.05000 - Train loss: 9.04907 - Test loss: 113.63248\n",
      "Epoch 206 - lr: 0.05000 - Train loss: 9.04908 - Test loss: 113.63135\n",
      "Epoch 207 - lr: 0.05000 - Train loss: 9.04909 - Test loss: 113.62991\n",
      "Epoch 208 - lr: 0.05000 - Train loss: 9.04911 - Test loss: 113.62796\n",
      "Epoch 209 - lr: 0.05000 - Train loss: 9.04915 - Test loss: 113.62511\n",
      "Epoch 210 - lr: 0.05000 - Train loss: 9.04922 - Test loss: 113.62039\n",
      "Epoch 211 - lr: 0.05000 - Train loss: 9.04939 - Test loss: 113.61083\n",
      "Epoch 212 - lr: 0.05000 - Train loss: 9.05010 - Test loss: 113.58270\n",
      "Epoch 213 - lr: 0.05000 - Train loss: 9.42261 - Test loss: 113.20963\n",
      "Epoch 214 - lr: 0.05000 - Train loss: 9.66119 - Test loss: 110.66023\n",
      "Epoch 215 - lr: 0.05000 - Train loss: 9.98774 - Test loss: 110.65703\n",
      "Epoch 216 - lr: 0.05000 - Train loss: 9.98743 - Test loss: 110.65336\n",
      "Epoch 217 - lr: 0.05000 - Train loss: 9.98707 - Test loss: 110.64854\n",
      "Epoch 218 - lr: 0.05000 - Train loss: 9.98660 - Test loss: 110.64025\n",
      "Epoch 219 - lr: 0.05000 - Train loss: 9.98572 - Test loss: 110.61305\n",
      "Epoch 220 - lr: 0.05000 - Train loss: 9.99616 - Test loss: 110.76201\n",
      "Epoch 221 - lr: 0.05000 - Train loss: 9.81750 - Test loss: 110.63940\n",
      "Epoch 222 - lr: 0.05000 - Train loss: 9.98499 - Test loss: 110.56535\n",
      "Epoch 223 - lr: 0.05000 - Train loss: 9.87925 - Test loss: 113.11563\n",
      "Epoch 224 - lr: 0.05000 - Train loss: 9.63180 - Test loss: 110.65385\n",
      "Epoch 225 - lr: 0.05000 - Train loss: 9.98666 - Test loss: 110.65316\n",
      "Epoch 226 - lr: 0.05000 - Train loss: 9.98653 - Test loss: 110.65258\n",
      "Epoch 227 - lr: 0.05000 - Train loss: 9.98642 - Test loss: 110.65201\n",
      "Epoch 228 - lr: 0.05000 - Train loss: 9.98631 - Test loss: 110.65130\n",
      "Epoch 229 - lr: 0.05000 - Train loss: 9.98618 - Test loss: 110.65017\n",
      "Epoch 230 - lr: 0.05000 - Train loss: 9.98602 - Test loss: 110.64792\n",
      "Epoch 231 - lr: 0.05000 - Train loss: 9.98572 - Test loss: 110.64184\n",
      "Epoch 232 - lr: 0.05000 - Train loss: 9.98469 - Test loss: 110.60272\n",
      "Epoch 233 - lr: 0.05000 - Train loss: 9.88345 - Test loss: 113.43201\n",
      "Epoch 234 - lr: 0.05000 - Train loss: 9.28963 - Test loss: 105.80593\n",
      "Epoch 235 - lr: 0.05000 - Train loss: 9.42879 - Test loss: 110.64825\n",
      "Epoch 236 - lr: 0.05000 - Train loss: 9.98652 - Test loss: 110.64927\n",
      "Epoch 237 - lr: 0.05000 - Train loss: 9.98644 - Test loss: 110.65034\n",
      "Epoch 238 - lr: 0.05000 - Train loss: 9.98638 - Test loss: 110.65138\n",
      "Epoch 239 - lr: 0.05000 - Train loss: 9.98634 - Test loss: 110.65234\n",
      "Epoch 240 - lr: 0.05000 - Train loss: 9.98631 - Test loss: 110.65317\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 241 - lr: 0.05000 - Train loss: 9.98630 - Test loss: 110.65389\n",
      "Epoch 242 - lr: 0.05000 - Train loss: 9.98629 - Test loss: 110.65447\n",
      "Epoch 243 - lr: 0.05000 - Train loss: 9.98629 - Test loss: 110.65494\n",
      "Epoch 244 - lr: 0.05000 - Train loss: 9.98629 - Test loss: 110.65530\n",
      "Epoch 245 - lr: 0.05000 - Train loss: 9.98629 - Test loss: 110.65557\n",
      "Epoch 246 - lr: 0.05000 - Train loss: 9.98630 - Test loss: 110.65575\n",
      "Epoch 247 - lr: 0.05000 - Train loss: 9.98630 - Test loss: 110.65586\n",
      "Epoch 248 - lr: 0.05000 - Train loss: 9.98630 - Test loss: 110.65591\n",
      "Epoch 249 - lr: 0.05000 - Train loss: 9.98629 - Test loss: 110.65590\n",
      "Epoch 250 - lr: 0.05000 - Train loss: 9.98629 - Test loss: 110.65585\n",
      "Epoch 251 - lr: 0.05000 - Train loss: 9.98628 - Test loss: 110.65575\n",
      "Epoch 252 - lr: 0.05000 - Train loss: 9.98627 - Test loss: 110.65563\n",
      "Epoch 253 - lr: 0.05000 - Train loss: 9.98626 - Test loss: 110.65547\n",
      "Epoch 254 - lr: 0.05000 - Train loss: 9.98624 - Test loss: 110.65528\n",
      "Epoch 255 - lr: 0.05000 - Train loss: 9.98623 - Test loss: 110.65507\n",
      "Epoch 256 - lr: 0.05000 - Train loss: 9.98621 - Test loss: 110.65483\n",
      "Epoch 257 - lr: 0.05000 - Train loss: 9.98619 - Test loss: 110.65457\n",
      "Epoch 258 - lr: 0.05000 - Train loss: 9.98617 - Test loss: 110.65428\n",
      "Epoch 259 - lr: 0.05000 - Train loss: 9.98615 - Test loss: 110.65397\n",
      "Epoch 260 - lr: 0.05000 - Train loss: 9.98612 - Test loss: 110.65364\n",
      "Epoch 261 - lr: 0.05000 - Train loss: 9.98609 - Test loss: 110.65328\n",
      "Epoch 262 - lr: 0.05000 - Train loss: 9.98606 - Test loss: 110.65290\n",
      "Epoch 263 - lr: 0.05000 - Train loss: 9.98603 - Test loss: 110.65249\n",
      "Epoch 264 - lr: 0.05000 - Train loss: 9.98600 - Test loss: 110.65207\n",
      "Epoch 265 - lr: 0.05000 - Train loss: 9.98596 - Test loss: 110.65163\n",
      "Epoch 266 - lr: 0.05000 - Train loss: 9.98592 - Test loss: 110.65119\n",
      "Epoch 267 - lr: 0.05000 - Train loss: 9.98587 - Test loss: 110.65075\n",
      "Epoch 268 - lr: 0.05000 - Train loss: 9.98583 - Test loss: 110.65033\n",
      "Epoch 269 - lr: 0.05000 - Train loss: 9.98578 - Test loss: 110.64994\n",
      "Epoch 270 - lr: 0.05000 - Train loss: 9.98574 - Test loss: 110.64962\n",
      "Epoch 271 - lr: 0.05000 - Train loss: 9.98569 - Test loss: 110.64936\n",
      "Epoch 272 - lr: 0.05000 - Train loss: 9.98565 - Test loss: 110.64917\n",
      "Epoch 273 - lr: 0.05000 - Train loss: 9.98561 - Test loss: 110.64905\n",
      "Epoch 274 - lr: 0.05000 - Train loss: 9.98557 - Test loss: 110.64898\n",
      "Epoch 275 - lr: 0.05000 - Train loss: 9.98554 - Test loss: 110.64894\n",
      "Epoch 276 - lr: 0.05000 - Train loss: 9.98551 - Test loss: 110.64891\n",
      "Epoch 277 - lr: 0.05000 - Train loss: 9.98548 - Test loss: 110.64888\n",
      "Epoch 278 - lr: 0.05000 - Train loss: 9.98545 - Test loss: 110.64883\n",
      "Epoch 279 - lr: 0.05000 - Train loss: 9.98543 - Test loss: 110.64876\n",
      "Epoch 280 - lr: 0.05000 - Train loss: 9.98541 - Test loss: 110.64866\n",
      "Epoch 281 - lr: 0.05000 - Train loss: 9.98538 - Test loss: 110.64852\n",
      "Epoch 282 - lr: 0.05000 - Train loss: 9.98536 - Test loss: 110.64836\n",
      "Epoch 283 - lr: 0.05000 - Train loss: 9.98534 - Test loss: 110.64816\n",
      "Epoch 284 - lr: 0.05000 - Train loss: 9.98532 - Test loss: 110.64792\n",
      "Epoch 285 - lr: 0.05000 - Train loss: 9.98529 - Test loss: 110.64766\n",
      "Epoch 286 - lr: 0.05000 - Train loss: 9.98527 - Test loss: 110.64736\n",
      "Epoch 287 - lr: 0.05000 - Train loss: 9.98524 - Test loss: 110.64702\n",
      "Epoch 288 - lr: 0.05000 - Train loss: 9.98521 - Test loss: 110.64665\n",
      "Epoch 289 - lr: 0.05000 - Train loss: 9.98518 - Test loss: 110.64624\n",
      "Epoch 290 - lr: 0.05000 - Train loss: 9.98514 - Test loss: 110.64579\n",
      "Epoch 291 - lr: 0.05000 - Train loss: 9.98510 - Test loss: 110.64531\n",
      "Epoch 292 - lr: 0.05000 - Train loss: 9.98506 - Test loss: 110.64479\n",
      "Epoch 293 - lr: 0.05000 - Train loss: 9.98502 - Test loss: 110.64425\n",
      "Epoch 294 - lr: 0.05000 - Train loss: 9.98497 - Test loss: 110.64368\n",
      "Epoch 295 - lr: 0.05000 - Train loss: 9.98492 - Test loss: 110.64310\n",
      "Epoch 296 - lr: 0.05000 - Train loss: 9.98486 - Test loss: 110.64253\n",
      "Epoch 297 - lr: 0.05000 - Train loss: 9.98480 - Test loss: 110.64199\n",
      "Epoch 298 - lr: 0.05000 - Train loss: 9.98474 - Test loss: 110.64150\n",
      "Epoch 299 - lr: 0.05000 - Train loss: 9.98468 - Test loss: 110.64109\n",
      "Epoch 300 - lr: 0.05000 - Train loss: 9.98462 - Test loss: 110.64075\n",
      "Epoch 301 - lr: 0.05000 - Train loss: 9.98456 - Test loss: 110.64049\n",
      "Epoch 302 - lr: 0.05000 - Train loss: 9.98451 - Test loss: 110.64028\n",
      "Epoch 303 - lr: 0.05000 - Train loss: 9.98445 - Test loss: 110.64009\n",
      "Epoch 304 - lr: 0.05000 - Train loss: 9.98440 - Test loss: 110.63991\n",
      "Epoch 305 - lr: 0.05000 - Train loss: 9.98435 - Test loss: 110.63970\n",
      "Epoch 306 - lr: 0.05000 - Train loss: 9.98430 - Test loss: 110.63946\n",
      "Epoch 307 - lr: 0.05000 - Train loss: 9.98425 - Test loss: 110.63917\n",
      "Epoch 308 - lr: 0.05000 - Train loss: 9.98420 - Test loss: 110.63882\n",
      "Epoch 309 - lr: 0.05000 - Train loss: 9.98415 - Test loss: 110.63844\n",
      "Epoch 310 - lr: 0.05000 - Train loss: 9.98409 - Test loss: 110.63800\n",
      "Epoch 311 - lr: 0.05000 - Train loss: 9.98404 - Test loss: 110.63754\n",
      "Epoch 312 - lr: 0.05000 - Train loss: 9.98398 - Test loss: 110.63706\n",
      "Epoch 313 - lr: 0.05000 - Train loss: 9.98392 - Test loss: 110.63657\n",
      "Epoch 314 - lr: 0.05000 - Train loss: 9.98385 - Test loss: 110.63608\n",
      "Epoch 315 - lr: 0.05000 - Train loss: 9.98379 - Test loss: 110.63560\n",
      "Epoch 316 - lr: 0.05000 - Train loss: 9.98373 - Test loss: 110.63515\n",
      "Epoch 317 - lr: 0.05000 - Train loss: 9.98366 - Test loss: 110.63473\n",
      "Epoch 318 - lr: 0.05000 - Train loss: 9.98360 - Test loss: 110.63434\n",
      "Epoch 319 - lr: 0.05000 - Train loss: 9.98354 - Test loss: 110.63399\n",
      "Epoch 320 - lr: 0.05000 - Train loss: 9.98348 - Test loss: 110.63370\n",
      "Epoch 321 - lr: 0.05000 - Train loss: 9.98342 - Test loss: 110.63347\n",
      "Epoch 322 - lr: 0.05000 - Train loss: 9.98337 - Test loss: 110.63332\n",
      "Epoch 323 - lr: 0.05000 - Train loss: 9.98331 - Test loss: 110.63325\n",
      "Epoch 324 - lr: 0.05000 - Train loss: 9.98327 - Test loss: 110.63325\n",
      "Epoch 325 - lr: 0.05000 - Train loss: 9.98322 - Test loss: 110.63332\n",
      "Epoch 326 - lr: 0.05000 - Train loss: 9.98318 - Test loss: 110.63343\n",
      "Epoch 327 - lr: 0.05000 - Train loss: 9.98315 - Test loss: 110.63356\n",
      "Epoch 328 - lr: 0.05000 - Train loss: 9.98312 - Test loss: 110.63368\n",
      "Epoch 329 - lr: 0.05000 - Train loss: 9.98309 - Test loss: 110.63378\n",
      "Epoch 330 - lr: 0.05000 - Train loss: 9.98307 - Test loss: 110.63386\n",
      "Epoch 331 - lr: 0.05000 - Train loss: 9.98305 - Test loss: 110.63391\n",
      "Epoch 332 - lr: 0.05000 - Train loss: 9.98303 - Test loss: 110.63392\n",
      "Epoch 333 - lr: 0.05000 - Train loss: 9.98301 - Test loss: 110.63389\n",
      "Epoch 334 - lr: 0.05000 - Train loss: 9.98299 - Test loss: 110.63383\n",
      "Epoch 335 - lr: 0.05000 - Train loss: 9.98298 - Test loss: 110.63374\n",
      "Epoch 336 - lr: 0.05000 - Train loss: 9.98296 - Test loss: 110.63360\n",
      "Epoch 337 - lr: 0.05000 - Train loss: 9.98294 - Test loss: 110.63343\n",
      "Epoch 338 - lr: 0.05000 - Train loss: 9.98293 - Test loss: 110.63322\n",
      "Epoch 339 - lr: 0.05000 - Train loss: 9.98291 - Test loss: 110.63295\n",
      "Epoch 340 - lr: 0.05000 - Train loss: 9.98289 - Test loss: 110.63263\n",
      "Epoch 341 - lr: 0.05000 - Train loss: 9.98288 - Test loss: 110.63224\n",
      "Epoch 342 - lr: 0.05000 - Train loss: 9.98287 - Test loss: 110.63175\n",
      "Epoch 343 - lr: 0.05000 - Train loss: 9.98286 - Test loss: 110.63114\n",
      "Epoch 344 - lr: 0.05000 - Train loss: 9.98285 - Test loss: 110.63034\n",
      "Epoch 345 - lr: 0.05000 - Train loss: 9.98285 - Test loss: 110.62927\n",
      "Epoch 346 - lr: 0.05000 - Train loss: 9.98286 - Test loss: 110.62775\n",
      "Epoch 347 - lr: 0.05000 - Train loss: 9.98289 - Test loss: 110.62544\n",
      "Epoch 348 - lr: 0.05000 - Train loss: 9.98296 - Test loss: 110.62150\n",
      "Epoch 349 - lr: 0.05000 - Train loss: 9.98312 - Test loss: 110.61343\n",
      "Epoch 350 - lr: 0.05000 - Train loss: 9.98375 - Test loss: 110.58918\n",
      "Epoch 351 - lr: 0.05000 - Train loss: 10.78014 - Test loss: 104.80304\n",
      "Epoch 352 - lr: 0.05000 - Train loss: 11.47765 - Test loss: 109.66635\n",
      "Epoch 353 - lr: 0.05000 - Train loss: 9.97035 - Test loss: 110.58371\n",
      "Epoch 354 - lr: 0.05000 - Train loss: 9.98500 - Test loss: 110.60963\n",
      "Epoch 355 - lr: 0.05000 - Train loss: 9.99470 - Test loss: 110.59826\n",
      "Epoch 356 - lr: 0.05000 - Train loss: 10.01321 - Test loss: 110.62017\n",
      "Epoch 357 - lr: 0.05000 - Train loss: 9.98195 - Test loss: 110.62236\n",
      "Epoch 358 - lr: 0.05000 - Train loss: 9.98173 - Test loss: 110.62460\n",
      "Epoch 359 - lr: 0.05000 - Train loss: 9.98159 - Test loss: 110.62668\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 360 - lr: 0.05000 - Train loss: 9.98151 - Test loss: 110.62850\n",
      "Epoch 361 - lr: 0.05000 - Train loss: 9.98147 - Test loss: 110.63001\n",
      "Epoch 362 - lr: 0.05000 - Train loss: 9.98145 - Test loss: 110.63122\n",
      "Epoch 363 - lr: 0.05000 - Train loss: 9.98145 - Test loss: 110.63218\n",
      "Epoch 364 - lr: 0.05000 - Train loss: 9.98145 - Test loss: 110.63291\n",
      "Epoch 365 - lr: 0.05000 - Train loss: 9.98145 - Test loss: 110.63346\n",
      "Epoch 366 - lr: 0.05000 - Train loss: 9.98144 - Test loss: 110.63387\n",
      "Epoch 367 - lr: 0.05000 - Train loss: 9.98143 - Test loss: 110.63416\n",
      "Epoch 368 - lr: 0.05000 - Train loss: 9.98141 - Test loss: 110.63436\n",
      "Epoch 369 - lr: 0.05000 - Train loss: 9.98138 - Test loss: 110.63449\n",
      "Epoch 370 - lr: 0.05000 - Train loss: 9.98135 - Test loss: 110.63457\n",
      "Epoch 371 - lr: 0.05000 - Train loss: 9.98132 - Test loss: 110.63462\n",
      "Epoch 372 - lr: 0.05000 - Train loss: 9.98127 - Test loss: 110.63464\n",
      "Epoch 373 - lr: 0.05000 - Train loss: 9.98123 - Test loss: 110.63464\n",
      "Epoch 374 - lr: 0.05000 - Train loss: 9.98118 - Test loss: 110.63463\n",
      "Epoch 375 - lr: 0.05000 - Train loss: 9.98112 - Test loss: 110.63461\n",
      "Epoch 376 - lr: 0.05000 - Train loss: 9.98106 - Test loss: 110.63458\n",
      "Epoch 377 - lr: 0.05000 - Train loss: 9.98101 - Test loss: 110.63456\n",
      "Epoch 378 - lr: 0.05000 - Train loss: 9.98094 - Test loss: 110.63453\n",
      "Epoch 379 - lr: 0.05000 - Train loss: 9.98088 - Test loss: 110.63451\n",
      "Epoch 380 - lr: 0.05000 - Train loss: 9.98081 - Test loss: 110.63448\n",
      "Epoch 381 - lr: 0.05000 - Train loss: 9.98074 - Test loss: 110.63446\n",
      "Epoch 382 - lr: 0.05000 - Train loss: 9.98067 - Test loss: 110.63443\n",
      "Epoch 383 - lr: 0.05000 - Train loss: 9.98060 - Test loss: 110.63441\n",
      "Epoch 384 - lr: 0.05000 - Train loss: 9.98053 - Test loss: 110.63439\n",
      "Epoch 385 - lr: 0.05000 - Train loss: 9.98045 - Test loss: 110.63437\n",
      "Epoch 386 - lr: 0.05000 - Train loss: 9.98037 - Test loss: 110.63435\n",
      "Epoch 387 - lr: 0.05000 - Train loss: 9.98029 - Test loss: 110.63433\n",
      "Epoch 388 - lr: 0.05000 - Train loss: 9.98021 - Test loss: 110.63431\n",
      "Epoch 389 - lr: 0.05000 - Train loss: 9.98012 - Test loss: 110.63429\n",
      "Epoch 390 - lr: 0.05000 - Train loss: 9.98003 - Test loss: 110.63427\n",
      "Epoch 391 - lr: 0.05000 - Train loss: 9.97994 - Test loss: 110.63425\n",
      "Epoch 392 - lr: 0.05000 - Train loss: 9.97985 - Test loss: 110.63423\n",
      "Epoch 393 - lr: 0.05000 - Train loss: 9.97975 - Test loss: 110.63421\n",
      "Epoch 394 - lr: 0.05000 - Train loss: 9.97965 - Test loss: 110.63420\n",
      "Epoch 395 - lr: 0.05000 - Train loss: 9.97955 - Test loss: 110.63418\n",
      "Epoch 396 - lr: 0.05000 - Train loss: 9.97944 - Test loss: 110.63416\n",
      "Epoch 397 - lr: 0.05000 - Train loss: 9.97933 - Test loss: 110.63414\n",
      "Epoch 398 - lr: 0.05000 - Train loss: 9.97922 - Test loss: 110.63412\n",
      "Epoch 399 - lr: 0.05000 - Train loss: 9.97910 - Test loss: 110.63410\n",
      "Epoch 400 - lr: 0.05000 - Train loss: 9.97898 - Test loss: 110.63408\n",
      "Epoch 401 - lr: 0.05000 - Train loss: 9.97885 - Test loss: 110.63406\n",
      "Epoch 402 - lr: 0.05000 - Train loss: 9.97872 - Test loss: 110.63404\n",
      "Epoch 403 - lr: 0.05000 - Train loss: 9.97859 - Test loss: 110.63402\n",
      "Epoch 404 - lr: 0.05000 - Train loss: 9.97845 - Test loss: 110.63400\n",
      "Epoch 405 - lr: 0.05000 - Train loss: 9.97830 - Test loss: 110.63398\n",
      "Epoch 406 - lr: 0.05000 - Train loss: 9.97815 - Test loss: 110.63396\n",
      "Epoch 407 - lr: 0.05000 - Train loss: 9.97799 - Test loss: 110.63393\n",
      "Epoch 408 - lr: 0.05000 - Train loss: 9.97783 - Test loss: 110.63391\n",
      "Epoch 409 - lr: 0.05000 - Train loss: 9.97766 - Test loss: 110.63389\n",
      "Epoch 410 - lr: 0.05000 - Train loss: 9.97749 - Test loss: 110.63386\n",
      "Epoch 411 - lr: 0.05000 - Train loss: 9.97731 - Test loss: 110.63383\n",
      "Epoch 412 - lr: 0.05000 - Train loss: 9.97712 - Test loss: 110.63381\n",
      "Epoch 413 - lr: 0.05000 - Train loss: 9.97692 - Test loss: 110.63378\n",
      "Epoch 414 - lr: 0.05000 - Train loss: 9.97671 - Test loss: 110.63375\n",
      "Epoch 415 - lr: 0.05000 - Train loss: 9.97650 - Test loss: 110.63372\n",
      "Epoch 416 - lr: 0.05000 - Train loss: 9.97627 - Test loss: 110.63368\n",
      "Epoch 417 - lr: 0.05000 - Train loss: 9.97604 - Test loss: 110.63365\n",
      "Epoch 418 - lr: 0.05000 - Train loss: 9.97579 - Test loss: 110.63361\n",
      "Epoch 419 - lr: 0.05000 - Train loss: 9.97554 - Test loss: 110.63357\n",
      "Epoch 420 - lr: 0.05000 - Train loss: 9.97527 - Test loss: 110.63353\n",
      "Epoch 421 - lr: 0.05000 - Train loss: 9.97499 - Test loss: 110.63348\n",
      "Epoch 422 - lr: 0.05000 - Train loss: 9.97470 - Test loss: 110.63344\n",
      "Epoch 423 - lr: 0.05000 - Train loss: 9.97439 - Test loss: 110.63338\n",
      "Epoch 424 - lr: 0.05000 - Train loss: 9.97407 - Test loss: 110.63333\n",
      "Epoch 425 - lr: 0.05000 - Train loss: 9.97373 - Test loss: 110.63327\n",
      "Epoch 426 - lr: 0.05000 - Train loss: 9.97338 - Test loss: 110.63320\n",
      "Epoch 427 - lr: 0.05000 - Train loss: 9.97301 - Test loss: 110.63313\n",
      "Epoch 428 - lr: 0.05000 - Train loss: 9.97262 - Test loss: 110.63306\n",
      "Epoch 429 - lr: 0.05000 - Train loss: 9.97220 - Test loss: 110.63297\n",
      "Epoch 430 - lr: 0.05000 - Train loss: 9.97177 - Test loss: 110.63288\n",
      "Epoch 431 - lr: 0.05000 - Train loss: 9.97131 - Test loss: 110.63278\n",
      "Epoch 432 - lr: 0.05000 - Train loss: 9.97083 - Test loss: 110.63268\n",
      "Epoch 433 - lr: 0.05000 - Train loss: 9.97032 - Test loss: 110.63256\n",
      "Epoch 434 - lr: 0.05000 - Train loss: 9.96978 - Test loss: 110.63243\n",
      "Epoch 435 - lr: 0.05000 - Train loss: 9.96920 - Test loss: 110.63229\n",
      "Epoch 436 - lr: 0.05000 - Train loss: 9.96860 - Test loss: 110.63213\n",
      "Epoch 437 - lr: 0.05000 - Train loss: 9.96796 - Test loss: 110.63196\n",
      "Epoch 438 - lr: 0.05000 - Train loss: 9.96728 - Test loss: 110.63177\n",
      "Epoch 439 - lr: 0.05000 - Train loss: 9.96655 - Test loss: 110.63155\n",
      "Epoch 440 - lr: 0.05000 - Train loss: 9.96578 - Test loss: 110.63132\n",
      "Epoch 441 - lr: 0.05000 - Train loss: 9.96497 - Test loss: 110.63106\n",
      "Epoch 442 - lr: 0.05000 - Train loss: 9.96409 - Test loss: 110.63078\n",
      "Epoch 443 - lr: 0.05000 - Train loss: 9.96316 - Test loss: 110.63046\n",
      "Epoch 444 - lr: 0.05000 - Train loss: 9.96217 - Test loss: 110.63011\n",
      "Epoch 445 - lr: 0.05000 - Train loss: 9.96111 - Test loss: 110.62971\n",
      "Epoch 446 - lr: 0.05000 - Train loss: 9.95998 - Test loss: 110.62928\n",
      "Epoch 447 - lr: 0.05000 - Train loss: 9.95877 - Test loss: 110.62879\n",
      "Epoch 448 - lr: 0.05000 - Train loss: 9.95747 - Test loss: 110.62825\n",
      "Epoch 449 - lr: 0.05000 - Train loss: 9.95608 - Test loss: 110.62765\n",
      "Epoch 450 - lr: 0.05000 - Train loss: 9.95459 - Test loss: 110.62699\n",
      "Epoch 451 - lr: 0.05000 - Train loss: 9.95298 - Test loss: 110.62624\n",
      "Epoch 452 - lr: 0.05000 - Train loss: 9.95126 - Test loss: 110.62542\n",
      "Epoch 453 - lr: 0.05000 - Train loss: 9.94941 - Test loss: 110.62450\n",
      "Epoch 454 - lr: 0.05000 - Train loss: 9.94742 - Test loss: 110.62347\n",
      "Epoch 455 - lr: 0.05000 - Train loss: 9.94527 - Test loss: 110.62234\n",
      "Epoch 456 - lr: 0.05000 - Train loss: 9.94296 - Test loss: 110.62108\n",
      "Epoch 457 - lr: 0.05000 - Train loss: 9.94048 - Test loss: 110.61968\n",
      "Epoch 458 - lr: 0.05000 - Train loss: 9.93779 - Test loss: 110.61813\n",
      "Epoch 459 - lr: 0.05000 - Train loss: 9.93490 - Test loss: 110.61642\n",
      "Epoch 460 - lr: 0.05000 - Train loss: 9.93178 - Test loss: 110.61453\n",
      "Epoch 461 - lr: 0.05000 - Train loss: 9.92842 - Test loss: 110.61245\n",
      "Epoch 462 - lr: 0.05000 - Train loss: 9.92478 - Test loss: 110.61017\n",
      "Epoch 463 - lr: 0.05000 - Train loss: 9.92086 - Test loss: 110.60765\n",
      "Epoch 464 - lr: 0.05000 - Train loss: 9.91662 - Test loss: 110.60490\n",
      "Epoch 465 - lr: 0.05000 - Train loss: 9.91204 - Test loss: 110.60188\n",
      "Epoch 466 - lr: 0.05000 - Train loss: 9.90708 - Test loss: 110.59857\n",
      "Epoch 467 - lr: 0.05000 - Train loss: 9.90173 - Test loss: 110.59493\n",
      "Epoch 468 - lr: 0.05000 - Train loss: 9.89595 - Test loss: 110.59093\n",
      "Epoch 469 - lr: 0.05000 - Train loss: 9.88973 - Test loss: 110.58650\n",
      "Epoch 470 - lr: 0.05000 - Train loss: 9.88306 - Test loss: 110.58154\n",
      "Epoch 471 - lr: 0.05000 - Train loss: 9.87597 - Test loss: 110.57595\n",
      "Epoch 472 - lr: 0.05000 - Train loss: 9.86854 - Test loss: 110.56957\n",
      "Epoch 473 - lr: 0.05000 - Train loss: 9.86090 - Test loss: 110.56222\n",
      "Epoch 474 - lr: 0.05000 - Train loss: 9.85329 - Test loss: 110.55355\n",
      "Epoch 475 - lr: 0.05000 - Train loss: 9.84609 - Test loss: 110.54255\n",
      "Epoch 476 - lr: 0.05000 - Train loss: 9.84125 - Test loss: 110.52415\n",
      "Epoch 477 - lr: 0.05000 - Train loss: 9.39126 - Test loss: 112.98537\n",
      "Epoch 478 - lr: 0.05000 - Train loss: 8.79347 - Test loss: 113.16570\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 479 - lr: 0.05000 - Train loss: 8.92139 - Test loss: 113.11375\n",
      "Epoch 480 - lr: 0.05000 - Train loss: 8.88335 - Test loss: 112.71961\n",
      "Epoch 481 - lr: 0.05000 - Train loss: 9.24411 - Test loss: 112.43506\n",
      "Epoch 482 - lr: 0.05000 - Train loss: 9.71214 - Test loss: 110.66151\n",
      "Epoch 483 - lr: 0.05000 - Train loss: 9.21086 - Test loss: 114.85042\n",
      "Epoch 484 - lr: 0.05000 - Train loss: 9.01600 - Test loss: 113.57795\n",
      "Epoch 485 - lr: 0.05000 - Train loss: 9.42395 - Test loss: 110.64667\n",
      "Epoch 486 - lr: 0.05000 - Train loss: 9.93932 - Test loss: 110.67530\n",
      "Epoch 487 - lr: 0.05000 - Train loss: 9.96478 - Test loss: 110.66413\n",
      "Epoch 488 - lr: 0.05000 - Train loss: 9.98773 - Test loss: 110.62255\n",
      "Epoch 489 - lr: 0.05000 - Train loss: 9.98573 - Test loss: 110.53190\n",
      "Epoch 490 - lr: 0.05000 - Train loss: 9.92699 - Test loss: 110.69485\n",
      "Epoch 491 - lr: 0.05000 - Train loss: 9.89453 - Test loss: 110.72003\n",
      "Epoch 492 - lr: 0.05000 - Train loss: 9.99404 - Test loss: 110.52040\n",
      "Epoch 493 - lr: 0.05000 - Train loss: 9.98106 - Test loss: 110.67619\n",
      "Epoch 494 - lr: 0.05000 - Train loss: 9.98720 - Test loss: 110.65438\n",
      "Epoch 495 - lr: 0.05000 - Train loss: 9.93594 - Test loss: 110.65803\n",
      "Epoch 496 - lr: 0.05000 - Train loss: 9.99529 - Test loss: 110.47200\n",
      "Epoch 497 - lr: 0.05000 - Train loss: 10.00400 - Test loss: 110.08043\n",
      "Epoch 498 - lr: 0.05000 - Train loss: 9.46065 - Test loss: 109.77510\n",
      "Epoch 499 - lr: 0.05000 - Train loss: 9.33570 - Test loss: 110.68944\n",
      "Epoch 500 - lr: 0.05000 - Train loss: 9.99119 - Test loss: 110.67608\n",
      "Epoch 501 - lr: 0.05000 - Train loss: 9.99043 - Test loss: 110.62153\n",
      "Epoch 502 - lr: 0.05000 - Train loss: 9.99592 - Test loss: 109.61719\n",
      "Epoch 503 - lr: 0.05000 - Train loss: 9.73484 - Test loss: 110.69388\n",
      "Epoch 504 - lr: 0.05000 - Train loss: 9.99073 - Test loss: 110.68759\n",
      "Epoch 505 - lr: 0.05000 - Train loss: 9.98970 - Test loss: 110.67050\n",
      "Epoch 506 - lr: 0.05000 - Train loss: 10.00986 - Test loss: 110.08144\n",
      "Epoch 507 - lr: 0.05000 - Train loss: 9.88079 - Test loss: 110.12128\n",
      "Epoch 508 - lr: 0.05000 - Train loss: 9.43235 - Test loss: 113.23430\n",
      "Epoch 509 - lr: 0.05000 - Train loss: 9.13391 - Test loss: 91.21582\n",
      "Epoch 510 - lr: 0.05000 - Train loss: 8.13762 - Test loss: 75.25702\n",
      "Epoch 511 - lr: 0.05000 - Train loss: 8.61180 - Test loss: 110.70341\n",
      "Epoch 512 - lr: 0.05000 - Train loss: 9.98766 - Test loss: 110.70344\n",
      "Epoch 513 - lr: 0.05000 - Train loss: 9.98769 - Test loss: 110.70261\n",
      "Epoch 514 - lr: 0.05000 - Train loss: 9.98681 - Test loss: 110.70270\n",
      "Epoch 515 - lr: 0.05000 - Train loss: 9.98704 - Test loss: 110.70153\n",
      "Epoch 516 - lr: 0.05000 - Train loss: 9.98554 - Test loss: 110.70243\n",
      "Epoch 517 - lr: 0.05000 - Train loss: 9.98716 - Test loss: 110.69946\n",
      "Epoch 518 - lr: 0.05000 - Train loss: 9.98297 - Test loss: 110.70453\n",
      "Epoch 519 - lr: 0.05000 - Train loss: 9.98991 - Test loss: 110.69732\n",
      "Epoch 520 - lr: 0.05000 - Train loss: 9.98230 - Test loss: 110.70335\n",
      "Epoch 521 - lr: 0.05000 - Train loss: 9.98933 - Test loss: 110.69514\n",
      "Epoch 522 - lr: 0.05000 - Train loss: 9.98019 - Test loss: 110.70521\n",
      "Epoch 523 - lr: 0.05000 - Train loss: 9.99060 - Test loss: 110.69615\n",
      "Epoch 524 - lr: 0.05000 - Train loss: 9.98387 - Test loss: 110.69534\n",
      "Epoch 525 - lr: 0.05000 - Train loss: 9.97887 - Test loss: 110.70333\n",
      "Epoch 526 - lr: 0.05000 - Train loss: 9.99004 - Test loss: 110.69197\n",
      "Epoch 527 - lr: 0.05000 - Train loss: 9.97796 - Test loss: 110.70149\n",
      "Epoch 528 - lr: 0.05000 - Train loss: 9.98933 - Test loss: 110.68791\n",
      "Epoch 529 - lr: 0.05000 - Train loss: 9.97928 - Test loss: 110.70050\n",
      "Epoch 530 - lr: 0.05000 - Train loss: 9.98934 - Test loss: 110.68495\n",
      "Epoch 531 - lr: 0.05000 - Train loss: 9.97838 - Test loss: 110.70580\n",
      "Epoch 532 - lr: 0.05000 - Train loss: 9.99132 - Test loss: 110.69364\n",
      "Epoch 533 - lr: 0.05000 - Train loss: 9.99053 - Test loss: 110.68583\n",
      "Epoch 534 - lr: 0.05000 - Train loss: 9.98826 - Test loss: 110.66464\n",
      "Epoch 535 - lr: 0.05000 - Train loss: 9.95304 - Test loss: 110.63737\n",
      "Epoch 536 - lr: 0.05000 - Train loss: 9.77881 - Test loss: 113.68968\n",
      "Epoch 537 - lr: 0.05000 - Train loss: 9.05138 - Test loss: 113.68117\n",
      "Epoch 538 - lr: 0.05000 - Train loss: 9.05114 - Test loss: 113.68053\n",
      "Epoch 539 - lr: 0.05000 - Train loss: 9.05107 - Test loss: 113.67989\n",
      "Epoch 540 - lr: 0.05000 - Train loss: 9.05099 - Test loss: 113.67924\n",
      "Epoch 541 - lr: 0.05000 - Train loss: 9.05089 - Test loss: 113.67861\n",
      "Epoch 542 - lr: 0.05000 - Train loss: 9.05076 - Test loss: 113.67802\n",
      "Epoch 543 - lr: 0.05000 - Train loss: 9.05060 - Test loss: 113.67753\n",
      "Epoch 544 - lr: 0.05000 - Train loss: 9.05040 - Test loss: 113.67721\n",
      "Epoch 545 - lr: 0.05000 - Train loss: 9.05015 - Test loss: 113.67709\n",
      "Epoch 546 - lr: 0.05000 - Train loss: 9.04990 - Test loss: 113.67714\n",
      "Epoch 547 - lr: 0.05000 - Train loss: 9.04965 - Test loss: 113.67725\n",
      "Epoch 548 - lr: 0.05000 - Train loss: 9.04945 - Test loss: 113.67732\n",
      "Epoch 549 - lr: 0.05000 - Train loss: 9.04926 - Test loss: 113.67728\n",
      "Epoch 550 - lr: 0.05000 - Train loss: 9.04908 - Test loss: 113.67716\n",
      "Epoch 551 - lr: 0.05000 - Train loss: 9.04889 - Test loss: 113.67694\n",
      "Epoch 552 - lr: 0.05000 - Train loss: 9.04868 - Test loss: 113.67666\n",
      "Epoch 553 - lr: 0.05000 - Train loss: 9.04845 - Test loss: 113.67633\n",
      "Epoch 554 - lr: 0.05000 - Train loss: 9.04820 - Test loss: 113.67594\n",
      "Epoch 555 - lr: 0.05000 - Train loss: 9.04792 - Test loss: 113.67551\n",
      "Epoch 556 - lr: 0.05000 - Train loss: 9.04763 - Test loss: 113.67504\n",
      "Epoch 557 - lr: 0.05000 - Train loss: 9.04732 - Test loss: 113.67453\n",
      "Epoch 558 - lr: 0.05000 - Train loss: 9.04699 - Test loss: 113.67398\n",
      "Epoch 559 - lr: 0.05000 - Train loss: 9.04663 - Test loss: 113.67339\n",
      "Epoch 560 - lr: 0.05000 - Train loss: 9.04626 - Test loss: 113.67277\n",
      "Epoch 561 - lr: 0.05000 - Train loss: 9.04586 - Test loss: 113.67211\n",
      "Epoch 562 - lr: 0.05000 - Train loss: 9.04543 - Test loss: 113.67142\n",
      "Epoch 563 - lr: 0.05000 - Train loss: 9.04498 - Test loss: 113.67069\n",
      "Epoch 564 - lr: 0.05000 - Train loss: 9.04450 - Test loss: 113.66993\n",
      "Epoch 565 - lr: 0.05000 - Train loss: 9.04398 - Test loss: 113.66912\n",
      "Epoch 566 - lr: 0.05000 - Train loss: 9.04344 - Test loss: 113.66826\n",
      "Epoch 567 - lr: 0.05000 - Train loss: 9.04285 - Test loss: 113.66734\n",
      "Epoch 568 - lr: 0.05000 - Train loss: 9.04222 - Test loss: 113.66633\n",
      "Epoch 569 - lr: 0.05000 - Train loss: 9.04155 - Test loss: 113.66521\n",
      "Epoch 570 - lr: 0.05000 - Train loss: 9.04083 - Test loss: 113.66392\n",
      "Epoch 571 - lr: 0.05000 - Train loss: 9.04005 - Test loss: 113.66234\n",
      "Epoch 572 - lr: 0.05000 - Train loss: 9.03919 - Test loss: 113.66029\n",
      "Epoch 573 - lr: 0.05000 - Train loss: 9.03826 - Test loss: 113.65710\n",
      "Epoch 574 - lr: 0.05000 - Train loss: 9.03707 - Test loss: 113.65113\n",
      "Epoch 575 - lr: 0.05000 - Train loss: 9.03480 - Test loss: 113.62824\n",
      "Epoch 576 - lr: 0.05000 - Train loss: 9.00264 - Test loss: 113.67200\n",
      "Epoch 577 - lr: 0.05000 - Train loss: 9.03550 - Test loss: 113.67028\n",
      "Epoch 578 - lr: 0.05000 - Train loss: 9.04329 - Test loss: 113.64166\n",
      "Epoch 579 - lr: 0.05000 - Train loss: 8.78630 - Test loss: 113.93849\n",
      "Epoch 580 - lr: 0.05000 - Train loss: 8.67345 - Test loss: 115.23352\n",
      "Epoch 581 - lr: 0.05000 - Train loss: 8.75777 - Test loss: 108.53725\n",
      "Epoch 582 - lr: 0.05000 - Train loss: 8.74015 - Test loss: 113.68226\n",
      "Epoch 583 - lr: 0.05000 - Train loss: 9.05108 - Test loss: 113.68054\n",
      "Epoch 584 - lr: 0.05000 - Train loss: 9.05094 - Test loss: 113.67723\n",
      "Epoch 585 - lr: 0.05000 - Train loss: 9.05063 - Test loss: 113.66810\n",
      "Epoch 586 - lr: 0.05000 - Train loss: 9.04926 - Test loss: 113.58486\n",
      "Epoch 587 - lr: 0.05000 - Train loss: 8.88714 - Test loss: 113.59238\n",
      "Epoch 588 - lr: 0.05000 - Train loss: 8.71498 - Test loss: 113.68674\n",
      "Epoch 589 - lr: 0.05000 - Train loss: 9.05138 - Test loss: 113.68671\n",
      "Epoch 590 - lr: 0.05000 - Train loss: 9.05138 - Test loss: 113.68667\n",
      "Epoch 591 - lr: 0.05000 - Train loss: 9.05138 - Test loss: 113.68663\n",
      "Epoch 592 - lr: 0.05000 - Train loss: 9.05137 - Test loss: 113.68660\n",
      "Epoch 593 - lr: 0.05000 - Train loss: 9.05137 - Test loss: 113.68656\n",
      "Epoch 594 - lr: 0.05000 - Train loss: 9.05137 - Test loss: 113.68653\n",
      "Epoch 595 - lr: 0.05000 - Train loss: 9.05136 - Test loss: 113.68650\n",
      "Epoch 596 - lr: 0.05000 - Train loss: 9.05136 - Test loss: 113.68647\n",
      "Epoch 597 - lr: 0.05000 - Train loss: 9.05136 - Test loss: 113.68644\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 598 - lr: 0.05000 - Train loss: 9.05136 - Test loss: 113.68642\n",
      "Epoch 599 - lr: 0.05000 - Train loss: 9.05135 - Test loss: 113.68639\n",
      "Epoch 600 - lr: 0.05000 - Train loss: 9.05135 - Test loss: 113.68636\n",
      "Epoch 601 - lr: 0.05000 - Train loss: 9.05135 - Test loss: 113.68634\n",
      "Epoch 602 - lr: 0.05000 - Train loss: 9.05135 - Test loss: 113.68631\n",
      "Epoch 603 - lr: 0.05000 - Train loss: 9.05134 - Test loss: 113.68629\n",
      "Epoch 604 - lr: 0.05000 - Train loss: 9.05134 - Test loss: 113.68626\n",
      "Epoch 605 - lr: 0.05000 - Train loss: 9.05134 - Test loss: 113.68624\n",
      "Epoch 606 - lr: 0.05000 - Train loss: 9.05134 - Test loss: 113.68622\n",
      "Epoch 607 - lr: 0.05000 - Train loss: 9.05133 - Test loss: 113.68619\n",
      "Epoch 608 - lr: 0.05000 - Train loss: 9.05133 - Test loss: 113.68617\n",
      "Epoch 609 - lr: 0.05000 - Train loss: 9.05133 - Test loss: 113.68615\n",
      "Epoch 610 - lr: 0.05000 - Train loss: 9.05133 - Test loss: 113.68613\n",
      "Epoch 611 - lr: 0.05000 - Train loss: 9.05133 - Test loss: 113.68610\n",
      "Epoch 612 - lr: 0.05000 - Train loss: 9.05132 - Test loss: 113.68608\n",
      "Epoch 613 - lr: 0.05000 - Train loss: 9.05132 - Test loss: 113.68606\n",
      "Epoch 614 - lr: 0.05000 - Train loss: 9.05132 - Test loss: 113.68604\n",
      "Epoch 615 - lr: 0.05000 - Train loss: 9.05132 - Test loss: 113.68602\n",
      "Epoch 616 - lr: 0.05000 - Train loss: 9.05131 - Test loss: 113.68599\n",
      "Epoch 617 - lr: 0.05000 - Train loss: 9.05131 - Test loss: 113.68597\n",
      "Epoch 618 - lr: 0.05000 - Train loss: 9.05131 - Test loss: 113.68595\n",
      "Epoch 619 - lr: 0.05000 - Train loss: 9.05131 - Test loss: 113.68593\n",
      "Epoch 620 - lr: 0.05000 - Train loss: 9.05131 - Test loss: 113.68591\n",
      "Epoch 621 - lr: 0.05000 - Train loss: 9.05130 - Test loss: 113.68588\n",
      "Epoch 622 - lr: 0.05000 - Train loss: 9.05130 - Test loss: 113.68586\n",
      "Epoch 623 - lr: 0.05000 - Train loss: 9.05130 - Test loss: 113.68584\n",
      "Epoch 624 - lr: 0.05000 - Train loss: 9.05130 - Test loss: 113.68582\n",
      "Epoch 625 - lr: 0.05000 - Train loss: 9.05129 - Test loss: 113.68579\n",
      "Epoch 626 - lr: 0.05000 - Train loss: 9.05129 - Test loss: 113.68577\n",
      "Epoch 627 - lr: 0.05000 - Train loss: 9.05129 - Test loss: 113.68574\n",
      "Epoch 628 - lr: 0.05000 - Train loss: 9.05129 - Test loss: 113.68572\n",
      "Epoch 629 - lr: 0.05000 - Train loss: 9.05128 - Test loss: 113.68569\n",
      "Epoch 630 - lr: 0.05000 - Train loss: 9.05128 - Test loss: 113.68567\n",
      "Epoch 631 - lr: 0.05000 - Train loss: 9.05128 - Test loss: 113.68564\n",
      "Epoch 632 - lr: 0.05000 - Train loss: 9.05127 - Test loss: 113.68562\n",
      "Epoch 633 - lr: 0.05000 - Train loss: 9.05127 - Test loss: 113.68559\n",
      "Epoch 634 - lr: 0.05000 - Train loss: 9.05127 - Test loss: 113.68556\n",
      "Epoch 635 - lr: 0.05000 - Train loss: 9.05127 - Test loss: 113.68553\n",
      "Epoch 636 - lr: 0.05000 - Train loss: 9.05126 - Test loss: 113.68550\n",
      "Epoch 637 - lr: 0.05000 - Train loss: 9.05126 - Test loss: 113.68547\n",
      "Epoch 638 - lr: 0.05000 - Train loss: 9.05125 - Test loss: 113.68544\n",
      "Epoch 639 - lr: 0.05000 - Train loss: 9.05125 - Test loss: 113.68541\n",
      "Epoch 640 - lr: 0.05000 - Train loss: 9.05125 - Test loss: 113.68537\n",
      "Epoch 641 - lr: 0.05000 - Train loss: 9.05124 - Test loss: 113.68534\n",
      "Epoch 642 - lr: 0.05000 - Train loss: 9.05124 - Test loss: 113.68530\n",
      "Epoch 643 - lr: 0.05000 - Train loss: 9.05123 - Test loss: 113.68526\n",
      "Epoch 644 - lr: 0.05000 - Train loss: 9.05123 - Test loss: 113.68522\n",
      "Epoch 645 - lr: 0.05000 - Train loss: 9.05122 - Test loss: 113.68517\n",
      "Epoch 646 - lr: 0.05000 - Train loss: 9.05121 - Test loss: 113.68513\n",
      "Epoch 647 - lr: 0.05000 - Train loss: 9.05121 - Test loss: 113.68508\n",
      "Epoch 648 - lr: 0.05000 - Train loss: 9.05120 - Test loss: 113.68503\n",
      "Epoch 649 - lr: 0.05000 - Train loss: 9.05119 - Test loss: 113.68498\n",
      "Epoch 650 - lr: 0.05000 - Train loss: 9.05118 - Test loss: 113.68492\n",
      "Epoch 651 - lr: 0.05000 - Train loss: 9.05117 - Test loss: 113.68486\n",
      "Epoch 652 - lr: 0.05000 - Train loss: 9.05116 - Test loss: 113.68479\n",
      "Epoch 653 - lr: 0.05000 - Train loss: 9.05115 - Test loss: 113.68472\n",
      "Epoch 654 - lr: 0.05000 - Train loss: 9.05114 - Test loss: 113.68464\n",
      "Epoch 655 - lr: 0.05000 - Train loss: 9.05112 - Test loss: 113.68456\n",
      "Epoch 656 - lr: 0.05000 - Train loss: 9.05110 - Test loss: 113.68446\n",
      "Epoch 657 - lr: 0.05000 - Train loss: 9.05108 - Test loss: 113.68436\n",
      "Epoch 658 - lr: 0.05000 - Train loss: 9.05105 - Test loss: 113.68424\n",
      "Epoch 659 - lr: 0.05000 - Train loss: 9.05102 - Test loss: 113.68410\n",
      "Epoch 660 - lr: 0.05000 - Train loss: 9.05097 - Test loss: 113.68394\n",
      "Epoch 661 - lr: 0.05000 - Train loss: 9.05090 - Test loss: 113.68374\n",
      "Epoch 662 - lr: 0.05000 - Train loss: 9.05080 - Test loss: 113.68347\n",
      "Epoch 663 - lr: 0.05000 - Train loss: 9.05063 - Test loss: 113.68305\n",
      "Epoch 664 - lr: 0.05000 - Train loss: 9.05025 - Test loss: 113.68208\n",
      "Epoch 665 - lr: 0.05000 - Train loss: 9.04933 - Test loss: 113.66727\n",
      "Epoch 666 - lr: 0.05000 - Train loss: 9.03979 - Test loss: 113.67593\n",
      "Epoch 667 - lr: 0.05000 - Train loss: 9.57695 - Test loss: 110.71240\n",
      "Epoch 668 - lr: 0.05000 - Train loss: 9.99221 - Test loss: 110.71048\n",
      "Epoch 669 - lr: 0.05000 - Train loss: 9.99212 - Test loss: 110.70971\n",
      "Epoch 670 - lr: 0.05000 - Train loss: 9.99205 - Test loss: 110.70880\n",
      "Epoch 671 - lr: 0.05000 - Train loss: 9.99198 - Test loss: 110.70762\n",
      "Epoch 672 - lr: 0.05000 - Train loss: 9.99187 - Test loss: 110.70590\n",
      "Epoch 673 - lr: 0.05000 - Train loss: 9.99171 - Test loss: 110.70295\n",
      "Epoch 674 - lr: 0.05000 - Train loss: 9.99139 - Test loss: 110.69608\n",
      "Epoch 675 - lr: 0.05000 - Train loss: 9.99019 - Test loss: 110.64924\n",
      "Epoch 676 - lr: 0.05000 - Train loss: 9.74295 - Test loss: 93.70983\n",
      "Epoch 677 - lr: 0.05000 - Train loss: 8.48365 - Test loss: 113.72736\n",
      "Epoch 678 - lr: 0.05000 - Train loss: 9.06573 - Test loss: 109.06564\n",
      "Epoch 679 - lr: 0.05000 - Train loss: 8.62135 - Test loss: 103.66613\n",
      "Epoch 680 - lr: 0.05000 - Train loss: 8.50476 - Test loss: 113.68557\n",
      "Epoch 681 - lr: 0.05000 - Train loss: 9.05174 - Test loss: 113.68570\n",
      "Epoch 682 - lr: 0.05000 - Train loss: 9.05169 - Test loss: 113.68579\n",
      "Epoch 683 - lr: 0.05000 - Train loss: 9.05165 - Test loss: 113.68585\n",
      "Epoch 684 - lr: 0.05000 - Train loss: 9.05162 - Test loss: 113.68589\n",
      "Epoch 685 - lr: 0.05000 - Train loss: 9.05159 - Test loss: 113.68593\n",
      "Epoch 686 - lr: 0.05000 - Train loss: 9.05157 - Test loss: 113.68595\n",
      "Epoch 687 - lr: 0.05000 - Train loss: 9.05156 - Test loss: 113.68597\n",
      "Epoch 688 - lr: 0.05000 - Train loss: 9.05154 - Test loss: 113.68599\n",
      "Epoch 689 - lr: 0.05000 - Train loss: 9.05153 - Test loss: 113.68600\n",
      "Epoch 690 - lr: 0.05000 - Train loss: 9.05151 - Test loss: 113.68601\n",
      "Epoch 691 - lr: 0.05000 - Train loss: 9.05150 - Test loss: 113.68602\n",
      "Epoch 692 - lr: 0.05000 - Train loss: 9.05149 - Test loss: 113.68602\n",
      "Epoch 693 - lr: 0.05000 - Train loss: 9.05149 - Test loss: 113.68603\n",
      "Epoch 694 - lr: 0.05000 - Train loss: 9.05148 - Test loss: 113.68603\n",
      "Epoch 695 - lr: 0.05000 - Train loss: 9.05147 - Test loss: 113.68603\n",
      "Epoch 696 - lr: 0.05000 - Train loss: 9.05146 - Test loss: 113.68603\n",
      "Epoch 697 - lr: 0.05000 - Train loss: 9.05146 - Test loss: 113.68603\n",
      "Epoch 698 - lr: 0.05000 - Train loss: 9.05145 - Test loss: 113.68604\n",
      "Epoch 699 - lr: 0.05000 - Train loss: 9.05145 - Test loss: 113.68604\n",
      "Epoch 700 - lr: 0.05000 - Train loss: 9.05144 - Test loss: 113.68603\n",
      "Epoch 701 - lr: 0.05000 - Train loss: 9.05144 - Test loss: 113.68603\n",
      "Epoch 702 - lr: 0.05000 - Train loss: 9.05144 - Test loss: 113.68603\n",
      "Epoch 703 - lr: 0.05000 - Train loss: 9.05143 - Test loss: 113.68603\n",
      "Epoch 704 - lr: 0.05000 - Train loss: 9.05143 - Test loss: 113.68603\n",
      "Epoch 705 - lr: 0.05000 - Train loss: 9.05142 - Test loss: 113.68603\n",
      "Epoch 706 - lr: 0.05000 - Train loss: 9.05142 - Test loss: 113.68603\n",
      "Epoch 707 - lr: 0.05000 - Train loss: 9.05142 - Test loss: 113.68603\n",
      "Epoch 708 - lr: 0.05000 - Train loss: 9.05142 - Test loss: 113.68602\n",
      "Epoch 709 - lr: 0.05000 - Train loss: 9.05141 - Test loss: 113.68602\n",
      "Epoch 710 - lr: 0.05000 - Train loss: 9.05141 - Test loss: 113.68602\n",
      "Epoch 711 - lr: 0.05000 - Train loss: 9.05141 - Test loss: 113.68602\n",
      "Epoch 712 - lr: 0.05000 - Train loss: 9.05141 - Test loss: 113.68601\n",
      "Epoch 713 - lr: 0.05000 - Train loss: 9.05140 - Test loss: 113.68601\n",
      "Epoch 714 - lr: 0.05000 - Train loss: 9.05140 - Test loss: 113.68601\n",
      "Epoch 715 - lr: 0.05000 - Train loss: 9.05140 - Test loss: 113.68601\n",
      "Epoch 716 - lr: 0.05000 - Train loss: 9.05140 - Test loss: 113.68600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 717 - lr: 0.05000 - Train loss: 9.05140 - Test loss: 113.68600\n",
      "Epoch 718 - lr: 0.05000 - Train loss: 9.05139 - Test loss: 113.68600\n",
      "Epoch 719 - lr: 0.05000 - Train loss: 9.05139 - Test loss: 113.68599\n",
      "Epoch 720 - lr: 0.05000 - Train loss: 9.05139 - Test loss: 113.68599\n",
      "Epoch 721 - lr: 0.05000 - Train loss: 9.05139 - Test loss: 113.68599\n",
      "Epoch 722 - lr: 0.05000 - Train loss: 9.05139 - Test loss: 113.68599\n",
      "Epoch 723 - lr: 0.05000 - Train loss: 9.05139 - Test loss: 113.68598\n",
      "Epoch 724 - lr: 0.05000 - Train loss: 9.05138 - Test loss: 113.68598\n",
      "Epoch 725 - lr: 0.05000 - Train loss: 9.05138 - Test loss: 113.68598\n",
      "Epoch 726 - lr: 0.05000 - Train loss: 9.05138 - Test loss: 113.68597\n",
      "Epoch 727 - lr: 0.05000 - Train loss: 9.05138 - Test loss: 113.68597\n",
      "Epoch 728 - lr: 0.05000 - Train loss: 9.05138 - Test loss: 113.68597\n",
      "Epoch 729 - lr: 0.05000 - Train loss: 9.05138 - Test loss: 113.68596\n",
      "Epoch 730 - lr: 0.05000 - Train loss: 9.05138 - Test loss: 113.68596\n",
      "Epoch 731 - lr: 0.05000 - Train loss: 9.05138 - Test loss: 113.68596\n",
      "Epoch 732 - lr: 0.05000 - Train loss: 9.05137 - Test loss: 113.68595\n",
      "Epoch 733 - lr: 0.05000 - Train loss: 9.05137 - Test loss: 113.68595\n",
      "Epoch 734 - lr: 0.05000 - Train loss: 9.05137 - Test loss: 113.68595\n",
      "Epoch 735 - lr: 0.05000 - Train loss: 9.05137 - Test loss: 113.68594\n",
      "Epoch 736 - lr: 0.05000 - Train loss: 9.05137 - Test loss: 113.68594\n",
      "Epoch 737 - lr: 0.05000 - Train loss: 9.05137 - Test loss: 113.68594\n",
      "Epoch 738 - lr: 0.05000 - Train loss: 9.05137 - Test loss: 113.68593\n",
      "Epoch 739 - lr: 0.05000 - Train loss: 9.05137 - Test loss: 113.68593\n",
      "Epoch 740 - lr: 0.05000 - Train loss: 9.05137 - Test loss: 113.68593\n",
      "Epoch 741 - lr: 0.05000 - Train loss: 9.05137 - Test loss: 113.68592\n",
      "Epoch 742 - lr: 0.05000 - Train loss: 9.05136 - Test loss: 113.68592\n",
      "Epoch 743 - lr: 0.05000 - Train loss: 9.05136 - Test loss: 113.68592\n",
      "Epoch 744 - lr: 0.05000 - Train loss: 9.05136 - Test loss: 113.68591\n",
      "Epoch 745 - lr: 0.05000 - Train loss: 9.05136 - Test loss: 113.68591\n",
      "Epoch 746 - lr: 0.05000 - Train loss: 9.05136 - Test loss: 113.68591\n",
      "Epoch 747 - lr: 0.05000 - Train loss: 9.05136 - Test loss: 113.68590\n",
      "Epoch 748 - lr: 0.05000 - Train loss: 9.05136 - Test loss: 113.68590\n",
      "Epoch 749 - lr: 0.05000 - Train loss: 9.05136 - Test loss: 113.68590\n",
      "Epoch 750 - lr: 0.05000 - Train loss: 9.05136 - Test loss: 113.68589\n",
      "Epoch 751 - lr: 0.05000 - Train loss: 9.05136 - Test loss: 113.68589\n",
      "Epoch 752 - lr: 0.05000 - Train loss: 9.05136 - Test loss: 113.68588\n",
      "Epoch 753 - lr: 0.05000 - Train loss: 9.05136 - Test loss: 113.68588\n",
      "Epoch 754 - lr: 0.05000 - Train loss: 9.05136 - Test loss: 113.68588\n",
      "Epoch 755 - lr: 0.05000 - Train loss: 9.05135 - Test loss: 113.68587\n",
      "Epoch 756 - lr: 0.05000 - Train loss: 9.05135 - Test loss: 113.68587\n",
      "Epoch 757 - lr: 0.05000 - Train loss: 9.05135 - Test loss: 113.68587\n",
      "Epoch 758 - lr: 0.05000 - Train loss: 9.05135 - Test loss: 113.68586\n",
      "Epoch 759 - lr: 0.05000 - Train loss: 9.05135 - Test loss: 113.68586\n",
      "Epoch 760 - lr: 0.05000 - Train loss: 9.05135 - Test loss: 113.68586\n",
      "Epoch 761 - lr: 0.05000 - Train loss: 9.05135 - Test loss: 113.68585\n",
      "Epoch 762 - lr: 0.05000 - Train loss: 9.05135 - Test loss: 113.68585\n",
      "Epoch 763 - lr: 0.05000 - Train loss: 9.05135 - Test loss: 113.68585\n",
      "Epoch 764 - lr: 0.05000 - Train loss: 9.05135 - Test loss: 113.68584\n",
      "Epoch 765 - lr: 0.05000 - Train loss: 9.05135 - Test loss: 113.68584\n",
      "Epoch 766 - lr: 0.05000 - Train loss: 9.05135 - Test loss: 113.68583\n",
      "Epoch 767 - lr: 0.05000 - Train loss: 9.05135 - Test loss: 113.68583\n",
      "Epoch 768 - lr: 0.05000 - Train loss: 9.05135 - Test loss: 113.68583\n",
      "Epoch 769 - lr: 0.05000 - Train loss: 9.05135 - Test loss: 113.68582\n",
      "Epoch 770 - lr: 0.05000 - Train loss: 9.05135 - Test loss: 113.68582\n",
      "Epoch 771 - lr: 0.05000 - Train loss: 9.05135 - Test loss: 113.68582\n",
      "Epoch 772 - lr: 0.05000 - Train loss: 9.05134 - Test loss: 113.68581\n",
      "Epoch 773 - lr: 0.05000 - Train loss: 9.05134 - Test loss: 113.68581\n",
      "Epoch 774 - lr: 0.05000 - Train loss: 9.05134 - Test loss: 113.68580\n",
      "Epoch 775 - lr: 0.05000 - Train loss: 9.05134 - Test loss: 113.68580\n",
      "Epoch 776 - lr: 0.05000 - Train loss: 9.05134 - Test loss: 113.68580\n",
      "Epoch 777 - lr: 0.05000 - Train loss: 9.05134 - Test loss: 113.68579\n",
      "Epoch 778 - lr: 0.05000 - Train loss: 9.05134 - Test loss: 113.68579\n",
      "Epoch 779 - lr: 0.05000 - Train loss: 9.05134 - Test loss: 113.68578\n",
      "Epoch 780 - lr: 0.05000 - Train loss: 9.05134 - Test loss: 113.68578\n",
      "Epoch 781 - lr: 0.05000 - Train loss: 9.05134 - Test loss: 113.68578\n",
      "Epoch 782 - lr: 0.05000 - Train loss: 9.05134 - Test loss: 113.68577\n",
      "Epoch 783 - lr: 0.05000 - Train loss: 9.05134 - Test loss: 113.68577\n",
      "Epoch 784 - lr: 0.05000 - Train loss: 9.05134 - Test loss: 113.68576\n",
      "Epoch 785 - lr: 0.05000 - Train loss: 9.05134 - Test loss: 113.68576\n",
      "Epoch 786 - lr: 0.05000 - Train loss: 9.05134 - Test loss: 113.68576\n",
      "Epoch 787 - lr: 0.05000 - Train loss: 9.05134 - Test loss: 113.68575\n",
      "Epoch 788 - lr: 0.05000 - Train loss: 9.05134 - Test loss: 113.68575\n",
      "Epoch 789 - lr: 0.05000 - Train loss: 9.05134 - Test loss: 113.68574\n",
      "Epoch 790 - lr: 0.05000 - Train loss: 9.05134 - Test loss: 113.68574\n",
      "Epoch 791 - lr: 0.05000 - Train loss: 9.05134 - Test loss: 113.68574\n",
      "Epoch 792 - lr: 0.05000 - Train loss: 9.05134 - Test loss: 113.68573\n",
      "Epoch 793 - lr: 0.05000 - Train loss: 9.05133 - Test loss: 113.68573\n",
      "Epoch 794 - lr: 0.05000 - Train loss: 9.05133 - Test loss: 113.68572\n",
      "Epoch 795 - lr: 0.05000 - Train loss: 9.05133 - Test loss: 113.68572\n",
      "Epoch 796 - lr: 0.05000 - Train loss: 9.05133 - Test loss: 113.68571\n",
      "Epoch 797 - lr: 0.05000 - Train loss: 9.05133 - Test loss: 113.68571\n",
      "Epoch 798 - lr: 0.05000 - Train loss: 9.05133 - Test loss: 113.68571\n",
      "Epoch 799 - lr: 0.05000 - Train loss: 9.05133 - Test loss: 113.68570\n",
      "Epoch 800 - lr: 0.05000 - Train loss: 9.05133 - Test loss: 113.68570\n",
      "Epoch 801 - lr: 0.05000 - Train loss: 9.05133 - Test loss: 113.68569\n",
      "Epoch 802 - lr: 0.05000 - Train loss: 9.05133 - Test loss: 113.68569\n",
      "Epoch 803 - lr: 0.05000 - Train loss: 9.05133 - Test loss: 113.68568\n",
      "Epoch 804 - lr: 0.05000 - Train loss: 9.05133 - Test loss: 113.68568\n",
      "Epoch 805 - lr: 0.05000 - Train loss: 9.05133 - Test loss: 113.68567\n",
      "Epoch 806 - lr: 0.05000 - Train loss: 9.05133 - Test loss: 113.68567\n",
      "Epoch 807 - lr: 0.05000 - Train loss: 9.05133 - Test loss: 113.68566\n",
      "Epoch 808 - lr: 0.05000 - Train loss: 9.05133 - Test loss: 113.68566\n",
      "Epoch 809 - lr: 0.05000 - Train loss: 9.05133 - Test loss: 113.68566\n",
      "Epoch 810 - lr: 0.05000 - Train loss: 9.05133 - Test loss: 113.68565\n",
      "Epoch 811 - lr: 0.05000 - Train loss: 9.05133 - Test loss: 113.68565\n",
      "Epoch 812 - lr: 0.05000 - Train loss: 9.05133 - Test loss: 113.68564\n",
      "Epoch 813 - lr: 0.05000 - Train loss: 9.05133 - Test loss: 113.68564\n",
      "Epoch 814 - lr: 0.05000 - Train loss: 9.05133 - Test loss: 113.68563\n",
      "Epoch 815 - lr: 0.05000 - Train loss: 9.05133 - Test loss: 113.68563\n",
      "Epoch 816 - lr: 0.05000 - Train loss: 9.05132 - Test loss: 113.68562\n",
      "Epoch 817 - lr: 0.05000 - Train loss: 9.05132 - Test loss: 113.68562\n",
      "Epoch 818 - lr: 0.05000 - Train loss: 9.05132 - Test loss: 113.68561\n",
      "Epoch 819 - lr: 0.05000 - Train loss: 9.05132 - Test loss: 113.68561\n",
      "Epoch 820 - lr: 0.05000 - Train loss: 9.05132 - Test loss: 113.68560\n",
      "Epoch 821 - lr: 0.05000 - Train loss: 9.05132 - Test loss: 113.68560\n",
      "Epoch 822 - lr: 0.05000 - Train loss: 9.05132 - Test loss: 113.68559\n",
      "Epoch 823 - lr: 0.05000 - Train loss: 9.05132 - Test loss: 113.68559\n",
      "Epoch 824 - lr: 0.05000 - Train loss: 9.05132 - Test loss: 113.68558\n",
      "Epoch 825 - lr: 0.05000 - Train loss: 9.05132 - Test loss: 113.68557\n",
      "Epoch 826 - lr: 0.05000 - Train loss: 9.05132 - Test loss: 113.68557\n",
      "Epoch 827 - lr: 0.05000 - Train loss: 9.05132 - Test loss: 113.68556\n",
      "Epoch 828 - lr: 0.05000 - Train loss: 9.05132 - Test loss: 113.68556\n",
      "Epoch 829 - lr: 0.05000 - Train loss: 9.05132 - Test loss: 113.68555\n",
      "Epoch 830 - lr: 0.05000 - Train loss: 9.05132 - Test loss: 113.68555\n",
      "Epoch 831 - lr: 0.05000 - Train loss: 9.05132 - Test loss: 113.68554\n",
      "Epoch 832 - lr: 0.05000 - Train loss: 9.05132 - Test loss: 113.68554\n",
      "Epoch 833 - lr: 0.05000 - Train loss: 9.05132 - Test loss: 113.68553\n",
      "Epoch 834 - lr: 0.05000 - Train loss: 9.05132 - Test loss: 113.68552\n",
      "Epoch 835 - lr: 0.05000 - Train loss: 9.05132 - Test loss: 113.68552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 836 - lr: 0.05000 - Train loss: 9.05132 - Test loss: 113.68551\n",
      "Epoch 837 - lr: 0.05000 - Train loss: 9.05132 - Test loss: 113.68551\n",
      "Epoch 838 - lr: 0.05000 - Train loss: 9.05132 - Test loss: 113.68550\n",
      "Epoch 839 - lr: 0.05000 - Train loss: 9.05132 - Test loss: 113.68549\n",
      "Epoch 840 - lr: 0.05000 - Train loss: 9.05132 - Test loss: 113.68549\n",
      "Epoch 841 - lr: 0.05000 - Train loss: 9.05131 - Test loss: 113.68548\n",
      "Epoch 842 - lr: 0.05000 - Train loss: 9.05131 - Test loss: 113.68547\n",
      "Epoch 843 - lr: 0.05000 - Train loss: 9.05131 - Test loss: 113.68547\n",
      "Epoch 844 - lr: 0.05000 - Train loss: 9.05131 - Test loss: 113.68546\n",
      "Epoch 845 - lr: 0.05000 - Train loss: 9.05131 - Test loss: 113.68545\n",
      "Epoch 846 - lr: 0.05000 - Train loss: 9.05131 - Test loss: 113.68545\n",
      "Epoch 847 - lr: 0.05000 - Train loss: 9.05131 - Test loss: 113.68544\n",
      "Epoch 848 - lr: 0.05000 - Train loss: 9.05131 - Test loss: 113.68543\n",
      "Epoch 849 - lr: 0.05000 - Train loss: 9.05131 - Test loss: 113.68543\n",
      "Epoch 850 - lr: 0.05000 - Train loss: 9.05131 - Test loss: 113.68542\n",
      "Epoch 851 - lr: 0.05000 - Train loss: 9.05131 - Test loss: 113.68541\n",
      "Epoch 852 - lr: 0.05000 - Train loss: 9.05131 - Test loss: 113.68540\n",
      "Epoch 853 - lr: 0.05000 - Train loss: 9.05131 - Test loss: 113.68540\n",
      "Epoch 854 - lr: 0.05000 - Train loss: 9.05131 - Test loss: 113.68539\n",
      "Epoch 855 - lr: 0.05000 - Train loss: 9.05131 - Test loss: 113.68538\n",
      "Epoch 856 - lr: 0.05000 - Train loss: 9.05131 - Test loss: 113.68537\n",
      "Epoch 857 - lr: 0.05000 - Train loss: 9.05131 - Test loss: 113.68537\n",
      "Epoch 858 - lr: 0.05000 - Train loss: 9.05131 - Test loss: 113.68536\n",
      "Epoch 859 - lr: 0.05000 - Train loss: 9.05131 - Test loss: 113.68535\n",
      "Epoch 860 - lr: 0.05000 - Train loss: 9.05131 - Test loss: 113.68534\n",
      "Epoch 861 - lr: 0.05000 - Train loss: 9.05131 - Test loss: 113.68533\n",
      "Epoch 862 - lr: 0.05000 - Train loss: 9.05130 - Test loss: 113.68532\n",
      "Epoch 863 - lr: 0.05000 - Train loss: 9.05130 - Test loss: 113.68532\n",
      "Epoch 864 - lr: 0.05000 - Train loss: 9.05130 - Test loss: 113.68531\n",
      "Epoch 865 - lr: 0.05000 - Train loss: 9.05130 - Test loss: 113.68530\n",
      "Epoch 866 - lr: 0.05000 - Train loss: 9.05130 - Test loss: 113.68529\n",
      "Epoch 867 - lr: 0.05000 - Train loss: 9.05130 - Test loss: 113.68528\n",
      "Epoch 868 - lr: 0.05000 - Train loss: 9.05130 - Test loss: 113.68527\n",
      "Epoch 869 - lr: 0.05000 - Train loss: 9.05130 - Test loss: 113.68526\n",
      "Epoch 870 - lr: 0.05000 - Train loss: 9.05130 - Test loss: 113.68525\n",
      "Epoch 871 - lr: 0.05000 - Train loss: 9.05130 - Test loss: 113.68524\n",
      "Epoch 872 - lr: 0.05000 - Train loss: 9.05130 - Test loss: 113.68523\n",
      "Epoch 873 - lr: 0.05000 - Train loss: 9.05130 - Test loss: 113.68522\n",
      "Epoch 874 - lr: 0.05000 - Train loss: 9.05130 - Test loss: 113.68521\n",
      "Epoch 875 - lr: 0.05000 - Train loss: 9.05130 - Test loss: 113.68520\n",
      "Epoch 876 - lr: 0.05000 - Train loss: 9.05130 - Test loss: 113.68519\n",
      "Epoch 877 - lr: 0.05000 - Train loss: 9.05130 - Test loss: 113.68518\n",
      "Epoch 878 - lr: 0.05000 - Train loss: 9.05130 - Test loss: 113.68517\n",
      "Epoch 879 - lr: 0.05000 - Train loss: 9.05129 - Test loss: 113.68515\n",
      "Epoch 880 - lr: 0.05000 - Train loss: 9.05129 - Test loss: 113.68514\n",
      "Epoch 881 - lr: 0.05000 - Train loss: 9.05129 - Test loss: 113.68513\n",
      "Epoch 882 - lr: 0.05000 - Train loss: 9.05129 - Test loss: 113.68512\n",
      "Epoch 883 - lr: 0.05000 - Train loss: 9.05129 - Test loss: 113.68510\n",
      "Epoch 884 - lr: 0.05000 - Train loss: 9.05129 - Test loss: 113.68509\n",
      "Epoch 885 - lr: 0.05000 - Train loss: 9.05129 - Test loss: 113.68508\n",
      "Epoch 886 - lr: 0.05000 - Train loss: 9.05129 - Test loss: 113.68506\n",
      "Epoch 887 - lr: 0.05000 - Train loss: 9.05129 - Test loss: 113.68505\n",
      "Epoch 888 - lr: 0.05000 - Train loss: 9.05129 - Test loss: 113.68504\n",
      "Epoch 889 - lr: 0.05000 - Train loss: 9.05129 - Test loss: 113.68502\n",
      "Epoch 890 - lr: 0.05000 - Train loss: 9.05129 - Test loss: 113.68501\n",
      "Epoch 891 - lr: 0.05000 - Train loss: 9.05129 - Test loss: 113.68499\n",
      "Epoch 892 - lr: 0.05000 - Train loss: 9.05128 - Test loss: 113.68498\n",
      "Epoch 893 - lr: 0.05000 - Train loss: 9.05128 - Test loss: 113.68496\n",
      "Epoch 894 - lr: 0.05000 - Train loss: 9.05128 - Test loss: 113.68494\n",
      "Epoch 895 - lr: 0.05000 - Train loss: 9.05128 - Test loss: 113.68492\n",
      "Epoch 896 - lr: 0.05000 - Train loss: 9.05128 - Test loss: 113.68491\n",
      "Epoch 897 - lr: 0.05000 - Train loss: 9.05128 - Test loss: 113.68489\n",
      "Epoch 898 - lr: 0.05000 - Train loss: 9.05128 - Test loss: 113.68487\n",
      "Epoch 899 - lr: 0.05000 - Train loss: 9.05128 - Test loss: 113.68485\n",
      "Epoch 900 - lr: 0.05000 - Train loss: 9.05128 - Test loss: 113.68483\n",
      "Epoch 901 - lr: 0.05000 - Train loss: 9.05128 - Test loss: 113.68481\n",
      "Epoch 902 - lr: 0.05000 - Train loss: 9.05127 - Test loss: 113.68479\n",
      "Epoch 903 - lr: 0.05000 - Train loss: 9.05127 - Test loss: 113.68476\n",
      "Epoch 904 - lr: 0.05000 - Train loss: 9.05127 - Test loss: 113.68474\n",
      "Epoch 905 - lr: 0.05000 - Train loss: 9.05127 - Test loss: 113.68472\n",
      "Epoch 906 - lr: 0.05000 - Train loss: 9.05127 - Test loss: 113.68469\n",
      "Epoch 907 - lr: 0.05000 - Train loss: 9.05127 - Test loss: 113.68467\n",
      "Epoch 908 - lr: 0.05000 - Train loss: 9.05127 - Test loss: 113.68464\n",
      "Epoch 909 - lr: 0.05000 - Train loss: 9.05126 - Test loss: 113.68461\n",
      "Epoch 910 - lr: 0.05000 - Train loss: 9.05126 - Test loss: 113.68458\n",
      "Epoch 911 - lr: 0.05000 - Train loss: 9.05126 - Test loss: 113.68455\n",
      "Epoch 912 - lr: 0.05000 - Train loss: 9.05126 - Test loss: 113.68452\n",
      "Epoch 913 - lr: 0.05000 - Train loss: 9.05126 - Test loss: 113.68448\n",
      "Epoch 914 - lr: 0.05000 - Train loss: 9.05126 - Test loss: 113.68445\n",
      "Epoch 915 - lr: 0.05000 - Train loss: 9.05125 - Test loss: 113.68441\n",
      "Epoch 916 - lr: 0.05000 - Train loss: 9.05125 - Test loss: 113.68437\n",
      "Epoch 917 - lr: 0.05000 - Train loss: 9.05125 - Test loss: 113.68433\n",
      "Epoch 918 - lr: 0.05000 - Train loss: 9.05125 - Test loss: 113.68428\n",
      "Epoch 919 - lr: 0.05000 - Train loss: 9.05124 - Test loss: 113.68423\n",
      "Epoch 920 - lr: 0.05000 - Train loss: 9.05124 - Test loss: 113.68418\n",
      "Epoch 921 - lr: 0.05000 - Train loss: 9.05124 - Test loss: 113.68413\n",
      "Epoch 922 - lr: 0.05000 - Train loss: 9.05124 - Test loss: 113.68407\n",
      "Epoch 923 - lr: 0.05000 - Train loss: 9.05123 - Test loss: 113.68400\n",
      "Epoch 924 - lr: 0.05000 - Train loss: 9.05123 - Test loss: 113.68394\n",
      "Epoch 925 - lr: 0.05000 - Train loss: 9.05122 - Test loss: 113.68386\n",
      "Epoch 926 - lr: 0.05000 - Train loss: 9.05122 - Test loss: 113.68378\n",
      "Epoch 927 - lr: 0.05000 - Train loss: 9.05121 - Test loss: 113.68369\n",
      "Epoch 928 - lr: 0.05000 - Train loss: 9.05121 - Test loss: 113.68358\n",
      "Epoch 929 - lr: 0.05000 - Train loss: 9.05120 - Test loss: 113.68347\n",
      "Epoch 930 - lr: 0.05000 - Train loss: 9.05120 - Test loss: 113.68334\n",
      "Epoch 931 - lr: 0.05000 - Train loss: 9.05119 - Test loss: 113.68319\n",
      "Epoch 932 - lr: 0.05000 - Train loss: 9.05118 - Test loss: 113.68302\n",
      "Epoch 933 - lr: 0.05000 - Train loss: 9.05117 - Test loss: 113.68282\n",
      "Epoch 934 - lr: 0.05000 - Train loss: 9.05115 - Test loss: 113.68258\n",
      "Epoch 935 - lr: 0.05000 - Train loss: 9.05114 - Test loss: 113.68229\n",
      "Epoch 936 - lr: 0.05000 - Train loss: 9.05112 - Test loss: 113.68192\n",
      "Epoch 937 - lr: 0.05000 - Train loss: 9.05109 - Test loss: 113.68143\n",
      "Epoch 938 - lr: 0.05000 - Train loss: 9.05106 - Test loss: 113.68077\n",
      "Epoch 939 - lr: 0.05000 - Train loss: 9.05102 - Test loss: 113.67981\n",
      "Epoch 940 - lr: 0.05000 - Train loss: 9.05094 - Test loss: 113.67826\n",
      "Epoch 941 - lr: 0.05000 - Train loss: 9.05082 - Test loss: 113.67535\n",
      "Epoch 942 - lr: 0.05000 - Train loss: 9.05057 - Test loss: 113.66795\n",
      "Epoch 943 - lr: 0.05000 - Train loss: 9.04962 - Test loss: 113.62171\n",
      "Epoch 944 - lr: 0.05000 - Train loss: 8.98834 - Test loss: 103.29494\n",
      "Epoch 945 - lr: 0.05000 - Train loss: 8.26869 - Test loss: 102.19251\n",
      "Epoch 946 - lr: 0.05000 - Train loss: 8.37179 - Test loss: 113.68436\n",
      "Epoch 947 - lr: 0.05000 - Train loss: 9.05126 - Test loss: 113.68432\n",
      "Epoch 948 - lr: 0.05000 - Train loss: 9.05126 - Test loss: 113.68427\n",
      "Epoch 949 - lr: 0.05000 - Train loss: 9.05125 - Test loss: 113.68422\n",
      "Epoch 950 - lr: 0.05000 - Train loss: 9.05125 - Test loss: 113.68417\n",
      "Epoch 951 - lr: 0.05000 - Train loss: 9.05125 - Test loss: 113.68412\n",
      "Epoch 952 - lr: 0.05000 - Train loss: 9.05125 - Test loss: 113.68406\n",
      "Epoch 953 - lr: 0.05000 - Train loss: 9.05124 - Test loss: 113.68400\n",
      "Epoch 954 - lr: 0.05000 - Train loss: 9.05124 - Test loss: 113.68394\n",
      "Epoch 955 - lr: 0.05000 - Train loss: 9.05124 - Test loss: 113.68387\n",
      "Epoch 956 - lr: 0.05000 - Train loss: 9.05123 - Test loss: 113.68379\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 957 - lr: 0.05000 - Train loss: 9.05123 - Test loss: 113.68371\n",
      "Epoch 958 - lr: 0.05000 - Train loss: 9.05122 - Test loss: 113.68363\n",
      "Epoch 959 - lr: 0.05000 - Train loss: 9.05122 - Test loss: 113.68353\n",
      "Epoch 960 - lr: 0.05000 - Train loss: 9.05121 - Test loss: 113.68343\n",
      "Epoch 961 - lr: 0.05000 - Train loss: 9.05121 - Test loss: 113.68332\n",
      "Epoch 962 - lr: 0.05000 - Train loss: 9.05120 - Test loss: 113.68320\n",
      "Epoch 963 - lr: 0.05000 - Train loss: 9.05119 - Test loss: 113.68307\n",
      "Epoch 964 - lr: 0.05000 - Train loss: 9.05118 - Test loss: 113.68292\n",
      "Epoch 965 - lr: 0.05000 - Train loss: 9.05117 - Test loss: 113.68276\n",
      "Epoch 966 - lr: 0.05000 - Train loss: 9.05116 - Test loss: 113.68258\n",
      "Epoch 967 - lr: 0.05000 - Train loss: 9.05115 - Test loss: 113.68238\n",
      "Epoch 968 - lr: 0.05000 - Train loss: 9.05113 - Test loss: 113.68215\n",
      "Epoch 969 - lr: 0.05000 - Train loss: 9.05111 - Test loss: 113.68189\n",
      "Epoch 970 - lr: 0.05000 - Train loss: 9.05109 - Test loss: 113.68159\n",
      "Epoch 971 - lr: 0.05000 - Train loss: 9.05107 - Test loss: 113.68125\n",
      "Epoch 972 - lr: 0.05000 - Train loss: 9.05103 - Test loss: 113.68086\n",
      "Epoch 973 - lr: 0.05000 - Train loss: 9.05099 - Test loss: 113.68041\n",
      "Epoch 974 - lr: 0.05000 - Train loss: 9.05094 - Test loss: 113.67987\n",
      "Epoch 975 - lr: 0.05000 - Train loss: 9.05087 - Test loss: 113.67925\n",
      "Epoch 976 - lr: 0.05000 - Train loss: 9.05078 - Test loss: 113.67854\n",
      "Epoch 977 - lr: 0.05000 - Train loss: 9.05066 - Test loss: 113.67774\n",
      "Epoch 978 - lr: 0.05000 - Train loss: 9.05047 - Test loss: 113.67691\n",
      "Epoch 979 - lr: 0.05000 - Train loss: 9.05021 - Test loss: 113.67617\n",
      "Epoch 980 - lr: 0.05000 - Train loss: 9.04985 - Test loss: 113.67569\n",
      "Epoch 981 - lr: 0.05000 - Train loss: 9.04943 - Test loss: 113.67557\n",
      "Epoch 982 - lr: 0.05000 - Train loss: 9.04907 - Test loss: 113.67561\n",
      "Epoch 983 - lr: 0.05000 - Train loss: 9.04881 - Test loss: 113.67553\n",
      "Epoch 984 - lr: 0.05000 - Train loss: 9.04859 - Test loss: 113.67530\n",
      "Epoch 985 - lr: 0.05000 - Train loss: 9.04833 - Test loss: 113.67497\n",
      "Epoch 986 - lr: 0.05000 - Train loss: 9.04805 - Test loss: 113.67457\n",
      "Epoch 987 - lr: 0.05000 - Train loss: 9.04775 - Test loss: 113.67411\n",
      "Epoch 988 - lr: 0.05000 - Train loss: 9.04742 - Test loss: 113.67362\n",
      "Epoch 989 - lr: 0.05000 - Train loss: 9.04706 - Test loss: 113.67308\n",
      "Epoch 990 - lr: 0.05000 - Train loss: 9.04668 - Test loss: 113.67251\n",
      "Epoch 991 - lr: 0.05000 - Train loss: 9.04627 - Test loss: 113.67191\n",
      "Epoch 992 - lr: 0.05000 - Train loss: 9.04584 - Test loss: 113.67128\n",
      "Epoch 993 - lr: 0.05000 - Train loss: 9.04537 - Test loss: 113.67064\n",
      "Epoch 994 - lr: 0.05000 - Train loss: 9.04487 - Test loss: 113.66997\n",
      "Epoch 995 - lr: 0.05000 - Train loss: 9.04434 - Test loss: 113.66929\n",
      "Epoch 996 - lr: 0.05000 - Train loss: 9.04377 - Test loss: 113.66861\n",
      "Epoch 997 - lr: 0.05000 - Train loss: 9.04317 - Test loss: 113.66787\n",
      "Epoch 998 - lr: 0.05000 - Train loss: 9.04247 - Test loss: 113.66728\n",
      "Epoch 999 - lr: 0.05000 - Train loss: 9.04191 - Test loss: 113.66614\n",
      "Epoch 1000 - lr: 0.05000 - Train loss: 9.04079 - Test loss: 113.66663\n",
      "Epoch 1001 - lr: 0.05000 - Train loss: 9.04112 - Test loss: 113.66210\n",
      "Epoch 1002 - lr: 0.05000 - Train loss: 9.03877 - Test loss: 113.66858\n",
      "Epoch 1003 - lr: 0.05000 - Train loss: 9.04346 - Test loss: 113.65064\n",
      "Epoch 1004 - lr: 0.05000 - Train loss: 8.76758 - Test loss: 113.84728\n",
      "Epoch 1005 - lr: 0.05000 - Train loss: 8.59494 - Test loss: 116.15482\n",
      "Epoch 1006 - lr: 0.05000 - Train loss: 8.56304 - Test loss: 110.70128\n",
      "Epoch 1007 - lr: 0.05000 - Train loss: 8.67379 - Test loss: 102.78147\n",
      "Epoch 1008 - lr: 0.05000 - Train loss: 8.74907 - Test loss: 102.65301\n",
      "Epoch 1009 - lr: 0.05000 - Train loss: 8.78129 - Test loss: 113.68489\n",
      "Epoch 1010 - lr: 0.05000 - Train loss: 9.05129 - Test loss: 113.68486\n",
      "Epoch 1011 - lr: 0.05000 - Train loss: 9.05128 - Test loss: 113.68483\n",
      "Epoch 1012 - lr: 0.05000 - Train loss: 9.05128 - Test loss: 113.68480\n",
      "Epoch 1013 - lr: 0.05000 - Train loss: 9.05128 - Test loss: 113.68476\n",
      "Epoch 1014 - lr: 0.05000 - Train loss: 9.05128 - Test loss: 113.68473\n",
      "Epoch 1015 - lr: 0.05000 - Train loss: 9.05128 - Test loss: 113.68469\n",
      "Epoch 1016 - lr: 0.05000 - Train loss: 9.05128 - Test loss: 113.68465\n",
      "Epoch 1017 - lr: 0.05000 - Train loss: 9.05127 - Test loss: 113.68460\n",
      "Epoch 1018 - lr: 0.05000 - Train loss: 9.05127 - Test loss: 113.68456\n",
      "Epoch 1019 - lr: 0.05000 - Train loss: 9.05127 - Test loss: 113.68451\n",
      "Epoch 1020 - lr: 0.05000 - Train loss: 9.05127 - Test loss: 113.68445\n",
      "Epoch 1021 - lr: 0.05000 - Train loss: 9.05127 - Test loss: 113.68440\n",
      "Epoch 1022 - lr: 0.05000 - Train loss: 9.05126 - Test loss: 113.68433\n",
      "Epoch 1023 - lr: 0.05000 - Train loss: 9.05126 - Test loss: 113.68427\n",
      "Epoch 1024 - lr: 0.05000 - Train loss: 9.05126 - Test loss: 113.68419\n",
      "Epoch 1025 - lr: 0.05000 - Train loss: 9.05125 - Test loss: 113.68411\n",
      "Epoch 1026 - lr: 0.05000 - Train loss: 9.05125 - Test loss: 113.68402\n",
      "Epoch 1027 - lr: 0.05000 - Train loss: 9.05125 - Test loss: 113.68392\n",
      "Epoch 1028 - lr: 0.05000 - Train loss: 9.05124 - Test loss: 113.68381\n",
      "Epoch 1029 - lr: 0.05000 - Train loss: 9.05124 - Test loss: 113.68368\n",
      "Epoch 1030 - lr: 0.05000 - Train loss: 9.05123 - Test loss: 113.68354\n",
      "Epoch 1031 - lr: 0.05000 - Train loss: 9.05123 - Test loss: 113.68338\n",
      "Epoch 1032 - lr: 0.05000 - Train loss: 9.05122 - Test loss: 113.68320\n",
      "Epoch 1033 - lr: 0.05000 - Train loss: 9.05121 - Test loss: 113.68298\n",
      "Epoch 1034 - lr: 0.05000 - Train loss: 9.05120 - Test loss: 113.68272\n",
      "Epoch 1035 - lr: 0.05000 - Train loss: 9.05119 - Test loss: 113.68241\n",
      "Epoch 1036 - lr: 0.05000 - Train loss: 9.05117 - Test loss: 113.68202\n",
      "Epoch 1037 - lr: 0.05000 - Train loss: 9.05115 - Test loss: 113.68154\n",
      "Epoch 1038 - lr: 0.05000 - Train loss: 9.05113 - Test loss: 113.68091\n",
      "Epoch 1039 - lr: 0.05000 - Train loss: 9.05110 - Test loss: 113.68005\n",
      "Epoch 1040 - lr: 0.05000 - Train loss: 9.05106 - Test loss: 113.67881\n",
      "Epoch 1041 - lr: 0.05000 - Train loss: 9.05099 - Test loss: 113.67688\n",
      "Epoch 1042 - lr: 0.05000 - Train loss: 9.05089 - Test loss: 113.67346\n",
      "Epoch 1043 - lr: 0.05000 - Train loss: 9.05069 - Test loss: 113.66584\n",
      "Epoch 1044 - lr: 0.05000 - Train loss: 9.05019 - Test loss: 113.63774\n",
      "Epoch 1045 - lr: 0.05000 - Train loss: 9.08643 - Test loss: 110.89871\n",
      "Epoch 1046 - lr: 0.05000 - Train loss: 8.81225 - Test loss: 113.68551\n",
      "Epoch 1047 - lr: 0.05000 - Train loss: 9.05130 - Test loss: 113.68550\n",
      "Epoch 1048 - lr: 0.05000 - Train loss: 9.05130 - Test loss: 113.68549\n",
      "Epoch 1049 - lr: 0.05000 - Train loss: 9.05130 - Test loss: 113.68548\n",
      "Epoch 1050 - lr: 0.05000 - Train loss: 9.05130 - Test loss: 113.68546\n",
      "Epoch 1051 - lr: 0.05000 - Train loss: 9.05130 - Test loss: 113.68545\n",
      "Epoch 1052 - lr: 0.05000 - Train loss: 9.05130 - Test loss: 113.68544\n",
      "Epoch 1053 - lr: 0.05000 - Train loss: 9.05130 - Test loss: 113.68543\n",
      "Epoch 1054 - lr: 0.05000 - Train loss: 9.05130 - Test loss: 113.68542\n",
      "Epoch 1055 - lr: 0.05000 - Train loss: 9.05130 - Test loss: 113.68540\n",
      "Epoch 1056 - lr: 0.05000 - Train loss: 9.05130 - Test loss: 113.68539\n",
      "Epoch 1057 - lr: 0.05000 - Train loss: 9.05130 - Test loss: 113.68537\n",
      "Epoch 1058 - lr: 0.05000 - Train loss: 9.05130 - Test loss: 113.68536\n",
      "Epoch 1059 - lr: 0.05000 - Train loss: 9.05130 - Test loss: 113.68534\n",
      "Epoch 1060 - lr: 0.05000 - Train loss: 9.05129 - Test loss: 113.68533\n",
      "Epoch 1061 - lr: 0.05000 - Train loss: 9.05129 - Test loss: 113.68531\n",
      "Epoch 1062 - lr: 0.05000 - Train loss: 9.05129 - Test loss: 113.68529\n",
      "Epoch 1063 - lr: 0.05000 - Train loss: 9.05129 - Test loss: 113.68528\n",
      "Epoch 1064 - lr: 0.05000 - Train loss: 9.05129 - Test loss: 113.68526\n",
      "Epoch 1065 - lr: 0.05000 - Train loss: 9.05129 - Test loss: 113.68524\n",
      "Epoch 1066 - lr: 0.05000 - Train loss: 9.05129 - Test loss: 113.68522\n",
      "Epoch 1067 - lr: 0.05000 - Train loss: 9.05129 - Test loss: 113.68520\n",
      "Epoch 1068 - lr: 0.05000 - Train loss: 9.05129 - Test loss: 113.68517\n",
      "Epoch 1069 - lr: 0.05000 - Train loss: 9.05129 - Test loss: 113.68515\n",
      "Epoch 1070 - lr: 0.05000 - Train loss: 9.05129 - Test loss: 113.68512\n",
      "Epoch 1071 - lr: 0.05000 - Train loss: 9.05128 - Test loss: 113.68510\n",
      "Epoch 1072 - lr: 0.05000 - Train loss: 9.05128 - Test loss: 113.68507\n",
      "Epoch 1073 - lr: 0.05000 - Train loss: 9.05128 - Test loss: 113.68504\n",
      "Epoch 1074 - lr: 0.05000 - Train loss: 9.05128 - Test loss: 113.68501\n",
      "Epoch 1075 - lr: 0.05000 - Train loss: 9.05128 - Test loss: 113.68498\n",
      "Epoch 1076 - lr: 0.05000 - Train loss: 9.05128 - Test loss: 113.68494\n",
      "Epoch 1077 - lr: 0.05000 - Train loss: 9.05128 - Test loss: 113.68490\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1078 - lr: 0.05000 - Train loss: 9.05127 - Test loss: 113.68486\n",
      "Epoch 1079 - lr: 0.05000 - Train loss: 9.05127 - Test loss: 113.68482\n",
      "Epoch 1080 - lr: 0.05000 - Train loss: 9.05127 - Test loss: 113.68477\n",
      "Epoch 1081 - lr: 0.05000 - Train loss: 9.05127 - Test loss: 113.68472\n",
      "Epoch 1082 - lr: 0.05000 - Train loss: 9.05126 - Test loss: 113.68467\n",
      "Epoch 1083 - lr: 0.05000 - Train loss: 9.05126 - Test loss: 113.68461\n",
      "Epoch 1084 - lr: 0.05000 - Train loss: 9.05126 - Test loss: 113.68454\n",
      "Epoch 1085 - lr: 0.05000 - Train loss: 9.05126 - Test loss: 113.68447\n",
      "Epoch 1086 - lr: 0.05000 - Train loss: 9.05125 - Test loss: 113.68439\n",
      "Epoch 1087 - lr: 0.05000 - Train loss: 9.05125 - Test loss: 113.68431\n",
      "Epoch 1088 - lr: 0.05000 - Train loss: 9.05124 - Test loss: 113.68421\n",
      "Epoch 1089 - lr: 0.05000 - Train loss: 9.05124 - Test loss: 113.68410\n",
      "Epoch 1090 - lr: 0.05000 - Train loss: 9.05124 - Test loss: 113.68398\n",
      "Epoch 1091 - lr: 0.05000 - Train loss: 9.05123 - Test loss: 113.68384\n",
      "Epoch 1092 - lr: 0.05000 - Train loss: 9.05122 - Test loss: 113.68368\n",
      "Epoch 1093 - lr: 0.05000 - Train loss: 9.05121 - Test loss: 113.68349\n",
      "Epoch 1094 - lr: 0.05000 - Train loss: 9.05121 - Test loss: 113.68327\n",
      "Epoch 1095 - lr: 0.05000 - Train loss: 9.05119 - Test loss: 113.68300\n",
      "Epoch 1096 - lr: 0.05000 - Train loss: 9.05118 - Test loss: 113.68268\n",
      "Epoch 1097 - lr: 0.05000 - Train loss: 9.05116 - Test loss: 113.68228\n",
      "Epoch 1098 - lr: 0.05000 - Train loss: 9.05114 - Test loss: 113.68176\n",
      "Epoch 1099 - lr: 0.05000 - Train loss: 9.05112 - Test loss: 113.68106\n",
      "Epoch 1100 - lr: 0.05000 - Train loss: 9.05108 - Test loss: 113.68009\n",
      "Epoch 1101 - lr: 0.05000 - Train loss: 9.05103 - Test loss: 113.67864\n",
      "Epoch 1102 - lr: 0.05000 - Train loss: 9.05095 - Test loss: 113.67623\n",
      "Epoch 1103 - lr: 0.05000 - Train loss: 9.05080 - Test loss: 113.67149\n",
      "Epoch 1104 - lr: 0.05000 - Train loss: 9.05049 - Test loss: 113.65838\n",
      "Epoch 1105 - lr: 0.05000 - Train loss: 9.04963 - Test loss: 113.57481\n",
      "Epoch 1106 - lr: 0.05000 - Train loss: 8.92723 - Test loss: 116.18662\n",
      "Epoch 1107 - lr: 0.05000 - Train loss: 8.45467 - Test loss: 116.17540\n",
      "Epoch 1108 - lr: 0.05000 - Train loss: 8.45446 - Test loss: 116.17540\n",
      "Epoch 1109 - lr: 0.05000 - Train loss: 8.45446 - Test loss: 116.17539\n",
      "Epoch 1110 - lr: 0.05000 - Train loss: 8.45446 - Test loss: 116.17539\n",
      "Epoch 1111 - lr: 0.05000 - Train loss: 8.45446 - Test loss: 116.17538\n",
      "Epoch 1112 - lr: 0.05000 - Train loss: 8.45446 - Test loss: 116.17537\n",
      "Epoch 1113 - lr: 0.05000 - Train loss: 8.45446 - Test loss: 116.17537\n",
      "Epoch 1114 - lr: 0.05000 - Train loss: 8.45446 - Test loss: 116.17536\n",
      "Epoch 1115 - lr: 0.05000 - Train loss: 8.45446 - Test loss: 116.17536\n",
      "Epoch 1116 - lr: 0.05000 - Train loss: 8.45446 - Test loss: 116.17535\n",
      "Epoch 1117 - lr: 0.05000 - Train loss: 8.45446 - Test loss: 116.17534\n",
      "Epoch 1118 - lr: 0.05000 - Train loss: 8.45446 - Test loss: 116.17534\n",
      "Epoch 1119 - lr: 0.05000 - Train loss: 8.45446 - Test loss: 116.17533\n",
      "Epoch 1120 - lr: 0.05000 - Train loss: 8.45446 - Test loss: 116.17532\n",
      "Epoch 1121 - lr: 0.05000 - Train loss: 8.45445 - Test loss: 116.17532\n",
      "Epoch 1122 - lr: 0.05000 - Train loss: 8.45445 - Test loss: 116.17531\n",
      "Epoch 1123 - lr: 0.05000 - Train loss: 8.45445 - Test loss: 116.17530\n",
      "Epoch 1124 - lr: 0.05000 - Train loss: 8.45445 - Test loss: 116.17529\n",
      "Epoch 1125 - lr: 0.05000 - Train loss: 8.45445 - Test loss: 116.17529\n",
      "Epoch 1126 - lr: 0.05000 - Train loss: 8.45445 - Test loss: 116.17528\n",
      "Epoch 1127 - lr: 0.05000 - Train loss: 8.45445 - Test loss: 116.17527\n",
      "Epoch 1128 - lr: 0.05000 - Train loss: 8.45445 - Test loss: 116.17526\n",
      "Epoch 1129 - lr: 0.05000 - Train loss: 8.45445 - Test loss: 116.17525\n",
      "Epoch 1130 - lr: 0.05000 - Train loss: 8.45445 - Test loss: 116.17524\n",
      "Epoch 1131 - lr: 0.05000 - Train loss: 8.45445 - Test loss: 116.17523\n",
      "Epoch 1132 - lr: 0.05000 - Train loss: 8.45445 - Test loss: 116.17522\n",
      "Epoch 1133 - lr: 0.05000 - Train loss: 8.45445 - Test loss: 116.17521\n",
      "Epoch 1134 - lr: 0.05000 - Train loss: 8.45445 - Test loss: 116.17520\n",
      "Epoch 1135 - lr: 0.05000 - Train loss: 8.45445 - Test loss: 116.17518\n",
      "Epoch 1136 - lr: 0.05000 - Train loss: 8.45445 - Test loss: 116.17517\n",
      "Epoch 1137 - lr: 0.05000 - Train loss: 8.45444 - Test loss: 116.17516\n",
      "Epoch 1138 - lr: 0.05000 - Train loss: 8.45444 - Test loss: 116.17515\n",
      "Epoch 1139 - lr: 0.05000 - Train loss: 8.45444 - Test loss: 116.17513\n",
      "Epoch 1140 - lr: 0.05000 - Train loss: 8.45444 - Test loss: 116.17512\n",
      "Epoch 1141 - lr: 0.05000 - Train loss: 8.45444 - Test loss: 116.17510\n",
      "Epoch 1142 - lr: 0.05000 - Train loss: 8.45444 - Test loss: 116.17508\n",
      "Epoch 1143 - lr: 0.05000 - Train loss: 8.45444 - Test loss: 116.17507\n",
      "Epoch 1144 - lr: 0.05000 - Train loss: 8.45444 - Test loss: 116.17505\n",
      "Epoch 1145 - lr: 0.05000 - Train loss: 8.45444 - Test loss: 116.17503\n",
      "Epoch 1146 - lr: 0.05000 - Train loss: 8.45443 - Test loss: 116.17501\n",
      "Epoch 1147 - lr: 0.05000 - Train loss: 8.45443 - Test loss: 116.17498\n",
      "Epoch 1148 - lr: 0.05000 - Train loss: 8.45443 - Test loss: 116.17496\n",
      "Epoch 1149 - lr: 0.05000 - Train loss: 8.45443 - Test loss: 116.17493\n",
      "Epoch 1150 - lr: 0.05000 - Train loss: 8.45443 - Test loss: 116.17491\n",
      "Epoch 1151 - lr: 0.05000 - Train loss: 8.45443 - Test loss: 116.17488\n",
      "Epoch 1152 - lr: 0.05000 - Train loss: 8.45442 - Test loss: 116.17485\n",
      "Epoch 1153 - lr: 0.05000 - Train loss: 8.45442 - Test loss: 116.17481\n",
      "Epoch 1154 - lr: 0.05000 - Train loss: 8.45442 - Test loss: 116.17478\n",
      "Epoch 1155 - lr: 0.05000 - Train loss: 8.45442 - Test loss: 116.17474\n",
      "Epoch 1156 - lr: 0.05000 - Train loss: 8.45442 - Test loss: 116.17469\n",
      "Epoch 1157 - lr: 0.05000 - Train loss: 8.45441 - Test loss: 116.17464\n",
      "Epoch 1158 - lr: 0.05000 - Train loss: 8.45441 - Test loss: 116.17459\n",
      "Epoch 1159 - lr: 0.05000 - Train loss: 8.45441 - Test loss: 116.17453\n",
      "Epoch 1160 - lr: 0.05000 - Train loss: 8.45440 - Test loss: 116.17446\n",
      "Epoch 1161 - lr: 0.05000 - Train loss: 8.45440 - Test loss: 116.17439\n",
      "Epoch 1162 - lr: 0.05000 - Train loss: 8.45439 - Test loss: 116.17431\n",
      "Epoch 1163 - lr: 0.05000 - Train loss: 8.45439 - Test loss: 116.17421\n",
      "Epoch 1164 - lr: 0.05000 - Train loss: 8.45438 - Test loss: 116.17410\n",
      "Epoch 1165 - lr: 0.05000 - Train loss: 8.45437 - Test loss: 116.17397\n",
      "Epoch 1166 - lr: 0.05000 - Train loss: 8.45436 - Test loss: 116.17382\n",
      "Epoch 1167 - lr: 0.05000 - Train loss: 8.45435 - Test loss: 116.17364\n",
      "Epoch 1168 - lr: 0.05000 - Train loss: 8.45434 - Test loss: 116.17341\n",
      "Epoch 1169 - lr: 0.05000 - Train loss: 8.45432 - Test loss: 116.17313\n",
      "Epoch 1170 - lr: 0.05000 - Train loss: 8.45430 - Test loss: 116.17277\n",
      "Epoch 1171 - lr: 0.05000 - Train loss: 8.45427 - Test loss: 116.17229\n",
      "Epoch 1172 - lr: 0.05000 - Train loss: 8.45424 - Test loss: 116.17161\n",
      "Epoch 1173 - lr: 0.05000 - Train loss: 8.45418 - Test loss: 116.17059\n",
      "Epoch 1174 - lr: 0.05000 - Train loss: 8.45410 - Test loss: 116.16887\n",
      "Epoch 1175 - lr: 0.05000 - Train loss: 8.45395 - Test loss: 116.16544\n",
      "Epoch 1176 - lr: 0.05000 - Train loss: 8.45358 - Test loss: 116.15534\n",
      "Epoch 1177 - lr: 0.05000 - Train loss: 8.45135 - Test loss: 116.02305\n",
      "Epoch 1178 - lr: 0.05000 - Train loss: 8.20295 - Test loss: 98.75080\n",
      "Epoch 1179 - lr: 0.05000 - Train loss: 8.05606 - Test loss: 116.17512\n",
      "Epoch 1180 - lr: 0.05000 - Train loss: 8.45444 - Test loss: 116.17511\n",
      "Epoch 1181 - lr: 0.05000 - Train loss: 8.45444 - Test loss: 116.17509\n",
      "Epoch 1182 - lr: 0.05000 - Train loss: 8.45444 - Test loss: 116.17508\n",
      "Epoch 1183 - lr: 0.05000 - Train loss: 8.45444 - Test loss: 116.17506\n",
      "Epoch 1184 - lr: 0.05000 - Train loss: 8.45444 - Test loss: 116.17504\n",
      "Epoch 1185 - lr: 0.05000 - Train loss: 8.45443 - Test loss: 116.17502\n",
      "Epoch 1186 - lr: 0.05000 - Train loss: 8.45443 - Test loss: 116.17501\n",
      "Epoch 1187 - lr: 0.05000 - Train loss: 8.45443 - Test loss: 116.17499\n",
      "Epoch 1188 - lr: 0.05000 - Train loss: 8.45443 - Test loss: 116.17496\n",
      "Epoch 1189 - lr: 0.05000 - Train loss: 8.45443 - Test loss: 116.17494\n",
      "Epoch 1190 - lr: 0.05000 - Train loss: 8.45443 - Test loss: 116.17492\n",
      "Epoch 1191 - lr: 0.05000 - Train loss: 8.45443 - Test loss: 116.17489\n",
      "Epoch 1192 - lr: 0.05000 - Train loss: 8.45442 - Test loss: 116.17486\n",
      "Epoch 1193 - lr: 0.05000 - Train loss: 8.45442 - Test loss: 116.17483\n",
      "Epoch 1194 - lr: 0.05000 - Train loss: 8.45442 - Test loss: 116.17480\n",
      "Epoch 1195 - lr: 0.05000 - Train loss: 8.45442 - Test loss: 116.17476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1196 - lr: 0.05000 - Train loss: 8.45441 - Test loss: 116.17473\n",
      "Epoch 1197 - lr: 0.05000 - Train loss: 8.45441 - Test loss: 116.17468\n",
      "Epoch 1198 - lr: 0.05000 - Train loss: 8.45441 - Test loss: 116.17464\n",
      "Epoch 1199 - lr: 0.05000 - Train loss: 8.45441 - Test loss: 116.17459\n",
      "Epoch 1200 - lr: 0.05000 - Train loss: 8.45440 - Test loss: 116.17453\n",
      "Epoch 1201 - lr: 0.05000 - Train loss: 8.45440 - Test loss: 116.17447\n",
      "Epoch 1202 - lr: 0.05000 - Train loss: 8.45439 - Test loss: 116.17440\n",
      "Epoch 1203 - lr: 0.05000 - Train loss: 8.45439 - Test loss: 116.17432\n",
      "Epoch 1204 - lr: 0.05000 - Train loss: 8.45438 - Test loss: 116.17423\n",
      "Epoch 1205 - lr: 0.05000 - Train loss: 8.45438 - Test loss: 116.17413\n",
      "Epoch 1206 - lr: 0.05000 - Train loss: 8.45437 - Test loss: 116.17400\n",
      "Epoch 1207 - lr: 0.05000 - Train loss: 8.45436 - Test loss: 116.17386\n",
      "Epoch 1208 - lr: 0.05000 - Train loss: 8.45435 - Test loss: 116.17369\n",
      "Epoch 1209 - lr: 0.05000 - Train loss: 8.45434 - Test loss: 116.17348\n",
      "Epoch 1210 - lr: 0.05000 - Train loss: 8.45432 - Test loss: 116.17321\n",
      "Epoch 1211 - lr: 0.05000 - Train loss: 8.45430 - Test loss: 116.17287\n",
      "Epoch 1212 - lr: 0.05000 - Train loss: 8.45427 - Test loss: 116.17242\n",
      "Epoch 1213 - lr: 0.05000 - Train loss: 8.45424 - Test loss: 116.17179\n",
      "Epoch 1214 - lr: 0.05000 - Train loss: 8.45418 - Test loss: 116.17083\n",
      "Epoch 1215 - lr: 0.05000 - Train loss: 8.45410 - Test loss: 116.16924\n",
      "Epoch 1216 - lr: 0.05000 - Train loss: 8.45395 - Test loss: 116.16605\n",
      "Epoch 1217 - lr: 0.05000 - Train loss: 8.45358 - Test loss: 116.15679\n",
      "Epoch 1218 - lr: 0.05000 - Train loss: 8.45134 - Test loss: 116.03466\n",
      "Epoch 1219 - lr: 0.05000 - Train loss: 8.10190 - Test loss: 101.79506\n",
      "Epoch 1220 - lr: 0.05000 - Train loss: 7.83783 - Test loss: 116.17550\n",
      "Epoch 1221 - lr: 0.05000 - Train loss: 8.45447 - Test loss: 116.17550\n",
      "Epoch 1222 - lr: 0.05000 - Train loss: 8.45447 - Test loss: 116.17549\n",
      "Epoch 1223 - lr: 0.05000 - Train loss: 8.45447 - Test loss: 116.17549\n",
      "Epoch 1224 - lr: 0.05000 - Train loss: 8.45447 - Test loss: 116.17549\n",
      "Epoch 1225 - lr: 0.05000 - Train loss: 8.45447 - Test loss: 116.17549\n",
      "Epoch 1226 - lr: 0.05000 - Train loss: 8.45447 - Test loss: 116.17548\n",
      "Epoch 1227 - lr: 0.05000 - Train loss: 8.45447 - Test loss: 116.17548\n",
      "Epoch 1228 - lr: 0.05000 - Train loss: 8.45447 - Test loss: 116.17548\n",
      "Epoch 1229 - lr: 0.05000 - Train loss: 8.45447 - Test loss: 116.17548\n",
      "Epoch 1230 - lr: 0.05000 - Train loss: 8.45447 - Test loss: 116.17547\n",
      "Epoch 1231 - lr: 0.05000 - Train loss: 8.45447 - Test loss: 116.17547\n",
      "Epoch 1232 - lr: 0.05000 - Train loss: 8.45447 - Test loss: 116.17547\n",
      "Epoch 1233 - lr: 0.05000 - Train loss: 8.45447 - Test loss: 116.17546\n",
      "Epoch 1234 - lr: 0.05000 - Train loss: 8.45447 - Test loss: 116.17546\n",
      "Epoch 1235 - lr: 0.05000 - Train loss: 8.45447 - Test loss: 116.17546\n",
      "Epoch 1236 - lr: 0.05000 - Train loss: 8.45447 - Test loss: 116.17546\n",
      "Epoch 1237 - lr: 0.05000 - Train loss: 8.45447 - Test loss: 116.17545\n",
      "Epoch 1238 - lr: 0.05000 - Train loss: 8.45447 - Test loss: 116.17545\n",
      "Epoch 1239 - lr: 0.05000 - Train loss: 8.45447 - Test loss: 116.17545\n",
      "Epoch 1240 - lr: 0.05000 - Train loss: 8.45447 - Test loss: 116.17544\n",
      "Epoch 1241 - lr: 0.05000 - Train loss: 8.45447 - Test loss: 116.17544\n",
      "Epoch 1242 - lr: 0.05000 - Train loss: 8.45447 - Test loss: 116.17544\n",
      "Epoch 1243 - lr: 0.05000 - Train loss: 8.45447 - Test loss: 116.17544\n",
      "Epoch 1244 - lr: 0.05000 - Train loss: 8.45447 - Test loss: 116.17543\n",
      "Epoch 1245 - lr: 0.05000 - Train loss: 8.45447 - Test loss: 116.17543\n",
      "Epoch 1246 - lr: 0.05000 - Train loss: 8.45447 - Test loss: 116.17543\n",
      "Epoch 1247 - lr: 0.05000 - Train loss: 8.45447 - Test loss: 116.17542\n",
      "Epoch 1248 - lr: 0.05000 - Train loss: 8.45447 - Test loss: 116.17542\n",
      "Epoch 1249 - lr: 0.05000 - Train loss: 8.45447 - Test loss: 116.17542\n",
      "Epoch 1250 - lr: 0.05000 - Train loss: 8.45447 - Test loss: 116.17541\n",
      "Epoch 1251 - lr: 0.05000 - Train loss: 8.45447 - Test loss: 116.17541\n",
      "Epoch 1252 - lr: 0.05000 - Train loss: 8.45447 - Test loss: 116.17541\n",
      "Epoch 1253 - lr: 0.05000 - Train loss: 8.45447 - Test loss: 116.17540\n",
      "Epoch 1254 - lr: 0.05000 - Train loss: 8.45447 - Test loss: 116.17540\n",
      "Epoch 1255 - lr: 0.05000 - Train loss: 8.45447 - Test loss: 116.17540\n",
      "Epoch 1256 - lr: 0.05000 - Train loss: 8.45447 - Test loss: 116.17539\n",
      "Epoch 1257 - lr: 0.05000 - Train loss: 8.45447 - Test loss: 116.17539\n",
      "Epoch 1258 - lr: 0.05000 - Train loss: 8.45447 - Test loss: 116.17539\n",
      "Epoch 1259 - lr: 0.05000 - Train loss: 8.45447 - Test loss: 116.17538\n",
      "Epoch 1260 - lr: 0.05000 - Train loss: 8.45447 - Test loss: 116.17538\n",
      "Epoch 1261 - lr: 0.05000 - Train loss: 8.45447 - Test loss: 116.17538\n",
      "Epoch 1262 - lr: 0.05000 - Train loss: 8.45447 - Test loss: 116.17537\n",
      "Epoch 1263 - lr: 0.05000 - Train loss: 8.45447 - Test loss: 116.17537\n",
      "Epoch 1264 - lr: 0.05000 - Train loss: 8.45447 - Test loss: 116.17537\n",
      "Epoch 1265 - lr: 0.05000 - Train loss: 8.45447 - Test loss: 116.17536\n",
      "Epoch 1266 - lr: 0.05000 - Train loss: 8.45447 - Test loss: 116.17536\n",
      "Epoch 1267 - lr: 0.05000 - Train loss: 8.45446 - Test loss: 116.17535\n",
      "Epoch 1268 - lr: 0.05000 - Train loss: 8.45446 - Test loss: 116.17535\n",
      "Epoch 1269 - lr: 0.05000 - Train loss: 8.45446 - Test loss: 116.17535\n",
      "Epoch 1270 - lr: 0.05000 - Train loss: 8.45446 - Test loss: 116.17534\n",
      "Epoch 1271 - lr: 0.05000 - Train loss: 8.45446 - Test loss: 116.17534\n",
      "Epoch 1272 - lr: 0.05000 - Train loss: 8.45446 - Test loss: 116.17534\n",
      "Epoch 1273 - lr: 0.05000 - Train loss: 8.45446 - Test loss: 116.17533\n",
      "Epoch 1274 - lr: 0.05000 - Train loss: 8.45446 - Test loss: 116.17533\n",
      "Epoch 1275 - lr: 0.05000 - Train loss: 8.45446 - Test loss: 116.17532\n",
      "Epoch 1276 - lr: 0.05000 - Train loss: 8.45446 - Test loss: 116.17532\n",
      "Epoch 1277 - lr: 0.05000 - Train loss: 8.45446 - Test loss: 116.17531\n",
      "Epoch 1278 - lr: 0.05000 - Train loss: 8.45446 - Test loss: 116.17531\n",
      "Epoch 1279 - lr: 0.05000 - Train loss: 8.45446 - Test loss: 116.17531\n",
      "Epoch 1280 - lr: 0.05000 - Train loss: 8.45446 - Test loss: 116.17530\n",
      "Epoch 1281 - lr: 0.05000 - Train loss: 8.45446 - Test loss: 116.17530\n",
      "Epoch 1282 - lr: 0.05000 - Train loss: 8.45446 - Test loss: 116.17529\n",
      "Epoch 1283 - lr: 0.05000 - Train loss: 8.45446 - Test loss: 116.17529\n",
      "Epoch 1284 - lr: 0.05000 - Train loss: 8.45446 - Test loss: 116.17528\n",
      "Epoch 1285 - lr: 0.05000 - Train loss: 8.45446 - Test loss: 116.17528\n",
      "Epoch 1286 - lr: 0.05000 - Train loss: 8.45446 - Test loss: 116.17527\n",
      "Epoch 1287 - lr: 0.05000 - Train loss: 8.45446 - Test loss: 116.17527\n",
      "Epoch 1288 - lr: 0.05000 - Train loss: 8.45446 - Test loss: 116.17526\n",
      "Epoch 1289 - lr: 0.05000 - Train loss: 8.45446 - Test loss: 116.17526\n",
      "Epoch 1290 - lr: 0.05000 - Train loss: 8.45446 - Test loss: 116.17525\n",
      "Epoch 1291 - lr: 0.05000 - Train loss: 8.45446 - Test loss: 116.17525\n",
      "Epoch 1292 - lr: 0.05000 - Train loss: 8.45446 - Test loss: 116.17524\n",
      "Epoch 1293 - lr: 0.05000 - Train loss: 8.45446 - Test loss: 116.17524\n",
      "Epoch 1294 - lr: 0.05000 - Train loss: 8.45446 - Test loss: 116.17523\n",
      "Epoch 1295 - lr: 0.05000 - Train loss: 8.45446 - Test loss: 116.17523\n",
      "Epoch 1296 - lr: 0.05000 - Train loss: 8.45446 - Test loss: 116.17522\n",
      "Epoch 1297 - lr: 0.05000 - Train loss: 8.45446 - Test loss: 116.17522\n",
      "Epoch 1298 - lr: 0.05000 - Train loss: 8.45446 - Test loss: 116.17521\n",
      "Epoch 1299 - lr: 0.05000 - Train loss: 8.45446 - Test loss: 116.17521\n",
      "Epoch 1300 - lr: 0.05000 - Train loss: 8.45446 - Test loss: 116.17520\n",
      "Epoch 1301 - lr: 0.05000 - Train loss: 8.45446 - Test loss: 116.17519\n",
      "Epoch 1302 - lr: 0.05000 - Train loss: 8.45446 - Test loss: 116.17519\n",
      "Epoch 1303 - lr: 0.05000 - Train loss: 8.45446 - Test loss: 116.17518\n",
      "Epoch 1304 - lr: 0.05000 - Train loss: 8.45446 - Test loss: 116.17518\n",
      "Epoch 1305 - lr: 0.05000 - Train loss: 8.45446 - Test loss: 116.17517\n",
      "Epoch 1306 - lr: 0.05000 - Train loss: 8.45446 - Test loss: 116.17516\n",
      "Epoch 1307 - lr: 0.05000 - Train loss: 8.45446 - Test loss: 116.17516\n",
      "Epoch 1308 - lr: 0.05000 - Train loss: 8.45446 - Test loss: 116.17515\n",
      "Epoch 1309 - lr: 0.05000 - Train loss: 8.45446 - Test loss: 116.17514\n",
      "Epoch 1310 - lr: 0.05000 - Train loss: 8.45446 - Test loss: 116.17513\n",
      "Epoch 1311 - lr: 0.05000 - Train loss: 8.45446 - Test loss: 116.17513\n",
      "Epoch 1312 - lr: 0.05000 - Train loss: 8.45446 - Test loss: 116.17512\n",
      "Epoch 1313 - lr: 0.05000 - Train loss: 8.45445 - Test loss: 116.17511\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1314 - lr: 0.05000 - Train loss: 8.45445 - Test loss: 116.17511\n",
      "Epoch 1315 - lr: 0.05000 - Train loss: 8.45445 - Test loss: 116.17510\n",
      "Epoch 1316 - lr: 0.05000 - Train loss: 8.45445 - Test loss: 116.17509\n",
      "Epoch 1317 - lr: 0.05000 - Train loss: 8.45445 - Test loss: 116.17508\n",
      "Epoch 1318 - lr: 0.05000 - Train loss: 8.45445 - Test loss: 116.17507\n",
      "Epoch 1319 - lr: 0.05000 - Train loss: 8.45445 - Test loss: 116.17507\n",
      "Epoch 1320 - lr: 0.05000 - Train loss: 8.45445 - Test loss: 116.17506\n",
      "Epoch 1321 - lr: 0.05000 - Train loss: 8.45445 - Test loss: 116.17505\n",
      "Epoch 1322 - lr: 0.05000 - Train loss: 8.45445 - Test loss: 116.17504\n",
      "Epoch 1323 - lr: 0.05000 - Train loss: 8.45445 - Test loss: 116.17503\n",
      "Epoch 1324 - lr: 0.05000 - Train loss: 8.45445 - Test loss: 116.17502\n",
      "Epoch 1325 - lr: 0.05000 - Train loss: 8.45445 - Test loss: 116.17501\n",
      "Epoch 1326 - lr: 0.05000 - Train loss: 8.45445 - Test loss: 116.17500\n",
      "Epoch 1327 - lr: 0.05000 - Train loss: 8.45445 - Test loss: 116.17499\n",
      "Epoch 1328 - lr: 0.05000 - Train loss: 8.45445 - Test loss: 116.17498\n",
      "Epoch 1329 - lr: 0.05000 - Train loss: 8.45445 - Test loss: 116.17497\n",
      "Epoch 1330 - lr: 0.05000 - Train loss: 8.45445 - Test loss: 116.17496\n",
      "Epoch 1331 - lr: 0.05000 - Train loss: 8.45445 - Test loss: 116.17495\n",
      "Epoch 1332 - lr: 0.05000 - Train loss: 8.45445 - Test loss: 116.17494\n",
      "Epoch 1333 - lr: 0.05000 - Train loss: 8.45445 - Test loss: 116.17492\n",
      "Epoch 1334 - lr: 0.05000 - Train loss: 8.45445 - Test loss: 116.17491\n",
      "Epoch 1335 - lr: 0.05000 - Train loss: 8.45445 - Test loss: 116.17490\n",
      "Epoch 1336 - lr: 0.05000 - Train loss: 8.45445 - Test loss: 116.17489\n",
      "Epoch 1337 - lr: 0.05000 - Train loss: 8.45445 - Test loss: 116.17487\n",
      "Epoch 1338 - lr: 0.05000 - Train loss: 8.45445 - Test loss: 116.17486\n",
      "Epoch 1339 - lr: 0.05000 - Train loss: 8.45444 - Test loss: 116.17484\n",
      "Epoch 1340 - lr: 0.05000 - Train loss: 8.45444 - Test loss: 116.17483\n",
      "Epoch 1341 - lr: 0.05000 - Train loss: 8.45444 - Test loss: 116.17481\n",
      "Epoch 1342 - lr: 0.05000 - Train loss: 8.45444 - Test loss: 116.17480\n",
      "Epoch 1343 - lr: 0.05000 - Train loss: 8.45444 - Test loss: 116.17478\n",
      "Epoch 1344 - lr: 0.05000 - Train loss: 8.45444 - Test loss: 116.17477\n",
      "Epoch 1345 - lr: 0.05000 - Train loss: 8.45444 - Test loss: 116.17475\n",
      "Epoch 1346 - lr: 0.05000 - Train loss: 8.45444 - Test loss: 116.17473\n",
      "Epoch 1347 - lr: 0.05000 - Train loss: 8.45444 - Test loss: 116.17471\n",
      "Epoch 1348 - lr: 0.05000 - Train loss: 8.45444 - Test loss: 116.17469\n",
      "Epoch 1349 - lr: 0.05000 - Train loss: 8.45444 - Test loss: 116.17467\n",
      "Epoch 1350 - lr: 0.05000 - Train loss: 8.45444 - Test loss: 116.17465\n",
      "Epoch 1351 - lr: 0.05000 - Train loss: 8.45444 - Test loss: 116.17463\n",
      "Epoch 1352 - lr: 0.05000 - Train loss: 8.45444 - Test loss: 116.17460\n",
      "Epoch 1353 - lr: 0.05000 - Train loss: 8.45443 - Test loss: 116.17458\n",
      "Epoch 1354 - lr: 0.05000 - Train loss: 8.45443 - Test loss: 116.17455\n",
      "Epoch 1355 - lr: 0.05000 - Train loss: 8.45443 - Test loss: 116.17453\n",
      "Epoch 1356 - lr: 0.05000 - Train loss: 8.45443 - Test loss: 116.17450\n",
      "Epoch 1357 - lr: 0.05000 - Train loss: 8.45443 - Test loss: 116.17447\n",
      "Epoch 1358 - lr: 0.05000 - Train loss: 8.45443 - Test loss: 116.17444\n",
      "Epoch 1359 - lr: 0.05000 - Train loss: 8.45443 - Test loss: 116.17440\n",
      "Epoch 1360 - lr: 0.05000 - Train loss: 8.45443 - Test loss: 116.17437\n",
      "Epoch 1361 - lr: 0.05000 - Train loss: 8.45443 - Test loss: 116.17433\n",
      "Epoch 1362 - lr: 0.05000 - Train loss: 8.45442 - Test loss: 116.17429\n",
      "Epoch 1363 - lr: 0.05000 - Train loss: 8.45442 - Test loss: 116.17425\n",
      "Epoch 1364 - lr: 0.05000 - Train loss: 8.45442 - Test loss: 116.17421\n",
      "Epoch 1365 - lr: 0.05000 - Train loss: 8.45442 - Test loss: 116.17416\n",
      "Epoch 1366 - lr: 0.05000 - Train loss: 8.45442 - Test loss: 116.17411\n",
      "Epoch 1367 - lr: 0.05000 - Train loss: 8.45441 - Test loss: 116.17405\n",
      "Epoch 1368 - lr: 0.05000 - Train loss: 8.45441 - Test loss: 116.17399\n",
      "Epoch 1369 - lr: 0.05000 - Train loss: 8.45441 - Test loss: 116.17393\n",
      "Epoch 1370 - lr: 0.05000 - Train loss: 8.45441 - Test loss: 116.17386\n",
      "Epoch 1371 - lr: 0.05000 - Train loss: 8.45440 - Test loss: 116.17378\n",
      "Epoch 1372 - lr: 0.05000 - Train loss: 8.45440 - Test loss: 116.17370\n",
      "Epoch 1373 - lr: 0.05000 - Train loss: 8.45440 - Test loss: 116.17360\n",
      "Epoch 1374 - lr: 0.05000 - Train loss: 8.45439 - Test loss: 116.17350\n",
      "Epoch 1375 - lr: 0.05000 - Train loss: 8.45439 - Test loss: 116.17338\n",
      "Epoch 1376 - lr: 0.05000 - Train loss: 8.45438 - Test loss: 116.17325\n",
      "Epoch 1377 - lr: 0.05000 - Train loss: 8.45438 - Test loss: 116.17311\n",
      "Epoch 1378 - lr: 0.05000 - Train loss: 8.45437 - Test loss: 116.17294\n",
      "Epoch 1379 - lr: 0.05000 - Train loss: 8.45436 - Test loss: 116.17274\n",
      "Epoch 1380 - lr: 0.05000 - Train loss: 8.45435 - Test loss: 116.17251\n",
      "Epoch 1381 - lr: 0.05000 - Train loss: 8.45434 - Test loss: 116.17224\n",
      "Epoch 1382 - lr: 0.05000 - Train loss: 8.45433 - Test loss: 116.17191\n",
      "Epoch 1383 - lr: 0.05000 - Train loss: 8.45432 - Test loss: 116.17151\n",
      "Epoch 1384 - lr: 0.05000 - Train loss: 8.45430 - Test loss: 116.17100\n",
      "Epoch 1385 - lr: 0.05000 - Train loss: 8.45427 - Test loss: 116.17033\n",
      "Epoch 1386 - lr: 0.05000 - Train loss: 8.45424 - Test loss: 116.16942\n",
      "Epoch 1387 - lr: 0.05000 - Train loss: 8.45420 - Test loss: 116.16812\n",
      "Epoch 1388 - lr: 0.05000 - Train loss: 8.45413 - Test loss: 116.16608\n",
      "Epoch 1389 - lr: 0.05000 - Train loss: 8.45402 - Test loss: 116.16249\n",
      "Epoch 1390 - lr: 0.05000 - Train loss: 8.45382 - Test loss: 116.15458\n",
      "Epoch 1391 - lr: 0.05000 - Train loss: 8.45328 - Test loss: 116.12614\n",
      "Epoch 1392 - lr: 0.05000 - Train loss: 8.47737 - Test loss: 115.89123\n",
      "Epoch 1393 - lr: 0.05000 - Train loss: 8.47520 - Test loss: 107.53204\n",
      "Epoch 1394 - lr: 0.05000 - Train loss: 8.19612 - Test loss: 116.17001\n",
      "Epoch 1395 - lr: 0.05000 - Train loss: 8.45418 - Test loss: 116.16876\n",
      "Epoch 1396 - lr: 0.05000 - Train loss: 8.45410 - Test loss: 116.16673\n",
      "Epoch 1397 - lr: 0.05000 - Train loss: 8.45397 - Test loss: 116.16295\n",
      "Epoch 1398 - lr: 0.05000 - Train loss: 8.45369 - Test loss: 116.15349\n",
      "Epoch 1399 - lr: 0.05000 - Train loss: 8.45276 - Test loss: 116.10214\n",
      "Epoch 1400 - lr: 0.05000 - Train loss: 8.37920 - Test loss: 110.34077\n",
      "Epoch 1401 - lr: 0.05000 - Train loss: 8.15163 - Test loss: 116.17521\n",
      "Epoch 1402 - lr: 0.05000 - Train loss: 8.45446 - Test loss: 116.17521\n",
      "Epoch 1403 - lr: 0.05000 - Train loss: 8.45446 - Test loss: 116.17520\n",
      "Epoch 1404 - lr: 0.05000 - Train loss: 8.45446 - Test loss: 116.17519\n",
      "Epoch 1405 - lr: 0.05000 - Train loss: 8.45446 - Test loss: 116.17519\n",
      "Epoch 1406 - lr: 0.05000 - Train loss: 8.45446 - Test loss: 116.17518\n",
      "Epoch 1407 - lr: 0.05000 - Train loss: 8.45446 - Test loss: 116.17517\n",
      "Epoch 1408 - lr: 0.05000 - Train loss: 8.45446 - Test loss: 116.17516\n",
      "Epoch 1409 - lr: 0.05000 - Train loss: 8.45446 - Test loss: 116.17516\n",
      "Epoch 1410 - lr: 0.05000 - Train loss: 8.45446 - Test loss: 116.17515\n",
      "Epoch 1411 - lr: 0.05000 - Train loss: 8.45446 - Test loss: 116.17514\n",
      "Epoch 1412 - lr: 0.05000 - Train loss: 8.45446 - Test loss: 116.17513\n",
      "Epoch 1413 - lr: 0.05000 - Train loss: 8.45446 - Test loss: 116.17513\n",
      "Epoch 1414 - lr: 0.05000 - Train loss: 8.45446 - Test loss: 116.17512\n",
      "Epoch 1415 - lr: 0.05000 - Train loss: 8.45446 - Test loss: 116.17511\n",
      "Epoch 1416 - lr: 0.05000 - Train loss: 8.45446 - Test loss: 116.17510\n",
      "Epoch 1417 - lr: 0.05000 - Train loss: 8.45446 - Test loss: 116.17509\n",
      "Epoch 1418 - lr: 0.05000 - Train loss: 8.45446 - Test loss: 116.17509\n",
      "Epoch 1419 - lr: 0.05000 - Train loss: 8.45446 - Test loss: 116.17508\n",
      "Epoch 1420 - lr: 0.05000 - Train loss: 8.45446 - Test loss: 116.17507\n",
      "Epoch 1421 - lr: 0.05000 - Train loss: 8.45446 - Test loss: 116.17506\n",
      "Epoch 1422 - lr: 0.05000 - Train loss: 8.45446 - Test loss: 116.17505\n",
      "Epoch 1423 - lr: 0.05000 - Train loss: 8.45446 - Test loss: 116.17504\n",
      "Epoch 1424 - lr: 0.05000 - Train loss: 8.45446 - Test loss: 116.17503\n",
      "Epoch 1425 - lr: 0.05000 - Train loss: 8.45446 - Test loss: 116.17502\n",
      "Epoch 1426 - lr: 0.05000 - Train loss: 8.45446 - Test loss: 116.17501\n",
      "Epoch 1427 - lr: 0.05000 - Train loss: 8.45446 - Test loss: 116.17500\n",
      "Epoch 1428 - lr: 0.05000 - Train loss: 8.45446 - Test loss: 116.17499\n",
      "Epoch 1429 - lr: 0.05000 - Train loss: 8.45446 - Test loss: 116.17498\n",
      "Epoch 1430 - lr: 0.05000 - Train loss: 8.45446 - Test loss: 116.17497\n",
      "Epoch 1431 - lr: 0.05000 - Train loss: 8.45446 - Test loss: 116.17496\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1432 - lr: 0.05000 - Train loss: 8.45446 - Test loss: 116.17495\n",
      "Epoch 1433 - lr: 0.05000 - Train loss: 8.45446 - Test loss: 116.17493\n",
      "Epoch 1434 - lr: 0.05000 - Train loss: 8.45446 - Test loss: 116.17492\n",
      "Epoch 1435 - lr: 0.05000 - Train loss: 8.45446 - Test loss: 116.17491\n",
      "Epoch 1436 - lr: 0.05000 - Train loss: 8.45446 - Test loss: 116.17490\n",
      "Epoch 1437 - lr: 0.05000 - Train loss: 8.45446 - Test loss: 116.17488\n",
      "Epoch 1438 - lr: 0.05000 - Train loss: 8.45446 - Test loss: 116.17487\n",
      "Epoch 1439 - lr: 0.05000 - Train loss: 8.45446 - Test loss: 116.17486\n",
      "Epoch 1440 - lr: 0.05000 - Train loss: 8.45446 - Test loss: 116.17484\n",
      "Epoch 1441 - lr: 0.05000 - Train loss: 8.45446 - Test loss: 116.17483\n",
      "Epoch 1442 - lr: 0.05000 - Train loss: 8.45446 - Test loss: 116.17481\n",
      "Epoch 1443 - lr: 0.05000 - Train loss: 8.45446 - Test loss: 116.17480\n",
      "Epoch 1444 - lr: 0.05000 - Train loss: 8.45446 - Test loss: 116.17478\n",
      "Epoch 1445 - lr: 0.05000 - Train loss: 8.45446 - Test loss: 116.17477\n",
      "Epoch 1446 - lr: 0.05000 - Train loss: 8.45446 - Test loss: 116.17475\n",
      "Epoch 1447 - lr: 0.05000 - Train loss: 8.45446 - Test loss: 116.17473\n",
      "Epoch 1448 - lr: 0.05000 - Train loss: 8.45445 - Test loss: 116.17471\n",
      "Epoch 1449 - lr: 0.05000 - Train loss: 8.45445 - Test loss: 116.17470\n",
      "Epoch 1450 - lr: 0.05000 - Train loss: 8.45445 - Test loss: 116.17468\n",
      "Epoch 1451 - lr: 0.05000 - Train loss: 8.45445 - Test loss: 116.17466\n",
      "Epoch 1452 - lr: 0.05000 - Train loss: 8.45445 - Test loss: 116.17464\n",
      "Epoch 1453 - lr: 0.05000 - Train loss: 8.45445 - Test loss: 116.17462\n",
      "Epoch 1454 - lr: 0.05000 - Train loss: 8.45445 - Test loss: 116.17460\n",
      "Epoch 1455 - lr: 0.05000 - Train loss: 8.45445 - Test loss: 116.17457\n",
      "Epoch 1456 - lr: 0.05000 - Train loss: 8.45445 - Test loss: 116.17455\n",
      "Epoch 1457 - lr: 0.05000 - Train loss: 8.45445 - Test loss: 116.17453\n",
      "Epoch 1458 - lr: 0.05000 - Train loss: 8.45445 - Test loss: 116.17450\n",
      "Epoch 1459 - lr: 0.05000 - Train loss: 8.45445 - Test loss: 116.17448\n",
      "Epoch 1460 - lr: 0.05000 - Train loss: 8.45445 - Test loss: 116.17445\n",
      "Epoch 1461 - lr: 0.05000 - Train loss: 8.45445 - Test loss: 116.17442\n",
      "Epoch 1462 - lr: 0.05000 - Train loss: 8.45445 - Test loss: 116.17440\n",
      "Epoch 1463 - lr: 0.05000 - Train loss: 8.45445 - Test loss: 116.17437\n",
      "Epoch 1464 - lr: 0.05000 - Train loss: 8.45445 - Test loss: 116.17434\n",
      "Epoch 1465 - lr: 0.05000 - Train loss: 8.45445 - Test loss: 116.17430\n",
      "Epoch 1466 - lr: 0.05000 - Train loss: 8.45445 - Test loss: 116.17427\n",
      "Epoch 1467 - lr: 0.05000 - Train loss: 8.45445 - Test loss: 116.17424\n",
      "Epoch 1468 - lr: 0.05000 - Train loss: 8.45445 - Test loss: 116.17420\n",
      "Epoch 1469 - lr: 0.05000 - Train loss: 8.45445 - Test loss: 116.17416\n",
      "Epoch 1470 - lr: 0.05000 - Train loss: 8.45445 - Test loss: 116.17412\n",
      "Epoch 1471 - lr: 0.05000 - Train loss: 8.45445 - Test loss: 116.17408\n",
      "Epoch 1472 - lr: 0.05000 - Train loss: 8.45445 - Test loss: 116.17403\n",
      "Epoch 1473 - lr: 0.05000 - Train loss: 8.45445 - Test loss: 116.17399\n",
      "Epoch 1474 - lr: 0.05000 - Train loss: 8.45445 - Test loss: 116.17394\n",
      "Epoch 1475 - lr: 0.05000 - Train loss: 8.45444 - Test loss: 116.17389\n",
      "Epoch 1476 - lr: 0.05000 - Train loss: 8.45444 - Test loss: 116.17383\n",
      "Epoch 1477 - lr: 0.05000 - Train loss: 8.45444 - Test loss: 116.17378\n",
      "Epoch 1478 - lr: 0.05000 - Train loss: 8.45444 - Test loss: 116.17372\n",
      "Epoch 1479 - lr: 0.05000 - Train loss: 8.45444 - Test loss: 116.17365\n",
      "Epoch 1480 - lr: 0.05000 - Train loss: 8.45444 - Test loss: 116.17358\n",
      "Epoch 1481 - lr: 0.05000 - Train loss: 8.45444 - Test loss: 116.17351\n",
      "Epoch 1482 - lr: 0.05000 - Train loss: 8.45444 - Test loss: 116.17343\n",
      "Epoch 1483 - lr: 0.05000 - Train loss: 8.45444 - Test loss: 116.17334\n",
      "Epoch 1484 - lr: 0.05000 - Train loss: 8.45444 - Test loss: 116.17325\n",
      "Epoch 1485 - lr: 0.05000 - Train loss: 8.45444 - Test loss: 116.17316\n",
      "Epoch 1486 - lr: 0.05000 - Train loss: 8.45444 - Test loss: 116.17305\n",
      "Epoch 1487 - lr: 0.05000 - Train loss: 8.45443 - Test loss: 116.17294\n",
      "Epoch 1488 - lr: 0.05000 - Train loss: 8.45443 - Test loss: 116.17281\n",
      "Epoch 1489 - lr: 0.05000 - Train loss: 8.45443 - Test loss: 116.17267\n",
      "Epoch 1490 - lr: 0.05000 - Train loss: 8.45443 - Test loss: 116.17252\n",
      "Epoch 1491 - lr: 0.05000 - Train loss: 8.45443 - Test loss: 116.17236\n",
      "Epoch 1492 - lr: 0.05000 - Train loss: 8.45443 - Test loss: 116.17217\n",
      "Epoch 1493 - lr: 0.05000 - Train loss: 8.45443 - Test loss: 116.17197\n",
      "Epoch 1494 - lr: 0.05000 - Train loss: 8.45442 - Test loss: 116.17173\n",
      "Epoch 1495 - lr: 0.05000 - Train loss: 8.45442 - Test loss: 116.17147\n",
      "Epoch 1496 - lr: 0.05000 - Train loss: 8.45442 - Test loss: 116.17117\n",
      "Epoch 1497 - lr: 0.05000 - Train loss: 8.45442 - Test loss: 116.17082\n",
      "Epoch 1498 - lr: 0.05000 - Train loss: 8.45441 - Test loss: 116.17041\n",
      "Epoch 1499 - lr: 0.05000 - Train loss: 8.45441 - Test loss: 116.16992\n",
      "Epoch 1500 - lr: 0.05000 - Train loss: 8.45441 - Test loss: 116.16933\n",
      "Epoch 1501 - lr: 0.05000 - Train loss: 8.45440 - Test loss: 116.16861\n",
      "Epoch 1502 - lr: 0.05000 - Train loss: 8.45439 - Test loss: 116.16769\n",
      "Epoch 1503 - lr: 0.05000 - Train loss: 8.45439 - Test loss: 116.16650\n",
      "Epoch 1504 - lr: 0.05000 - Train loss: 8.45437 - Test loss: 116.16488\n",
      "Epoch 1505 - lr: 0.05000 - Train loss: 8.45436 - Test loss: 116.16254\n",
      "Epoch 1506 - lr: 0.05000 - Train loss: 8.45433 - Test loss: 116.15891\n",
      "Epoch 1507 - lr: 0.05000 - Train loss: 8.45429 - Test loss: 116.15253\n",
      "Epoch 1508 - lr: 0.05000 - Train loss: 8.45420 - Test loss: 116.13863\n",
      "Epoch 1509 - lr: 0.05000 - Train loss: 8.45388 - Test loss: 116.08974\n",
      "Epoch 1510 - lr: 0.05000 - Train loss: 8.45622 - Test loss: 115.87358\n",
      "Epoch 1511 - lr: 0.05000 - Train loss: 8.64638 - Test loss: 102.52768\n",
      "Epoch 1512 - lr: 0.05000 - Train loss: 8.71517 - Test loss: 113.68513\n",
      "Epoch 1513 - lr: 0.05000 - Train loss: 9.05125 - Test loss: 113.68509\n",
      "Epoch 1514 - lr: 0.05000 - Train loss: 9.05125 - Test loss: 113.68505\n",
      "Epoch 1515 - lr: 0.05000 - Train loss: 9.05125 - Test loss: 113.68501\n",
      "Epoch 1516 - lr: 0.05000 - Train loss: 9.05125 - Test loss: 113.68496\n",
      "Epoch 1517 - lr: 0.05000 - Train loss: 9.05124 - Test loss: 113.68491\n",
      "Epoch 1518 - lr: 0.05000 - Train loss: 9.05124 - Test loss: 113.68485\n",
      "Epoch 1519 - lr: 0.05000 - Train loss: 9.05124 - Test loss: 113.68479\n",
      "Epoch 1520 - lr: 0.05000 - Train loss: 9.05124 - Test loss: 113.68473\n",
      "Epoch 1521 - lr: 0.05000 - Train loss: 9.05123 - Test loss: 113.68465\n",
      "Epoch 1522 - lr: 0.05000 - Train loss: 9.05123 - Test loss: 113.68457\n",
      "Epoch 1523 - lr: 0.05000 - Train loss: 9.05122 - Test loss: 113.68448\n",
      "Epoch 1524 - lr: 0.05000 - Train loss: 9.05122 - Test loss: 113.68437\n",
      "Epoch 1525 - lr: 0.05000 - Train loss: 9.05121 - Test loss: 113.68426\n",
      "Epoch 1526 - lr: 0.05000 - Train loss: 9.05120 - Test loss: 113.68412\n",
      "Epoch 1527 - lr: 0.05000 - Train loss: 9.05120 - Test loss: 113.68396\n",
      "Epoch 1528 - lr: 0.05000 - Train loss: 9.05119 - Test loss: 113.68378\n",
      "Epoch 1529 - lr: 0.05000 - Train loss: 9.05117 - Test loss: 113.68356\n",
      "Epoch 1530 - lr: 0.05000 - Train loss: 9.05116 - Test loss: 113.68329\n",
      "Epoch 1531 - lr: 0.05000 - Train loss: 9.05114 - Test loss: 113.68295\n",
      "Epoch 1532 - lr: 0.05000 - Train loss: 9.05112 - Test loss: 113.68252\n",
      "Epoch 1533 - lr: 0.05000 - Train loss: 9.05110 - Test loss: 113.68196\n",
      "Epoch 1534 - lr: 0.05000 - Train loss: 9.05106 - Test loss: 113.68116\n",
      "Epoch 1535 - lr: 0.05000 - Train loss: 9.05101 - Test loss: 113.67999\n",
      "Epoch 1536 - lr: 0.05000 - Train loss: 9.05093 - Test loss: 113.67805\n",
      "Epoch 1537 - lr: 0.05000 - Train loss: 9.05079 - Test loss: 113.67428\n",
      "Epoch 1538 - lr: 0.05000 - Train loss: 9.05047 - Test loss: 113.66401\n",
      "Epoch 1539 - lr: 0.05000 - Train loss: 9.04925 - Test loss: 113.58212\n",
      "Epoch 1540 - lr: 0.05000 - Train loss: 8.92030 - Test loss: 111.08372\n",
      "Epoch 1541 - lr: 0.05000 - Train loss: 8.26712 - Test loss: 115.44013\n",
      "Epoch 1542 - lr: 0.05000 - Train loss: 8.59526 - Test loss: 113.68474\n",
      "Epoch 1543 - lr: 0.05000 - Train loss: 9.05120 - Test loss: 113.68465\n",
      "Epoch 1544 - lr: 0.05000 - Train loss: 9.05119 - Test loss: 113.68455\n",
      "Epoch 1545 - lr: 0.05000 - Train loss: 9.05118 - Test loss: 113.68442\n",
      "Epoch 1546 - lr: 0.05000 - Train loss: 9.05117 - Test loss: 113.68428\n",
      "Epoch 1547 - lr: 0.05000 - Train loss: 9.05116 - Test loss: 113.68410\n",
      "Epoch 1548 - lr: 0.05000 - Train loss: 9.05114 - Test loss: 113.68389\n",
      "Epoch 1549 - lr: 0.05000 - Train loss: 9.05112 - Test loss: 113.68362\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1550 - lr: 0.05000 - Train loss: 9.05109 - Test loss: 113.68326\n",
      "Epoch 1551 - lr: 0.05000 - Train loss: 9.05106 - Test loss: 113.68277\n",
      "Epoch 1552 - lr: 0.05000 - Train loss: 9.05101 - Test loss: 113.68205\n",
      "Epoch 1553 - lr: 0.05000 - Train loss: 9.05093 - Test loss: 113.68087\n",
      "Epoch 1554 - lr: 0.05000 - Train loss: 9.05079 - Test loss: 113.67858\n",
      "Epoch 1555 - lr: 0.05000 - Train loss: 9.05046 - Test loss: 113.67215\n",
      "Epoch 1556 - lr: 0.05000 - Train loss: 9.04849 - Test loss: 113.59359\n",
      "Epoch 1557 - lr: 0.05000 - Train loss: 8.62163 - Test loss: 81.08297\n",
      "Epoch 1558 - lr: 0.05000 - Train loss: 8.28056 - Test loss: 113.68434\n",
      "Epoch 1559 - lr: 0.05000 - Train loss: 9.05117 - Test loss: 113.68421\n",
      "Epoch 1560 - lr: 0.05000 - Train loss: 9.05116 - Test loss: 113.68407\n",
      "Epoch 1561 - lr: 0.05000 - Train loss: 9.05115 - Test loss: 113.68391\n",
      "Epoch 1562 - lr: 0.05000 - Train loss: 9.05113 - Test loss: 113.68372\n",
      "Epoch 1563 - lr: 0.05000 - Train loss: 9.05112 - Test loss: 113.68350\n",
      "Epoch 1564 - lr: 0.05000 - Train loss: 9.05110 - Test loss: 113.68324\n",
      "Epoch 1565 - lr: 0.05000 - Train loss: 9.05108 - Test loss: 113.68292\n",
      "Epoch 1566 - lr: 0.05000 - Train loss: 9.05105 - Test loss: 113.68252\n",
      "Epoch 1567 - lr: 0.05000 - Train loss: 9.05102 - Test loss: 113.68202\n",
      "Epoch 1568 - lr: 0.05000 - Train loss: 9.05097 - Test loss: 113.68137\n",
      "Epoch 1569 - lr: 0.05000 - Train loss: 9.05091 - Test loss: 113.68048\n",
      "Epoch 1570 - lr: 0.05000 - Train loss: 9.05083 - Test loss: 113.67918\n",
      "Epoch 1571 - lr: 0.05000 - Train loss: 9.05070 - Test loss: 113.67713\n",
      "Epoch 1572 - lr: 0.05000 - Train loss: 9.05048 - Test loss: 113.67335\n",
      "Epoch 1573 - lr: 0.05000 - Train loss: 9.05000 - Test loss: 113.66381\n",
      "Epoch 1574 - lr: 0.05000 - Train loss: 9.04721 - Test loss: 113.54959\n",
      "Epoch 1575 - lr: 0.05000 - Train loss: 8.34892 - Test loss: 93.69958\n",
      "Epoch 1576 - lr: 0.05000 - Train loss: 7.59086 - Test loss: 96.13737\n",
      "Epoch 1577 - lr: 0.05000 - Train loss: 8.10870 - Test loss: 116.17933\n",
      "Epoch 1578 - lr: 0.05000 - Train loss: 8.17508 - Test loss: 73.15316\n",
      "Epoch 1579 - lr: 0.05000 - Train loss: 6.89958 - Test loss: 60.59773\n",
      "Epoch 1580 - lr: 0.05000 - Train loss: 5.92161 - Test loss: 45.97934\n",
      "Epoch 1581 - lr: 0.05000 - Train loss: 6.11946 - Test loss: 103.62297\n",
      "Epoch 1582 - lr: 0.05000 - Train loss: 8.21514 - Test loss: 116.17552\n",
      "Epoch 1583 - lr: 0.05000 - Train loss: 8.45443 - Test loss: 116.17546\n",
      "Epoch 1584 - lr: 0.05000 - Train loss: 8.45443 - Test loss: 116.17540\n",
      "Epoch 1585 - lr: 0.05000 - Train loss: 8.45442 - Test loss: 116.17534\n",
      "Epoch 1586 - lr: 0.05000 - Train loss: 8.45441 - Test loss: 116.17527\n",
      "Epoch 1587 - lr: 0.05000 - Train loss: 8.45441 - Test loss: 116.17520\n",
      "Epoch 1588 - lr: 0.05000 - Train loss: 8.45440 - Test loss: 116.17512\n",
      "Epoch 1589 - lr: 0.05000 - Train loss: 8.45439 - Test loss: 116.17503\n",
      "Epoch 1590 - lr: 0.05000 - Train loss: 8.45438 - Test loss: 116.17492\n",
      "Epoch 1591 - lr: 0.05000 - Train loss: 8.45437 - Test loss: 116.17481\n",
      "Epoch 1592 - lr: 0.05000 - Train loss: 8.45435 - Test loss: 116.17467\n",
      "Epoch 1593 - lr: 0.05000 - Train loss: 8.45434 - Test loss: 116.17450\n",
      "Epoch 1594 - lr: 0.05000 - Train loss: 8.45432 - Test loss: 116.17430\n",
      "Epoch 1595 - lr: 0.05000 - Train loss: 8.45429 - Test loss: 116.17404\n",
      "Epoch 1596 - lr: 0.05000 - Train loss: 8.45426 - Test loss: 116.17368\n",
      "Epoch 1597 - lr: 0.05000 - Train loss: 8.45421 - Test loss: 116.17316\n",
      "Epoch 1598 - lr: 0.05000 - Train loss: 8.45414 - Test loss: 116.17235\n",
      "Epoch 1599 - lr: 0.05000 - Train loss: 8.45401 - Test loss: 116.17083\n",
      "Epoch 1600 - lr: 0.05000 - Train loss: 8.45375 - Test loss: 116.16699\n",
      "Epoch 1601 - lr: 0.05000 - Train loss: 8.45266 - Test loss: 116.14198\n",
      "Epoch 1602 - lr: 0.05000 - Train loss: 8.58878 - Test loss: 107.96850\n",
      "Epoch 1603 - lr: 0.05000 - Train loss: 8.18024 - Test loss: 90.21803\n",
      "Epoch 1604 - lr: 0.05000 - Train loss: 6.85680 - Test loss: 73.82950\n",
      "Epoch 1605 - lr: 0.05000 - Train loss: 7.28140 - Test loss: 118.83655\n",
      "Epoch 1606 - lr: 0.05000 - Train loss: 8.07546 - Test loss: 118.83612\n",
      "Epoch 1607 - lr: 0.05000 - Train loss: 8.07541 - Test loss: 118.83579\n",
      "Epoch 1608 - lr: 0.05000 - Train loss: 8.07538 - Test loss: 118.83553\n",
      "Epoch 1609 - lr: 0.05000 - Train loss: 8.07535 - Test loss: 118.83531\n",
      "Epoch 1610 - lr: 0.05000 - Train loss: 8.07532 - Test loss: 118.83514\n",
      "Epoch 1611 - lr: 0.05000 - Train loss: 8.07530 - Test loss: 118.83499\n",
      "Epoch 1612 - lr: 0.05000 - Train loss: 8.07529 - Test loss: 118.83486\n",
      "Epoch 1613 - lr: 0.05000 - Train loss: 8.07527 - Test loss: 118.83475\n",
      "Epoch 1614 - lr: 0.05000 - Train loss: 8.07526 - Test loss: 118.83465\n",
      "Epoch 1615 - lr: 0.05000 - Train loss: 8.07525 - Test loss: 118.83457\n",
      "Epoch 1616 - lr: 0.05000 - Train loss: 8.07524 - Test loss: 118.83449\n",
      "Epoch 1617 - lr: 0.05000 - Train loss: 8.07523 - Test loss: 118.83442\n",
      "Epoch 1618 - lr: 0.05000 - Train loss: 8.07522 - Test loss: 118.83436\n",
      "Epoch 1619 - lr: 0.05000 - Train loss: 8.07522 - Test loss: 118.83430\n",
      "Epoch 1620 - lr: 0.05000 - Train loss: 8.07521 - Test loss: 118.83424\n",
      "Epoch 1621 - lr: 0.05000 - Train loss: 8.07520 - Test loss: 118.83420\n",
      "Epoch 1622 - lr: 0.05000 - Train loss: 8.07520 - Test loss: 118.83415\n",
      "Epoch 1623 - lr: 0.05000 - Train loss: 8.07519 - Test loss: 118.83411\n",
      "Epoch 1624 - lr: 0.05000 - Train loss: 8.07519 - Test loss: 118.83407\n",
      "Epoch 1625 - lr: 0.05000 - Train loss: 8.07518 - Test loss: 118.83403\n",
      "Epoch 1626 - lr: 0.05000 - Train loss: 8.07518 - Test loss: 118.83400\n",
      "Epoch 1627 - lr: 0.05000 - Train loss: 8.07518 - Test loss: 118.83396\n",
      "Epoch 1628 - lr: 0.05000 - Train loss: 8.07517 - Test loss: 118.83393\n",
      "Epoch 1629 - lr: 0.05000 - Train loss: 8.07517 - Test loss: 118.83390\n",
      "Epoch 1630 - lr: 0.05000 - Train loss: 8.07517 - Test loss: 118.83388\n",
      "Epoch 1631 - lr: 0.05000 - Train loss: 8.07516 - Test loss: 118.83385\n",
      "Epoch 1632 - lr: 0.05000 - Train loss: 8.07516 - Test loss: 118.83382\n",
      "Epoch 1633 - lr: 0.05000 - Train loss: 8.07516 - Test loss: 118.83380\n",
      "Epoch 1634 - lr: 0.05000 - Train loss: 8.07516 - Test loss: 118.83378\n",
      "Epoch 1635 - lr: 0.05000 - Train loss: 8.07515 - Test loss: 118.83375\n",
      "Epoch 1636 - lr: 0.05000 - Train loss: 8.07515 - Test loss: 118.83373\n",
      "Epoch 1637 - lr: 0.05000 - Train loss: 8.07515 - Test loss: 118.83371\n",
      "Epoch 1638 - lr: 0.05000 - Train loss: 8.07515 - Test loss: 118.83369\n",
      "Epoch 1639 - lr: 0.05000 - Train loss: 8.07515 - Test loss: 118.83367\n",
      "Epoch 1640 - lr: 0.05000 - Train loss: 8.07514 - Test loss: 118.83365\n",
      "Epoch 1641 - lr: 0.05000 - Train loss: 8.07514 - Test loss: 118.83364\n",
      "Epoch 1642 - lr: 0.05000 - Train loss: 8.07514 - Test loss: 118.83362\n",
      "Epoch 1643 - lr: 0.05000 - Train loss: 8.07514 - Test loss: 118.83360\n",
      "Epoch 1644 - lr: 0.05000 - Train loss: 8.07514 - Test loss: 118.83359\n",
      "Epoch 1645 - lr: 0.05000 - Train loss: 8.07514 - Test loss: 118.83357\n",
      "Epoch 1646 - lr: 0.05000 - Train loss: 8.07514 - Test loss: 118.83355\n",
      "Epoch 1647 - lr: 0.05000 - Train loss: 8.07513 - Test loss: 118.83354\n",
      "Epoch 1648 - lr: 0.05000 - Train loss: 8.07513 - Test loss: 118.83352\n",
      "Epoch 1649 - lr: 0.05000 - Train loss: 8.07513 - Test loss: 118.83351\n",
      "Epoch 1650 - lr: 0.05000 - Train loss: 8.07513 - Test loss: 118.83349\n",
      "Epoch 1651 - lr: 0.05000 - Train loss: 8.07513 - Test loss: 118.83348\n",
      "Epoch 1652 - lr: 0.05000 - Train loss: 8.07513 - Test loss: 118.83347\n",
      "Epoch 1653 - lr: 0.05000 - Train loss: 8.07513 - Test loss: 118.83345\n",
      "Epoch 1654 - lr: 0.05000 - Train loss: 8.07512 - Test loss: 118.83344\n",
      "Epoch 1655 - lr: 0.05000 - Train loss: 8.07512 - Test loss: 118.83343\n",
      "Epoch 1656 - lr: 0.05000 - Train loss: 8.07512 - Test loss: 118.83341\n",
      "Epoch 1657 - lr: 0.05000 - Train loss: 8.07512 - Test loss: 118.83340\n",
      "Epoch 1658 - lr: 0.05000 - Train loss: 8.07512 - Test loss: 118.83339\n",
      "Epoch 1659 - lr: 0.05000 - Train loss: 8.07512 - Test loss: 118.83338\n",
      "Epoch 1660 - lr: 0.05000 - Train loss: 8.07512 - Test loss: 118.83336\n",
      "Epoch 1661 - lr: 0.05000 - Train loss: 8.07512 - Test loss: 118.83335\n",
      "Epoch 1662 - lr: 0.05000 - Train loss: 8.07512 - Test loss: 118.83334\n",
      "Epoch 1663 - lr: 0.05000 - Train loss: 8.07512 - Test loss: 118.83333\n",
      "Epoch 1664 - lr: 0.05000 - Train loss: 8.07512 - Test loss: 118.83332\n",
      "Epoch 1665 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83331\n",
      "Epoch 1666 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83329\n",
      "Epoch 1667 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1668 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83327\n",
      "Epoch 1669 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83326\n",
      "Epoch 1670 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83325\n",
      "Epoch 1671 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83324\n",
      "Epoch 1672 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83323\n",
      "Epoch 1673 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83322\n",
      "Epoch 1674 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83321\n",
      "Epoch 1675 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83319\n",
      "Epoch 1676 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83318\n",
      "Epoch 1677 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83317\n",
      "Epoch 1678 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83316\n",
      "Epoch 1679 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83315\n",
      "Epoch 1680 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83314\n",
      "Epoch 1681 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83313\n",
      "Epoch 1682 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83312\n",
      "Epoch 1683 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83311\n",
      "Epoch 1684 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83310\n",
      "Epoch 1685 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83309\n",
      "Epoch 1686 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83308\n",
      "Epoch 1687 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83306\n",
      "Epoch 1688 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83305\n",
      "Epoch 1689 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83304\n",
      "Epoch 1690 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83303\n",
      "Epoch 1691 - lr: 0.05000 - Train loss: 8.07509 - Test loss: 118.83302\n",
      "Epoch 1692 - lr: 0.05000 - Train loss: 8.07509 - Test loss: 118.83301\n",
      "Epoch 1693 - lr: 0.05000 - Train loss: 8.07509 - Test loss: 118.83300\n",
      "Epoch 1694 - lr: 0.05000 - Train loss: 8.07509 - Test loss: 118.83299\n",
      "Epoch 1695 - lr: 0.05000 - Train loss: 8.07509 - Test loss: 118.83297\n",
      "Epoch 1696 - lr: 0.05000 - Train loss: 8.07509 - Test loss: 118.83296\n",
      "Epoch 1697 - lr: 0.05000 - Train loss: 8.07509 - Test loss: 118.83295\n",
      "Epoch 1698 - lr: 0.05000 - Train loss: 8.07509 - Test loss: 118.83294\n",
      "Epoch 1699 - lr: 0.05000 - Train loss: 8.07509 - Test loss: 118.83293\n",
      "Epoch 1700 - lr: 0.05000 - Train loss: 8.07509 - Test loss: 118.83292\n",
      "Epoch 1701 - lr: 0.05000 - Train loss: 8.07509 - Test loss: 118.83290\n",
      "Epoch 1702 - lr: 0.05000 - Train loss: 8.07509 - Test loss: 118.83289\n",
      "Epoch 1703 - lr: 0.05000 - Train loss: 8.07509 - Test loss: 118.83288\n",
      "Epoch 1704 - lr: 0.05000 - Train loss: 8.07509 - Test loss: 118.83287\n",
      "Epoch 1705 - lr: 0.05000 - Train loss: 8.07508 - Test loss: 118.83285\n",
      "Epoch 1706 - lr: 0.05000 - Train loss: 8.07508 - Test loss: 118.83284\n",
      "Epoch 1707 - lr: 0.05000 - Train loss: 8.07508 - Test loss: 118.83283\n",
      "Epoch 1708 - lr: 0.05000 - Train loss: 8.07508 - Test loss: 118.83282\n",
      "Epoch 1709 - lr: 0.05000 - Train loss: 8.07508 - Test loss: 118.83280\n",
      "Epoch 1710 - lr: 0.05000 - Train loss: 8.07508 - Test loss: 118.83279\n",
      "Epoch 1711 - lr: 0.05000 - Train loss: 8.07508 - Test loss: 118.83278\n",
      "Epoch 1712 - lr: 0.05000 - Train loss: 8.07508 - Test loss: 118.83276\n",
      "Epoch 1713 - lr: 0.05000 - Train loss: 8.07508 - Test loss: 118.83275\n",
      "Epoch 1714 - lr: 0.05000 - Train loss: 8.07508 - Test loss: 118.83273\n",
      "Epoch 1715 - lr: 0.05000 - Train loss: 8.07508 - Test loss: 118.83272\n",
      "Epoch 1716 - lr: 0.05000 - Train loss: 8.07508 - Test loss: 118.83270\n",
      "Epoch 1717 - lr: 0.05000 - Train loss: 8.07508 - Test loss: 118.83269\n",
      "Epoch 1718 - lr: 0.05000 - Train loss: 8.07508 - Test loss: 118.83267\n",
      "Epoch 1719 - lr: 0.05000 - Train loss: 8.07507 - Test loss: 118.83266\n",
      "Epoch 1720 - lr: 0.05000 - Train loss: 8.07507 - Test loss: 118.83264\n",
      "Epoch 1721 - lr: 0.05000 - Train loss: 8.07507 - Test loss: 118.83262\n",
      "Epoch 1722 - lr: 0.05000 - Train loss: 8.07507 - Test loss: 118.83261\n",
      "Epoch 1723 - lr: 0.05000 - Train loss: 8.07507 - Test loss: 118.83259\n",
      "Epoch 1724 - lr: 0.05000 - Train loss: 8.07507 - Test loss: 118.83257\n",
      "Epoch 1725 - lr: 0.05000 - Train loss: 8.07507 - Test loss: 118.83256\n",
      "Epoch 1726 - lr: 0.05000 - Train loss: 8.07507 - Test loss: 118.83254\n",
      "Epoch 1727 - lr: 0.05000 - Train loss: 8.07507 - Test loss: 118.83252\n",
      "Epoch 1728 - lr: 0.05000 - Train loss: 8.07507 - Test loss: 118.83250\n",
      "Epoch 1729 - lr: 0.05000 - Train loss: 8.07507 - Test loss: 118.83248\n",
      "Epoch 1730 - lr: 0.05000 - Train loss: 8.07506 - Test loss: 118.83246\n",
      "Epoch 1731 - lr: 0.05000 - Train loss: 8.07506 - Test loss: 118.83244\n",
      "Epoch 1732 - lr: 0.05000 - Train loss: 8.07506 - Test loss: 118.83242\n",
      "Epoch 1733 - lr: 0.05000 - Train loss: 8.07506 - Test loss: 118.83240\n",
      "Epoch 1734 - lr: 0.05000 - Train loss: 8.07506 - Test loss: 118.83237\n",
      "Epoch 1735 - lr: 0.05000 - Train loss: 8.07506 - Test loss: 118.83235\n",
      "Epoch 1736 - lr: 0.05000 - Train loss: 8.07506 - Test loss: 118.83233\n",
      "Epoch 1737 - lr: 0.05000 - Train loss: 8.07506 - Test loss: 118.83230\n",
      "Epoch 1738 - lr: 0.05000 - Train loss: 8.07506 - Test loss: 118.83228\n",
      "Epoch 1739 - lr: 0.05000 - Train loss: 8.07505 - Test loss: 118.83225\n",
      "Epoch 1740 - lr: 0.05000 - Train loss: 8.07505 - Test loss: 118.83222\n",
      "Epoch 1741 - lr: 0.05000 - Train loss: 8.07505 - Test loss: 118.83220\n",
      "Epoch 1742 - lr: 0.05000 - Train loss: 8.07505 - Test loss: 118.83217\n",
      "Epoch 1743 - lr: 0.05000 - Train loss: 8.07505 - Test loss: 118.83214\n",
      "Epoch 1744 - lr: 0.05000 - Train loss: 8.07505 - Test loss: 118.83211\n",
      "Epoch 1745 - lr: 0.05000 - Train loss: 8.07505 - Test loss: 118.83207\n",
      "Epoch 1746 - lr: 0.05000 - Train loss: 8.07504 - Test loss: 118.83204\n",
      "Epoch 1747 - lr: 0.05000 - Train loss: 8.07504 - Test loss: 118.83201\n",
      "Epoch 1748 - lr: 0.05000 - Train loss: 8.07504 - Test loss: 118.83197\n",
      "Epoch 1749 - lr: 0.05000 - Train loss: 8.07504 - Test loss: 118.83193\n",
      "Epoch 1750 - lr: 0.05000 - Train loss: 8.07504 - Test loss: 118.83189\n",
      "Epoch 1751 - lr: 0.05000 - Train loss: 8.07504 - Test loss: 118.83185\n",
      "Epoch 1752 - lr: 0.05000 - Train loss: 8.07503 - Test loss: 118.83181\n",
      "Epoch 1753 - lr: 0.05000 - Train loss: 8.07503 - Test loss: 118.83176\n",
      "Epoch 1754 - lr: 0.05000 - Train loss: 8.07503 - Test loss: 118.83172\n",
      "Epoch 1755 - lr: 0.05000 - Train loss: 8.07503 - Test loss: 118.83167\n",
      "Epoch 1756 - lr: 0.05000 - Train loss: 8.07502 - Test loss: 118.83161\n",
      "Epoch 1757 - lr: 0.05000 - Train loss: 8.07502 - Test loss: 118.83156\n",
      "Epoch 1758 - lr: 0.05000 - Train loss: 8.07502 - Test loss: 118.83150\n",
      "Epoch 1759 - lr: 0.05000 - Train loss: 8.07502 - Test loss: 118.83144\n",
      "Epoch 1760 - lr: 0.05000 - Train loss: 8.07501 - Test loss: 118.83138\n",
      "Epoch 1761 - lr: 0.05000 - Train loss: 8.07501 - Test loss: 118.83131\n",
      "Epoch 1762 - lr: 0.05000 - Train loss: 8.07501 - Test loss: 118.83123\n",
      "Epoch 1763 - lr: 0.05000 - Train loss: 8.07500 - Test loss: 118.83116\n",
      "Epoch 1764 - lr: 0.05000 - Train loss: 8.07500 - Test loss: 118.83107\n",
      "Epoch 1765 - lr: 0.05000 - Train loss: 8.07500 - Test loss: 118.83098\n",
      "Epoch 1766 - lr: 0.05000 - Train loss: 8.07499 - Test loss: 118.83089\n",
      "Epoch 1767 - lr: 0.05000 - Train loss: 8.07499 - Test loss: 118.83079\n",
      "Epoch 1768 - lr: 0.05000 - Train loss: 8.07498 - Test loss: 118.83068\n",
      "Epoch 1769 - lr: 0.05000 - Train loss: 8.07498 - Test loss: 118.83056\n",
      "Epoch 1770 - lr: 0.05000 - Train loss: 8.07497 - Test loss: 118.83043\n",
      "Epoch 1771 - lr: 0.05000 - Train loss: 8.07496 - Test loss: 118.83028\n",
      "Epoch 1772 - lr: 0.05000 - Train loss: 8.07496 - Test loss: 118.83013\n",
      "Epoch 1773 - lr: 0.05000 - Train loss: 8.07495 - Test loss: 118.82995\n",
      "Epoch 1774 - lr: 0.05000 - Train loss: 8.07494 - Test loss: 118.82976\n",
      "Epoch 1775 - lr: 0.05000 - Train loss: 8.07493 - Test loss: 118.82955\n",
      "Epoch 1776 - lr: 0.05000 - Train loss: 8.07492 - Test loss: 118.82931\n",
      "Epoch 1777 - lr: 0.05000 - Train loss: 8.07491 - Test loss: 118.82904\n",
      "Epoch 1778 - lr: 0.05000 - Train loss: 8.07489 - Test loss: 118.82873\n",
      "Epoch 1779 - lr: 0.05000 - Train loss: 8.07488 - Test loss: 118.82837\n",
      "Epoch 1780 - lr: 0.05000 - Train loss: 8.07486 - Test loss: 118.82796\n",
      "Epoch 1781 - lr: 0.05000 - Train loss: 8.07484 - Test loss: 118.82746\n",
      "Epoch 1782 - lr: 0.05000 - Train loss: 8.07481 - Test loss: 118.82686\n",
      "Epoch 1783 - lr: 0.05000 - Train loss: 8.07478 - Test loss: 118.82612\n",
      "Epoch 1784 - lr: 0.05000 - Train loss: 8.07474 - Test loss: 118.82517\n",
      "Epoch 1785 - lr: 0.05000 - Train loss: 8.07468 - Test loss: 118.82391\n",
      "Epoch 1786 - lr: 0.05000 - Train loss: 8.07461 - Test loss: 118.82214\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1787 - lr: 0.05000 - Train loss: 8.07450 - Test loss: 118.81941\n",
      "Epoch 1788 - lr: 0.05000 - Train loss: 8.07431 - Test loss: 118.81452\n",
      "Epoch 1789 - lr: 0.05000 - Train loss: 8.07392 - Test loss: 118.80265\n",
      "Epoch 1790 - lr: 0.05000 - Train loss: 8.07243 - Test loss: 118.73013\n",
      "Epoch 1791 - lr: 0.05000 - Train loss: 8.03017 - Test loss: 109.14171\n",
      "Epoch 1792 - lr: 0.05000 - Train loss: 7.58367 - Test loss: 89.00860\n",
      "Epoch 1793 - lr: 0.05000 - Train loss: 7.54425 - Test loss: 118.50628\n",
      "Epoch 1794 - lr: 0.05000 - Train loss: 7.69362 - Test loss: 83.36785\n",
      "Epoch 1795 - lr: 0.05000 - Train loss: 7.46272 - Test loss: 118.79076\n",
      "Epoch 1796 - lr: 0.05000 - Train loss: 8.07262 - Test loss: 118.69165\n",
      "Epoch 1797 - lr: 0.05000 - Train loss: 7.99416 - Test loss: 117.28034\n",
      "Epoch 1798 - lr: 0.05000 - Train loss: 7.83760 - Test loss: 118.47384\n",
      "Epoch 1799 - lr: 0.05000 - Train loss: 7.85378 - Test loss: 99.00323\n",
      "Epoch 1800 - lr: 0.05000 - Train loss: 7.48062 - Test loss: 98.99353\n",
      "Epoch 1801 - lr: 0.05000 - Train loss: 7.22571 - Test loss: 77.15899\n",
      "Epoch 1802 - lr: 0.05000 - Train loss: 6.38453 - Test loss: 79.70198\n",
      "Epoch 1803 - lr: 0.05000 - Train loss: 7.06426 - Test loss: 81.09466\n",
      "Epoch 1804 - lr: 0.05000 - Train loss: 6.55461 - Test loss: 93.97994\n",
      "Epoch 1805 - lr: 0.05000 - Train loss: 7.63586 - Test loss: 71.01432\n",
      "Epoch 1806 - lr: 0.05000 - Train loss: 6.07941 - Test loss: 54.04684\n",
      "Epoch 1807 - lr: 0.05000 - Train loss: 5.54330 - Test loss: 42.66212\n",
      "Epoch 1808 - lr: 0.05000 - Train loss: 5.13420 - Test loss: 33.92768\n",
      "Epoch 1809 - lr: 0.05000 - Train loss: 4.81889 - Test loss: 27.22238\n",
      "Epoch 1810 - lr: 0.05000 - Train loss: 4.57596 - Test loss: 22.06844\n",
      "Epoch 1811 - lr: 0.05000 - Train loss: 4.38740 - Test loss: 18.10311\n",
      "Epoch 1812 - lr: 0.05000 - Train loss: 4.24056 - Test loss: 15.05404\n",
      "Epoch 1813 - lr: 0.05000 - Train loss: 4.12695 - Test loss: 12.70658\n",
      "Epoch 1814 - lr: 0.05000 - Train loss: 4.03893 - Test loss: 10.89626\n",
      "Epoch 1815 - lr: 0.05000 - Train loss: 3.96972 - Test loss: 9.50387\n",
      "Epoch 1816 - lr: 0.05000 - Train loss: 3.91304 - Test loss: 8.44525\n",
      "Epoch 1817 - lr: 0.05000 - Train loss: 3.87303 - Test loss: 7.62393\n",
      "Epoch 1818 - lr: 0.05000 - Train loss: 3.83211 - Test loss: 7.02420\n",
      "Epoch 1819 - lr: 0.05000 - Train loss: 3.81406 - Test loss: 6.44357\n",
      "Epoch 1820 - lr: 0.05000 - Train loss: 3.66895 - Test loss: 104.64769\n",
      "Epoch 1821 - lr: 0.05000 - Train loss: 17.81167 - Test loss: 83.23064\n",
      "Epoch 1822 - lr: 0.05000 - Train loss: 8.62140 - Test loss: 99.27549\n",
      "Epoch 1823 - lr: 0.05000 - Train loss: 7.80029 - Test loss: 102.13217\n",
      "Epoch 1824 - lr: 0.05000 - Train loss: 7.74716 - Test loss: 100.16973\n",
      "Epoch 1825 - lr: 0.05000 - Train loss: 7.51870 - Test loss: 89.89297\n",
      "Epoch 1826 - lr: 0.05000 - Train loss: 6.98680 - Test loss: 72.54515\n",
      "Epoch 1827 - lr: 0.05000 - Train loss: 6.21202 - Test loss: 56.80548\n",
      "Epoch 1828 - lr: 0.05000 - Train loss: 5.65235 - Test loss: 44.71517\n",
      "Epoch 1829 - lr: 0.05000 - Train loss: 5.21881 - Test loss: 35.43281\n",
      "Epoch 1830 - lr: 0.05000 - Train loss: 4.88455 - Test loss: 28.30272\n",
      "Epoch 1831 - lr: 0.05000 - Train loss: 4.62562 - Test loss: 22.82833\n",
      "Epoch 1832 - lr: 0.05000 - Train loss: 4.42309 - Test loss: 18.63836\n",
      "Epoch 1833 - lr: 0.05000 - Train loss: 4.26674 - Test loss: 15.42887\n",
      "Epoch 1834 - lr: 0.05000 - Train loss: 4.14459 - Test loss: 12.97599\n",
      "Epoch 1835 - lr: 0.05000 - Train loss: 4.05150 - Test loss: 11.09360\n",
      "Epoch 1836 - lr: 0.05000 - Train loss: 3.97532 - Test loss: 9.66665\n",
      "Epoch 1837 - lr: 0.05000 - Train loss: 3.92549 - Test loss: 8.54943\n",
      "Epoch 1838 - lr: 0.05000 - Train loss: 3.86842 - Test loss: 7.74941\n",
      "Epoch 1839 - lr: 0.05000 - Train loss: 3.83688 - Test loss: 7.12416\n",
      "Epoch 1840 - lr: 0.05000 - Train loss: 3.81979 - Test loss: 6.61401\n",
      "Epoch 1841 - lr: 0.05000 - Train loss: 3.78799 - Test loss: 6.26071\n",
      "Epoch 1842 - lr: 0.05000 - Train loss: 3.77273 - Test loss: 5.98331\n",
      "Epoch 1843 - lr: 0.05000 - Train loss: 3.76326 - Test loss: 5.75490\n",
      "Epoch 1844 - lr: 0.05000 - Train loss: 3.75779 - Test loss: 5.57500\n",
      "Epoch 1845 - lr: 0.05000 - Train loss: 3.74050 - Test loss: 5.45963\n",
      "Epoch 1846 - lr: 0.05000 - Train loss: 3.73444 - Test loss: 5.36916\n",
      "Epoch 1847 - lr: 0.05000 - Train loss: 3.72909 - Test loss: 5.29648\n",
      "Epoch 1848 - lr: 0.05000 - Train loss: 3.71781 - Test loss: 5.14062\n",
      "Epoch 1849 - lr: 0.05000 - Train loss: 3.34326 - Test loss: 3.67089\n",
      "Epoch 1850 - lr: 0.05000 - Train loss: 5.02510 - Test loss: 112.92423\n",
      "Epoch 1851 - lr: 0.05000 - Train loss: 7.77275 - Test loss: 118.18572\n",
      "Epoch 1852 - lr: 0.05000 - Train loss: 8.43868 - Test loss: 116.65544\n",
      "Epoch 1853 - lr: 0.05000 - Train loss: 7.92943 - Test loss: 118.13136\n",
      "Epoch 1854 - lr: 0.05000 - Train loss: 7.58910 - Test loss: 118.75701\n",
      "Epoch 1855 - lr: 0.05000 - Train loss: 8.10355 - Test loss: 118.83288\n",
      "Epoch 1856 - lr: 0.05000 - Train loss: 8.07478 - Test loss: 118.83287\n",
      "Epoch 1857 - lr: 0.05000 - Train loss: 8.07470 - Test loss: 118.83284\n",
      "Epoch 1858 - lr: 0.05000 - Train loss: 8.07458 - Test loss: 118.83280\n",
      "Epoch 1859 - lr: 0.05000 - Train loss: 8.07435 - Test loss: 118.83272\n",
      "Epoch 1860 - lr: 0.05000 - Train loss: 8.07375 - Test loss: 118.83242\n",
      "Epoch 1861 - lr: 0.05000 - Train loss: 8.07907 - Test loss: 118.79822\n",
      "Epoch 1862 - lr: 0.05000 - Train loss: 8.11785 - Test loss: 118.03659\n",
      "Epoch 1863 - lr: 0.05000 - Train loss: 7.88753 - Test loss: 117.22521\n",
      "Epoch 1864 - lr: 0.05000 - Train loss: 7.88069 - Test loss: 118.34445\n",
      "Epoch 1865 - lr: 0.05000 - Train loss: 7.65661 - Test loss: 118.60177\n",
      "Epoch 1866 - lr: 0.05000 - Train loss: 7.70156 - Test loss: 118.50329\n",
      "Epoch 1867 - lr: 0.05000 - Train loss: 7.62035 - Test loss: 118.81126\n",
      "Epoch 1868 - lr: 0.05000 - Train loss: 7.73240 - Test loss: 118.81750\n",
      "Epoch 1869 - lr: 0.05000 - Train loss: 8.26717 - Test loss: 117.78173\n",
      "Epoch 1870 - lr: 0.05000 - Train loss: 7.61964 - Test loss: 118.78923\n",
      "Epoch 1871 - lr: 0.05000 - Train loss: 7.90553 - Test loss: 118.84886\n",
      "Epoch 1872 - lr: 0.05000 - Train loss: 7.73650 - Test loss: 118.50506\n",
      "Epoch 1873 - lr: 0.05000 - Train loss: 7.80358 - Test loss: 118.62147\n",
      "Epoch 1874 - lr: 0.05000 - Train loss: 7.55675 - Test loss: 118.81686\n",
      "Epoch 1875 - lr: 0.05000 - Train loss: 7.78934 - Test loss: 118.84459\n",
      "Epoch 1876 - lr: 0.05000 - Train loss: 7.60699 - Test loss: 118.90012\n",
      "Epoch 1877 - lr: 0.05000 - Train loss: 7.48537 - Test loss: 119.24992\n",
      "Epoch 1878 - lr: 0.05000 - Train loss: 8.14953 - Test loss: 118.83259\n",
      "Epoch 1879 - lr: 0.05000 - Train loss: 8.07512 - Test loss: 118.83259\n",
      "Epoch 1880 - lr: 0.05000 - Train loss: 8.07512 - Test loss: 118.83260\n",
      "Epoch 1881 - lr: 0.05000 - Train loss: 8.07512 - Test loss: 118.83260\n",
      "Epoch 1882 - lr: 0.05000 - Train loss: 8.07512 - Test loss: 118.83261\n",
      "Epoch 1883 - lr: 0.05000 - Train loss: 8.07512 - Test loss: 118.83261\n",
      "Epoch 1884 - lr: 0.05000 - Train loss: 8.07512 - Test loss: 118.83261\n",
      "Epoch 1885 - lr: 0.05000 - Train loss: 8.07512 - Test loss: 118.83262\n",
      "Epoch 1886 - lr: 0.05000 - Train loss: 8.07512 - Test loss: 118.83262\n",
      "Epoch 1887 - lr: 0.05000 - Train loss: 8.07512 - Test loss: 118.83262\n",
      "Epoch 1888 - lr: 0.05000 - Train loss: 8.07512 - Test loss: 118.83263\n",
      "Epoch 1889 - lr: 0.05000 - Train loss: 8.07512 - Test loss: 118.83263\n",
      "Epoch 1890 - lr: 0.05000 - Train loss: 8.07512 - Test loss: 118.83263\n",
      "Epoch 1891 - lr: 0.05000 - Train loss: 8.07512 - Test loss: 118.83264\n",
      "Epoch 1892 - lr: 0.05000 - Train loss: 8.07512 - Test loss: 118.83264\n",
      "Epoch 1893 - lr: 0.05000 - Train loss: 8.07512 - Test loss: 118.83264\n",
      "Epoch 1894 - lr: 0.05000 - Train loss: 8.07512 - Test loss: 118.83265\n",
      "Epoch 1895 - lr: 0.05000 - Train loss: 8.07512 - Test loss: 118.83265\n",
      "Epoch 1896 - lr: 0.05000 - Train loss: 8.07512 - Test loss: 118.83265\n",
      "Epoch 1897 - lr: 0.05000 - Train loss: 8.07512 - Test loss: 118.83266\n",
      "Epoch 1898 - lr: 0.05000 - Train loss: 8.07512 - Test loss: 118.83266\n",
      "Epoch 1899 - lr: 0.05000 - Train loss: 8.07512 - Test loss: 118.83266\n",
      "Epoch 1900 - lr: 0.05000 - Train loss: 8.07512 - Test loss: 118.83266\n",
      "Epoch 1901 - lr: 0.05000 - Train loss: 8.07512 - Test loss: 118.83267\n",
      "Epoch 1902 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83267\n",
      "Epoch 1903 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83267\n",
      "Epoch 1904 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83267\n",
      "Epoch 1905 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83268\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1906 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83268\n",
      "Epoch 1907 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83268\n",
      "Epoch 1908 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83268\n",
      "Epoch 1909 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83269\n",
      "Epoch 1910 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83269\n",
      "Epoch 1911 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83269\n",
      "Epoch 1912 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83269\n",
      "Epoch 1913 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83269\n",
      "Epoch 1914 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83270\n",
      "Epoch 1915 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83270\n",
      "Epoch 1916 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83270\n",
      "Epoch 1917 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83270\n",
      "Epoch 1918 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83271\n",
      "Epoch 1919 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83271\n",
      "Epoch 1920 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83271\n",
      "Epoch 1921 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83271\n",
      "Epoch 1922 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83271\n",
      "Epoch 1923 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83271\n",
      "Epoch 1924 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83272\n",
      "Epoch 1925 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83272\n",
      "Epoch 1926 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83272\n",
      "Epoch 1927 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83272\n",
      "Epoch 1928 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83272\n",
      "Epoch 1929 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83273\n",
      "Epoch 1930 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83273\n",
      "Epoch 1931 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83273\n",
      "Epoch 1932 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83273\n",
      "Epoch 1933 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83273\n",
      "Epoch 1934 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83273\n",
      "Epoch 1935 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83273\n",
      "Epoch 1936 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83274\n",
      "Epoch 1937 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83274\n",
      "Epoch 1938 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83274\n",
      "Epoch 1939 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83274\n",
      "Epoch 1940 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83274\n",
      "Epoch 1941 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83274\n",
      "Epoch 1942 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83274\n",
      "Epoch 1943 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83275\n",
      "Epoch 1944 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83275\n",
      "Epoch 1945 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83275\n",
      "Epoch 1946 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83275\n",
      "Epoch 1947 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83275\n",
      "Epoch 1948 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83275\n",
      "Epoch 1949 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83275\n",
      "Epoch 1950 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83275\n",
      "Epoch 1951 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83276\n",
      "Epoch 1952 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83276\n",
      "Epoch 1953 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83276\n",
      "Epoch 1954 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83276\n",
      "Epoch 1955 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83276\n",
      "Epoch 1956 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83276\n",
      "Epoch 1957 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83276\n",
      "Epoch 1958 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83276\n",
      "Epoch 1959 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83276\n",
      "Epoch 1960 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83276\n",
      "Epoch 1961 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83277\n",
      "Epoch 1962 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83277\n",
      "Epoch 1963 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83277\n",
      "Epoch 1964 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83277\n",
      "Epoch 1965 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83277\n",
      "Epoch 1966 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83277\n",
      "Epoch 1967 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83277\n",
      "Epoch 1968 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83277\n",
      "Epoch 1969 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83277\n",
      "Epoch 1970 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83277\n",
      "Epoch 1971 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83277\n",
      "Epoch 1972 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83277\n",
      "Epoch 1973 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83278\n",
      "Epoch 1974 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83278\n",
      "Epoch 1975 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83278\n",
      "Epoch 1976 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83278\n",
      "Epoch 1977 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83278\n",
      "Epoch 1978 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83278\n",
      "Epoch 1979 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83278\n",
      "Epoch 1980 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83278\n",
      "Epoch 1981 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83278\n",
      "Epoch 1982 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83278\n",
      "Epoch 1983 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83278\n",
      "Epoch 1984 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83278\n",
      "Epoch 1985 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83278\n",
      "Epoch 1986 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83278\n",
      "Epoch 1987 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83278\n",
      "Epoch 1988 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83278\n",
      "Epoch 1989 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83278\n",
      "Epoch 1990 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83279\n",
      "Epoch 1991 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83279\n",
      "Epoch 1992 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83279\n",
      "Epoch 1993 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83279\n",
      "Epoch 1994 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83279\n",
      "Epoch 1995 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83279\n",
      "Epoch 1996 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83279\n",
      "Epoch 1997 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83279\n",
      "Epoch 1998 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83279\n",
      "Epoch 1999 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83279\n",
      "Epoch 2000 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83279\n",
      "Epoch 2001 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83279\n",
      "Epoch 2002 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83279\n",
      "Epoch 2003 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83279\n",
      "Epoch 2004 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83279\n",
      "Epoch 2005 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83279\n",
      "Epoch 2006 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83279\n",
      "Epoch 2007 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83279\n",
      "Epoch 2008 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83279\n",
      "Epoch 2009 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83279\n",
      "Epoch 2010 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83279\n",
      "Epoch 2011 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83279\n",
      "Epoch 2012 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83279\n",
      "Epoch 2013 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83279\n",
      "Epoch 2014 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83279\n",
      "Epoch 2015 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83279\n",
      "Epoch 2016 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83279\n",
      "Epoch 2017 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83279\n",
      "Epoch 2018 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83279\n",
      "Epoch 2019 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83279\n",
      "Epoch 2020 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83279\n",
      "Epoch 2021 - lr: 0.05000 - Train loss: 8.07511 - Test loss: 118.83279\n",
      "Epoch 2022 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83279\n",
      "Epoch 2023 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83279\n",
      "Epoch 2024 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83279\n",
      "Epoch 2025 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83279\n",
      "Epoch 2026 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83279\n",
      "Epoch 2027 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83279\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2028 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83279\n",
      "Epoch 2029 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83279\n",
      "Epoch 2030 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83279\n",
      "Epoch 2031 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83279\n",
      "Epoch 2032 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83279\n",
      "Epoch 2033 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83279\n",
      "Epoch 2034 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83279\n",
      "Epoch 2035 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83279\n",
      "Epoch 2036 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83279\n",
      "Epoch 2037 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83279\n",
      "Epoch 2038 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83279\n",
      "Epoch 2039 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83279\n",
      "Epoch 2040 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83279\n",
      "Epoch 2041 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83279\n",
      "Epoch 2042 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83279\n",
      "Epoch 2043 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83279\n",
      "Epoch 2044 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83279\n",
      "Epoch 2045 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83279\n",
      "Epoch 2046 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83279\n",
      "Epoch 2047 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83279\n",
      "Epoch 2048 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83279\n",
      "Epoch 2049 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83279\n",
      "Epoch 2050 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83278\n",
      "Epoch 2051 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83278\n",
      "Epoch 2052 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83278\n",
      "Epoch 2053 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83278\n",
      "Epoch 2054 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83278\n",
      "Epoch 2055 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83278\n",
      "Epoch 2056 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83278\n",
      "Epoch 2057 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83278\n",
      "Epoch 2058 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83278\n",
      "Epoch 2059 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83278\n",
      "Epoch 2060 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83278\n",
      "Epoch 2061 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83278\n",
      "Epoch 2062 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83278\n",
      "Epoch 2063 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83278\n",
      "Epoch 2064 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83278\n",
      "Epoch 2065 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83278\n",
      "Epoch 2066 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83278\n",
      "Epoch 2067 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83278\n",
      "Epoch 2068 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83277\n",
      "Epoch 2069 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83277\n",
      "Epoch 2070 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83277\n",
      "Epoch 2071 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83277\n",
      "Epoch 2072 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83277\n",
      "Epoch 2073 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83277\n",
      "Epoch 2074 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83277\n",
      "Epoch 2075 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83277\n",
      "Epoch 2076 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83277\n",
      "Epoch 2077 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83277\n",
      "Epoch 2078 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83277\n",
      "Epoch 2079 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83277\n",
      "Epoch 2080 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83276\n",
      "Epoch 2081 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83276\n",
      "Epoch 2082 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83276\n",
      "Epoch 2083 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83276\n",
      "Epoch 2084 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83276\n",
      "Epoch 2085 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83276\n",
      "Epoch 2086 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83276\n",
      "Epoch 2087 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83276\n",
      "Epoch 2088 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83276\n",
      "Epoch 2089 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83275\n",
      "Epoch 2090 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83275\n",
      "Epoch 2091 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83275\n",
      "Epoch 2092 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83275\n",
      "Epoch 2093 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83275\n",
      "Epoch 2094 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83275\n",
      "Epoch 2095 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83275\n",
      "Epoch 2096 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83275\n",
      "Epoch 2097 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83275\n",
      "Epoch 2098 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83274\n",
      "Epoch 2099 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83274\n",
      "Epoch 2100 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83274\n",
      "Epoch 2101 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83274\n",
      "Epoch 2102 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83274\n",
      "Epoch 2103 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83274\n",
      "Epoch 2104 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83274\n",
      "Epoch 2105 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83273\n",
      "Epoch 2106 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83273\n",
      "Epoch 2107 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83273\n",
      "Epoch 2108 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83273\n",
      "Epoch 2109 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83273\n",
      "Epoch 2110 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83273\n",
      "Epoch 2111 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83272\n",
      "Epoch 2112 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83272\n",
      "Epoch 2113 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83272\n",
      "Epoch 2114 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83272\n",
      "Epoch 2115 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83272\n",
      "Epoch 2116 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83272\n",
      "Epoch 2117 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83271\n",
      "Epoch 2118 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83271\n",
      "Epoch 2119 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83271\n",
      "Epoch 2120 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83271\n",
      "Epoch 2121 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83271\n",
      "Epoch 2122 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83270\n",
      "Epoch 2123 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83270\n",
      "Epoch 2124 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83270\n",
      "Epoch 2125 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83270\n",
      "Epoch 2126 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83270\n",
      "Epoch 2127 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83269\n",
      "Epoch 2128 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83269\n",
      "Epoch 2129 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83269\n",
      "Epoch 2130 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83269\n",
      "Epoch 2131 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83268\n",
      "Epoch 2132 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83268\n",
      "Epoch 2133 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83268\n",
      "Epoch 2134 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83268\n",
      "Epoch 2135 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83268\n",
      "Epoch 2136 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83267\n",
      "Epoch 2137 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83267\n",
      "Epoch 2138 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83267\n",
      "Epoch 2139 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83267\n",
      "Epoch 2140 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83266\n",
      "Epoch 2141 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83266\n",
      "Epoch 2142 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83266\n",
      "Epoch 2143 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83265\n",
      "Epoch 2144 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83265\n",
      "Epoch 2145 - lr: 0.05000 - Train loss: 8.07509 - Test loss: 118.83265\n",
      "Epoch 2146 - lr: 0.05000 - Train loss: 8.07509 - Test loss: 118.83265\n",
      "Epoch 2147 - lr: 0.05000 - Train loss: 8.07509 - Test loss: 118.83264\n",
      "Epoch 2148 - lr: 0.05000 - Train loss: 8.07509 - Test loss: 118.83264\n",
      "Epoch 2149 - lr: 0.05000 - Train loss: 8.07509 - Test loss: 118.83264\n",
      "Epoch 2150 - lr: 0.05000 - Train loss: 8.07509 - Test loss: 118.83263\n",
      "Epoch 2151 - lr: 0.05000 - Train loss: 8.07509 - Test loss: 118.83263\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2152 - lr: 0.05000 - Train loss: 8.07509 - Test loss: 118.83263\n",
      "Epoch 2153 - lr: 0.05000 - Train loss: 8.07509 - Test loss: 118.83262\n",
      "Epoch 2154 - lr: 0.05000 - Train loss: 8.07509 - Test loss: 118.83262\n",
      "Epoch 2155 - lr: 0.05000 - Train loss: 8.07509 - Test loss: 118.83262\n",
      "Epoch 2156 - lr: 0.05000 - Train loss: 8.07509 - Test loss: 118.83261\n",
      "Epoch 2157 - lr: 0.05000 - Train loss: 8.07509 - Test loss: 118.83261\n",
      "Epoch 2158 - lr: 0.05000 - Train loss: 8.07509 - Test loss: 118.83260\n",
      "Epoch 2159 - lr: 0.05000 - Train loss: 8.07509 - Test loss: 118.83260\n",
      "Epoch 2160 - lr: 0.05000 - Train loss: 8.07509 - Test loss: 118.83260\n",
      "Epoch 2161 - lr: 0.05000 - Train loss: 8.07509 - Test loss: 118.83259\n",
      "Epoch 2162 - lr: 0.05000 - Train loss: 8.07509 - Test loss: 118.83259\n",
      "Epoch 2163 - lr: 0.05000 - Train loss: 8.07509 - Test loss: 118.83259\n",
      "Epoch 2164 - lr: 0.05000 - Train loss: 8.07509 - Test loss: 118.83258\n",
      "Epoch 2165 - lr: 0.05000 - Train loss: 8.07509 - Test loss: 118.83258\n",
      "Epoch 2166 - lr: 0.05000 - Train loss: 8.07509 - Test loss: 118.83257\n",
      "Epoch 2167 - lr: 0.05000 - Train loss: 8.07509 - Test loss: 118.83257\n",
      "Epoch 2168 - lr: 0.05000 - Train loss: 8.07509 - Test loss: 118.83256\n",
      "Epoch 2169 - lr: 0.05000 - Train loss: 8.07509 - Test loss: 118.83256\n",
      "Epoch 2170 - lr: 0.05000 - Train loss: 8.07509 - Test loss: 118.83255\n",
      "Epoch 2171 - lr: 0.05000 - Train loss: 8.07509 - Test loss: 118.83255\n",
      "Epoch 2172 - lr: 0.05000 - Train loss: 8.07509 - Test loss: 118.83254\n",
      "Epoch 2173 - lr: 0.05000 - Train loss: 8.07509 - Test loss: 118.83254\n",
      "Epoch 2174 - lr: 0.05000 - Train loss: 8.07509 - Test loss: 118.83253\n",
      "Epoch 2175 - lr: 0.05000 - Train loss: 8.07509 - Test loss: 118.83253\n",
      "Epoch 2176 - lr: 0.05000 - Train loss: 8.07509 - Test loss: 118.83252\n",
      "Epoch 2177 - lr: 0.05000 - Train loss: 8.07509 - Test loss: 118.83252\n",
      "Epoch 2178 - lr: 0.05000 - Train loss: 8.07509 - Test loss: 118.83251\n",
      "Epoch 2179 - lr: 0.05000 - Train loss: 8.07509 - Test loss: 118.83251\n",
      "Epoch 2180 - lr: 0.05000 - Train loss: 8.07509 - Test loss: 118.83250\n",
      "Epoch 2181 - lr: 0.05000 - Train loss: 8.07509 - Test loss: 118.83250\n",
      "Epoch 2182 - lr: 0.05000 - Train loss: 8.07509 - Test loss: 118.83249\n",
      "Epoch 2183 - lr: 0.05000 - Train loss: 8.07509 - Test loss: 118.83248\n",
      "Epoch 2184 - lr: 0.05000 - Train loss: 8.07509 - Test loss: 118.83248\n",
      "Epoch 2185 - lr: 0.05000 - Train loss: 8.07509 - Test loss: 118.83247\n",
      "Epoch 2186 - lr: 0.05000 - Train loss: 8.07509 - Test loss: 118.83247\n",
      "Epoch 2187 - lr: 0.05000 - Train loss: 8.07509 - Test loss: 118.83246\n",
      "Epoch 2188 - lr: 0.05000 - Train loss: 8.07509 - Test loss: 118.83245\n",
      "Epoch 2189 - lr: 0.05000 - Train loss: 8.07509 - Test loss: 118.83244\n",
      "Epoch 2190 - lr: 0.05000 - Train loss: 8.07509 - Test loss: 118.83244\n",
      "Epoch 2191 - lr: 0.05000 - Train loss: 8.07509 - Test loss: 118.83243\n",
      "Epoch 2192 - lr: 0.05000 - Train loss: 8.07509 - Test loss: 118.83242\n",
      "Epoch 2193 - lr: 0.05000 - Train loss: 8.07509 - Test loss: 118.83241\n",
      "Epoch 2194 - lr: 0.05000 - Train loss: 8.07509 - Test loss: 118.83241\n",
      "Epoch 2195 - lr: 0.05000 - Train loss: 8.07509 - Test loss: 118.83240\n",
      "Epoch 2196 - lr: 0.05000 - Train loss: 8.07508 - Test loss: 118.83239\n",
      "Epoch 2197 - lr: 0.05000 - Train loss: 8.07508 - Test loss: 118.83238\n",
      "Epoch 2198 - lr: 0.05000 - Train loss: 8.07508 - Test loss: 118.83237\n",
      "Epoch 2199 - lr: 0.05000 - Train loss: 8.07508 - Test loss: 118.83236\n",
      "Epoch 2200 - lr: 0.05000 - Train loss: 8.07508 - Test loss: 118.83235\n",
      "Epoch 2201 - lr: 0.05000 - Train loss: 8.07508 - Test loss: 118.83235\n",
      "Epoch 2202 - lr: 0.05000 - Train loss: 8.07508 - Test loss: 118.83234\n",
      "Epoch 2203 - lr: 0.05000 - Train loss: 8.07508 - Test loss: 118.83233\n",
      "Epoch 2204 - lr: 0.05000 - Train loss: 8.07508 - Test loss: 118.83231\n",
      "Epoch 2205 - lr: 0.05000 - Train loss: 8.07508 - Test loss: 118.83230\n",
      "Epoch 2206 - lr: 0.05000 - Train loss: 8.07508 - Test loss: 118.83229\n",
      "Epoch 2207 - lr: 0.05000 - Train loss: 8.07508 - Test loss: 118.83228\n",
      "Epoch 2208 - lr: 0.05000 - Train loss: 8.07508 - Test loss: 118.83227\n",
      "Epoch 2209 - lr: 0.05000 - Train loss: 8.07508 - Test loss: 118.83226\n",
      "Epoch 2210 - lr: 0.05000 - Train loss: 8.07508 - Test loss: 118.83225\n",
      "Epoch 2211 - lr: 0.05000 - Train loss: 8.07508 - Test loss: 118.83223\n",
      "Epoch 2212 - lr: 0.05000 - Train loss: 8.07508 - Test loss: 118.83222\n",
      "Epoch 2213 - lr: 0.05000 - Train loss: 8.07508 - Test loss: 118.83221\n",
      "Epoch 2214 - lr: 0.05000 - Train loss: 8.07508 - Test loss: 118.83219\n",
      "Epoch 2215 - lr: 0.05000 - Train loss: 8.07508 - Test loss: 118.83218\n",
      "Epoch 2216 - lr: 0.05000 - Train loss: 8.07508 - Test loss: 118.83216\n",
      "Epoch 2217 - lr: 0.05000 - Train loss: 8.07508 - Test loss: 118.83215\n",
      "Epoch 2218 - lr: 0.05000 - Train loss: 8.07507 - Test loss: 118.83213\n",
      "Epoch 2219 - lr: 0.05000 - Train loss: 8.07507 - Test loss: 118.83212\n",
      "Epoch 2220 - lr: 0.05000 - Train loss: 8.07507 - Test loss: 118.83210\n",
      "Epoch 2221 - lr: 0.05000 - Train loss: 8.07507 - Test loss: 118.83208\n",
      "Epoch 2222 - lr: 0.05000 - Train loss: 8.07507 - Test loss: 118.83206\n",
      "Epoch 2223 - lr: 0.05000 - Train loss: 8.07507 - Test loss: 118.83204\n",
      "Epoch 2224 - lr: 0.05000 - Train loss: 8.07507 - Test loss: 118.83202\n",
      "Epoch 2225 - lr: 0.05000 - Train loss: 8.07507 - Test loss: 118.83200\n",
      "Epoch 2226 - lr: 0.05000 - Train loss: 8.07507 - Test loss: 118.83198\n",
      "Epoch 2227 - lr: 0.05000 - Train loss: 8.07507 - Test loss: 118.83196\n",
      "Epoch 2228 - lr: 0.05000 - Train loss: 8.07507 - Test loss: 118.83194\n",
      "Epoch 2229 - lr: 0.05000 - Train loss: 8.07507 - Test loss: 118.83191\n",
      "Epoch 2230 - lr: 0.05000 - Train loss: 8.07507 - Test loss: 118.83189\n",
      "Epoch 2231 - lr: 0.05000 - Train loss: 8.07506 - Test loss: 118.83186\n",
      "Epoch 2232 - lr: 0.05000 - Train loss: 8.07506 - Test loss: 118.83183\n",
      "Epoch 2233 - lr: 0.05000 - Train loss: 8.07506 - Test loss: 118.83180\n",
      "Epoch 2234 - lr: 0.05000 - Train loss: 8.07506 - Test loss: 118.83177\n",
      "Epoch 2235 - lr: 0.05000 - Train loss: 8.07506 - Test loss: 118.83174\n",
      "Epoch 2236 - lr: 0.05000 - Train loss: 8.07506 - Test loss: 118.83171\n",
      "Epoch 2237 - lr: 0.05000 - Train loss: 8.07506 - Test loss: 118.83167\n",
      "Epoch 2238 - lr: 0.05000 - Train loss: 8.07505 - Test loss: 118.83163\n",
      "Epoch 2239 - lr: 0.05000 - Train loss: 8.07505 - Test loss: 118.83160\n",
      "Epoch 2240 - lr: 0.05000 - Train loss: 8.07505 - Test loss: 118.83155\n",
      "Epoch 2241 - lr: 0.05000 - Train loss: 8.07505 - Test loss: 118.83151\n",
      "Epoch 2242 - lr: 0.05000 - Train loss: 8.07505 - Test loss: 118.83146\n",
      "Epoch 2243 - lr: 0.05000 - Train loss: 8.07504 - Test loss: 118.83141\n",
      "Epoch 2244 - lr: 0.05000 - Train loss: 8.07504 - Test loss: 118.83136\n",
      "Epoch 2245 - lr: 0.05000 - Train loss: 8.07504 - Test loss: 118.83130\n",
      "Epoch 2246 - lr: 0.05000 - Train loss: 8.07504 - Test loss: 118.83124\n",
      "Epoch 2247 - lr: 0.05000 - Train loss: 8.07503 - Test loss: 118.83118\n",
      "Epoch 2248 - lr: 0.05000 - Train loss: 8.07503 - Test loss: 118.83111\n",
      "Epoch 2249 - lr: 0.05000 - Train loss: 8.07503 - Test loss: 118.83104\n",
      "Epoch 2250 - lr: 0.05000 - Train loss: 8.07502 - Test loss: 118.83095\n",
      "Epoch 2251 - lr: 0.05000 - Train loss: 8.07502 - Test loss: 118.83087\n",
      "Epoch 2252 - lr: 0.05000 - Train loss: 8.07502 - Test loss: 118.83077\n",
      "Epoch 2253 - lr: 0.05000 - Train loss: 8.07501 - Test loss: 118.83067\n",
      "Epoch 2254 - lr: 0.05000 - Train loss: 8.07500 - Test loss: 118.83055\n",
      "Epoch 2255 - lr: 0.05000 - Train loss: 8.07500 - Test loss: 118.83043\n",
      "Epoch 2256 - lr: 0.05000 - Train loss: 8.07499 - Test loss: 118.83029\n",
      "Epoch 2257 - lr: 0.05000 - Train loss: 8.07498 - Test loss: 118.83013\n",
      "Epoch 2258 - lr: 0.05000 - Train loss: 8.07497 - Test loss: 118.82996\n",
      "Epoch 2259 - lr: 0.05000 - Train loss: 8.07496 - Test loss: 118.82977\n",
      "Epoch 2260 - lr: 0.05000 - Train loss: 8.07495 - Test loss: 118.82954\n",
      "Epoch 2261 - lr: 0.05000 - Train loss: 8.07494 - Test loss: 118.82929\n",
      "Epoch 2262 - lr: 0.05000 - Train loss: 8.07492 - Test loss: 118.82900\n",
      "Epoch 2263 - lr: 0.05000 - Train loss: 8.07490 - Test loss: 118.82866\n",
      "Epoch 2264 - lr: 0.05000 - Train loss: 8.07487 - Test loss: 118.82826\n",
      "Epoch 2265 - lr: 0.05000 - Train loss: 8.07484 - Test loss: 118.82778\n",
      "Epoch 2266 - lr: 0.05000 - Train loss: 8.07479 - Test loss: 118.82721\n",
      "Epoch 2267 - lr: 0.05000 - Train loss: 8.07473 - Test loss: 118.82651\n",
      "Epoch 2268 - lr: 0.05000 - Train loss: 8.07465 - Test loss: 118.82565\n",
      "Epoch 2269 - lr: 0.05000 - Train loss: 8.07452 - Test loss: 118.82460\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2270 - lr: 0.05000 - Train loss: 8.07432 - Test loss: 118.82335\n",
      "Epoch 2271 - lr: 0.05000 - Train loss: 8.07399 - Test loss: 118.82201\n",
      "Epoch 2272 - lr: 0.05000 - Train loss: 8.07346 - Test loss: 118.82087\n",
      "Epoch 2273 - lr: 0.05000 - Train loss: 8.07289 - Test loss: 118.82024\n",
      "Epoch 2274 - lr: 0.05000 - Train loss: 8.07269 - Test loss: 118.81956\n",
      "Epoch 2275 - lr: 0.05000 - Train loss: 8.07249 - Test loss: 118.81884\n",
      "Epoch 2276 - lr: 0.05000 - Train loss: 8.07226 - Test loss: 118.81809\n",
      "Epoch 2277 - lr: 0.05000 - Train loss: 8.07202 - Test loss: 118.81729\n",
      "Epoch 2278 - lr: 0.05000 - Train loss: 8.07177 - Test loss: 118.81647\n",
      "Epoch 2279 - lr: 0.05000 - Train loss: 8.07150 - Test loss: 118.81561\n",
      "Epoch 2280 - lr: 0.05000 - Train loss: 8.07120 - Test loss: 118.81474\n",
      "Epoch 2281 - lr: 0.05000 - Train loss: 8.07090 - Test loss: 118.81381\n",
      "Epoch 2282 - lr: 0.05000 - Train loss: 8.07055 - Test loss: 118.81296\n",
      "Epoch 2283 - lr: 0.05000 - Train loss: 8.07024 - Test loss: 118.81185\n",
      "Epoch 2284 - lr: 0.05000 - Train loss: 8.06974 - Test loss: 118.81143\n",
      "Epoch 2285 - lr: 0.05000 - Train loss: 8.06971 - Test loss: 118.80890\n",
      "Epoch 2286 - lr: 0.05000 - Train loss: 8.06841 - Test loss: 118.81279\n",
      "Epoch 2287 - lr: 0.05000 - Train loss: 8.07109 - Test loss: 118.80055\n",
      "Epoch 2288 - lr: 0.05000 - Train loss: 8.06839 - Test loss: 118.81630\n",
      "Epoch 2289 - lr: 0.05000 - Train loss: 8.07273 - Test loss: 118.80243\n",
      "Epoch 2290 - lr: 0.05000 - Train loss: 8.06832 - Test loss: 118.81552\n",
      "Epoch 2291 - lr: 0.05000 - Train loss: 8.07215 - Test loss: 118.79836\n",
      "Epoch 2292 - lr: 0.05000 - Train loss: 8.06878 - Test loss: 118.81156\n",
      "Epoch 2293 - lr: 0.05000 - Train loss: 8.06937 - Test loss: 118.78995\n",
      "Epoch 2294 - lr: 0.05000 - Train loss: 8.07834 - Test loss: 118.81029\n",
      "Epoch 2295 - lr: 0.05000 - Train loss: 8.06477 - Test loss: 118.80689\n",
      "Epoch 2296 - lr: 0.05000 - Train loss: 8.06497 - Test loss: 118.79263\n",
      "Epoch 2297 - lr: 0.05000 - Train loss: 8.06833 - Test loss: 118.79794\n",
      "Epoch 2298 - lr: 0.05000 - Train loss: 8.07032 - Test loss: 118.80459\n",
      "Epoch 2299 - lr: 0.05000 - Train loss: 8.06628 - Test loss: 118.79099\n",
      "Epoch 2300 - lr: 0.05000 - Train loss: 8.06745 - Test loss: 118.80994\n",
      "Epoch 2301 - lr: 0.05000 - Train loss: 8.06637 - Test loss: 118.75403\n",
      "Epoch 2302 - lr: 0.05000 - Train loss: 8.08578 - Test loss: 118.83293\n",
      "Epoch 2303 - lr: 0.05000 - Train loss: 8.07504 - Test loss: 118.83276\n",
      "Epoch 2304 - lr: 0.05000 - Train loss: 8.07503 - Test loss: 118.83274\n",
      "Epoch 2305 - lr: 0.05000 - Train loss: 8.07503 - Test loss: 118.83271\n",
      "Epoch 2306 - lr: 0.05000 - Train loss: 8.07502 - Test loss: 118.83268\n",
      "Epoch 2307 - lr: 0.05000 - Train loss: 8.07502 - Test loss: 118.83264\n",
      "Epoch 2308 - lr: 0.05000 - Train loss: 8.07501 - Test loss: 118.83261\n",
      "Epoch 2309 - lr: 0.05000 - Train loss: 8.07501 - Test loss: 118.83256\n",
      "Epoch 2310 - lr: 0.05000 - Train loss: 8.07500 - Test loss: 118.83251\n",
      "Epoch 2311 - lr: 0.05000 - Train loss: 8.07499 - Test loss: 118.83246\n",
      "Epoch 2312 - lr: 0.05000 - Train loss: 8.07498 - Test loss: 118.83240\n",
      "Epoch 2313 - lr: 0.05000 - Train loss: 8.07497 - Test loss: 118.83232\n",
      "Epoch 2314 - lr: 0.05000 - Train loss: 8.07496 - Test loss: 118.83224\n",
      "Epoch 2315 - lr: 0.05000 - Train loss: 8.07494 - Test loss: 118.83213\n",
      "Epoch 2316 - lr: 0.05000 - Train loss: 8.07492 - Test loss: 118.83201\n",
      "Epoch 2317 - lr: 0.05000 - Train loss: 8.07490 - Test loss: 118.83185\n",
      "Epoch 2318 - lr: 0.05000 - Train loss: 8.07487 - Test loss: 118.83165\n",
      "Epoch 2319 - lr: 0.05000 - Train loss: 8.07483 - Test loss: 118.83138\n",
      "Epoch 2320 - lr: 0.05000 - Train loss: 8.07477 - Test loss: 118.83101\n",
      "Epoch 2321 - lr: 0.05000 - Train loss: 8.07467 - Test loss: 118.83046\n",
      "Epoch 2322 - lr: 0.05000 - Train loss: 8.07451 - Test loss: 118.82955\n",
      "Epoch 2323 - lr: 0.05000 - Train loss: 8.07415 - Test loss: 118.82778\n",
      "Epoch 2324 - lr: 0.05000 - Train loss: 8.07275 - Test loss: 118.82289\n",
      "Epoch 2325 - lr: 0.05000 - Train loss: 7.96484 - Test loss: 119.55504\n",
      "Epoch 2326 - lr: 0.05000 - Train loss: 7.89404 - Test loss: 118.77940\n",
      "Epoch 2327 - lr: 0.05000 - Train loss: 8.07415 - Test loss: 118.78542\n",
      "Epoch 2328 - lr: 0.05000 - Train loss: 8.07419 - Test loss: 118.78170\n",
      "Epoch 2329 - lr: 0.05000 - Train loss: 8.07405 - Test loss: 118.77424\n",
      "Epoch 2330 - lr: 0.05000 - Train loss: 8.07385 - Test loss: 118.76686\n",
      "Epoch 2331 - lr: 0.05000 - Train loss: 8.07363 - Test loss: 118.76249\n",
      "Epoch 2332 - lr: 0.05000 - Train loss: 8.07342 - Test loss: 118.75928\n",
      "Epoch 2333 - lr: 0.05000 - Train loss: 8.07321 - Test loss: 118.75570\n",
      "Epoch 2334 - lr: 0.05000 - Train loss: 8.07311 - Test loss: 118.75122\n",
      "Epoch 2335 - lr: 0.05000 - Train loss: 8.07374 - Test loss: 118.74652\n",
      "Epoch 2336 - lr: 0.05000 - Train loss: 8.07590 - Test loss: 118.73350\n",
      "Epoch 2337 - lr: 0.05000 - Train loss: 8.11965 - Test loss: 116.91156\n",
      "Epoch 2338 - lr: 0.05000 - Train loss: 7.93214 - Test loss: 110.61736\n",
      "Epoch 2339 - lr: 0.05000 - Train loss: 7.81664 - Test loss: 118.83231\n",
      "Epoch 2340 - lr: 0.05000 - Train loss: 8.07506 - Test loss: 118.83227\n",
      "Epoch 2341 - lr: 0.05000 - Train loss: 8.07505 - Test loss: 118.83222\n",
      "Epoch 2342 - lr: 0.05000 - Train loss: 8.07505 - Test loss: 118.83217\n",
      "Epoch 2343 - lr: 0.05000 - Train loss: 8.07505 - Test loss: 118.83212\n",
      "Epoch 2344 - lr: 0.05000 - Train loss: 8.07505 - Test loss: 118.83206\n",
      "Epoch 2345 - lr: 0.05000 - Train loss: 8.07504 - Test loss: 118.83199\n",
      "Epoch 2346 - lr: 0.05000 - Train loss: 8.07504 - Test loss: 118.83192\n",
      "Epoch 2347 - lr: 0.05000 - Train loss: 8.07504 - Test loss: 118.83184\n",
      "Epoch 2348 - lr: 0.05000 - Train loss: 8.07503 - Test loss: 118.83175\n",
      "Epoch 2349 - lr: 0.05000 - Train loss: 8.07503 - Test loss: 118.83165\n",
      "Epoch 2350 - lr: 0.05000 - Train loss: 8.07502 - Test loss: 118.83153\n",
      "Epoch 2351 - lr: 0.05000 - Train loss: 8.07502 - Test loss: 118.83140\n",
      "Epoch 2352 - lr: 0.05000 - Train loss: 8.07501 - Test loss: 118.83125\n",
      "Epoch 2353 - lr: 0.05000 - Train loss: 8.07500 - Test loss: 118.83106\n",
      "Epoch 2354 - lr: 0.05000 - Train loss: 8.07500 - Test loss: 118.83085\n",
      "Epoch 2355 - lr: 0.05000 - Train loss: 8.07498 - Test loss: 118.83059\n",
      "Epoch 2356 - lr: 0.05000 - Train loss: 8.07497 - Test loss: 118.83026\n",
      "Epoch 2357 - lr: 0.05000 - Train loss: 8.07495 - Test loss: 118.82985\n",
      "Epoch 2358 - lr: 0.05000 - Train loss: 8.07493 - Test loss: 118.82931\n",
      "Epoch 2359 - lr: 0.05000 - Train loss: 8.07490 - Test loss: 118.82858\n",
      "Epoch 2360 - lr: 0.05000 - Train loss: 8.07486 - Test loss: 118.82752\n",
      "Epoch 2361 - lr: 0.05000 - Train loss: 8.07480 - Test loss: 118.82587\n",
      "Epoch 2362 - lr: 0.05000 - Train loss: 8.07470 - Test loss: 118.82292\n",
      "Epoch 2363 - lr: 0.05000 - Train loss: 8.07451 - Test loss: 118.81631\n",
      "Epoch 2364 - lr: 0.05000 - Train loss: 8.07396 - Test loss: 118.79104\n",
      "Epoch 2365 - lr: 0.05000 - Train loss: 8.08174 - Test loss: 118.69458\n",
      "Epoch 2366 - lr: 0.05000 - Train loss: 7.88055 - Test loss: 101.12708\n",
      "Epoch 2367 - lr: 0.05000 - Train loss: 7.66963 - Test loss: 118.83333\n",
      "Epoch 2368 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2369 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2370 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2371 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2372 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2373 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2374 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2375 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2376 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2377 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2378 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2379 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2380 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2381 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2382 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2383 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2384 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2385 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2386 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2387 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2388 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2389 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2390 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2391 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2392 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2393 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2394 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2395 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2396 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2397 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2398 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2399 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2400 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2401 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2402 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2403 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2404 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2405 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2406 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2407 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2408 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2409 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2410 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2411 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2412 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2413 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2414 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2415 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2416 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2417 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2418 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2419 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2420 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2421 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2422 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2423 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2424 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2425 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2426 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2427 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2428 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2429 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2430 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2431 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2432 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2433 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2434 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2435 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2436 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2437 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2438 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2439 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2440 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2441 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2442 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2443 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2444 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2445 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2446 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2447 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2448 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2449 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2450 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2451 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2452 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2453 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2454 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2455 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2456 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2457 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2458 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2459 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2460 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2461 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2462 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2463 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2464 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2465 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2466 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2467 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2468 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2469 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2470 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2471 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2472 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2473 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2474 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2475 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2476 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2477 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2478 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2479 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2480 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2481 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2482 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2483 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2484 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2485 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2486 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2487 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2488 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2489 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2490 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2491 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2492 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2493 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2494 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2495 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2496 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2497 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2498 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2499 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2500 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2501 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2502 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2503 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2504 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2505 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2506 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2507 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2508 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2509 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2510 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2511 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2512 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2513 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2514 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2515 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2516 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2517 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2518 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2519 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2520 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2521 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2522 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2523 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2524 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2525 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2526 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2527 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2528 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2529 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2530 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2531 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2532 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2533 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2534 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2535 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2536 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2537 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2538 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2539 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2540 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2541 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2542 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2543 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2544 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2545 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2546 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2547 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2548 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2549 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2550 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2551 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2552 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2553 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2554 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2555 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2556 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2557 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2558 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2559 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2560 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2561 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2562 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2563 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2564 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2565 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2566 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2567 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2568 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2569 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2570 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2571 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2572 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2573 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2574 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2575 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2576 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2577 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2578 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2579 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2580 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2581 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2582 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2583 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2584 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2585 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2586 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2587 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2588 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2589 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2590 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2591 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2592 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2593 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2594 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2595 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2596 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2597 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2598 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2599 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2600 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2601 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2602 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2603 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2604 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2605 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2606 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2607 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2608 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2609 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2610 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2611 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2612 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2613 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2614 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2615 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2616 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2617 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2618 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2619 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2620 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2621 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2622 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2623 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2624 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2625 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2626 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2627 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2628 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2629 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2630 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2631 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2632 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2633 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2634 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2635 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2636 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2637 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2638 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2639 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2640 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2641 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2642 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2643 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2644 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2645 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2646 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2647 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2648 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2649 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2650 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2651 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2652 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2653 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2654 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2655 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2656 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2657 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2658 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2659 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2660 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2661 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2662 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2663 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2664 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2665 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2666 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2667 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2668 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2669 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2670 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2671 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2672 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2673 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2674 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2675 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2676 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2677 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2678 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2679 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2680 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2681 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2682 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2683 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2684 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2685 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2686 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2687 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2688 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2689 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2690 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2691 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2692 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2693 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2694 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2695 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2696 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2697 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2698 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2699 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2700 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2701 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2702 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2703 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2704 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2705 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2706 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2707 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2708 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2709 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2710 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2711 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2712 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2713 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2714 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2715 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2716 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2717 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2718 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2719 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2720 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2721 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2722 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2723 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2724 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2725 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2726 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2727 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2728 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2729 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2730 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2731 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2732 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2733 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2734 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2735 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2736 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2737 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2738 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2739 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2740 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2741 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2742 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2743 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2744 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2745 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2746 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2747 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2748 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2749 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2750 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2751 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2752 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2753 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2754 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2755 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2756 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2757 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2758 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2759 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2760 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2761 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2762 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2763 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2764 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2765 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2766 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2767 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2768 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2769 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2770 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2771 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2772 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2773 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2774 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2775 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2776 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2777 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2778 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2779 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2780 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2781 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2782 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2783 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2784 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2785 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2786 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2787 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2788 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2789 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2790 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2791 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2792 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2793 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2794 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2795 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2796 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2797 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2798 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2799 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2800 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2801 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2802 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2803 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2804 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2805 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2806 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2807 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2808 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2809 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2810 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2811 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2812 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2813 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2814 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2815 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2816 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2817 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2818 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2819 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2820 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2821 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2822 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2823 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2824 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2825 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2826 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2827 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2828 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2829 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2830 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2831 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2832 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2833 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2834 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2835 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2836 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2837 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2838 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2839 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2840 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2841 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2842 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2843 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2844 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2845 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2846 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2847 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2848 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2849 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2850 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2851 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2852 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2853 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2854 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2855 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2856 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2857 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2858 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2859 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2860 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2861 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2862 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2863 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2864 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2865 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2866 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2867 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2868 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2869 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2870 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2871 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2872 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2873 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2874 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2875 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2876 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2877 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2878 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2879 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2880 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2881 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2882 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2883 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2884 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2885 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2886 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2887 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2888 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2889 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2890 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2891 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2892 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2893 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2894 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2895 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2896 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2897 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2898 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2899 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2900 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2901 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2902 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2903 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2904 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2905 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2906 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2907 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2908 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2909 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2910 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2911 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2912 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2913 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2914 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2915 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2916 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2917 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2918 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2919 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2920 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2921 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2922 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2923 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2924 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2925 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2926 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2927 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2928 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2929 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2930 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2931 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2932 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2933 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2934 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2935 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2936 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2937 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2938 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2939 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2940 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2941 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2942 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2943 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2944 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2945 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2946 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2947 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2948 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2949 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2950 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2951 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2952 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2953 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2954 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2955 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2956 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2957 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2958 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2959 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2960 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2961 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2962 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2963 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2964 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2965 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2966 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2967 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2968 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2969 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2970 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2971 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2972 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2973 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2974 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2975 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2976 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2977 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2978 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2979 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83333\n",
      "Epoch 2980 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 2981 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 2982 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 2983 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 2984 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 2985 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 2986 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 2987 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2988 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 2989 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 2990 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 2991 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 2992 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 2993 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 2994 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 2995 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 2996 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 2997 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 2998 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 2999 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3000 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3001 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3002 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3003 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3004 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3005 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3006 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3007 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3008 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3009 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3010 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3011 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3012 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3013 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3014 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3015 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3016 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3017 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3018 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3019 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3020 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3021 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3022 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3023 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3024 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3025 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3026 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3027 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3028 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3029 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3030 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3031 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3032 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3033 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3034 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3035 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3036 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3037 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3038 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3039 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3040 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3041 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3042 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3043 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3044 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3045 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3046 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3047 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3048 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3049 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3050 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3051 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3052 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3053 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3054 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3055 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3056 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3057 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3058 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3059 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3060 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3061 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3062 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3063 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3064 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3065 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3066 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3067 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3068 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3069 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3070 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3071 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3072 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3073 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3074 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3075 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3076 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3077 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3078 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3079 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3080 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3081 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3082 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3083 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3084 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3085 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3086 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3087 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3088 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3089 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3090 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3091 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3092 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3093 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3094 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3095 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3096 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3097 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3098 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3099 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3100 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3101 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3102 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3103 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3104 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3105 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3106 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3107 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3108 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3109 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3110 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3111 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3112 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3113 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3114 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3115 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3116 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3117 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3118 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3119 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3120 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3121 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3122 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3123 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3124 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3125 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3126 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3127 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3128 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3129 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3130 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3131 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3132 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3133 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3134 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3135 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3136 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3137 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3138 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3139 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3140 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3141 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3142 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3143 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3144 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3145 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3146 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3147 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3148 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3149 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3150 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3151 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3152 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3153 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3154 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3155 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3156 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3157 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3158 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3159 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3160 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3161 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3162 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3163 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3164 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3165 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3166 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3167 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3168 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3169 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3170 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3171 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3172 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3173 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3174 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3175 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3176 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3177 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3178 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3179 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3180 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3181 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3182 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3183 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3184 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3185 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3186 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3187 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3188 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3189 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3190 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3191 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3192 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3193 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3194 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3195 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3196 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3197 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3198 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3199 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3200 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3201 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3202 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3203 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3204 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3205 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3206 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3207 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3208 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3209 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3210 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3211 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3212 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3213 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3214 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3215 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3216 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3217 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3218 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3219 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3220 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3221 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3222 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3223 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3224 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3225 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3226 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3227 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3228 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3229 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3230 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3231 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3232 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3233 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3234 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3235 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3236 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3237 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3238 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3239 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3240 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3241 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3242 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3243 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3244 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3245 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3246 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3247 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3248 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3249 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3250 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3251 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3252 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3253 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3254 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3255 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3256 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3257 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3258 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3259 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3260 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3261 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3262 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3263 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3264 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3265 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3266 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3267 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3268 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3269 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3270 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3271 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3272 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3273 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3274 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3275 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3276 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3277 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3278 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3279 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3280 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3281 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3282 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3283 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3284 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3285 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3286 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3287 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3288 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3289 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3290 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3291 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3292 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3293 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3294 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3295 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3296 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3297 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3298 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3299 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3300 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3301 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3302 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3303 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3304 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3305 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3306 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3307 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3308 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3309 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3310 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3311 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3312 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3313 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3314 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3315 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3316 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3317 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3318 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3319 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3320 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3321 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3322 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3323 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3324 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3325 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3326 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3327 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3328 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3329 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3330 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3331 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3332 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3333 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3334 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3335 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3336 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3337 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3338 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3339 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3340 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3341 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3342 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3343 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3344 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3345 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3346 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3347 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3348 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3349 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3350 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3351 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3352 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3353 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3354 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3355 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3356 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3357 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3358 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3359 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3360 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3361 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3362 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3363 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3364 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3365 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3366 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3367 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3368 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3369 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3370 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3371 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3372 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3373 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3374 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3375 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3376 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3377 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3378 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3379 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3380 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3381 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3382 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3383 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3384 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3385 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3386 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3387 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3388 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3389 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3390 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3391 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3392 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3393 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3394 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3395 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3396 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3397 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3398 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3399 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3400 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3401 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3402 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3403 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3404 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3405 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3406 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3407 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3408 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3409 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3410 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3411 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3412 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3413 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3414 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3415 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3416 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3417 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3418 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3419 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3420 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3421 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3422 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3423 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3424 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3425 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3426 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3427 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3428 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3429 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3430 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3431 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3432 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3433 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3434 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3435 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3436 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3437 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3438 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3439 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3440 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3441 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3442 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3443 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3444 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3445 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3446 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3447 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3448 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3449 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3450 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3451 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3452 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3453 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3454 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3455 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3456 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3457 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3458 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3459 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3460 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3461 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3462 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3463 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3464 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3465 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3466 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3467 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3468 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3469 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3470 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3471 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3472 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3473 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3474 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3475 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3476 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3477 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3478 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3479 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3480 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3481 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3482 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3483 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3484 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3485 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3486 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3487 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3488 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3489 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3490 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3491 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3492 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3493 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3494 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3495 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3496 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3497 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3498 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3499 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3500 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3501 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3502 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3503 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3504 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3505 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3506 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3507 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3508 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3509 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3510 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3511 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3512 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3513 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3514 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3515 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3516 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3517 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3518 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3519 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3520 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3521 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3522 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3523 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3524 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3525 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3526 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3527 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3528 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3529 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3530 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3531 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3532 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3533 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3534 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3535 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3536 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3537 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3538 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3539 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3540 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3541 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3542 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3543 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3544 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3545 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3546 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3547 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3548 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3549 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3550 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3551 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3552 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3553 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3554 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3555 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3556 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3557 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3558 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3559 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3560 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3561 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3562 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3563 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3564 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3565 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3566 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3567 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3568 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3569 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3570 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3571 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3572 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3573 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3574 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3575 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3576 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3577 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3578 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3579 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3580 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3581 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3582 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3583 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3584 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3585 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3586 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3587 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3588 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3589 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3590 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3591 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3592 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3593 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3594 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3595 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3596 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3597 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3598 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3599 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3600 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3601 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3602 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3603 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3604 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3605 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3606 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3607 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3608 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3609 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3610 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3611 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3612 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3613 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3614 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3615 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3616 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3617 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3618 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3619 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3620 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3621 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3622 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3623 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3624 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3625 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3626 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3627 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3628 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3629 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3630 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3631 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3632 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3633 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3634 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3635 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3636 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3637 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3638 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3639 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3640 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3641 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3642 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3643 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3644 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3645 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3646 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3647 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3648 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3649 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3650 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3651 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3652 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3653 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3654 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3655 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3656 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3657 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3658 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3659 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3660 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3661 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3662 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3663 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3664 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3665 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3666 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3667 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3668 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3669 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3670 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3671 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3672 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3673 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3674 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3675 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3676 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3677 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3678 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3679 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3680 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3681 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3682 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3683 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3684 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3685 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3686 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3687 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3688 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3689 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3690 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3691 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3692 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3693 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3694 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3695 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3696 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3697 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3698 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3699 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3700 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3701 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3702 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3703 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3704 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3705 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3706 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3707 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3708 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3709 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3710 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3711 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3712 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3713 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3714 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3715 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3716 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3717 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3718 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3719 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3720 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3721 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3722 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3723 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3724 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3725 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3726 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3727 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3728 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3729 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3730 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3731 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3732 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3733 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3734 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3735 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3736 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3737 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3738 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3739 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3740 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3741 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3742 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3743 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3744 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3745 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3746 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3747 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3748 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3749 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3750 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3751 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3752 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3753 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3754 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3755 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3756 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3757 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3758 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3759 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3760 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3761 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3762 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3763 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3764 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3765 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3766 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3767 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3768 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3769 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3770 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3771 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3772 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3773 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3774 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3775 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3776 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3777 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3778 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3779 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3780 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3781 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3782 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3783 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3784 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3785 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3786 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3787 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3788 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3789 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3790 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3791 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3792 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3793 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3794 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3795 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3796 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3797 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3798 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3799 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3800 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3801 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3802 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3803 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3804 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3805 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3806 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3807 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3808 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3809 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3810 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3811 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3812 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3813 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3814 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3815 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3816 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3817 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3818 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3819 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3820 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3821 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3822 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3823 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3824 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3825 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3826 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3827 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3828 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3829 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3830 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3831 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3832 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3833 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3834 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3835 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3836 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3837 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3838 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3839 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3840 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3841 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3842 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3843 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3844 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3845 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3846 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3847 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3848 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3849 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3850 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3851 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3852 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3853 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3854 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3855 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3856 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3857 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3858 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3859 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3860 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3861 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3862 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3863 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3864 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3865 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3866 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3867 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3868 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3869 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3870 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3871 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3872 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3873 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3874 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3875 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3876 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3877 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3878 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3879 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3880 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3881 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3882 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3883 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3884 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3885 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3886 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83332\n",
      "Epoch 3887 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3888 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3889 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3890 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3891 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3892 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3893 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3894 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3895 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3896 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3897 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3898 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3899 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3900 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3901 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3902 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3903 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3904 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3905 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3906 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3907 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3908 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3909 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3910 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3911 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3912 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3913 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3914 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3915 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3916 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3917 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3918 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3919 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3920 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3921 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3922 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3923 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3924 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3925 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3926 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3927 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3928 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3929 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3930 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3931 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3932 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3933 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3934 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3935 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3936 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3937 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3938 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3939 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3940 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3941 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3942 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3943 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3944 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3945 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3946 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3947 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3948 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3949 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3950 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3951 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3952 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3953 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3954 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3955 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3956 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3957 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3958 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3959 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3960 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3961 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3962 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3963 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3964 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3965 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3966 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3967 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3968 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3969 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3970 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3971 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3972 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3973 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3974 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3975 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3976 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3977 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3978 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3979 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3980 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3981 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3982 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3983 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3984 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3985 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3986 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3987 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3988 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3989 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3990 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3991 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3992 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3993 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3994 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3995 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3996 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3997 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3998 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 3999 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4000 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4001 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4002 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4003 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4004 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4005 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4006 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4007 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4008 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4009 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4010 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4011 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4012 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4013 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4014 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4015 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4016 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4017 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4018 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4019 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4020 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4021 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4022 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4023 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4024 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4025 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4026 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4027 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4028 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4029 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4030 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4031 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4032 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4033 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4034 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4035 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4036 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4037 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4038 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4039 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4040 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4041 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4042 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4043 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4044 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4045 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4046 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4047 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4048 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4049 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4050 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4051 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4052 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4053 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4054 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4055 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4056 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4057 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4058 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4059 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4060 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4061 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4062 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4063 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4064 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4065 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4066 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4067 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4068 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4069 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4070 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4071 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4072 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4073 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4074 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4075 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4076 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4077 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4078 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4079 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4080 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4081 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4082 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4083 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4084 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4085 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4086 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4087 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4088 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4089 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4090 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4091 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4092 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4093 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4094 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4095 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4096 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4097 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4098 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4099 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4100 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4101 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4102 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4103 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4104 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4105 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4106 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4107 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4108 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4109 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4110 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4111 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4112 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4113 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4114 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4115 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4116 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4117 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4118 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4119 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4120 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4121 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4122 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4123 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4124 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4125 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4126 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4127 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4128 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4129 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4130 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4131 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4132 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4133 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4134 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4135 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4136 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4137 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4138 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4139 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4140 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4141 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4142 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4143 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4144 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4145 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4146 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4147 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4148 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4149 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4150 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4151 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4152 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4153 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4154 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4155 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4156 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4157 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4158 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4159 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4160 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4161 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4162 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4163 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4164 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4165 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4166 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4167 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4168 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4169 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4170 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4171 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4172 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4173 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4174 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4175 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4176 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4177 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4178 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4179 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4180 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4181 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4182 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4183 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4184 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4185 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4186 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4187 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4188 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4189 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4190 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4191 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4192 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4193 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4194 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4195 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4196 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4197 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4198 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4199 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4200 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4201 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4202 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4203 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4204 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4205 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4206 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4207 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4208 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4209 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4210 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4211 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4212 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4213 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4214 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4215 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4216 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4217 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4218 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4219 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4220 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4221 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4222 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4223 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4224 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4225 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4226 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4227 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4228 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4229 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4230 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4231 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4232 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4233 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4234 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4235 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4236 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4237 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4238 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4239 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4240 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4241 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4242 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4243 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4244 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4245 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4246 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4247 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4248 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4249 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4250 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4251 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4252 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4253 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4254 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4255 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4256 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4257 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4258 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4259 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4260 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4261 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4262 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4263 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4264 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4265 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4266 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4267 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4268 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4269 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4270 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4271 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4272 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4273 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4274 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4275 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4276 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4277 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4278 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4279 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4280 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4281 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4282 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4283 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4284 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4285 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4286 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4287 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4288 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4289 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4290 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4291 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4292 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4293 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4294 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4295 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4296 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4297 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4298 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4299 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4300 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4301 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4302 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4303 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4304 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4305 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4306 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4307 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4308 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4309 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4310 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4311 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4312 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4313 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4314 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4315 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4316 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4317 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4318 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4319 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4320 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4321 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4322 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4323 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4324 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4325 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4326 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4327 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4328 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4329 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4330 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4331 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4332 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4333 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4334 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4335 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4336 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4337 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4338 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4339 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4340 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4341 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4342 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4343 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4344 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4345 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4346 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4347 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4348 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4349 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4350 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4351 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4352 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4353 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4354 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4355 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4356 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4357 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4358 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4359 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4360 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4361 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4362 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4363 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4364 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4365 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4366 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4367 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4368 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4369 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4370 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4371 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4372 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4373 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4374 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4375 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4376 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4377 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4378 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4379 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4380 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4381 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4382 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4383 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4384 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4385 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4386 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4387 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4388 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4389 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4390 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4391 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4392 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4393 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4394 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4395 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4396 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4397 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4398 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4399 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4400 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4401 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4402 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4403 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4404 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4405 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4406 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4407 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4408 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4409 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4410 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4411 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4412 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4413 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4414 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4415 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4416 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4417 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4418 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4419 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4420 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4421 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4422 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4423 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4424 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4425 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4426 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4427 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4428 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4429 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4430 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4431 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4432 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4433 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4434 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4435 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4436 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4437 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4438 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4439 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4440 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4441 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4442 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4443 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4444 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4445 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4446 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4447 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4448 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4449 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4450 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4451 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4452 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4453 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4454 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4455 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4456 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4457 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4458 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4459 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4460 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4461 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4462 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4463 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4464 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4465 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4466 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4467 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4468 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4469 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4470 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4471 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4472 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4473 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4474 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4475 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4476 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4477 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4478 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4479 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4480 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4481 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4482 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4483 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4484 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4485 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4486 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4487 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4488 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4489 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4490 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4491 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4492 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4493 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4494 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4495 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4496 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4497 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4498 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4499 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4500 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4501 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4502 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4503 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4504 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4505 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4506 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4507 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4508 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4509 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4510 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4511 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4512 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4513 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4514 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4515 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4516 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4517 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4518 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4519 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4520 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4521 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4522 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4523 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4524 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4525 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4526 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4527 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4528 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4529 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4530 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4531 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4532 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4533 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4534 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4535 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4536 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4537 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4538 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4539 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4540 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4541 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4542 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4543 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4544 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4545 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4546 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4547 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4548 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4549 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4550 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4551 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4552 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4553 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4554 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4555 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4556 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4557 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4558 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4559 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4560 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4561 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4562 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4563 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4564 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4565 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4566 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4567 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4568 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4569 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4570 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4571 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4572 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4573 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4574 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4575 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4576 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4577 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4578 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4579 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4580 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4581 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4582 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4583 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4584 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4585 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4586 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4587 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4588 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4589 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4590 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4591 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4592 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4593 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4594 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4595 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4596 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4597 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4598 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4599 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4600 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4601 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4602 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4603 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4604 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4605 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4606 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4607 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4608 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4609 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4610 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4611 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4612 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4613 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4614 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4615 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4616 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4617 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4618 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4619 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4620 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4621 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4622 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4623 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4624 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4625 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4626 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4627 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4628 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4629 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4630 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4631 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4632 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4633 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4634 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4635 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4636 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4637 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4638 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4639 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4640 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4641 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4642 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4643 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4644 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4645 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4646 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4647 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4648 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4649 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4650 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4651 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4652 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4653 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4654 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4655 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4656 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4657 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4658 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4659 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4660 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4661 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4662 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4663 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4664 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4665 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4666 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4667 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4668 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4669 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4670 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4671 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4672 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4673 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4674 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4675 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4676 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4677 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4678 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4679 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4680 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4681 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4682 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4683 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4684 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4685 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4686 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4687 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4688 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4689 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4690 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4691 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4692 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4693 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4694 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4695 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4696 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4697 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4698 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4699 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4700 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4701 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4702 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4703 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4704 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4705 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4706 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4707 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4708 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4709 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4710 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4711 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4712 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4713 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4714 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4715 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4716 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4717 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4718 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4719 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4720 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4721 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4722 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4723 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4724 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4725 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4726 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4727 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4728 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4729 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4730 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4731 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4732 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4733 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4734 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4735 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4736 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4737 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4738 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4739 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4740 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4741 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4742 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4743 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4744 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4745 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4746 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4747 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4748 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4749 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4750 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4751 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4752 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4753 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4754 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4755 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4756 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4757 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4758 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4759 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4760 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4761 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4762 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4763 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4764 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4765 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4766 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4767 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4768 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4769 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4770 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4771 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4772 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4773 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4774 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4775 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4776 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4777 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4778 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4779 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4780 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4781 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4782 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4783 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4784 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4785 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4786 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4787 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4788 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4789 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4790 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4791 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4792 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4793 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4794 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4795 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4796 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4797 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4798 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4799 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4800 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4801 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4802 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4803 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4804 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4805 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4806 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4807 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4808 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4809 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4810 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4811 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4812 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4813 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4814 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4815 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4816 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4817 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4818 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4819 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4820 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4821 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4822 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4823 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4824 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4825 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4826 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4827 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4828 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4829 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4830 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4831 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4832 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4833 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4834 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4835 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4836 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4837 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4838 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4839 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4840 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4841 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4842 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4843 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4844 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4845 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4846 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4847 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4848 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4849 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4850 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4851 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4852 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4853 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4854 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4855 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4856 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4857 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4858 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4859 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4860 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4861 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4862 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4863 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4864 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4865 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4866 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4867 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4868 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4869 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4870 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4871 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4872 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4873 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4874 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4875 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4876 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4877 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4878 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4879 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4880 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4881 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4882 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4883 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4884 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4885 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4886 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4887 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4888 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4889 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4890 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4891 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4892 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4893 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4894 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4895 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4896 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4897 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4898 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4899 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4900 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4901 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4902 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4903 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4904 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4905 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4906 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4907 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4908 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4909 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4910 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4911 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4912 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4913 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4914 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4915 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4916 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4917 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4918 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4919 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4920 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4921 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4922 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4923 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4924 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4925 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4926 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4927 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4928 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4929 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4930 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4931 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4932 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4933 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4934 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4935 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4936 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4937 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4938 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4939 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4940 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4941 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4942 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4943 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4944 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4945 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4946 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4947 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4948 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4949 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4950 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4951 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4952 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4953 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4954 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4955 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4956 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4957 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4958 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4959 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4960 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4961 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4962 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4963 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4964 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4965 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4966 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4967 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4968 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4969 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4970 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4971 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4972 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4973 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4974 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4975 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4976 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4977 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4978 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4979 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4980 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4981 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4982 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4983 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4984 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4985 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4986 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4987 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4988 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4989 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4990 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4991 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4992 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4993 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4994 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4995 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4996 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4997 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4998 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 4999 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "Epoch 5000 - lr: 0.05000 - Train loss: 8.07510 - Test loss: 118.83331\n",
      "number of hidden neurons in first layer is: 480 number of hidden neurons in second layer is: 120\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(0,len(Nh1_vec)):\n",
    "    print('number of hidden neurons in first layer is:', int(Nh1_vec[i]), 'number of hidden neurons in second layer is:',int(Nh2_vec[i] ))\n",
    "    net_try= Network(Ni, Nh1_vec[i], Nh2_vec[i], No)\n",
    "    #now I train for each network\n",
    "    #%% TRAINING\n",
    "\n",
    "    num_epochs = 5000\n",
    "    lr = 0.05\n",
    "    en_decay = False\n",
    "    lr_final = 0.005\n",
    "    lr_decay = (lr_final / lr)**(1 / num_epochs)\n",
    "\n",
    "    train_loss_log = []\n",
    "    test_loss_log = []\n",
    "    for num_ep in range(num_epochs):\n",
    "    # Learning rate decay\n",
    "        if en_decay:\n",
    "            lr *= lr_decay\n",
    "    # Train single epoch (sample by sample, no batch for now)\n",
    "        train_loss_vec = [net_try.update(x, y, lr) for x, y in zip(x_train, y_train)]\n",
    "        avg_train_loss = np.mean(train_loss_vec)\n",
    "    # Test network\n",
    "        y_test_est = np.array([net_try.forward(x) for x in x_test])\n",
    "        avg_test_loss = np.mean((y_test_est - y_test)**2/2)\n",
    "    # Log\n",
    "        train_loss_log.append(avg_train_loss)\n",
    "        test_loss_log.append(avg_test_loss)\n",
    "        print('Epoch %d - lr: %.5f - Train loss: %.5f - Test loss: %.5f' % (num_ep + 1, lr, avg_train_loss, avg_test_loss))\n",
    "\n",
    "        \n",
    "        \n",
    "        # Plot losses\n",
    "    print('number of hidden neurons in first layer is:', int(Nh1_vec[i]), 'number of hidden neurons in second layer is:',int(Nh2_vec[i] ))\n",
    "    \n",
    "    plt.close('all')\n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.semilogy(train_loss_log, label='Train loss')\n",
    "    plt.semilogy(test_loss_log, label='Test loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.tight_layout()# Plot losses\n",
    "    plt.close('all')\n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.semilogy(train_loss_log, label='Train loss')\n",
    "    plt.semilogy(test_loss_log, label='Test loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot weights after training\n",
    "    net_try.plot_weights()\n",
    "\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Plot weights after training\n",
    "    net_try.plot_weights()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ni = 1 \n",
    "Nh1= 260\n",
    "Nh2= 330\n",
    "No= 1 \n",
    "### Initialize network\n",
    "net = Network(Ni, Nh1, Nh2, No)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st hidden layer weigth matrix shape: (260, 2)\n",
      "2nd hidden layer weigth matrix shape: (330, 261)\n",
      "Output layer weigth matrix shape: (1, 331)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Access the class members\n",
    "print('1st hidden layer weigth matrix shape:', net.WBh1.shape)\n",
    "print('2nd hidden layer weigth matrix shape:', net.WBh2.shape)\n",
    "print('Output layer weigth matrix shape:', net.WBo.shape)\n",
    "\n",
    "# Plot weights\n",
    "plt.close('all')\n",
    "net.plot_weights()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the x vector\n",
    "x_highres = np.linspace(-4, 2, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_net_output = []\n",
    "for x in x_highres:\n",
    "    net_out = net.forward(x)\n",
    "    initial_net_output.append(net_out)\n",
    "initial_net_output = np.array(initial_net_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Plot\n",
    "plt.close('all')\n",
    "plt.figure(figsize=(12,8))\n",
    "x_highres = np.linspace(-4,2,1000)\n",
    "plt.plot(x_highres, poly_model(x_highres, beta_true), color='b', ls='--', label='True data model')\n",
    "plt.plot(x_train, y_train, color='r', ls='', marker='.', label='Train data points')\n",
    "plt.plot(x_highres, initial_net_output, color='g', ls='--', label='Network output (random weights)')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#something went wrong with thw network initial output not sure why"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - lr: 0.01000 - Train loss: 7.25395 - Test loss: 152.43795\n",
      "Epoch 2 - lr: 0.01000 - Train loss: 7.93365 - Test loss: 135.23752\n",
      "Epoch 3 - lr: 0.01000 - Train loss: 8.38045 - Test loss: 123.72388\n",
      "Epoch 4 - lr: 0.01000 - Train loss: 8.98477 - Test loss: 110.32210\n",
      "Epoch 5 - lr: 0.01000 - Train loss: 9.49942 - Test loss: 93.28482\n",
      "Epoch 6 - lr: 0.01000 - Train loss: 9.80629 - Test loss: 73.16684\n",
      "Epoch 7 - lr: 0.01000 - Train loss: 9.73307 - Test loss: 52.49142\n",
      "Epoch 8 - lr: 0.01000 - Train loss: 9.15553 - Test loss: 34.89602\n",
      "Epoch 9 - lr: 0.01000 - Train loss: 8.23715 - Test loss: 22.59399\n",
      "Epoch 10 - lr: 0.01000 - Train loss: 7.27520 - Test loss: 15.08122\n",
      "Epoch 11 - lr: 0.01000 - Train loss: 6.43894 - Test loss: 10.72266\n",
      "Epoch 12 - lr: 0.01000 - Train loss: 5.75496 - Test loss: 8.16658\n",
      "Epoch 13 - lr: 0.01000 - Train loss: 5.19370 - Test loss: 6.59850\n",
      "Epoch 14 - lr: 0.01000 - Train loss: 4.72400 - Test loss: 5.57935\n",
      "Epoch 15 - lr: 0.01000 - Train loss: 4.32632 - Test loss: 4.87889\n",
      "Epoch 16 - lr: 0.01000 - Train loss: 3.98927 - Test loss: 4.37433\n",
      "Epoch 17 - lr: 0.01000 - Train loss: 3.70475 - Test loss: 3.99719\n",
      "Epoch 18 - lr: 0.01000 - Train loss: 3.46553 - Test loss: 3.70702\n",
      "Epoch 19 - lr: 0.01000 - Train loss: 3.26472 - Test loss: 3.47851\n",
      "Epoch 20 - lr: 0.01000 - Train loss: 3.09600 - Test loss: 3.29509\n",
      "Epoch 21 - lr: 0.01000 - Train loss: 2.95375 - Test loss: 3.14541\n",
      "Epoch 22 - lr: 0.01000 - Train loss: 2.83325 - Test loss: 3.02154\n",
      "Epoch 23 - lr: 0.01000 - Train loss: 2.73058 - Test loss: 2.91772\n",
      "Epoch 24 - lr: 0.01000 - Train loss: 2.64256 - Test loss: 2.82975\n",
      "Epoch 25 - lr: 0.01000 - Train loss: 2.56661 - Test loss: 2.75447\n",
      "Epoch 26 - lr: 0.01000 - Train loss: 2.50067 - Test loss: 2.68948\n",
      "Epoch 27 - lr: 0.01000 - Train loss: 2.44308 - Test loss: 2.63293\n",
      "Epoch 28 - lr: 0.01000 - Train loss: 2.39248 - Test loss: 2.58338\n",
      "Epoch 29 - lr: 0.01000 - Train loss: 2.34780 - Test loss: 2.53968\n",
      "Epoch 30 - lr: 0.01000 - Train loss: 2.30813 - Test loss: 2.50091\n",
      "Epoch 31 - lr: 0.01000 - Train loss: 2.27276 - Test loss: 2.46635\n",
      "Epoch 32 - lr: 0.01000 - Train loss: 2.24109 - Test loss: 2.43538\n",
      "Epoch 33 - lr: 0.01000 - Train loss: 2.21260 - Test loss: 2.40753\n",
      "Epoch 34 - lr: 0.01000 - Train loss: 2.18689 - Test loss: 2.38237\n",
      "Epoch 35 - lr: 0.01000 - Train loss: 2.16361 - Test loss: 2.35956\n",
      "Epoch 36 - lr: 0.01000 - Train loss: 2.14246 - Test loss: 2.33882\n",
      "Epoch 37 - lr: 0.01000 - Train loss: 2.12318 - Test loss: 2.31991\n",
      "Epoch 38 - lr: 0.01000 - Train loss: 2.10557 - Test loss: 2.30261\n",
      "Epoch 39 - lr: 0.01000 - Train loss: 2.08944 - Test loss: 2.28675\n",
      "Epoch 40 - lr: 0.01000 - Train loss: 2.07462 - Test loss: 2.27217\n",
      "Epoch 41 - lr: 0.01000 - Train loss: 2.06099 - Test loss: 2.25875\n",
      "Epoch 42 - lr: 0.01000 - Train loss: 2.04842 - Test loss: 2.24637\n",
      "Epoch 43 - lr: 0.01000 - Train loss: 2.03681 - Test loss: 2.23492\n",
      "Epoch 44 - lr: 0.01000 - Train loss: 2.02606 - Test loss: 2.22432\n",
      "Epoch 45 - lr: 0.01000 - Train loss: 2.01610 - Test loss: 2.21449\n",
      "Epoch 46 - lr: 0.01000 - Train loss: 2.00685 - Test loss: 2.20537\n",
      "Epoch 47 - lr: 0.01000 - Train loss: 1.99825 - Test loss: 2.19688\n",
      "Epoch 48 - lr: 0.01000 - Train loss: 1.99024 - Test loss: 2.18898\n",
      "Epoch 49 - lr: 0.01000 - Train loss: 1.98278 - Test loss: 2.18161\n",
      "Epoch 50 - lr: 0.01000 - Train loss: 1.97581 - Test loss: 2.17474\n",
      "Epoch 51 - lr: 0.01000 - Train loss: 1.96929 - Test loss: 2.16832\n",
      "Epoch 52 - lr: 0.01000 - Train loss: 1.96320 - Test loss: 2.16233\n",
      "Epoch 53 - lr: 0.01000 - Train loss: 1.95750 - Test loss: 2.15671\n",
      "Epoch 54 - lr: 0.01000 - Train loss: 1.95215 - Test loss: 2.15146\n",
      "Epoch 55 - lr: 0.01000 - Train loss: 1.94713 - Test loss: 2.14654\n",
      "Epoch 56 - lr: 0.01000 - Train loss: 1.94241 - Test loss: 2.14192\n",
      "Epoch 57 - lr: 0.01000 - Train loss: 1.93799 - Test loss: 2.13759\n",
      "Epoch 58 - lr: 0.01000 - Train loss: 1.93382 - Test loss: 2.13352\n",
      "Epoch 59 - lr: 0.01000 - Train loss: 1.92990 - Test loss: 2.12971\n",
      "Epoch 60 - lr: 0.01000 - Train loss: 1.92620 - Test loss: 2.12612\n",
      "Epoch 61 - lr: 0.01000 - Train loss: 1.92272 - Test loss: 2.12274\n",
      "Epoch 62 - lr: 0.01000 - Train loss: 1.91944 - Test loss: 2.11957\n",
      "Epoch 63 - lr: 0.01000 - Train loss: 1.91634 - Test loss: 2.11659\n",
      "Epoch 64 - lr: 0.01000 - Train loss: 1.91341 - Test loss: 2.11378\n",
      "Epoch 65 - lr: 0.01000 - Train loss: 1.91064 - Test loss: 2.11114\n",
      "Epoch 66 - lr: 0.01000 - Train loss: 1.90803 - Test loss: 2.10864\n",
      "Epoch 67 - lr: 0.01000 - Train loss: 1.90555 - Test loss: 2.10630\n",
      "Epoch 68 - lr: 0.01000 - Train loss: 1.90321 - Test loss: 2.10409\n",
      "Epoch 69 - lr: 0.01000 - Train loss: 1.90099 - Test loss: 2.10201\n",
      "Epoch 70 - lr: 0.01000 - Train loss: 1.89889 - Test loss: 2.10004\n",
      "Epoch 71 - lr: 0.01000 - Train loss: 1.89690 - Test loss: 2.09819\n",
      "Epoch 72 - lr: 0.01000 - Train loss: 1.89501 - Test loss: 2.09645\n",
      "Epoch 73 - lr: 0.01000 - Train loss: 1.89321 - Test loss: 2.09480\n",
      "Epoch 74 - lr: 0.01000 - Train loss: 1.89151 - Test loss: 2.09325\n",
      "Epoch 75 - lr: 0.01000 - Train loss: 1.88989 - Test loss: 2.09179\n",
      "Epoch 76 - lr: 0.01000 - Train loss: 1.88836 - Test loss: 2.09041\n",
      "Epoch 77 - lr: 0.01000 - Train loss: 1.88690 - Test loss: 2.08911\n",
      "Epoch 78 - lr: 0.01000 - Train loss: 1.88551 - Test loss: 2.08788\n",
      "Epoch 79 - lr: 0.01000 - Train loss: 1.88418 - Test loss: 2.08672\n",
      "Epoch 80 - lr: 0.01000 - Train loss: 1.88292 - Test loss: 2.08563\n",
      "Epoch 81 - lr: 0.01000 - Train loss: 1.88173 - Test loss: 2.08460\n",
      "Epoch 82 - lr: 0.01000 - Train loss: 1.88058 - Test loss: 2.08363\n",
      "Epoch 83 - lr: 0.01000 - Train loss: 1.87949 - Test loss: 2.08271\n",
      "Epoch 84 - lr: 0.01000 - Train loss: 1.87846 - Test loss: 2.08185\n",
      "Epoch 85 - lr: 0.01000 - Train loss: 1.87746 - Test loss: 2.08104\n",
      "Epoch 86 - lr: 0.01000 - Train loss: 1.87652 - Test loss: 2.08027\n",
      "Epoch 87 - lr: 0.01000 - Train loss: 1.87561 - Test loss: 2.07955\n",
      "Epoch 88 - lr: 0.01000 - Train loss: 1.87475 - Test loss: 2.07886\n",
      "Epoch 89 - lr: 0.01000 - Train loss: 1.87392 - Test loss: 2.07822\n",
      "Epoch 90 - lr: 0.01000 - Train loss: 1.87313 - Test loss: 2.07762\n",
      "Epoch 91 - lr: 0.01000 - Train loss: 1.87238 - Test loss: 2.07705\n",
      "Epoch 92 - lr: 0.01000 - Train loss: 1.87165 - Test loss: 2.07651\n",
      "Epoch 93 - lr: 0.01000 - Train loss: 1.87096 - Test loss: 2.07601\n",
      "Epoch 94 - lr: 0.01000 - Train loss: 1.87029 - Test loss: 2.07553\n",
      "Epoch 95 - lr: 0.01000 - Train loss: 1.86965 - Test loss: 2.07508\n",
      "Epoch 96 - lr: 0.01000 - Train loss: 1.86903 - Test loss: 2.07466\n",
      "Epoch 97 - lr: 0.01000 - Train loss: 1.86844 - Test loss: 2.07426\n",
      "Epoch 98 - lr: 0.01000 - Train loss: 1.86788 - Test loss: 2.07389\n",
      "Epoch 99 - lr: 0.01000 - Train loss: 1.86733 - Test loss: 2.07354\n",
      "Epoch 100 - lr: 0.01000 - Train loss: 1.86680 - Test loss: 2.07321\n",
      "Epoch 101 - lr: 0.01000 - Train loss: 1.86629 - Test loss: 2.07290\n",
      "Epoch 102 - lr: 0.01000 - Train loss: 1.86580 - Test loss: 2.07261\n",
      "Epoch 103 - lr: 0.01000 - Train loss: 1.86533 - Test loss: 2.07233\n",
      "Epoch 104 - lr: 0.01000 - Train loss: 1.86487 - Test loss: 2.07207\n",
      "Epoch 105 - lr: 0.01000 - Train loss: 1.86443 - Test loss: 2.07183\n",
      "Epoch 106 - lr: 0.01000 - Train loss: 1.86399 - Test loss: 2.07160\n",
      "Epoch 107 - lr: 0.01000 - Train loss: 1.86358 - Test loss: 2.07138\n",
      "Epoch 108 - lr: 0.01000 - Train loss: 1.86317 - Test loss: 2.07118\n",
      "Epoch 109 - lr: 0.01000 - Train loss: 1.86277 - Test loss: 2.07099\n",
      "Epoch 110 - lr: 0.01000 - Train loss: 1.86238 - Test loss: 2.07080\n",
      "Epoch 111 - lr: 0.01000 - Train loss: 1.86200 - Test loss: 2.07063\n",
      "Epoch 112 - lr: 0.01000 - Train loss: 1.86163 - Test loss: 2.07046\n",
      "Epoch 113 - lr: 0.01000 - Train loss: 1.86126 - Test loss: 2.07031\n",
      "Epoch 114 - lr: 0.01000 - Train loss: 1.86090 - Test loss: 2.07015\n",
      "Epoch 115 - lr: 0.01000 - Train loss: 1.86053 - Test loss: 2.07000\n",
      "Epoch 116 - lr: 0.01000 - Train loss: 1.86017 - Test loss: 2.06986\n",
      "Epoch 117 - lr: 0.01000 - Train loss: 1.85981 - Test loss: 2.06971\n",
      "Epoch 118 - lr: 0.01000 - Train loss: 1.85944 - Test loss: 2.06957\n",
      "Epoch 119 - lr: 0.01000 - Train loss: 1.85907 - Test loss: 2.06942\n",
      "Epoch 120 - lr: 0.01000 - Train loss: 1.85868 - Test loss: 2.06925\n",
      "Epoch 121 - lr: 0.01000 - Train loss: 1.85827 - Test loss: 2.06908\n",
      "Epoch 122 - lr: 0.01000 - Train loss: 1.85783 - Test loss: 2.06888\n",
      "Epoch 123 - lr: 0.01000 - Train loss: 1.85734 - Test loss: 2.06863\n",
      "Epoch 124 - lr: 0.01000 - Train loss: 1.85680 - Test loss: 2.06833\n",
      "Epoch 125 - lr: 0.01000 - Train loss: 1.85615 - Test loss: 2.06792\n",
      "Epoch 126 - lr: 0.01000 - Train loss: 1.85534 - Test loss: 2.06733\n",
      "Epoch 127 - lr: 0.01000 - Train loss: 1.85426 - Test loss: 2.06641\n",
      "Epoch 128 - lr: 0.01000 - Train loss: 1.85272 - Test loss: 2.06484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 129 - lr: 0.01000 - Train loss: 1.85035 - Test loss: 2.06197\n",
      "Epoch 130 - lr: 0.01000 - Train loss: 1.84685 - Test loss: 2.05747\n",
      "Epoch 131 - lr: 0.01000 - Train loss: 1.84307 - Test loss: 2.05436\n",
      "Epoch 132 - lr: 0.01000 - Train loss: 1.84032 - Test loss: 2.05446\n",
      "Epoch 133 - lr: 0.01000 - Train loss: 1.83823 - Test loss: 2.05479\n",
      "Epoch 134 - lr: 0.01000 - Train loss: 1.83620 - Test loss: 2.05450\n",
      "Epoch 135 - lr: 0.01000 - Train loss: 1.83410 - Test loss: 2.05378\n",
      "Epoch 136 - lr: 0.01000 - Train loss: 1.83191 - Test loss: 2.05282\n",
      "Epoch 137 - lr: 0.01000 - Train loss: 1.82966 - Test loss: 2.05170\n",
      "Epoch 138 - lr: 0.01000 - Train loss: 1.82734 - Test loss: 2.05045\n",
      "Epoch 139 - lr: 0.01000 - Train loss: 1.82494 - Test loss: 2.04910\n",
      "Epoch 140 - lr: 0.01000 - Train loss: 1.82247 - Test loss: 2.04764\n",
      "Epoch 141 - lr: 0.01000 - Train loss: 1.81989 - Test loss: 2.04608\n",
      "Epoch 142 - lr: 0.01000 - Train loss: 1.81719 - Test loss: 2.04440\n",
      "Epoch 143 - lr: 0.01000 - Train loss: 1.81435 - Test loss: 2.04262\n",
      "Epoch 144 - lr: 0.01000 - Train loss: 1.81134 - Test loss: 2.04074\n",
      "Epoch 145 - lr: 0.01000 - Train loss: 1.80814 - Test loss: 2.03875\n",
      "Epoch 146 - lr: 0.01000 - Train loss: 1.80475 - Test loss: 2.03667\n",
      "Epoch 147 - lr: 0.01000 - Train loss: 1.80116 - Test loss: 2.03449\n",
      "Epoch 148 - lr: 0.01000 - Train loss: 1.79736 - Test loss: 2.03218\n",
      "Epoch 149 - lr: 0.01000 - Train loss: 1.79332 - Test loss: 2.02967\n",
      "Epoch 150 - lr: 0.01000 - Train loss: 1.78903 - Test loss: 2.02680\n",
      "Epoch 151 - lr: 0.01000 - Train loss: 1.78444 - Test loss: 2.02333\n",
      "Epoch 152 - lr: 0.01000 - Train loss: 1.77950 - Test loss: 2.01896\n",
      "Epoch 153 - lr: 0.01000 - Train loss: 1.77418 - Test loss: 2.01338\n",
      "Epoch 154 - lr: 0.01000 - Train loss: 1.76843 - Test loss: 2.00623\n",
      "Epoch 155 - lr: 0.01000 - Train loss: 1.76223 - Test loss: 1.99724\n",
      "Epoch 156 - lr: 0.01000 - Train loss: 1.75555 - Test loss: 1.98617\n",
      "Epoch 157 - lr: 0.01000 - Train loss: 1.74839 - Test loss: 1.97292\n",
      "Epoch 158 - lr: 0.01000 - Train loss: 1.74077 - Test loss: 1.95755\n",
      "Epoch 159 - lr: 0.01000 - Train loss: 1.73271 - Test loss: 1.94036\n",
      "Epoch 160 - lr: 0.01000 - Train loss: 1.72424 - Test loss: 1.92191\n",
      "Epoch 161 - lr: 0.01000 - Train loss: 1.71540 - Test loss: 1.90290\n",
      "Epoch 162 - lr: 0.01000 - Train loss: 1.70624 - Test loss: 1.88413\n",
      "Epoch 163 - lr: 0.01000 - Train loss: 1.69679 - Test loss: 1.86626\n",
      "Epoch 164 - lr: 0.01000 - Train loss: 1.68709 - Test loss: 1.84970\n",
      "Epoch 165 - lr: 0.01000 - Train loss: 1.67715 - Test loss: 1.83451\n",
      "Epoch 166 - lr: 0.01000 - Train loss: 1.66696 - Test loss: 1.82037\n",
      "Epoch 167 - lr: 0.01000 - Train loss: 1.65643 - Test loss: 1.80649\n",
      "Epoch 168 - lr: 0.01000 - Train loss: 1.64541 - Test loss: 1.79184\n",
      "Epoch 169 - lr: 0.01000 - Train loss: 1.63372 - Test loss: 1.77563\n",
      "Epoch 170 - lr: 0.01000 - Train loss: 1.62122 - Test loss: 1.75808\n",
      "Epoch 171 - lr: 0.01000 - Train loss: 1.60791 - Test loss: 1.74080\n",
      "Epoch 172 - lr: 0.01000 - Train loss: 1.59401 - Test loss: 1.72625\n",
      "Epoch 173 - lr: 0.01000 - Train loss: 1.57979 - Test loss: 1.71644\n",
      "Epoch 174 - lr: 0.01000 - Train loss: 1.56547 - Test loss: 1.71180\n",
      "Epoch 175 - lr: 0.01000 - Train loss: 1.55116 - Test loss: 1.71074\n",
      "Epoch 176 - lr: 0.01000 - Train loss: 1.53682 - Test loss: 1.70985\n",
      "Epoch 177 - lr: 0.01000 - Train loss: 1.52235 - Test loss: 1.70511\n",
      "Epoch 178 - lr: 0.01000 - Train loss: 1.50764 - Test loss: 1.69344\n",
      "Epoch 179 - lr: 0.01000 - Train loss: 1.49265 - Test loss: 1.67374\n",
      "Epoch 180 - lr: 0.01000 - Train loss: 1.47743 - Test loss: 1.64729\n",
      "Epoch 181 - lr: 0.01000 - Train loss: 1.46212 - Test loss: 1.61705\n",
      "Epoch 182 - lr: 0.01000 - Train loss: 1.44694 - Test loss: 1.58606\n",
      "Epoch 183 - lr: 0.01000 - Train loss: 1.43208 - Test loss: 1.55581\n",
      "Epoch 184 - lr: 0.01000 - Train loss: 1.41769 - Test loss: 1.52588\n",
      "Epoch 185 - lr: 0.01000 - Train loss: 1.40375 - Test loss: 1.49506\n",
      "Epoch 186 - lr: 0.01000 - Train loss: 1.39020 - Test loss: 1.46303\n",
      "Epoch 187 - lr: 0.01000 - Train loss: 1.37695 - Test loss: 1.43100\n",
      "Epoch 188 - lr: 0.01000 - Train loss: 1.36397 - Test loss: 1.40094\n",
      "Epoch 189 - lr: 0.01000 - Train loss: 1.35126 - Test loss: 1.37440\n",
      "Epoch 190 - lr: 0.01000 - Train loss: 1.33880 - Test loss: 1.35239\n",
      "Epoch 191 - lr: 0.01000 - Train loss: 1.32646 - Test loss: 1.33604\n",
      "Epoch 192 - lr: 0.01000 - Train loss: 1.31406 - Test loss: 1.32629\n",
      "Epoch 193 - lr: 0.01000 - Train loss: 1.30133 - Test loss: 1.32139\n",
      "Epoch 194 - lr: 0.01000 - Train loss: 1.28801 - Test loss: 1.32091\n",
      "Epoch 195 - lr: 0.01000 - Train loss: 1.27452 - Test loss: 1.35427\n",
      "Epoch 196 - lr: 0.01000 - Train loss: 1.26227 - Test loss: 1.48323\n",
      "Epoch 197 - lr: 0.01000 - Train loss: 1.24759 - Test loss: 1.62019\n",
      "Epoch 198 - lr: 0.01000 - Train loss: 1.23523 - Test loss: 1.66145\n",
      "Epoch 199 - lr: 0.01000 - Train loss: 1.22492 - Test loss: 1.66625\n",
      "Epoch 200 - lr: 0.01000 - Train loss: 1.21553 - Test loss: 1.66608\n",
      "Epoch 201 - lr: 0.01000 - Train loss: 1.20847 - Test loss: 1.66468\n",
      "Epoch 202 - lr: 0.01000 - Train loss: 1.20650 - Test loss: 1.66442\n",
      "Epoch 203 - lr: 0.01000 - Train loss: 1.21558 - Test loss: 1.59762\n",
      "Epoch 204 - lr: 0.01000 - Train loss: 1.21740 - Test loss: 1.35204\n",
      "Epoch 205 - lr: 0.01000 - Train loss: 1.20726 - Test loss: 1.31391\n",
      "Epoch 206 - lr: 0.01000 - Train loss: 1.19161 - Test loss: 1.44465\n",
      "Epoch 207 - lr: 0.01000 - Train loss: 1.17248 - Test loss: 1.59209\n",
      "Epoch 208 - lr: 0.01000 - Train loss: 1.16800 - Test loss: 1.60104\n",
      "Epoch 209 - lr: 0.01000 - Train loss: 1.17578 - Test loss: 1.38624\n",
      "Epoch 210 - lr: 0.01000 - Train loss: 1.18330 - Test loss: 1.15149\n",
      "Epoch 211 - lr: 0.01000 - Train loss: 1.18462 - Test loss: 1.13754\n",
      "Epoch 212 - lr: 0.01000 - Train loss: 1.19311 - Test loss: 1.13003\n",
      "Epoch 213 - lr: 0.01000 - Train loss: 1.19711 - Test loss: 1.13194\n",
      "Epoch 214 - lr: 0.01000 - Train loss: 1.19625 - Test loss: 1.13547\n",
      "Epoch 215 - lr: 0.01000 - Train loss: 1.19391 - Test loss: 1.13219\n",
      "Epoch 216 - lr: 0.01000 - Train loss: 1.19068 - Test loss: 1.12601\n",
      "Epoch 217 - lr: 0.01000 - Train loss: 1.18696 - Test loss: 1.12028\n",
      "Epoch 218 - lr: 0.01000 - Train loss: 1.18303 - Test loss: 1.11586\n",
      "Epoch 219 - lr: 0.01000 - Train loss: 1.17900 - Test loss: 1.11246\n",
      "Epoch 220 - lr: 0.01000 - Train loss: 1.17492 - Test loss: 1.10958\n",
      "Epoch 221 - lr: 0.01000 - Train loss: 1.17082 - Test loss: 1.10686\n",
      "Epoch 222 - lr: 0.01000 - Train loss: 1.16673 - Test loss: 1.10411\n",
      "Epoch 223 - lr: 0.01000 - Train loss: 1.16267 - Test loss: 1.10126\n",
      "Epoch 224 - lr: 0.01000 - Train loss: 1.15867 - Test loss: 1.09832\n",
      "Epoch 225 - lr: 0.01000 - Train loss: 1.15475 - Test loss: 1.09534\n",
      "Epoch 226 - lr: 0.01000 - Train loss: 1.15094 - Test loss: 1.09245\n",
      "Epoch 227 - lr: 0.01000 - Train loss: 1.14730 - Test loss: 1.08976\n",
      "Epoch 228 - lr: 0.01000 - Train loss: 1.14386 - Test loss: 1.08734\n",
      "Epoch 229 - lr: 0.01000 - Train loss: 1.14066 - Test loss: 1.08519\n",
      "Epoch 230 - lr: 0.01000 - Train loss: 1.13772 - Test loss: 1.08314\n",
      "Epoch 231 - lr: 0.01000 - Train loss: 1.13506 - Test loss: 1.08102\n",
      "Epoch 232 - lr: 0.01000 - Train loss: 1.13269 - Test loss: 1.07881\n",
      "Epoch 233 - lr: 0.01000 - Train loss: 1.13057 - Test loss: 1.07694\n",
      "Epoch 234 - lr: 0.01000 - Train loss: 1.12851 - Test loss: 1.07635\n",
      "Epoch 235 - lr: 0.01000 - Train loss: 1.12608 - Test loss: 1.07836\n",
      "Epoch 236 - lr: 0.01000 - Train loss: 1.12270 - Test loss: 1.08415\n",
      "Epoch 237 - lr: 0.01000 - Train loss: 1.11912 - Test loss: 1.09408\n",
      "Epoch 238 - lr: 0.01000 - Train loss: 1.11873 - Test loss: 1.10777\n",
      "Epoch 239 - lr: 0.01000 - Train loss: 1.12148 - Test loss: 1.12537\n",
      "Epoch 240 - lr: 0.01000 - Train loss: 1.12250 - Test loss: 1.14907\n",
      "Epoch 241 - lr: 0.01000 - Train loss: 1.12360 - Test loss: 1.18094\n",
      "Epoch 242 - lr: 0.01000 - Train loss: 1.12495 - Test loss: 1.22052\n",
      "Epoch 243 - lr: 0.01000 - Train loss: 1.12058 - Test loss: 1.26542\n",
      "Epoch 244 - lr: 0.01000 - Train loss: 1.11198 - Test loss: 1.31424\n",
      "Epoch 245 - lr: 0.01000 - Train loss: 1.10433 - Test loss: 1.36867\n",
      "Epoch 246 - lr: 0.01000 - Train loss: 1.09946 - Test loss: 1.42460\n",
      "Epoch 247 - lr: 0.01000 - Train loss: 1.09688 - Test loss: 1.46610\n",
      "Epoch 248 - lr: 0.01000 - Train loss: 1.09443 - Test loss: 1.47094\n",
      "Epoch 249 - lr: 0.01000 - Train loss: 1.08892 - Test loss: 1.43321\n",
      "Epoch 250 - lr: 0.01000 - Train loss: 1.08014 - Test loss: 1.37189\n",
      "Epoch 251 - lr: 0.01000 - Train loss: 1.07079 - Test loss: 1.31799\n",
      "Epoch 252 - lr: 0.01000 - Train loss: 1.06426 - Test loss: 1.29061\n",
      "Epoch 253 - lr: 0.01000 - Train loss: 1.05950 - Test loss: 1.28547\n",
      "Epoch 254 - lr: 0.01000 - Train loss: 1.05447 - Test loss: 1.28497\n",
      "Epoch 255 - lr: 0.01000 - Train loss: 1.04961 - Test loss: 1.27659\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 256 - lr: 0.01000 - Train loss: 1.04569 - Test loss: 1.26398\n",
      "Epoch 257 - lr: 0.01000 - Train loss: 1.04273 - Test loss: 1.25496\n",
      "Epoch 258 - lr: 0.01000 - Train loss: 1.04020 - Test loss: 1.24863\n",
      "Epoch 259 - lr: 0.01000 - Train loss: 1.03820 - Test loss: 1.24221\n",
      "Epoch 260 - lr: 0.01000 - Train loss: 1.03728 - Test loss: 1.23573\n",
      "Epoch 261 - lr: 0.01000 - Train loss: 1.03788 - Test loss: 1.23361\n",
      "Epoch 262 - lr: 0.01000 - Train loss: 1.04013 - Test loss: 1.24119\n",
      "Epoch 263 - lr: 0.01000 - Train loss: 1.04251 - Test loss: 1.24541\n",
      "Epoch 264 - lr: 0.01000 - Train loss: 1.04317 - Test loss: 1.24291\n",
      "Epoch 265 - lr: 0.01000 - Train loss: 1.04080 - Test loss: 1.23333\n",
      "Epoch 266 - lr: 0.01000 - Train loss: 1.03714 - Test loss: 1.21245\n",
      "Epoch 267 - lr: 0.01000 - Train loss: 1.03436 - Test loss: 1.20316\n",
      "Epoch 268 - lr: 0.01000 - Train loss: 1.02962 - Test loss: 1.19839\n",
      "Epoch 269 - lr: 0.01000 - Train loss: 1.02362 - Test loss: 1.19724\n",
      "Epoch 270 - lr: 0.01000 - Train loss: 1.01870 - Test loss: 1.19772\n",
      "Epoch 271 - lr: 0.01000 - Train loss: 1.01460 - Test loss: 1.19243\n",
      "Epoch 272 - lr: 0.01000 - Train loss: 1.01231 - Test loss: 1.18228\n",
      "Epoch 273 - lr: 0.01000 - Train loss: 1.00678 - Test loss: 1.19473\n",
      "Epoch 274 - lr: 0.01000 - Train loss: 1.00065 - Test loss: 1.17429\n",
      "Epoch 275 - lr: 0.01000 - Train loss: 1.02121 - Test loss: 1.19243\n",
      "Epoch 276 - lr: 0.01000 - Train loss: 0.99472 - Test loss: 1.17075\n",
      "Epoch 277 - lr: 0.01000 - Train loss: 1.02608 - Test loss: 1.18416\n",
      "Epoch 278 - lr: 0.01000 - Train loss: 0.99701 - Test loss: 1.18288\n",
      "Epoch 279 - lr: 0.01000 - Train loss: 1.00005 - Test loss: 1.19968\n",
      "Epoch 280 - lr: 0.01000 - Train loss: 0.99225 - Test loss: 1.19265\n",
      "Epoch 281 - lr: 0.01000 - Train loss: 0.99878 - Test loss: 1.20338\n",
      "Epoch 282 - lr: 0.01000 - Train loss: 0.99782 - Test loss: 1.20412\n",
      "Epoch 283 - lr: 0.01000 - Train loss: 1.00302 - Test loss: 1.20476\n",
      "Epoch 284 - lr: 0.01000 - Train loss: 1.00675 - Test loss: 1.20145\n",
      "Epoch 285 - lr: 0.01000 - Train loss: 1.01225 - Test loss: 1.18870\n",
      "Epoch 286 - lr: 0.01000 - Train loss: 1.02392 - Test loss: 1.16080\n",
      "Epoch 287 - lr: 0.01000 - Train loss: 1.00906 - Test loss: 1.20333\n",
      "Epoch 288 - lr: 0.01000 - Train loss: 1.02477 - Test loss: 1.16163\n",
      "Epoch 289 - lr: 0.01000 - Train loss: 1.00521 - Test loss: 1.20177\n",
      "Epoch 290 - lr: 0.01000 - Train loss: 1.00791 - Test loss: 1.20099\n",
      "Epoch 291 - lr: 0.01000 - Train loss: 1.00674 - Test loss: 1.20014\n",
      "Epoch 292 - lr: 0.01000 - Train loss: 1.00472 - Test loss: 1.19917\n",
      "Epoch 293 - lr: 0.01000 - Train loss: 1.00268 - Test loss: 1.19834\n",
      "Epoch 294 - lr: 0.01000 - Train loss: 1.00097 - Test loss: 1.19759\n",
      "Epoch 295 - lr: 0.01000 - Train loss: 0.99965 - Test loss: 1.19686\n",
      "Epoch 296 - lr: 0.01000 - Train loss: 0.99865 - Test loss: 1.19613\n",
      "Epoch 297 - lr: 0.01000 - Train loss: 0.99784 - Test loss: 1.19540\n",
      "Epoch 298 - lr: 0.01000 - Train loss: 0.99716 - Test loss: 1.19468\n",
      "Epoch 299 - lr: 0.01000 - Train loss: 0.99658 - Test loss: 1.19396\n",
      "Epoch 300 - lr: 0.01000 - Train loss: 0.99607 - Test loss: 1.19325\n",
      "Epoch 301 - lr: 0.01000 - Train loss: 0.99562 - Test loss: 1.19254\n",
      "Epoch 302 - lr: 0.01000 - Train loss: 0.99522 - Test loss: 1.19183\n",
      "Epoch 303 - lr: 0.01000 - Train loss: 0.99485 - Test loss: 1.19113\n",
      "Epoch 304 - lr: 0.01000 - Train loss: 0.99452 - Test loss: 1.19043\n",
      "Epoch 305 - lr: 0.01000 - Train loss: 0.99422 - Test loss: 1.18974\n",
      "Epoch 306 - lr: 0.01000 - Train loss: 0.99393 - Test loss: 1.18905\n",
      "Epoch 307 - lr: 0.01000 - Train loss: 0.99367 - Test loss: 1.18837\n",
      "Epoch 308 - lr: 0.01000 - Train loss: 0.99341 - Test loss: 1.18768\n",
      "Epoch 309 - lr: 0.01000 - Train loss: 0.99316 - Test loss: 1.18699\n",
      "Epoch 310 - lr: 0.01000 - Train loss: 0.99291 - Test loss: 1.18629\n",
      "Epoch 311 - lr: 0.01000 - Train loss: 0.99266 - Test loss: 1.18557\n",
      "Epoch 312 - lr: 0.01000 - Train loss: 0.99240 - Test loss: 1.18484\n",
      "Epoch 313 - lr: 0.01000 - Train loss: 0.99214 - Test loss: 1.18408\n",
      "Epoch 314 - lr: 0.01000 - Train loss: 0.99187 - Test loss: 1.18329\n",
      "Epoch 315 - lr: 0.01000 - Train loss: 0.99159 - Test loss: 1.18246\n",
      "Epoch 316 - lr: 0.01000 - Train loss: 0.99129 - Test loss: 1.18159\n",
      "Epoch 317 - lr: 0.01000 - Train loss: 0.99099 - Test loss: 1.18067\n",
      "Epoch 318 - lr: 0.01000 - Train loss: 0.99068 - Test loss: 1.17972\n",
      "Epoch 319 - lr: 0.01000 - Train loss: 0.99036 - Test loss: 1.17873\n",
      "Epoch 320 - lr: 0.01000 - Train loss: 0.99005 - Test loss: 1.17772\n",
      "Epoch 321 - lr: 0.01000 - Train loss: 0.98975 - Test loss: 1.17671\n",
      "Epoch 322 - lr: 0.01000 - Train loss: 0.98947 - Test loss: 1.17573\n",
      "Epoch 323 - lr: 0.01000 - Train loss: 0.98921 - Test loss: 1.17479\n",
      "Epoch 324 - lr: 0.01000 - Train loss: 0.98899 - Test loss: 1.17393\n",
      "Epoch 325 - lr: 0.01000 - Train loss: 0.98881 - Test loss: 1.17318\n",
      "Epoch 326 - lr: 0.01000 - Train loss: 0.98868 - Test loss: 1.17255\n",
      "Epoch 327 - lr: 0.01000 - Train loss: 0.98861 - Test loss: 1.17209\n",
      "Epoch 328 - lr: 0.01000 - Train loss: 0.98859 - Test loss: 1.17180\n",
      "Epoch 329 - lr: 0.01000 - Train loss: 0.98863 - Test loss: 1.17173\n",
      "Epoch 330 - lr: 0.01000 - Train loss: 0.98873 - Test loss: 1.17187\n",
      "Epoch 331 - lr: 0.01000 - Train loss: 0.98888 - Test loss: 1.17223\n",
      "Epoch 332 - lr: 0.01000 - Train loss: 0.98909 - Test loss: 1.17280\n",
      "Epoch 333 - lr: 0.01000 - Train loss: 0.98936 - Test loss: 1.17351\n",
      "Epoch 334 - lr: 0.01000 - Train loss: 0.98968 - Test loss: 1.17423\n",
      "Epoch 335 - lr: 0.01000 - Train loss: 0.99006 - Test loss: 1.17476\n",
      "Epoch 336 - lr: 0.01000 - Train loss: 0.99051 - Test loss: 1.17482\n",
      "Epoch 337 - lr: 0.01000 - Train loss: 0.99101 - Test loss: 1.17408\n",
      "Epoch 338 - lr: 0.01000 - Train loss: 0.99158 - Test loss: 1.17226\n",
      "Epoch 339 - lr: 0.01000 - Train loss: 0.99223 - Test loss: 1.16927\n",
      "Epoch 340 - lr: 0.01000 - Train loss: 0.99298 - Test loss: 1.16523\n",
      "Epoch 341 - lr: 0.01000 - Train loss: 0.99386 - Test loss: 1.16050\n",
      "Epoch 342 - lr: 0.01000 - Train loss: 0.99491 - Test loss: 1.15551\n",
      "Epoch 343 - lr: 0.01000 - Train loss: 0.99614 - Test loss: 1.15077\n",
      "Epoch 344 - lr: 0.01000 - Train loss: 0.99750 - Test loss: 1.14679\n",
      "Epoch 345 - lr: 0.01000 - Train loss: 0.99884 - Test loss: 1.14413\n",
      "Epoch 346 - lr: 0.01000 - Train loss: 0.99996 - Test loss: 1.14308\n",
      "Epoch 347 - lr: 0.01000 - Train loss: 1.00077 - Test loss: 1.14340\n",
      "Epoch 348 - lr: 0.01000 - Train loss: 1.00130 - Test loss: 1.14463\n",
      "Epoch 349 - lr: 0.01000 - Train loss: 1.00165 - Test loss: 1.14640\n",
      "Epoch 350 - lr: 0.01000 - Train loss: 1.00187 - Test loss: 1.14842\n",
      "Epoch 351 - lr: 0.01000 - Train loss: 1.00202 - Test loss: 1.15046\n",
      "Epoch 352 - lr: 0.01000 - Train loss: 1.00211 - Test loss: 1.15230\n",
      "Epoch 353 - lr: 0.01000 - Train loss: 1.00216 - Test loss: 1.15379\n",
      "Epoch 354 - lr: 0.01000 - Train loss: 1.00218 - Test loss: 1.15487\n",
      "Epoch 355 - lr: 0.01000 - Train loss: 1.00216 - Test loss: 1.15556\n",
      "Epoch 356 - lr: 0.01000 - Train loss: 1.00210 - Test loss: 1.15589\n",
      "Epoch 357 - lr: 0.01000 - Train loss: 1.00200 - Test loss: 1.15592\n",
      "Epoch 358 - lr: 0.01000 - Train loss: 1.00187 - Test loss: 1.15569\n",
      "Epoch 359 - lr: 0.01000 - Train loss: 1.00170 - Test loss: 1.15525\n",
      "Epoch 360 - lr: 0.01000 - Train loss: 1.00150 - Test loss: 1.15464\n",
      "Epoch 361 - lr: 0.01000 - Train loss: 1.00128 - Test loss: 1.15389\n",
      "Epoch 362 - lr: 0.01000 - Train loss: 1.00104 - Test loss: 1.15303\n",
      "Epoch 363 - lr: 0.01000 - Train loss: 1.00079 - Test loss: 1.15210\n",
      "Epoch 364 - lr: 0.01000 - Train loss: 1.00052 - Test loss: 1.15113\n",
      "Epoch 365 - lr: 0.01000 - Train loss: 1.00024 - Test loss: 1.15017\n",
      "Epoch 366 - lr: 0.01000 - Train loss: 0.99996 - Test loss: 1.14923\n",
      "Epoch 367 - lr: 0.01000 - Train loss: 0.99966 - Test loss: 1.14835\n",
      "Epoch 368 - lr: 0.01000 - Train loss: 0.99936 - Test loss: 1.14757\n",
      "Epoch 369 - lr: 0.01000 - Train loss: 0.99905 - Test loss: 1.14690\n",
      "Epoch 370 - lr: 0.01000 - Train loss: 0.99874 - Test loss: 1.14636\n",
      "Epoch 371 - lr: 0.01000 - Train loss: 0.99841 - Test loss: 1.14595\n",
      "Epoch 372 - lr: 0.01000 - Train loss: 0.99809 - Test loss: 1.14569\n",
      "Epoch 373 - lr: 0.01000 - Train loss: 0.99775 - Test loss: 1.14555\n",
      "Epoch 374 - lr: 0.01000 - Train loss: 0.99742 - Test loss: 1.14555\n",
      "Epoch 375 - lr: 0.01000 - Train loss: 0.99709 - Test loss: 1.14566\n",
      "Epoch 376 - lr: 0.01000 - Train loss: 0.99676 - Test loss: 1.14586\n",
      "Epoch 377 - lr: 0.01000 - Train loss: 0.99644 - Test loss: 1.14615\n",
      "Epoch 378 - lr: 0.01000 - Train loss: 0.99614 - Test loss: 1.14649\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 379 - lr: 0.01000 - Train loss: 0.99585 - Test loss: 1.14688\n",
      "Epoch 380 - lr: 0.01000 - Train loss: 0.99558 - Test loss: 1.14729\n",
      "Epoch 381 - lr: 0.01000 - Train loss: 0.99532 - Test loss: 1.14771\n",
      "Epoch 382 - lr: 0.01000 - Train loss: 0.99508 - Test loss: 1.14812\n",
      "Epoch 383 - lr: 0.01000 - Train loss: 0.99486 - Test loss: 1.14851\n",
      "Epoch 384 - lr: 0.01000 - Train loss: 0.99465 - Test loss: 1.14887\n",
      "Epoch 385 - lr: 0.01000 - Train loss: 0.99446 - Test loss: 1.14918\n",
      "Epoch 386 - lr: 0.01000 - Train loss: 0.99428 - Test loss: 1.14944\n",
      "Epoch 387 - lr: 0.01000 - Train loss: 0.99412 - Test loss: 1.14964\n",
      "Epoch 388 - lr: 0.01000 - Train loss: 0.99396 - Test loss: 1.14978\n",
      "Epoch 389 - lr: 0.01000 - Train loss: 0.99382 - Test loss: 1.14984\n",
      "Epoch 390 - lr: 0.01000 - Train loss: 0.99367 - Test loss: 1.14983\n",
      "Epoch 391 - lr: 0.01000 - Train loss: 0.99354 - Test loss: 1.14973\n",
      "Epoch 392 - lr: 0.01000 - Train loss: 0.99341 - Test loss: 1.14955\n",
      "Epoch 393 - lr: 0.01000 - Train loss: 0.99328 - Test loss: 1.14928\n",
      "Epoch 394 - lr: 0.01000 - Train loss: 0.99315 - Test loss: 1.14892\n",
      "Epoch 395 - lr: 0.01000 - Train loss: 0.99302 - Test loss: 1.14846\n",
      "Epoch 396 - lr: 0.01000 - Train loss: 0.99289 - Test loss: 1.14792\n",
      "Epoch 397 - lr: 0.01000 - Train loss: 0.99277 - Test loss: 1.14728\n",
      "Epoch 398 - lr: 0.01000 - Train loss: 0.99264 - Test loss: 1.14655\n",
      "Epoch 399 - lr: 0.01000 - Train loss: 0.99251 - Test loss: 1.14574\n",
      "Epoch 400 - lr: 0.01000 - Train loss: 0.99238 - Test loss: 1.14485\n",
      "Epoch 401 - lr: 0.01000 - Train loss: 0.99225 - Test loss: 1.14388\n",
      "Epoch 402 - lr: 0.01000 - Train loss: 0.99213 - Test loss: 1.14285\n",
      "Epoch 403 - lr: 0.01000 - Train loss: 0.99200 - Test loss: 1.14176\n",
      "Epoch 404 - lr: 0.01000 - Train loss: 0.99188 - Test loss: 1.14063\n",
      "Epoch 405 - lr: 0.01000 - Train loss: 0.99176 - Test loss: 1.13946\n",
      "Epoch 406 - lr: 0.01000 - Train loss: 0.99165 - Test loss: 1.13827\n",
      "Epoch 407 - lr: 0.01000 - Train loss: 0.99154 - Test loss: 1.13706\n",
      "Epoch 408 - lr: 0.01000 - Train loss: 0.99144 - Test loss: 1.13585\n",
      "Epoch 409 - lr: 0.01000 - Train loss: 0.99135 - Test loss: 1.13464\n",
      "Epoch 410 - lr: 0.01000 - Train loss: 0.99126 - Test loss: 1.13345\n",
      "Epoch 411 - lr: 0.01000 - Train loss: 0.99118 - Test loss: 1.13227\n",
      "Epoch 412 - lr: 0.01000 - Train loss: 0.99110 - Test loss: 1.13112\n",
      "Epoch 413 - lr: 0.01000 - Train loss: 0.99103 - Test loss: 1.13000\n",
      "Epoch 414 - lr: 0.01000 - Train loss: 0.99096 - Test loss: 1.12891\n",
      "Epoch 415 - lr: 0.01000 - Train loss: 0.99090 - Test loss: 1.12786\n",
      "Epoch 416 - lr: 0.01000 - Train loss: 0.99084 - Test loss: 1.12684\n",
      "Epoch 417 - lr: 0.01000 - Train loss: 0.99079 - Test loss: 1.12586\n",
      "Epoch 418 - lr: 0.01000 - Train loss: 0.99074 - Test loss: 1.12493\n",
      "Epoch 419 - lr: 0.01000 - Train loss: 0.99068 - Test loss: 1.12403\n",
      "Epoch 420 - lr: 0.01000 - Train loss: 0.99063 - Test loss: 1.12317\n",
      "Epoch 421 - lr: 0.01000 - Train loss: 0.99058 - Test loss: 1.12234\n",
      "Epoch 422 - lr: 0.01000 - Train loss: 0.99052 - Test loss: 1.12155\n",
      "Epoch 423 - lr: 0.01000 - Train loss: 0.99046 - Test loss: 1.12080\n",
      "Epoch 424 - lr: 0.01000 - Train loss: 0.99041 - Test loss: 1.12009\n",
      "Epoch 425 - lr: 0.01000 - Train loss: 0.99034 - Test loss: 1.11941\n",
      "Epoch 426 - lr: 0.01000 - Train loss: 0.99028 - Test loss: 1.11876\n",
      "Epoch 427 - lr: 0.01000 - Train loss: 0.99021 - Test loss: 1.11814\n",
      "Epoch 428 - lr: 0.01000 - Train loss: 0.99014 - Test loss: 1.11756\n",
      "Epoch 429 - lr: 0.01000 - Train loss: 0.99006 - Test loss: 1.11701\n",
      "Epoch 430 - lr: 0.01000 - Train loss: 0.98998 - Test loss: 1.11648\n",
      "Epoch 431 - lr: 0.01000 - Train loss: 0.98989 - Test loss: 1.11599\n",
      "Epoch 432 - lr: 0.01000 - Train loss: 0.98980 - Test loss: 1.11552\n",
      "Epoch 433 - lr: 0.01000 - Train loss: 0.98970 - Test loss: 1.11509\n",
      "Epoch 434 - lr: 0.01000 - Train loss: 0.98960 - Test loss: 1.11468\n",
      "Epoch 435 - lr: 0.01000 - Train loss: 0.98950 - Test loss: 1.11430\n",
      "Epoch 436 - lr: 0.01000 - Train loss: 0.98939 - Test loss: 1.11394\n",
      "Epoch 437 - lr: 0.01000 - Train loss: 0.98927 - Test loss: 1.11361\n",
      "Epoch 438 - lr: 0.01000 - Train loss: 0.98915 - Test loss: 1.11330\n",
      "Epoch 439 - lr: 0.01000 - Train loss: 0.98902 - Test loss: 1.11302\n",
      "Epoch 440 - lr: 0.01000 - Train loss: 0.98889 - Test loss: 1.11276\n",
      "Epoch 441 - lr: 0.01000 - Train loss: 0.98876 - Test loss: 1.11252\n",
      "Epoch 442 - lr: 0.01000 - Train loss: 0.98862 - Test loss: 1.11231\n",
      "Epoch 443 - lr: 0.01000 - Train loss: 0.98847 - Test loss: 1.11212\n",
      "Epoch 444 - lr: 0.01000 - Train loss: 0.98833 - Test loss: 1.11195\n",
      "Epoch 445 - lr: 0.01000 - Train loss: 0.98817 - Test loss: 1.11180\n",
      "Epoch 446 - lr: 0.01000 - Train loss: 0.98802 - Test loss: 1.11166\n",
      "Epoch 447 - lr: 0.01000 - Train loss: 0.98786 - Test loss: 1.11155\n",
      "Epoch 448 - lr: 0.01000 - Train loss: 0.98770 - Test loss: 1.11145\n",
      "Epoch 449 - lr: 0.01000 - Train loss: 0.98754 - Test loss: 1.11137\n",
      "Epoch 450 - lr: 0.01000 - Train loss: 0.98737 - Test loss: 1.11131\n",
      "Epoch 451 - lr: 0.01000 - Train loss: 0.98720 - Test loss: 1.11125\n",
      "Epoch 452 - lr: 0.01000 - Train loss: 0.98704 - Test loss: 1.11122\n",
      "Epoch 453 - lr: 0.01000 - Train loss: 0.98687 - Test loss: 1.11119\n",
      "Epoch 454 - lr: 0.01000 - Train loss: 0.98670 - Test loss: 1.11118\n",
      "Epoch 455 - lr: 0.01000 - Train loss: 0.98653 - Test loss: 1.11117\n",
      "Epoch 456 - lr: 0.01000 - Train loss: 0.98636 - Test loss: 1.11118\n",
      "Epoch 457 - lr: 0.01000 - Train loss: 0.98619 - Test loss: 1.11119\n",
      "Epoch 458 - lr: 0.01000 - Train loss: 0.98602 - Test loss: 1.11120\n",
      "Epoch 459 - lr: 0.01000 - Train loss: 0.98585 - Test loss: 1.11123\n",
      "Epoch 460 - lr: 0.01000 - Train loss: 0.98569 - Test loss: 1.11125\n",
      "Epoch 461 - lr: 0.01000 - Train loss: 0.98553 - Test loss: 1.11128\n",
      "Epoch 462 - lr: 0.01000 - Train loss: 0.98537 - Test loss: 1.11131\n",
      "Epoch 463 - lr: 0.01000 - Train loss: 0.98521 - Test loss: 1.11134\n",
      "Epoch 464 - lr: 0.01000 - Train loss: 0.98506 - Test loss: 1.11137\n",
      "Epoch 465 - lr: 0.01000 - Train loss: 0.98491 - Test loss: 1.11140\n",
      "Epoch 466 - lr: 0.01000 - Train loss: 0.98476 - Test loss: 1.11143\n",
      "Epoch 467 - lr: 0.01000 - Train loss: 0.98462 - Test loss: 1.11145\n",
      "Epoch 468 - lr: 0.01000 - Train loss: 0.98448 - Test loss: 1.11147\n",
      "Epoch 469 - lr: 0.01000 - Train loss: 0.98435 - Test loss: 1.11149\n",
      "Epoch 470 - lr: 0.01000 - Train loss: 0.98422 - Test loss: 1.11150\n",
      "Epoch 471 - lr: 0.01000 - Train loss: 0.98410 - Test loss: 1.11150\n",
      "Epoch 472 - lr: 0.01000 - Train loss: 0.98398 - Test loss: 1.11149\n",
      "Epoch 473 - lr: 0.01000 - Train loss: 0.98387 - Test loss: 1.11148\n",
      "Epoch 474 - lr: 0.01000 - Train loss: 0.98376 - Test loss: 1.11146\n",
      "Epoch 475 - lr: 0.01000 - Train loss: 0.98365 - Test loss: 1.11144\n",
      "Epoch 476 - lr: 0.01000 - Train loss: 0.98356 - Test loss: 1.11140\n",
      "Epoch 477 - lr: 0.01000 - Train loss: 0.98346 - Test loss: 1.11135\n",
      "Epoch 478 - lr: 0.01000 - Train loss: 0.98337 - Test loss: 1.11130\n",
      "Epoch 479 - lr: 0.01000 - Train loss: 0.98329 - Test loss: 1.11124\n",
      "Epoch 480 - lr: 0.01000 - Train loss: 0.98321 - Test loss: 1.11116\n",
      "Epoch 481 - lr: 0.01000 - Train loss: 0.98314 - Test loss: 1.11108\n",
      "Epoch 482 - lr: 0.01000 - Train loss: 0.98307 - Test loss: 1.11099\n",
      "Epoch 483 - lr: 0.01000 - Train loss: 0.98301 - Test loss: 1.11089\n",
      "Epoch 484 - lr: 0.01000 - Train loss: 0.98295 - Test loss: 1.11078\n",
      "Epoch 485 - lr: 0.01000 - Train loss: 0.98290 - Test loss: 1.11066\n",
      "Epoch 486 - lr: 0.01000 - Train loss: 0.98285 - Test loss: 1.11053\n",
      "Epoch 487 - lr: 0.01000 - Train loss: 0.98280 - Test loss: 1.11039\n",
      "Epoch 488 - lr: 0.01000 - Train loss: 0.98276 - Test loss: 1.11024\n",
      "Epoch 489 - lr: 0.01000 - Train loss: 0.98273 - Test loss: 1.11009\n",
      "Epoch 490 - lr: 0.01000 - Train loss: 0.98270 - Test loss: 1.10992\n",
      "Epoch 491 - lr: 0.01000 - Train loss: 0.98267 - Test loss: 1.10975\n",
      "Epoch 492 - lr: 0.01000 - Train loss: 0.98265 - Test loss: 1.10956\n",
      "Epoch 493 - lr: 0.01000 - Train loss: 0.98263 - Test loss: 1.10937\n",
      "Epoch 494 - lr: 0.01000 - Train loss: 0.98262 - Test loss: 1.10918\n",
      "Epoch 495 - lr: 0.01000 - Train loss: 0.98261 - Test loss: 1.10897\n",
      "Epoch 496 - lr: 0.01000 - Train loss: 0.98260 - Test loss: 1.10875\n",
      "Epoch 497 - lr: 0.01000 - Train loss: 0.98260 - Test loss: 1.10853\n",
      "Epoch 498 - lr: 0.01000 - Train loss: 0.98261 - Test loss: 1.10830\n",
      "Epoch 499 - lr: 0.01000 - Train loss: 0.98261 - Test loss: 1.10807\n",
      "Epoch 500 - lr: 0.01000 - Train loss: 0.98263 - Test loss: 1.10783\n",
      "Epoch 501 - lr: 0.01000 - Train loss: 0.98264 - Test loss: 1.10758\n",
      "Epoch 502 - lr: 0.01000 - Train loss: 0.98266 - Test loss: 1.10732\n",
      "Epoch 503 - lr: 0.01000 - Train loss: 0.98269 - Test loss: 1.10706\n",
      "Epoch 504 - lr: 0.01000 - Train loss: 0.98272 - Test loss: 1.10679\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 505 - lr: 0.01000 - Train loss: 0.98275 - Test loss: 1.10652\n",
      "Epoch 506 - lr: 0.01000 - Train loss: 0.98279 - Test loss: 1.10624\n",
      "Epoch 507 - lr: 0.01000 - Train loss: 0.98283 - Test loss: 1.10595\n",
      "Epoch 508 - lr: 0.01000 - Train loss: 0.98288 - Test loss: 1.10567\n",
      "Epoch 509 - lr: 0.01000 - Train loss: 0.98293 - Test loss: 1.10537\n",
      "Epoch 510 - lr: 0.01000 - Train loss: 0.98298 - Test loss: 1.10507\n",
      "Epoch 511 - lr: 0.01000 - Train loss: 0.98304 - Test loss: 1.10477\n",
      "Epoch 512 - lr: 0.01000 - Train loss: 0.98311 - Test loss: 1.10447\n",
      "Epoch 513 - lr: 0.01000 - Train loss: 0.98317 - Test loss: 1.10416\n",
      "Epoch 514 - lr: 0.01000 - Train loss: 0.98325 - Test loss: 1.10385\n",
      "Epoch 515 - lr: 0.01000 - Train loss: 0.98332 - Test loss: 1.10353\n",
      "Epoch 516 - lr: 0.01000 - Train loss: 0.98340 - Test loss: 1.10322\n",
      "Epoch 517 - lr: 0.01000 - Train loss: 0.98349 - Test loss: 1.10290\n",
      "Epoch 518 - lr: 0.01000 - Train loss: 0.98357 - Test loss: 1.10258\n",
      "Epoch 519 - lr: 0.01000 - Train loss: 0.98367 - Test loss: 1.10226\n",
      "Epoch 520 - lr: 0.01000 - Train loss: 0.98376 - Test loss: 1.10194\n",
      "Epoch 521 - lr: 0.01000 - Train loss: 0.98386 - Test loss: 1.10162\n",
      "Epoch 522 - lr: 0.01000 - Train loss: 0.98396 - Test loss: 1.10130\n",
      "Epoch 523 - lr: 0.01000 - Train loss: 0.98406 - Test loss: 1.10098\n",
      "Epoch 524 - lr: 0.01000 - Train loss: 0.98417 - Test loss: 1.10067\n",
      "Epoch 525 - lr: 0.01000 - Train loss: 0.98428 - Test loss: 1.10035\n",
      "Epoch 526 - lr: 0.01000 - Train loss: 0.98439 - Test loss: 1.10004\n",
      "Epoch 527 - lr: 0.01000 - Train loss: 0.98450 - Test loss: 1.09973\n",
      "Epoch 528 - lr: 0.01000 - Train loss: 0.98462 - Test loss: 1.09942\n",
      "Epoch 529 - lr: 0.01000 - Train loss: 0.98473 - Test loss: 1.09911\n",
      "Epoch 530 - lr: 0.01000 - Train loss: 0.98485 - Test loss: 1.09881\n",
      "Epoch 531 - lr: 0.01000 - Train loss: 0.98497 - Test loss: 1.09851\n",
      "Epoch 532 - lr: 0.01000 - Train loss: 0.98509 - Test loss: 1.09822\n",
      "Epoch 533 - lr: 0.01000 - Train loss: 0.98520 - Test loss: 1.09793\n",
      "Epoch 534 - lr: 0.01000 - Train loss: 0.98532 - Test loss: 1.09765\n",
      "Epoch 535 - lr: 0.01000 - Train loss: 0.98544 - Test loss: 1.09737\n",
      "Epoch 536 - lr: 0.01000 - Train loss: 0.98556 - Test loss: 1.09709\n",
      "Epoch 537 - lr: 0.01000 - Train loss: 0.98568 - Test loss: 1.09682\n",
      "Epoch 538 - lr: 0.01000 - Train loss: 0.98579 - Test loss: 1.09656\n",
      "Epoch 539 - lr: 0.01000 - Train loss: 0.98591 - Test loss: 1.09630\n",
      "Epoch 540 - lr: 0.01000 - Train loss: 0.98603 - Test loss: 1.09604\n",
      "Epoch 541 - lr: 0.01000 - Train loss: 0.98614 - Test loss: 1.09579\n",
      "Epoch 542 - lr: 0.01000 - Train loss: 0.98625 - Test loss: 1.09554\n",
      "Epoch 543 - lr: 0.01000 - Train loss: 0.98637 - Test loss: 1.09530\n",
      "Epoch 544 - lr: 0.01000 - Train loss: 0.98648 - Test loss: 1.09507\n",
      "Epoch 545 - lr: 0.01000 - Train loss: 0.98659 - Test loss: 1.09484\n",
      "Epoch 546 - lr: 0.01000 - Train loss: 0.98669 - Test loss: 1.09461\n",
      "Epoch 547 - lr: 0.01000 - Train loss: 0.98680 - Test loss: 1.09439\n",
      "Epoch 548 - lr: 0.01000 - Train loss: 0.98691 - Test loss: 1.09417\n",
      "Epoch 549 - lr: 0.01000 - Train loss: 0.98701 - Test loss: 1.09396\n",
      "Epoch 550 - lr: 0.01000 - Train loss: 0.98711 - Test loss: 1.09375\n",
      "Epoch 551 - lr: 0.01000 - Train loss: 0.98721 - Test loss: 1.09354\n",
      "Epoch 552 - lr: 0.01000 - Train loss: 0.98731 - Test loss: 1.09334\n",
      "Epoch 553 - lr: 0.01000 - Train loss: 0.98741 - Test loss: 1.09315\n",
      "Epoch 554 - lr: 0.01000 - Train loss: 0.98750 - Test loss: 1.09295\n",
      "Epoch 555 - lr: 0.01000 - Train loss: 0.98760 - Test loss: 1.09276\n",
      "Epoch 556 - lr: 0.01000 - Train loss: 0.98769 - Test loss: 1.09257\n",
      "Epoch 557 - lr: 0.01000 - Train loss: 0.98778 - Test loss: 1.09239\n",
      "Epoch 558 - lr: 0.01000 - Train loss: 0.98787 - Test loss: 1.09221\n",
      "Epoch 559 - lr: 0.01000 - Train loss: 0.98796 - Test loss: 1.09203\n",
      "Epoch 560 - lr: 0.01000 - Train loss: 0.98805 - Test loss: 1.09186\n",
      "Epoch 561 - lr: 0.01000 - Train loss: 0.98813 - Test loss: 1.09168\n",
      "Epoch 562 - lr: 0.01000 - Train loss: 0.98822 - Test loss: 1.09151\n",
      "Epoch 563 - lr: 0.01000 - Train loss: 0.98830 - Test loss: 1.09135\n",
      "Epoch 564 - lr: 0.01000 - Train loss: 0.98838 - Test loss: 1.09118\n",
      "Epoch 565 - lr: 0.01000 - Train loss: 0.98846 - Test loss: 1.09102\n",
      "Epoch 566 - lr: 0.01000 - Train loss: 0.98854 - Test loss: 1.09086\n",
      "Epoch 567 - lr: 0.01000 - Train loss: 0.98861 - Test loss: 1.09070\n",
      "Epoch 568 - lr: 0.01000 - Train loss: 0.98869 - Test loss: 1.09054\n",
      "Epoch 569 - lr: 0.01000 - Train loss: 0.98876 - Test loss: 1.09039\n",
      "Epoch 570 - lr: 0.01000 - Train loss: 0.98883 - Test loss: 1.09023\n",
      "Epoch 571 - lr: 0.01000 - Train loss: 0.98890 - Test loss: 1.09008\n",
      "Epoch 572 - lr: 0.01000 - Train loss: 0.98897 - Test loss: 1.08993\n",
      "Epoch 573 - lr: 0.01000 - Train loss: 0.98904 - Test loss: 1.08978\n",
      "Epoch 574 - lr: 0.01000 - Train loss: 0.98911 - Test loss: 1.08964\n",
      "Epoch 575 - lr: 0.01000 - Train loss: 0.98917 - Test loss: 1.08949\n",
      "Epoch 576 - lr: 0.01000 - Train loss: 0.98923 - Test loss: 1.08935\n",
      "Epoch 577 - lr: 0.01000 - Train loss: 0.98930 - Test loss: 1.08920\n",
      "Epoch 578 - lr: 0.01000 - Train loss: 0.98936 - Test loss: 1.08906\n",
      "Epoch 579 - lr: 0.01000 - Train loss: 0.98942 - Test loss: 1.08892\n",
      "Epoch 580 - lr: 0.01000 - Train loss: 0.98947 - Test loss: 1.08878\n",
      "Epoch 581 - lr: 0.01000 - Train loss: 0.98953 - Test loss: 1.08864\n",
      "Epoch 582 - lr: 0.01000 - Train loss: 0.98958 - Test loss: 1.08850\n",
      "Epoch 583 - lr: 0.01000 - Train loss: 0.98964 - Test loss: 1.08836\n",
      "Epoch 584 - lr: 0.01000 - Train loss: 0.98969 - Test loss: 1.08822\n",
      "Epoch 585 - lr: 0.01000 - Train loss: 0.98974 - Test loss: 1.08808\n",
      "Epoch 586 - lr: 0.01000 - Train loss: 0.98979 - Test loss: 1.08795\n",
      "Epoch 587 - lr: 0.01000 - Train loss: 0.98983 - Test loss: 1.08781\n",
      "Epoch 588 - lr: 0.01000 - Train loss: 0.98988 - Test loss: 1.08768\n",
      "Epoch 589 - lr: 0.01000 - Train loss: 0.98992 - Test loss: 1.08754\n",
      "Epoch 590 - lr: 0.01000 - Train loss: 0.98996 - Test loss: 1.08741\n",
      "Epoch 591 - lr: 0.01000 - Train loss: 0.99000 - Test loss: 1.08727\n",
      "Epoch 592 - lr: 0.01000 - Train loss: 0.99004 - Test loss: 1.08714\n",
      "Epoch 593 - lr: 0.01000 - Train loss: 0.99008 - Test loss: 1.08701\n",
      "Epoch 594 - lr: 0.01000 - Train loss: 0.99011 - Test loss: 1.08687\n",
      "Epoch 595 - lr: 0.01000 - Train loss: 0.99014 - Test loss: 1.08674\n",
      "Epoch 596 - lr: 0.01000 - Train loss: 0.99018 - Test loss: 1.08661\n",
      "Epoch 597 - lr: 0.01000 - Train loss: 0.99020 - Test loss: 1.08647\n",
      "Epoch 598 - lr: 0.01000 - Train loss: 0.99023 - Test loss: 1.08634\n",
      "Epoch 599 - lr: 0.01000 - Train loss: 0.99026 - Test loss: 1.08621\n",
      "Epoch 600 - lr: 0.01000 - Train loss: 0.99028 - Test loss: 1.08608\n",
      "Epoch 601 - lr: 0.01000 - Train loss: 0.99030 - Test loss: 1.08594\n",
      "Epoch 602 - lr: 0.01000 - Train loss: 0.99032 - Test loss: 1.08581\n",
      "Epoch 603 - lr: 0.01000 - Train loss: 0.99034 - Test loss: 1.08568\n",
      "Epoch 604 - lr: 0.01000 - Train loss: 0.99035 - Test loss: 1.08555\n",
      "Epoch 605 - lr: 0.01000 - Train loss: 0.99036 - Test loss: 1.08542\n",
      "Epoch 606 - lr: 0.01000 - Train loss: 0.99037 - Test loss: 1.08528\n",
      "Epoch 607 - lr: 0.01000 - Train loss: 0.99038 - Test loss: 1.08515\n",
      "Epoch 608 - lr: 0.01000 - Train loss: 0.99039 - Test loss: 1.08502\n",
      "Epoch 609 - lr: 0.01000 - Train loss: 0.99039 - Test loss: 1.08489\n",
      "Epoch 610 - lr: 0.01000 - Train loss: 0.99039 - Test loss: 1.08476\n",
      "Epoch 611 - lr: 0.01000 - Train loss: 0.99039 - Test loss: 1.08463\n",
      "Epoch 612 - lr: 0.01000 - Train loss: 0.99038 - Test loss: 1.08450\n",
      "Epoch 613 - lr: 0.01000 - Train loss: 0.99038 - Test loss: 1.08436\n",
      "Epoch 614 - lr: 0.01000 - Train loss: 0.99037 - Test loss: 1.08423\n",
      "Epoch 615 - lr: 0.01000 - Train loss: 0.99036 - Test loss: 1.08410\n",
      "Epoch 616 - lr: 0.01000 - Train loss: 0.99034 - Test loss: 1.08397\n",
      "Epoch 617 - lr: 0.01000 - Train loss: 0.99033 - Test loss: 1.08384\n",
      "Epoch 618 - lr: 0.01000 - Train loss: 0.99031 - Test loss: 1.08372\n",
      "Epoch 619 - lr: 0.01000 - Train loss: 0.99029 - Test loss: 1.08359\n",
      "Epoch 620 - lr: 0.01000 - Train loss: 0.99026 - Test loss: 1.08346\n",
      "Epoch 621 - lr: 0.01000 - Train loss: 0.99024 - Test loss: 1.08333\n",
      "Epoch 622 - lr: 0.01000 - Train loss: 0.99021 - Test loss: 1.08320\n",
      "Epoch 623 - lr: 0.01000 - Train loss: 0.99018 - Test loss: 1.08308\n",
      "Epoch 624 - lr: 0.01000 - Train loss: 0.99014 - Test loss: 1.08295\n",
      "Epoch 625 - lr: 0.01000 - Train loss: 0.99011 - Test loss: 1.08283\n",
      "Epoch 626 - lr: 0.01000 - Train loss: 0.99007 - Test loss: 1.08270\n",
      "Epoch 627 - lr: 0.01000 - Train loss: 0.99003 - Test loss: 1.08258\n",
      "Epoch 628 - lr: 0.01000 - Train loss: 0.98998 - Test loss: 1.08246\n",
      "Epoch 629 - lr: 0.01000 - Train loss: 0.98993 - Test loss: 1.08233\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 630 - lr: 0.01000 - Train loss: 0.98988 - Test loss: 1.08221\n",
      "Epoch 631 - lr: 0.01000 - Train loss: 0.98983 - Test loss: 1.08209\n",
      "Epoch 632 - lr: 0.01000 - Train loss: 0.98977 - Test loss: 1.08198\n",
      "Epoch 633 - lr: 0.01000 - Train loss: 0.98972 - Test loss: 1.08186\n",
      "Epoch 634 - lr: 0.01000 - Train loss: 0.98966 - Test loss: 1.08174\n",
      "Epoch 635 - lr: 0.01000 - Train loss: 0.98959 - Test loss: 1.08163\n",
      "Epoch 636 - lr: 0.01000 - Train loss: 0.98953 - Test loss: 1.08151\n",
      "Epoch 637 - lr: 0.01000 - Train loss: 0.98946 - Test loss: 1.08140\n",
      "Epoch 638 - lr: 0.01000 - Train loss: 0.98939 - Test loss: 1.08129\n",
      "Epoch 639 - lr: 0.01000 - Train loss: 0.98931 - Test loss: 1.08118\n",
      "Epoch 640 - lr: 0.01000 - Train loss: 0.98923 - Test loss: 1.08107\n",
      "Epoch 641 - lr: 0.01000 - Train loss: 0.98915 - Test loss: 1.08097\n",
      "Epoch 642 - lr: 0.01000 - Train loss: 0.98907 - Test loss: 1.08086\n",
      "Epoch 643 - lr: 0.01000 - Train loss: 0.98899 - Test loss: 1.08076\n",
      "Epoch 644 - lr: 0.01000 - Train loss: 0.98890 - Test loss: 1.08066\n",
      "Epoch 645 - lr: 0.01000 - Train loss: 0.98881 - Test loss: 1.08056\n",
      "Epoch 646 - lr: 0.01000 - Train loss: 0.98872 - Test loss: 1.08046\n",
      "Epoch 647 - lr: 0.01000 - Train loss: 0.98862 - Test loss: 1.08036\n",
      "Epoch 648 - lr: 0.01000 - Train loss: 0.98852 - Test loss: 1.08026\n",
      "Epoch 649 - lr: 0.01000 - Train loss: 0.98842 - Test loss: 1.08017\n",
      "Epoch 650 - lr: 0.01000 - Train loss: 0.98831 - Test loss: 1.08008\n",
      "Epoch 651 - lr: 0.01000 - Train loss: 0.98821 - Test loss: 1.07999\n",
      "Epoch 652 - lr: 0.01000 - Train loss: 0.98810 - Test loss: 1.07990\n",
      "Epoch 653 - lr: 0.01000 - Train loss: 0.98798 - Test loss: 1.07981\n",
      "Epoch 654 - lr: 0.01000 - Train loss: 0.98787 - Test loss: 1.07972\n",
      "Epoch 655 - lr: 0.01000 - Train loss: 0.98775 - Test loss: 1.07964\n",
      "Epoch 656 - lr: 0.01000 - Train loss: 0.98762 - Test loss: 1.07956\n",
      "Epoch 657 - lr: 0.01000 - Train loss: 0.98750 - Test loss: 1.07948\n",
      "Epoch 658 - lr: 0.01000 - Train loss: 0.98737 - Test loss: 1.07940\n",
      "Epoch 659 - lr: 0.01000 - Train loss: 0.98723 - Test loss: 1.07932\n",
      "Epoch 660 - lr: 0.01000 - Train loss: 0.98710 - Test loss: 1.07925\n",
      "Epoch 661 - lr: 0.01000 - Train loss: 0.98696 - Test loss: 1.07918\n",
      "Epoch 662 - lr: 0.01000 - Train loss: 0.98681 - Test loss: 1.07911\n",
      "Epoch 663 - lr: 0.01000 - Train loss: 0.98666 - Test loss: 1.07904\n",
      "Epoch 664 - lr: 0.01000 - Train loss: 0.98651 - Test loss: 1.07898\n",
      "Epoch 665 - lr: 0.01000 - Train loss: 0.98635 - Test loss: 1.07892\n",
      "Epoch 666 - lr: 0.01000 - Train loss: 0.98619 - Test loss: 1.07886\n",
      "Epoch 667 - lr: 0.01000 - Train loss: 0.98603 - Test loss: 1.07880\n",
      "Epoch 668 - lr: 0.01000 - Train loss: 0.98586 - Test loss: 1.07875\n",
      "Epoch 669 - lr: 0.01000 - Train loss: 0.98568 - Test loss: 1.07869\n",
      "Epoch 670 - lr: 0.01000 - Train loss: 0.98551 - Test loss: 1.07865\n",
      "Epoch 671 - lr: 0.01000 - Train loss: 0.98532 - Test loss: 1.07860\n",
      "Epoch 672 - lr: 0.01000 - Train loss: 0.98513 - Test loss: 1.07856\n",
      "Epoch 673 - lr: 0.01000 - Train loss: 0.98494 - Test loss: 1.07853\n",
      "Epoch 674 - lr: 0.01000 - Train loss: 0.98473 - Test loss: 1.07849\n",
      "Epoch 675 - lr: 0.01000 - Train loss: 0.98453 - Test loss: 1.07846\n",
      "Epoch 676 - lr: 0.01000 - Train loss: 0.98431 - Test loss: 1.07844\n",
      "Epoch 677 - lr: 0.01000 - Train loss: 0.98409 - Test loss: 1.07842\n",
      "Epoch 678 - lr: 0.01000 - Train loss: 0.98387 - Test loss: 1.07841\n",
      "Epoch 679 - lr: 0.01000 - Train loss: 0.98363 - Test loss: 1.07840\n",
      "Epoch 680 - lr: 0.01000 - Train loss: 0.98339 - Test loss: 1.07840\n",
      "Epoch 681 - lr: 0.01000 - Train loss: 0.98314 - Test loss: 1.07840\n",
      "Epoch 682 - lr: 0.01000 - Train loss: 0.98288 - Test loss: 1.07841\n",
      "Epoch 683 - lr: 0.01000 - Train loss: 0.98262 - Test loss: 1.07843\n",
      "Epoch 684 - lr: 0.01000 - Train loss: 0.98234 - Test loss: 1.07845\n",
      "Epoch 685 - lr: 0.01000 - Train loss: 0.98206 - Test loss: 1.07849\n",
      "Epoch 686 - lr: 0.01000 - Train loss: 0.98177 - Test loss: 1.07853\n",
      "Epoch 687 - lr: 0.01000 - Train loss: 0.98146 - Test loss: 1.07858\n",
      "Epoch 688 - lr: 0.01000 - Train loss: 0.98115 - Test loss: 1.07864\n",
      "Epoch 689 - lr: 0.01000 - Train loss: 0.98083 - Test loss: 1.07871\n",
      "Epoch 690 - lr: 0.01000 - Train loss: 0.98050 - Test loss: 1.07880\n",
      "Epoch 691 - lr: 0.01000 - Train loss: 0.98016 - Test loss: 1.07889\n",
      "Epoch 692 - lr: 0.01000 - Train loss: 0.97981 - Test loss: 1.07899\n",
      "Epoch 693 - lr: 0.01000 - Train loss: 0.97945 - Test loss: 1.07911\n",
      "Epoch 694 - lr: 0.01000 - Train loss: 0.97908 - Test loss: 1.07923\n",
      "Epoch 695 - lr: 0.01000 - Train loss: 0.97870 - Test loss: 1.07937\n",
      "Epoch 696 - lr: 0.01000 - Train loss: 0.97831 - Test loss: 1.07951\n",
      "Epoch 697 - lr: 0.01000 - Train loss: 0.97792 - Test loss: 1.07967\n",
      "Epoch 698 - lr: 0.01000 - Train loss: 0.97753 - Test loss: 1.07983\n",
      "Epoch 699 - lr: 0.01000 - Train loss: 0.97713 - Test loss: 1.07999\n",
      "Epoch 700 - lr: 0.01000 - Train loss: 0.97673 - Test loss: 1.08016\n",
      "Epoch 701 - lr: 0.01000 - Train loss: 0.97633 - Test loss: 1.08033\n",
      "Epoch 702 - lr: 0.01000 - Train loss: 0.97594 - Test loss: 1.08049\n",
      "Epoch 703 - lr: 0.01000 - Train loss: 0.97556 - Test loss: 1.08064\n",
      "Epoch 704 - lr: 0.01000 - Train loss: 0.97519 - Test loss: 1.08078\n",
      "Epoch 705 - lr: 0.01000 - Train loss: 0.97484 - Test loss: 1.08090\n",
      "Epoch 706 - lr: 0.01000 - Train loss: 0.97451 - Test loss: 1.08099\n",
      "Epoch 707 - lr: 0.01000 - Train loss: 0.97419 - Test loss: 1.08105\n",
      "Epoch 708 - lr: 0.01000 - Train loss: 0.97391 - Test loss: 1.08107\n",
      "Epoch 709 - lr: 0.01000 - Train loss: 0.97365 - Test loss: 1.08105\n",
      "Epoch 710 - lr: 0.01000 - Train loss: 0.97342 - Test loss: 1.08099\n",
      "Epoch 711 - lr: 0.01000 - Train loss: 0.97323 - Test loss: 1.08088\n",
      "Epoch 712 - lr: 0.01000 - Train loss: 0.97307 - Test loss: 1.08071\n",
      "Epoch 713 - lr: 0.01000 - Train loss: 0.97294 - Test loss: 1.08049\n",
      "Epoch 714 - lr: 0.01000 - Train loss: 0.97285 - Test loss: 1.08022\n",
      "Epoch 715 - lr: 0.01000 - Train loss: 0.97280 - Test loss: 1.07990\n",
      "Epoch 716 - lr: 0.01000 - Train loss: 0.97278 - Test loss: 1.07952\n",
      "Epoch 717 - lr: 0.01000 - Train loss: 0.97279 - Test loss: 1.07909\n",
      "Epoch 718 - lr: 0.01000 - Train loss: 0.97283 - Test loss: 1.07862\n",
      "Epoch 719 - lr: 0.01000 - Train loss: 0.97291 - Test loss: 1.07811\n",
      "Epoch 720 - lr: 0.01000 - Train loss: 0.97301 - Test loss: 1.07756\n",
      "Epoch 721 - lr: 0.01000 - Train loss: 0.97314 - Test loss: 1.07698\n",
      "Epoch 722 - lr: 0.01000 - Train loss: 0.97329 - Test loss: 1.07637\n",
      "Epoch 723 - lr: 0.01000 - Train loss: 0.97347 - Test loss: 1.07574\n",
      "Epoch 724 - lr: 0.01000 - Train loss: 0.97366 - Test loss: 1.07510\n",
      "Epoch 725 - lr: 0.01000 - Train loss: 0.97388 - Test loss: 1.07444\n",
      "Epoch 726 - lr: 0.01000 - Train loss: 0.97411 - Test loss: 1.07378\n",
      "Epoch 727 - lr: 0.01000 - Train loss: 0.97436 - Test loss: 1.07312\n",
      "Epoch 728 - lr: 0.01000 - Train loss: 0.97463 - Test loss: 1.07246\n",
      "Epoch 729 - lr: 0.01000 - Train loss: 0.97490 - Test loss: 1.07181\n",
      "Epoch 730 - lr: 0.01000 - Train loss: 0.97519 - Test loss: 1.07117\n",
      "Epoch 731 - lr: 0.01000 - Train loss: 0.97549 - Test loss: 1.07054\n",
      "Epoch 732 - lr: 0.01000 - Train loss: 0.97580 - Test loss: 1.06992\n",
      "Epoch 733 - lr: 0.01000 - Train loss: 0.97611 - Test loss: 1.06933\n",
      "Epoch 734 - lr: 0.01000 - Train loss: 0.97644 - Test loss: 1.06875\n",
      "Epoch 735 - lr: 0.01000 - Train loss: 0.97676 - Test loss: 1.06819\n",
      "Epoch 736 - lr: 0.01000 - Train loss: 0.97709 - Test loss: 1.06765\n",
      "Epoch 737 - lr: 0.01000 - Train loss: 0.97741 - Test loss: 1.06714\n",
      "Epoch 738 - lr: 0.01000 - Train loss: 0.97774 - Test loss: 1.06664\n",
      "Epoch 739 - lr: 0.01000 - Train loss: 0.97806 - Test loss: 1.06618\n",
      "Epoch 740 - lr: 0.01000 - Train loss: 0.97837 - Test loss: 1.06573\n",
      "Epoch 741 - lr: 0.01000 - Train loss: 0.97868 - Test loss: 1.06531\n",
      "Epoch 742 - lr: 0.01000 - Train loss: 0.97898 - Test loss: 1.06491\n",
      "Epoch 743 - lr: 0.01000 - Train loss: 0.97927 - Test loss: 1.06454\n",
      "Epoch 744 - lr: 0.01000 - Train loss: 0.97955 - Test loss: 1.06419\n",
      "Epoch 745 - lr: 0.01000 - Train loss: 0.97982 - Test loss: 1.06386\n",
      "Epoch 746 - lr: 0.01000 - Train loss: 0.98007 - Test loss: 1.06356\n",
      "Epoch 747 - lr: 0.01000 - Train loss: 0.98031 - Test loss: 1.06328\n",
      "Epoch 748 - lr: 0.01000 - Train loss: 0.98054 - Test loss: 1.06302\n",
      "Epoch 749 - lr: 0.01000 - Train loss: 0.98075 - Test loss: 1.06278\n",
      "Epoch 750 - lr: 0.01000 - Train loss: 0.98095 - Test loss: 1.06255\n",
      "Epoch 751 - lr: 0.01000 - Train loss: 0.98113 - Test loss: 1.06235\n",
      "Epoch 752 - lr: 0.01000 - Train loss: 0.98130 - Test loss: 1.06216\n",
      "Epoch 753 - lr: 0.01000 - Train loss: 0.98146 - Test loss: 1.06198\n",
      "Epoch 754 - lr: 0.01000 - Train loss: 0.98161 - Test loss: 1.06182\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 755 - lr: 0.01000 - Train loss: 0.98175 - Test loss: 1.06167\n",
      "Epoch 756 - lr: 0.01000 - Train loss: 0.98188 - Test loss: 1.06154\n",
      "Epoch 757 - lr: 0.01000 - Train loss: 0.98200 - Test loss: 1.06141\n",
      "Epoch 758 - lr: 0.01000 - Train loss: 0.98210 - Test loss: 1.06129\n",
      "Epoch 759 - lr: 0.01000 - Train loss: 0.98221 - Test loss: 1.06118\n",
      "Epoch 760 - lr: 0.01000 - Train loss: 0.98230 - Test loss: 1.06108\n",
      "Epoch 761 - lr: 0.01000 - Train loss: 0.98239 - Test loss: 1.06099\n",
      "Epoch 762 - lr: 0.01000 - Train loss: 0.98247 - Test loss: 1.06090\n",
      "Epoch 763 - lr: 0.01000 - Train loss: 0.98254 - Test loss: 1.06081\n",
      "Epoch 764 - lr: 0.01000 - Train loss: 0.98261 - Test loss: 1.06074\n",
      "Epoch 765 - lr: 0.01000 - Train loss: 0.98268 - Test loss: 1.06066\n",
      "Epoch 766 - lr: 0.01000 - Train loss: 0.98274 - Test loss: 1.06059\n",
      "Epoch 767 - lr: 0.01000 - Train loss: 0.98280 - Test loss: 1.06052\n",
      "Epoch 768 - lr: 0.01000 - Train loss: 0.98285 - Test loss: 1.06046\n",
      "Epoch 769 - lr: 0.01000 - Train loss: 0.98290 - Test loss: 1.06040\n",
      "Epoch 770 - lr: 0.01000 - Train loss: 0.98295 - Test loss: 1.06034\n",
      "Epoch 771 - lr: 0.01000 - Train loss: 0.98300 - Test loss: 1.06028\n",
      "Epoch 772 - lr: 0.01000 - Train loss: 0.98304 - Test loss: 1.06022\n",
      "Epoch 773 - lr: 0.01000 - Train loss: 0.98308 - Test loss: 1.06017\n",
      "Epoch 774 - lr: 0.01000 - Train loss: 0.98312 - Test loss: 1.06011\n",
      "Epoch 775 - lr: 0.01000 - Train loss: 0.98316 - Test loss: 1.06006\n",
      "Epoch 776 - lr: 0.01000 - Train loss: 0.98319 - Test loss: 1.06001\n",
      "Epoch 777 - lr: 0.01000 - Train loss: 0.98323 - Test loss: 1.05996\n",
      "Epoch 778 - lr: 0.01000 - Train loss: 0.98326 - Test loss: 1.05991\n",
      "Epoch 779 - lr: 0.01000 - Train loss: 0.98329 - Test loss: 1.05986\n",
      "Epoch 780 - lr: 0.01000 - Train loss: 0.98332 - Test loss: 1.05981\n",
      "Epoch 781 - lr: 0.01000 - Train loss: 0.98335 - Test loss: 1.05976\n",
      "Epoch 782 - lr: 0.01000 - Train loss: 0.98337 - Test loss: 1.05971\n",
      "Epoch 783 - lr: 0.01000 - Train loss: 0.98340 - Test loss: 1.05966\n",
      "Epoch 784 - lr: 0.01000 - Train loss: 0.98343 - Test loss: 1.05962\n",
      "Epoch 785 - lr: 0.01000 - Train loss: 0.98345 - Test loss: 1.05957\n",
      "Epoch 786 - lr: 0.01000 - Train loss: 0.98348 - Test loss: 1.05952\n",
      "Epoch 787 - lr: 0.01000 - Train loss: 0.98350 - Test loss: 1.05947\n",
      "Epoch 788 - lr: 0.01000 - Train loss: 0.98352 - Test loss: 1.05943\n",
      "Epoch 789 - lr: 0.01000 - Train loss: 0.98355 - Test loss: 1.05938\n",
      "Epoch 790 - lr: 0.01000 - Train loss: 0.98357 - Test loss: 1.05933\n",
      "Epoch 791 - lr: 0.01000 - Train loss: 0.98359 - Test loss: 1.05928\n",
      "Epoch 792 - lr: 0.01000 - Train loss: 0.98361 - Test loss: 1.05924\n",
      "Epoch 793 - lr: 0.01000 - Train loss: 0.98363 - Test loss: 1.05919\n",
      "Epoch 794 - lr: 0.01000 - Train loss: 0.98366 - Test loss: 1.05914\n",
      "Epoch 795 - lr: 0.01000 - Train loss: 0.98368 - Test loss: 1.05910\n",
      "Epoch 796 - lr: 0.01000 - Train loss: 0.98370 - Test loss: 1.05905\n",
      "Epoch 797 - lr: 0.01000 - Train loss: 0.98372 - Test loss: 1.05900\n",
      "Epoch 798 - lr: 0.01000 - Train loss: 0.98374 - Test loss: 1.05895\n",
      "Epoch 799 - lr: 0.01000 - Train loss: 0.98375 - Test loss: 1.05891\n",
      "Epoch 800 - lr: 0.01000 - Train loss: 0.98377 - Test loss: 1.05886\n",
      "Epoch 801 - lr: 0.01000 - Train loss: 0.98379 - Test loss: 1.05881\n",
      "Epoch 802 - lr: 0.01000 - Train loss: 0.98381 - Test loss: 1.05876\n",
      "Epoch 803 - lr: 0.01000 - Train loss: 0.98383 - Test loss: 1.05872\n",
      "Epoch 804 - lr: 0.01000 - Train loss: 0.98385 - Test loss: 1.05867\n",
      "Epoch 805 - lr: 0.01000 - Train loss: 0.98387 - Test loss: 1.05862\n",
      "Epoch 806 - lr: 0.01000 - Train loss: 0.98388 - Test loss: 1.05858\n",
      "Epoch 807 - lr: 0.01000 - Train loss: 0.98390 - Test loss: 1.05853\n",
      "Epoch 808 - lr: 0.01000 - Train loss: 0.98392 - Test loss: 1.05848\n",
      "Epoch 809 - lr: 0.01000 - Train loss: 0.98394 - Test loss: 1.05844\n",
      "Epoch 810 - lr: 0.01000 - Train loss: 0.98395 - Test loss: 1.05839\n",
      "Epoch 811 - lr: 0.01000 - Train loss: 0.98397 - Test loss: 1.05834\n",
      "Epoch 812 - lr: 0.01000 - Train loss: 0.98399 - Test loss: 1.05830\n",
      "Epoch 813 - lr: 0.01000 - Train loss: 0.98400 - Test loss: 1.05825\n",
      "Epoch 814 - lr: 0.01000 - Train loss: 0.98402 - Test loss: 1.05820\n",
      "Epoch 815 - lr: 0.01000 - Train loss: 0.98403 - Test loss: 1.05816\n",
      "Epoch 816 - lr: 0.01000 - Train loss: 0.98405 - Test loss: 1.05811\n",
      "Epoch 817 - lr: 0.01000 - Train loss: 0.98407 - Test loss: 1.05806\n",
      "Epoch 818 - lr: 0.01000 - Train loss: 0.98408 - Test loss: 1.05802\n",
      "Epoch 819 - lr: 0.01000 - Train loss: 0.98410 - Test loss: 1.05797\n",
      "Epoch 820 - lr: 0.01000 - Train loss: 0.98411 - Test loss: 1.05793\n",
      "Epoch 821 - lr: 0.01000 - Train loss: 0.98413 - Test loss: 1.05788\n",
      "Epoch 822 - lr: 0.01000 - Train loss: 0.98414 - Test loss: 1.05783\n",
      "Epoch 823 - lr: 0.01000 - Train loss: 0.98416 - Test loss: 1.05779\n",
      "Epoch 824 - lr: 0.01000 - Train loss: 0.98417 - Test loss: 1.05774\n",
      "Epoch 825 - lr: 0.01000 - Train loss: 0.98419 - Test loss: 1.05770\n",
      "Epoch 826 - lr: 0.01000 - Train loss: 0.98420 - Test loss: 1.05765\n",
      "Epoch 827 - lr: 0.01000 - Train loss: 0.98421 - Test loss: 1.05761\n",
      "Epoch 828 - lr: 0.01000 - Train loss: 0.98423 - Test loss: 1.05757\n",
      "Epoch 829 - lr: 0.01000 - Train loss: 0.98424 - Test loss: 1.05752\n",
      "Epoch 830 - lr: 0.01000 - Train loss: 0.98425 - Test loss: 1.05748\n",
      "Epoch 831 - lr: 0.01000 - Train loss: 0.98427 - Test loss: 1.05743\n",
      "Epoch 832 - lr: 0.01000 - Train loss: 0.98428 - Test loss: 1.05739\n",
      "Epoch 833 - lr: 0.01000 - Train loss: 0.98429 - Test loss: 1.05735\n",
      "Epoch 834 - lr: 0.01000 - Train loss: 0.98431 - Test loss: 1.05730\n",
      "Epoch 835 - lr: 0.01000 - Train loss: 0.98432 - Test loss: 1.05726\n",
      "Epoch 836 - lr: 0.01000 - Train loss: 0.98433 - Test loss: 1.05722\n",
      "Epoch 837 - lr: 0.01000 - Train loss: 0.98434 - Test loss: 1.05717\n",
      "Epoch 838 - lr: 0.01000 - Train loss: 0.98435 - Test loss: 1.05713\n",
      "Epoch 839 - lr: 0.01000 - Train loss: 0.98437 - Test loss: 1.05709\n",
      "Epoch 840 - lr: 0.01000 - Train loss: 0.98438 - Test loss: 1.05705\n",
      "Epoch 841 - lr: 0.01000 - Train loss: 0.98439 - Test loss: 1.05700\n",
      "Epoch 842 - lr: 0.01000 - Train loss: 0.98440 - Test loss: 1.05696\n",
      "Epoch 843 - lr: 0.01000 - Train loss: 0.98441 - Test loss: 1.05692\n",
      "Epoch 844 - lr: 0.01000 - Train loss: 0.98442 - Test loss: 1.05688\n",
      "Epoch 845 - lr: 0.01000 - Train loss: 0.98443 - Test loss: 1.05684\n",
      "Epoch 846 - lr: 0.01000 - Train loss: 0.98444 - Test loss: 1.05680\n",
      "Epoch 847 - lr: 0.01000 - Train loss: 0.98445 - Test loss: 1.05676\n",
      "Epoch 848 - lr: 0.01000 - Train loss: 0.98446 - Test loss: 1.05672\n",
      "Epoch 849 - lr: 0.01000 - Train loss: 0.98447 - Test loss: 1.05668\n",
      "Epoch 850 - lr: 0.01000 - Train loss: 0.98448 - Test loss: 1.05664\n",
      "Epoch 851 - lr: 0.01000 - Train loss: 0.98449 - Test loss: 1.05660\n",
      "Epoch 852 - lr: 0.01000 - Train loss: 0.98449 - Test loss: 1.05656\n",
      "Epoch 853 - lr: 0.01000 - Train loss: 0.98450 - Test loss: 1.05652\n",
      "Epoch 854 - lr: 0.01000 - Train loss: 0.98451 - Test loss: 1.05648\n",
      "Epoch 855 - lr: 0.01000 - Train loss: 0.98452 - Test loss: 1.05644\n",
      "Epoch 856 - lr: 0.01000 - Train loss: 0.98453 - Test loss: 1.05640\n",
      "Epoch 857 - lr: 0.01000 - Train loss: 0.98453 - Test loss: 1.05636\n",
      "Epoch 858 - lr: 0.01000 - Train loss: 0.98454 - Test loss: 1.05632\n",
      "Epoch 859 - lr: 0.01000 - Train loss: 0.98455 - Test loss: 1.05629\n",
      "Epoch 860 - lr: 0.01000 - Train loss: 0.98455 - Test loss: 1.05625\n",
      "Epoch 861 - lr: 0.01000 - Train loss: 0.98456 - Test loss: 1.05621\n",
      "Epoch 862 - lr: 0.01000 - Train loss: 0.98457 - Test loss: 1.05617\n",
      "Epoch 863 - lr: 0.01000 - Train loss: 0.98457 - Test loss: 1.05614\n",
      "Epoch 864 - lr: 0.01000 - Train loss: 0.98458 - Test loss: 1.05610\n",
      "Epoch 865 - lr: 0.01000 - Train loss: 0.98458 - Test loss: 1.05606\n",
      "Epoch 866 - lr: 0.01000 - Train loss: 0.98459 - Test loss: 1.05603\n",
      "Epoch 867 - lr: 0.01000 - Train loss: 0.98459 - Test loss: 1.05599\n",
      "Epoch 868 - lr: 0.01000 - Train loss: 0.98460 - Test loss: 1.05595\n",
      "Epoch 869 - lr: 0.01000 - Train loss: 0.98460 - Test loss: 1.05592\n",
      "Epoch 870 - lr: 0.01000 - Train loss: 0.98461 - Test loss: 1.05588\n",
      "Epoch 871 - lr: 0.01000 - Train loss: 0.98461 - Test loss: 1.05585\n",
      "Epoch 872 - lr: 0.01000 - Train loss: 0.98461 - Test loss: 1.05581\n",
      "Epoch 873 - lr: 0.01000 - Train loss: 0.98462 - Test loss: 1.05578\n",
      "Epoch 874 - lr: 0.01000 - Train loss: 0.98462 - Test loss: 1.05574\n",
      "Epoch 875 - lr: 0.01000 - Train loss: 0.98462 - Test loss: 1.05571\n",
      "Epoch 876 - lr: 0.01000 - Train loss: 0.98462 - Test loss: 1.05567\n",
      "Epoch 877 - lr: 0.01000 - Train loss: 0.98463 - Test loss: 1.05564\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 878 - lr: 0.01000 - Train loss: 0.98463 - Test loss: 1.05560\n",
      "Epoch 879 - lr: 0.01000 - Train loss: 0.98463 - Test loss: 1.05557\n",
      "Epoch 880 - lr: 0.01000 - Train loss: 0.98463 - Test loss: 1.05554\n",
      "Epoch 881 - lr: 0.01000 - Train loss: 0.98463 - Test loss: 1.05550\n",
      "Epoch 882 - lr: 0.01000 - Train loss: 0.98463 - Test loss: 1.05547\n",
      "Epoch 883 - lr: 0.01000 - Train loss: 0.98463 - Test loss: 1.05544\n",
      "Epoch 884 - lr: 0.01000 - Train loss: 0.98463 - Test loss: 1.05541\n",
      "Epoch 885 - lr: 0.01000 - Train loss: 0.98463 - Test loss: 1.05537\n",
      "Epoch 886 - lr: 0.01000 - Train loss: 0.98463 - Test loss: 1.05534\n",
      "Epoch 887 - lr: 0.01000 - Train loss: 0.98463 - Test loss: 1.05531\n",
      "Epoch 888 - lr: 0.01000 - Train loss: 0.98463 - Test loss: 1.05528\n",
      "Epoch 889 - lr: 0.01000 - Train loss: 0.98463 - Test loss: 1.05525\n",
      "Epoch 890 - lr: 0.01000 - Train loss: 0.98463 - Test loss: 1.05521\n",
      "Epoch 891 - lr: 0.01000 - Train loss: 0.98463 - Test loss: 1.05518\n",
      "Epoch 892 - lr: 0.01000 - Train loss: 0.98463 - Test loss: 1.05515\n",
      "Epoch 893 - lr: 0.01000 - Train loss: 0.98462 - Test loss: 1.05512\n",
      "Epoch 894 - lr: 0.01000 - Train loss: 0.98462 - Test loss: 1.05509\n",
      "Epoch 895 - lr: 0.01000 - Train loss: 0.98462 - Test loss: 1.05506\n",
      "Epoch 896 - lr: 0.01000 - Train loss: 0.98462 - Test loss: 1.05503\n",
      "Epoch 897 - lr: 0.01000 - Train loss: 0.98461 - Test loss: 1.05500\n",
      "Epoch 898 - lr: 0.01000 - Train loss: 0.98461 - Test loss: 1.05497\n",
      "Epoch 899 - lr: 0.01000 - Train loss: 0.98460 - Test loss: 1.05494\n",
      "Epoch 900 - lr: 0.01000 - Train loss: 0.98460 - Test loss: 1.05491\n",
      "Epoch 901 - lr: 0.01000 - Train loss: 0.98460 - Test loss: 1.05488\n",
      "Epoch 902 - lr: 0.01000 - Train loss: 0.98459 - Test loss: 1.05485\n",
      "Epoch 903 - lr: 0.01000 - Train loss: 0.98459 - Test loss: 1.05483\n",
      "Epoch 904 - lr: 0.01000 - Train loss: 0.98458 - Test loss: 1.05480\n",
      "Epoch 905 - lr: 0.01000 - Train loss: 0.98457 - Test loss: 1.05477\n",
      "Epoch 906 - lr: 0.01000 - Train loss: 0.98457 - Test loss: 1.05474\n",
      "Epoch 907 - lr: 0.01000 - Train loss: 0.98456 - Test loss: 1.05471\n",
      "Epoch 908 - lr: 0.01000 - Train loss: 0.98456 - Test loss: 1.05468\n",
      "Epoch 909 - lr: 0.01000 - Train loss: 0.98455 - Test loss: 1.05466\n",
      "Epoch 910 - lr: 0.01000 - Train loss: 0.98454 - Test loss: 1.05463\n",
      "Epoch 911 - lr: 0.01000 - Train loss: 0.98454 - Test loss: 1.05460\n",
      "Epoch 912 - lr: 0.01000 - Train loss: 0.98453 - Test loss: 1.05458\n",
      "Epoch 913 - lr: 0.01000 - Train loss: 0.98452 - Test loss: 1.05455\n",
      "Epoch 914 - lr: 0.01000 - Train loss: 0.98451 - Test loss: 1.05452\n",
      "Epoch 915 - lr: 0.01000 - Train loss: 0.98450 - Test loss: 1.05450\n",
      "Epoch 916 - lr: 0.01000 - Train loss: 0.98449 - Test loss: 1.05447\n",
      "Epoch 917 - lr: 0.01000 - Train loss: 0.98449 - Test loss: 1.05444\n",
      "Epoch 918 - lr: 0.01000 - Train loss: 0.98448 - Test loss: 1.05442\n",
      "Epoch 919 - lr: 0.01000 - Train loss: 0.98447 - Test loss: 1.05439\n",
      "Epoch 920 - lr: 0.01000 - Train loss: 0.98446 - Test loss: 1.05437\n",
      "Epoch 921 - lr: 0.01000 - Train loss: 0.98445 - Test loss: 1.05434\n",
      "Epoch 922 - lr: 0.01000 - Train loss: 0.98444 - Test loss: 1.05431\n",
      "Epoch 923 - lr: 0.01000 - Train loss: 0.98442 - Test loss: 1.05429\n",
      "Epoch 924 - lr: 0.01000 - Train loss: 0.98441 - Test loss: 1.05426\n",
      "Epoch 925 - lr: 0.01000 - Train loss: 0.98440 - Test loss: 1.05424\n",
      "Epoch 926 - lr: 0.01000 - Train loss: 0.98439 - Test loss: 1.05421\n",
      "Epoch 927 - lr: 0.01000 - Train loss: 0.98438 - Test loss: 1.05419\n",
      "Epoch 928 - lr: 0.01000 - Train loss: 0.98437 - Test loss: 1.05417\n",
      "Epoch 929 - lr: 0.01000 - Train loss: 0.98435 - Test loss: 1.05414\n",
      "Epoch 930 - lr: 0.01000 - Train loss: 0.98434 - Test loss: 1.05412\n",
      "Epoch 931 - lr: 0.01000 - Train loss: 0.98433 - Test loss: 1.05409\n",
      "Epoch 932 - lr: 0.01000 - Train loss: 0.98432 - Test loss: 1.05407\n",
      "Epoch 933 - lr: 0.01000 - Train loss: 0.98430 - Test loss: 1.05405\n",
      "Epoch 934 - lr: 0.01000 - Train loss: 0.98429 - Test loss: 1.05402\n",
      "Epoch 935 - lr: 0.01000 - Train loss: 0.98428 - Test loss: 1.05400\n",
      "Epoch 936 - lr: 0.01000 - Train loss: 0.98426 - Test loss: 1.05398\n",
      "Epoch 937 - lr: 0.01000 - Train loss: 0.98425 - Test loss: 1.05395\n",
      "Epoch 938 - lr: 0.01000 - Train loss: 0.98423 - Test loss: 1.05393\n",
      "Epoch 939 - lr: 0.01000 - Train loss: 0.98422 - Test loss: 1.05391\n",
      "Epoch 940 - lr: 0.01000 - Train loss: 0.98420 - Test loss: 1.05389\n",
      "Epoch 941 - lr: 0.01000 - Train loss: 0.98419 - Test loss: 1.05386\n",
      "Epoch 942 - lr: 0.01000 - Train loss: 0.98417 - Test loss: 1.05384\n",
      "Epoch 943 - lr: 0.01000 - Train loss: 0.98415 - Test loss: 1.05382\n",
      "Epoch 944 - lr: 0.01000 - Train loss: 0.98414 - Test loss: 1.05380\n",
      "Epoch 945 - lr: 0.01000 - Train loss: 0.98412 - Test loss: 1.05378\n",
      "Epoch 946 - lr: 0.01000 - Train loss: 0.98411 - Test loss: 1.05376\n",
      "Epoch 947 - lr: 0.01000 - Train loss: 0.98409 - Test loss: 1.05373\n",
      "Epoch 948 - lr: 0.01000 - Train loss: 0.98407 - Test loss: 1.05371\n",
      "Epoch 949 - lr: 0.01000 - Train loss: 0.98405 - Test loss: 1.05369\n",
      "Epoch 950 - lr: 0.01000 - Train loss: 0.98404 - Test loss: 1.05367\n",
      "Epoch 951 - lr: 0.01000 - Train loss: 0.98402 - Test loss: 1.05365\n",
      "Epoch 952 - lr: 0.01000 - Train loss: 0.98400 - Test loss: 1.05363\n",
      "Epoch 953 - lr: 0.01000 - Train loss: 0.98398 - Test loss: 1.05361\n",
      "Epoch 954 - lr: 0.01000 - Train loss: 0.98396 - Test loss: 1.05359\n",
      "Epoch 955 - lr: 0.01000 - Train loss: 0.98394 - Test loss: 1.05357\n",
      "Epoch 956 - lr: 0.01000 - Train loss: 0.98393 - Test loss: 1.05355\n",
      "Epoch 957 - lr: 0.01000 - Train loss: 0.98391 - Test loss: 1.05353\n",
      "Epoch 958 - lr: 0.01000 - Train loss: 0.98389 - Test loss: 1.05351\n",
      "Epoch 959 - lr: 0.01000 - Train loss: 0.98387 - Test loss: 1.05349\n",
      "Epoch 960 - lr: 0.01000 - Train loss: 0.98385 - Test loss: 1.05347\n",
      "Epoch 961 - lr: 0.01000 - Train loss: 0.98383 - Test loss: 1.05345\n",
      "Epoch 962 - lr: 0.01000 - Train loss: 0.98381 - Test loss: 1.05343\n",
      "Epoch 963 - lr: 0.01000 - Train loss: 0.98379 - Test loss: 1.05341\n",
      "Epoch 964 - lr: 0.01000 - Train loss: 0.98377 - Test loss: 1.05339\n",
      "Epoch 965 - lr: 0.01000 - Train loss: 0.98375 - Test loss: 1.05337\n",
      "Epoch 966 - lr: 0.01000 - Train loss: 0.98372 - Test loss: 1.05335\n",
      "Epoch 967 - lr: 0.01000 - Train loss: 0.98370 - Test loss: 1.05333\n",
      "Epoch 968 - lr: 0.01000 - Train loss: 0.98368 - Test loss: 1.05331\n",
      "Epoch 969 - lr: 0.01000 - Train loss: 0.98366 - Test loss: 1.05329\n",
      "Epoch 970 - lr: 0.01000 - Train loss: 0.98364 - Test loss: 1.05327\n",
      "Epoch 971 - lr: 0.01000 - Train loss: 0.98362 - Test loss: 1.05325\n",
      "Epoch 972 - lr: 0.01000 - Train loss: 0.98360 - Test loss: 1.05324\n",
      "Epoch 973 - lr: 0.01000 - Train loss: 0.98357 - Test loss: 1.05322\n",
      "Epoch 974 - lr: 0.01000 - Train loss: 0.98355 - Test loss: 1.05320\n",
      "Epoch 975 - lr: 0.01000 - Train loss: 0.98353 - Test loss: 1.05318\n",
      "Epoch 976 - lr: 0.01000 - Train loss: 0.98351 - Test loss: 1.05316\n",
      "Epoch 977 - lr: 0.01000 - Train loss: 0.98348 - Test loss: 1.05314\n",
      "Epoch 978 - lr: 0.01000 - Train loss: 0.98346 - Test loss: 1.05313\n",
      "Epoch 979 - lr: 0.01000 - Train loss: 0.98344 - Test loss: 1.05311\n",
      "Epoch 980 - lr: 0.01000 - Train loss: 0.98341 - Test loss: 1.05309\n",
      "Epoch 981 - lr: 0.01000 - Train loss: 0.98339 - Test loss: 1.05307\n",
      "Epoch 982 - lr: 0.01000 - Train loss: 0.98337 - Test loss: 1.05305\n",
      "Epoch 983 - lr: 0.01000 - Train loss: 0.98334 - Test loss: 1.05304\n",
      "Epoch 984 - lr: 0.01000 - Train loss: 0.98332 - Test loss: 1.05302\n",
      "Epoch 985 - lr: 0.01000 - Train loss: 0.98330 - Test loss: 1.05300\n",
      "Epoch 986 - lr: 0.01000 - Train loss: 0.98327 - Test loss: 1.05298\n",
      "Epoch 987 - lr: 0.01000 - Train loss: 0.98325 - Test loss: 1.05297\n",
      "Epoch 988 - lr: 0.01000 - Train loss: 0.98323 - Test loss: 1.05295\n",
      "Epoch 989 - lr: 0.01000 - Train loss: 0.98320 - Test loss: 1.05293\n",
      "Epoch 990 - lr: 0.01000 - Train loss: 0.98318 - Test loss: 1.05291\n",
      "Epoch 991 - lr: 0.01000 - Train loss: 0.98315 - Test loss: 1.05290\n",
      "Epoch 992 - lr: 0.01000 - Train loss: 0.98313 - Test loss: 1.05288\n",
      "Epoch 993 - lr: 0.01000 - Train loss: 0.98310 - Test loss: 1.05286\n",
      "Epoch 994 - lr: 0.01000 - Train loss: 0.98308 - Test loss: 1.05285\n",
      "Epoch 995 - lr: 0.01000 - Train loss: 0.98306 - Test loss: 1.05283\n",
      "Epoch 996 - lr: 0.01000 - Train loss: 0.98303 - Test loss: 1.05281\n",
      "Epoch 997 - lr: 0.01000 - Train loss: 0.98301 - Test loss: 1.05280\n",
      "Epoch 998 - lr: 0.01000 - Train loss: 0.98298 - Test loss: 1.05278\n",
      "Epoch 999 - lr: 0.01000 - Train loss: 0.98296 - Test loss: 1.05276\n",
      "Epoch 1000 - lr: 0.01000 - Train loss: 0.98293 - Test loss: 1.05274\n",
      "Epoch 1001 - lr: 0.01000 - Train loss: 0.98291 - Test loss: 1.05273\n",
      "Epoch 1002 - lr: 0.01000 - Train loss: 0.98288 - Test loss: 1.05271\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1003 - lr: 0.01000 - Train loss: 0.98286 - Test loss: 1.05269\n",
      "Epoch 1004 - lr: 0.01000 - Train loss: 0.98283 - Test loss: 1.05268\n",
      "Epoch 1005 - lr: 0.01000 - Train loss: 0.98281 - Test loss: 1.05266\n",
      "Epoch 1006 - lr: 0.01000 - Train loss: 0.98278 - Test loss: 1.05264\n",
      "Epoch 1007 - lr: 0.01000 - Train loss: 0.98276 - Test loss: 1.05263\n",
      "Epoch 1008 - lr: 0.01000 - Train loss: 0.98274 - Test loss: 1.05261\n",
      "Epoch 1009 - lr: 0.01000 - Train loss: 0.98271 - Test loss: 1.05259\n",
      "Epoch 1010 - lr: 0.01000 - Train loss: 0.98269 - Test loss: 1.05258\n",
      "Epoch 1011 - lr: 0.01000 - Train loss: 0.98266 - Test loss: 1.05256\n",
      "Epoch 1012 - lr: 0.01000 - Train loss: 0.98264 - Test loss: 1.05254\n",
      "Epoch 1013 - lr: 0.01000 - Train loss: 0.98261 - Test loss: 1.05253\n",
      "Epoch 1014 - lr: 0.01000 - Train loss: 0.98259 - Test loss: 1.05251\n",
      "Epoch 1015 - lr: 0.01000 - Train loss: 0.98256 - Test loss: 1.05249\n",
      "Epoch 1016 - lr: 0.01000 - Train loss: 0.98254 - Test loss: 1.05248\n",
      "Epoch 1017 - lr: 0.01000 - Train loss: 0.98251 - Test loss: 1.05246\n",
      "Epoch 1018 - lr: 0.01000 - Train loss: 0.98249 - Test loss: 1.05245\n",
      "Epoch 1019 - lr: 0.01000 - Train loss: 0.98247 - Test loss: 1.05243\n",
      "Epoch 1020 - lr: 0.01000 - Train loss: 0.98244 - Test loss: 1.05241\n",
      "Epoch 1021 - lr: 0.01000 - Train loss: 0.98242 - Test loss: 1.05240\n",
      "Epoch 1022 - lr: 0.01000 - Train loss: 0.98239 - Test loss: 1.05238\n",
      "Epoch 1023 - lr: 0.01000 - Train loss: 0.98237 - Test loss: 1.05236\n",
      "Epoch 1024 - lr: 0.01000 - Train loss: 0.98235 - Test loss: 1.05235\n",
      "Epoch 1025 - lr: 0.01000 - Train loss: 0.98232 - Test loss: 1.05233\n",
      "Epoch 1026 - lr: 0.01000 - Train loss: 0.98230 - Test loss: 1.05231\n",
      "Epoch 1027 - lr: 0.01000 - Train loss: 0.98228 - Test loss: 1.05230\n",
      "Epoch 1028 - lr: 0.01000 - Train loss: 0.98225 - Test loss: 1.05228\n",
      "Epoch 1029 - lr: 0.01000 - Train loss: 0.98223 - Test loss: 1.05226\n",
      "Epoch 1030 - lr: 0.01000 - Train loss: 0.98221 - Test loss: 1.05225\n",
      "Epoch 1031 - lr: 0.01000 - Train loss: 0.98218 - Test loss: 1.05223\n",
      "Epoch 1032 - lr: 0.01000 - Train loss: 0.98216 - Test loss: 1.05221\n",
      "Epoch 1033 - lr: 0.01000 - Train loss: 0.98214 - Test loss: 1.05220\n",
      "Epoch 1034 - lr: 0.01000 - Train loss: 0.98212 - Test loss: 1.05218\n",
      "Epoch 1035 - lr: 0.01000 - Train loss: 0.98209 - Test loss: 1.05216\n",
      "Epoch 1036 - lr: 0.01000 - Train loss: 0.98207 - Test loss: 1.05215\n",
      "Epoch 1037 - lr: 0.01000 - Train loss: 0.98205 - Test loss: 1.05213\n",
      "Epoch 1038 - lr: 0.01000 - Train loss: 0.98203 - Test loss: 1.05211\n",
      "Epoch 1039 - lr: 0.01000 - Train loss: 0.98201 - Test loss: 1.05210\n",
      "Epoch 1040 - lr: 0.01000 - Train loss: 0.98199 - Test loss: 1.05208\n",
      "Epoch 1041 - lr: 0.01000 - Train loss: 0.98196 - Test loss: 1.05206\n",
      "Epoch 1042 - lr: 0.01000 - Train loss: 0.98194 - Test loss: 1.05205\n",
      "Epoch 1043 - lr: 0.01000 - Train loss: 0.98192 - Test loss: 1.05203\n",
      "Epoch 1044 - lr: 0.01000 - Train loss: 0.98190 - Test loss: 1.05201\n",
      "Epoch 1045 - lr: 0.01000 - Train loss: 0.98188 - Test loss: 1.05200\n",
      "Epoch 1046 - lr: 0.01000 - Train loss: 0.98186 - Test loss: 1.05198\n",
      "Epoch 1047 - lr: 0.01000 - Train loss: 0.98184 - Test loss: 1.05196\n",
      "Epoch 1048 - lr: 0.01000 - Train loss: 0.98182 - Test loss: 1.05195\n",
      "Epoch 1049 - lr: 0.01000 - Train loss: 0.98180 - Test loss: 1.05193\n",
      "Epoch 1050 - lr: 0.01000 - Train loss: 0.98178 - Test loss: 1.05191\n",
      "Epoch 1051 - lr: 0.01000 - Train loss: 0.98176 - Test loss: 1.05189\n",
      "Epoch 1052 - lr: 0.01000 - Train loss: 0.98174 - Test loss: 1.05188\n",
      "Epoch 1053 - lr: 0.01000 - Train loss: 0.98172 - Test loss: 1.05186\n",
      "Epoch 1054 - lr: 0.01000 - Train loss: 0.98171 - Test loss: 1.05184\n",
      "Epoch 1055 - lr: 0.01000 - Train loss: 0.98169 - Test loss: 1.05183\n",
      "Epoch 1056 - lr: 0.01000 - Train loss: 0.98167 - Test loss: 1.05181\n",
      "Epoch 1057 - lr: 0.01000 - Train loss: 0.98165 - Test loss: 1.05179\n",
      "Epoch 1058 - lr: 0.01000 - Train loss: 0.98163 - Test loss: 1.05177\n",
      "Epoch 1059 - lr: 0.01000 - Train loss: 0.98162 - Test loss: 1.05176\n",
      "Epoch 1060 - lr: 0.01000 - Train loss: 0.98160 - Test loss: 1.05174\n",
      "Epoch 1061 - lr: 0.01000 - Train loss: 0.98158 - Test loss: 1.05172\n",
      "Epoch 1062 - lr: 0.01000 - Train loss: 0.98156 - Test loss: 1.05170\n",
      "Epoch 1063 - lr: 0.01000 - Train loss: 0.98155 - Test loss: 1.05169\n",
      "Epoch 1064 - lr: 0.01000 - Train loss: 0.98153 - Test loss: 1.05167\n",
      "Epoch 1065 - lr: 0.01000 - Train loss: 0.98152 - Test loss: 1.05165\n",
      "Epoch 1066 - lr: 0.01000 - Train loss: 0.98150 - Test loss: 1.05163\n",
      "Epoch 1067 - lr: 0.01000 - Train loss: 0.98148 - Test loss: 1.05162\n",
      "Epoch 1068 - lr: 0.01000 - Train loss: 0.98147 - Test loss: 1.05160\n",
      "Epoch 1069 - lr: 0.01000 - Train loss: 0.98145 - Test loss: 1.05158\n",
      "Epoch 1070 - lr: 0.01000 - Train loss: 0.98144 - Test loss: 1.05156\n",
      "Epoch 1071 - lr: 0.01000 - Train loss: 0.98142 - Test loss: 1.05155\n",
      "Epoch 1072 - lr: 0.01000 - Train loss: 0.98141 - Test loss: 1.05153\n",
      "Epoch 1073 - lr: 0.01000 - Train loss: 0.98139 - Test loss: 1.05151\n",
      "Epoch 1074 - lr: 0.01000 - Train loss: 0.98138 - Test loss: 1.05149\n",
      "Epoch 1075 - lr: 0.01000 - Train loss: 0.98137 - Test loss: 1.05147\n",
      "Epoch 1076 - lr: 0.01000 - Train loss: 0.98135 - Test loss: 1.05146\n",
      "Epoch 1077 - lr: 0.01000 - Train loss: 0.98134 - Test loss: 1.05144\n",
      "Epoch 1078 - lr: 0.01000 - Train loss: 0.98133 - Test loss: 1.05142\n",
      "Epoch 1079 - lr: 0.01000 - Train loss: 0.98132 - Test loss: 1.05140\n",
      "Epoch 1080 - lr: 0.01000 - Train loss: 0.98130 - Test loss: 1.05139\n",
      "Epoch 1081 - lr: 0.01000 - Train loss: 0.98129 - Test loss: 1.05137\n",
      "Epoch 1082 - lr: 0.01000 - Train loss: 0.98128 - Test loss: 1.05135\n",
      "Epoch 1083 - lr: 0.01000 - Train loss: 0.98127 - Test loss: 1.05133\n",
      "Epoch 1084 - lr: 0.01000 - Train loss: 0.98126 - Test loss: 1.05131\n",
      "Epoch 1085 - lr: 0.01000 - Train loss: 0.98124 - Test loss: 1.05130\n",
      "Epoch 1086 - lr: 0.01000 - Train loss: 0.98123 - Test loss: 1.05128\n",
      "Epoch 1087 - lr: 0.01000 - Train loss: 0.98122 - Test loss: 1.05126\n",
      "Epoch 1088 - lr: 0.01000 - Train loss: 0.98121 - Test loss: 1.05124\n",
      "Epoch 1089 - lr: 0.01000 - Train loss: 0.98120 - Test loss: 1.05122\n",
      "Epoch 1090 - lr: 0.01000 - Train loss: 0.98119 - Test loss: 1.05120\n",
      "Epoch 1091 - lr: 0.01000 - Train loss: 0.98118 - Test loss: 1.05119\n",
      "Epoch 1092 - lr: 0.01000 - Train loss: 0.98117 - Test loss: 1.05117\n",
      "Epoch 1093 - lr: 0.01000 - Train loss: 0.98116 - Test loss: 1.05115\n",
      "Epoch 1094 - lr: 0.01000 - Train loss: 0.98115 - Test loss: 1.05113\n",
      "Epoch 1095 - lr: 0.01000 - Train loss: 0.98115 - Test loss: 1.05111\n",
      "Epoch 1096 - lr: 0.01000 - Train loss: 0.98114 - Test loss: 1.05109\n",
      "Epoch 1097 - lr: 0.01000 - Train loss: 0.98113 - Test loss: 1.05108\n",
      "Epoch 1098 - lr: 0.01000 - Train loss: 0.98112 - Test loss: 1.05106\n",
      "Epoch 1099 - lr: 0.01000 - Train loss: 0.98111 - Test loss: 1.05104\n",
      "Epoch 1100 - lr: 0.01000 - Train loss: 0.98111 - Test loss: 1.05102\n",
      "Epoch 1101 - lr: 0.01000 - Train loss: 0.98110 - Test loss: 1.05100\n",
      "Epoch 1102 - lr: 0.01000 - Train loss: 0.98109 - Test loss: 1.05098\n",
      "Epoch 1103 - lr: 0.01000 - Train loss: 0.98108 - Test loss: 1.05097\n",
      "Epoch 1104 - lr: 0.01000 - Train loss: 0.98108 - Test loss: 1.05095\n",
      "Epoch 1105 - lr: 0.01000 - Train loss: 0.98107 - Test loss: 1.05093\n",
      "Epoch 1106 - lr: 0.01000 - Train loss: 0.98106 - Test loss: 1.05091\n",
      "Epoch 1107 - lr: 0.01000 - Train loss: 0.98106 - Test loss: 1.05089\n",
      "Epoch 1108 - lr: 0.01000 - Train loss: 0.98105 - Test loss: 1.05087\n",
      "Epoch 1109 - lr: 0.01000 - Train loss: 0.98105 - Test loss: 1.05085\n",
      "Epoch 1110 - lr: 0.01000 - Train loss: 0.98104 - Test loss: 1.05084\n",
      "Epoch 1111 - lr: 0.01000 - Train loss: 0.98104 - Test loss: 1.05082\n",
      "Epoch 1112 - lr: 0.01000 - Train loss: 0.98103 - Test loss: 1.05080\n",
      "Epoch 1113 - lr: 0.01000 - Train loss: 0.98103 - Test loss: 1.05078\n",
      "Epoch 1114 - lr: 0.01000 - Train loss: 0.98102 - Test loss: 1.05076\n",
      "Epoch 1115 - lr: 0.01000 - Train loss: 0.98102 - Test loss: 1.05074\n",
      "Epoch 1116 - lr: 0.01000 - Train loss: 0.98102 - Test loss: 1.05073\n",
      "Epoch 1117 - lr: 0.01000 - Train loss: 0.98101 - Test loss: 1.05071\n",
      "Epoch 1118 - lr: 0.01000 - Train loss: 0.98101 - Test loss: 1.05069\n",
      "Epoch 1119 - lr: 0.01000 - Train loss: 0.98100 - Test loss: 1.05067\n",
      "Epoch 1120 - lr: 0.01000 - Train loss: 0.98100 - Test loss: 1.05065\n",
      "Epoch 1121 - lr: 0.01000 - Train loss: 0.98100 - Test loss: 1.05063\n",
      "Epoch 1122 - lr: 0.01000 - Train loss: 0.98100 - Test loss: 1.05061\n",
      "Epoch 1123 - lr: 0.01000 - Train loss: 0.98099 - Test loss: 1.05060\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1124 - lr: 0.01000 - Train loss: 0.98099 - Test loss: 1.05058\n",
      "Epoch 1125 - lr: 0.01000 - Train loss: 0.98099 - Test loss: 1.05056\n",
      "Epoch 1126 - lr: 0.01000 - Train loss: 0.98099 - Test loss: 1.05054\n",
      "Epoch 1127 - lr: 0.01000 - Train loss: 0.98099 - Test loss: 1.05052\n",
      "Epoch 1128 - lr: 0.01000 - Train loss: 0.98098 - Test loss: 1.05050\n",
      "Epoch 1129 - lr: 0.01000 - Train loss: 0.98098 - Test loss: 1.05048\n",
      "Epoch 1130 - lr: 0.01000 - Train loss: 0.98098 - Test loss: 1.05047\n",
      "Epoch 1131 - lr: 0.01000 - Train loss: 0.98098 - Test loss: 1.05045\n",
      "Epoch 1132 - lr: 0.01000 - Train loss: 0.98098 - Test loss: 1.05043\n",
      "Epoch 1133 - lr: 0.01000 - Train loss: 0.98098 - Test loss: 1.05041\n",
      "Epoch 1134 - lr: 0.01000 - Train loss: 0.98098 - Test loss: 1.05039\n",
      "Epoch 1135 - lr: 0.01000 - Train loss: 0.98098 - Test loss: 1.05037\n",
      "Epoch 1136 - lr: 0.01000 - Train loss: 0.98098 - Test loss: 1.05035\n",
      "Epoch 1137 - lr: 0.01000 - Train loss: 0.98098 - Test loss: 1.05034\n",
      "Epoch 1138 - lr: 0.01000 - Train loss: 0.98098 - Test loss: 1.05032\n",
      "Epoch 1139 - lr: 0.01000 - Train loss: 0.98098 - Test loss: 1.05030\n",
      "Epoch 1140 - lr: 0.01000 - Train loss: 0.98098 - Test loss: 1.05028\n",
      "Epoch 1141 - lr: 0.01000 - Train loss: 0.98098 - Test loss: 1.05026\n",
      "Epoch 1142 - lr: 0.01000 - Train loss: 0.98098 - Test loss: 1.05024\n",
      "Epoch 1143 - lr: 0.01000 - Train loss: 0.98098 - Test loss: 1.05023\n",
      "Epoch 1144 - lr: 0.01000 - Train loss: 0.98098 - Test loss: 1.05021\n",
      "Epoch 1145 - lr: 0.01000 - Train loss: 0.98099 - Test loss: 1.05019\n",
      "Epoch 1146 - lr: 0.01000 - Train loss: 0.98099 - Test loss: 1.05017\n",
      "Epoch 1147 - lr: 0.01000 - Train loss: 0.98099 - Test loss: 1.05015\n",
      "Epoch 1148 - lr: 0.01000 - Train loss: 0.98099 - Test loss: 1.05013\n",
      "Epoch 1149 - lr: 0.01000 - Train loss: 0.98099 - Test loss: 1.05012\n",
      "Epoch 1150 - lr: 0.01000 - Train loss: 0.98100 - Test loss: 1.05010\n",
      "Epoch 1151 - lr: 0.01000 - Train loss: 0.98100 - Test loss: 1.05008\n",
      "Epoch 1152 - lr: 0.01000 - Train loss: 0.98100 - Test loss: 1.05006\n",
      "Epoch 1153 - lr: 0.01000 - Train loss: 0.98100 - Test loss: 1.05004\n",
      "Epoch 1154 - lr: 0.01000 - Train loss: 0.98101 - Test loss: 1.05002\n",
      "Epoch 1155 - lr: 0.01000 - Train loss: 0.98101 - Test loss: 1.05001\n",
      "Epoch 1156 - lr: 0.01000 - Train loss: 0.98101 - Test loss: 1.04999\n",
      "Epoch 1157 - lr: 0.01000 - Train loss: 0.98102 - Test loss: 1.04997\n",
      "Epoch 1158 - lr: 0.01000 - Train loss: 0.98102 - Test loss: 1.04995\n",
      "Epoch 1159 - lr: 0.01000 - Train loss: 0.98102 - Test loss: 1.04993\n",
      "Epoch 1160 - lr: 0.01000 - Train loss: 0.98103 - Test loss: 1.04992\n",
      "Epoch 1161 - lr: 0.01000 - Train loss: 0.98103 - Test loss: 1.04990\n",
      "Epoch 1162 - lr: 0.01000 - Train loss: 0.98104 - Test loss: 1.04988\n",
      "Epoch 1163 - lr: 0.01000 - Train loss: 0.98104 - Test loss: 1.04986\n",
      "Epoch 1164 - lr: 0.01000 - Train loss: 0.98105 - Test loss: 1.04984\n",
      "Epoch 1165 - lr: 0.01000 - Train loss: 0.98105 - Test loss: 1.04983\n",
      "Epoch 1166 - lr: 0.01000 - Train loss: 0.98105 - Test loss: 1.04981\n",
      "Epoch 1167 - lr: 0.01000 - Train loss: 0.98106 - Test loss: 1.04979\n",
      "Epoch 1168 - lr: 0.01000 - Train loss: 0.98106 - Test loss: 1.04977\n",
      "Epoch 1169 - lr: 0.01000 - Train loss: 0.98107 - Test loss: 1.04975\n",
      "Epoch 1170 - lr: 0.01000 - Train loss: 0.98107 - Test loss: 1.04974\n",
      "Epoch 1171 - lr: 0.01000 - Train loss: 0.98108 - Test loss: 1.04972\n",
      "Epoch 1172 - lr: 0.01000 - Train loss: 0.98109 - Test loss: 1.04970\n",
      "Epoch 1173 - lr: 0.01000 - Train loss: 0.98109 - Test loss: 1.04968\n",
      "Epoch 1174 - lr: 0.01000 - Train loss: 0.98110 - Test loss: 1.04966\n",
      "Epoch 1175 - lr: 0.01000 - Train loss: 0.98110 - Test loss: 1.04965\n",
      "Epoch 1176 - lr: 0.01000 - Train loss: 0.98111 - Test loss: 1.04963\n",
      "Epoch 1177 - lr: 0.01000 - Train loss: 0.98112 - Test loss: 1.04961\n",
      "Epoch 1178 - lr: 0.01000 - Train loss: 0.98112 - Test loss: 1.04959\n",
      "Epoch 1179 - lr: 0.01000 - Train loss: 0.98113 - Test loss: 1.04958\n",
      "Epoch 1180 - lr: 0.01000 - Train loss: 0.98113 - Test loss: 1.04956\n",
      "Epoch 1181 - lr: 0.01000 - Train loss: 0.98114 - Test loss: 1.04954\n",
      "Epoch 1182 - lr: 0.01000 - Train loss: 0.98115 - Test loss: 1.04952\n",
      "Epoch 1183 - lr: 0.01000 - Train loss: 0.98115 - Test loss: 1.04951\n",
      "Epoch 1184 - lr: 0.01000 - Train loss: 0.98116 - Test loss: 1.04949\n",
      "Epoch 1185 - lr: 0.01000 - Train loss: 0.98117 - Test loss: 1.04947\n",
      "Epoch 1186 - lr: 0.01000 - Train loss: 0.98118 - Test loss: 1.04945\n",
      "Epoch 1187 - lr: 0.01000 - Train loss: 0.98118 - Test loss: 1.04944\n",
      "Epoch 1188 - lr: 0.01000 - Train loss: 0.98119 - Test loss: 1.04942\n",
      "Epoch 1189 - lr: 0.01000 - Train loss: 0.98120 - Test loss: 1.04940\n",
      "Epoch 1190 - lr: 0.01000 - Train loss: 0.98121 - Test loss: 1.04938\n",
      "Epoch 1191 - lr: 0.01000 - Train loss: 0.98121 - Test loss: 1.04937\n",
      "Epoch 1192 - lr: 0.01000 - Train loss: 0.98122 - Test loss: 1.04935\n",
      "Epoch 1193 - lr: 0.01000 - Train loss: 0.98123 - Test loss: 1.04933\n",
      "Epoch 1194 - lr: 0.01000 - Train loss: 0.98124 - Test loss: 1.04931\n",
      "Epoch 1195 - lr: 0.01000 - Train loss: 0.98125 - Test loss: 1.04930\n",
      "Epoch 1196 - lr: 0.01000 - Train loss: 0.98125 - Test loss: 1.04928\n",
      "Epoch 1197 - lr: 0.01000 - Train loss: 0.98126 - Test loss: 1.04926\n",
      "Epoch 1198 - lr: 0.01000 - Train loss: 0.98127 - Test loss: 1.04925\n",
      "Epoch 1199 - lr: 0.01000 - Train loss: 0.98128 - Test loss: 1.04923\n",
      "Epoch 1200 - lr: 0.01000 - Train loss: 0.98129 - Test loss: 1.04921\n",
      "Epoch 1201 - lr: 0.01000 - Train loss: 0.98130 - Test loss: 1.04919\n",
      "Epoch 1202 - lr: 0.01000 - Train loss: 0.98131 - Test loss: 1.04918\n",
      "Epoch 1203 - lr: 0.01000 - Train loss: 0.98132 - Test loss: 1.04916\n",
      "Epoch 1204 - lr: 0.01000 - Train loss: 0.98133 - Test loss: 1.04914\n",
      "Epoch 1205 - lr: 0.01000 - Train loss: 0.98133 - Test loss: 1.04913\n",
      "Epoch 1206 - lr: 0.01000 - Train loss: 0.98134 - Test loss: 1.04911\n",
      "Epoch 1207 - lr: 0.01000 - Train loss: 0.98135 - Test loss: 1.04909\n",
      "Epoch 1208 - lr: 0.01000 - Train loss: 0.98136 - Test loss: 1.04908\n",
      "Epoch 1209 - lr: 0.01000 - Train loss: 0.98137 - Test loss: 1.04906\n",
      "Epoch 1210 - lr: 0.01000 - Train loss: 0.98138 - Test loss: 1.04904\n",
      "Epoch 1211 - lr: 0.01000 - Train loss: 0.98139 - Test loss: 1.04903\n",
      "Epoch 1212 - lr: 0.01000 - Train loss: 0.98140 - Test loss: 1.04901\n",
      "Epoch 1213 - lr: 0.01000 - Train loss: 0.98141 - Test loss: 1.04899\n",
      "Epoch 1214 - lr: 0.01000 - Train loss: 0.98142 - Test loss: 1.04898\n",
      "Epoch 1215 - lr: 0.01000 - Train loss: 0.98143 - Test loss: 1.04896\n",
      "Epoch 1216 - lr: 0.01000 - Train loss: 0.98144 - Test loss: 1.04894\n",
      "Epoch 1217 - lr: 0.01000 - Train loss: 0.98145 - Test loss: 1.04893\n",
      "Epoch 1218 - lr: 0.01000 - Train loss: 0.98147 - Test loss: 1.04891\n",
      "Epoch 1219 - lr: 0.01000 - Train loss: 0.98148 - Test loss: 1.04889\n",
      "Epoch 1220 - lr: 0.01000 - Train loss: 0.98149 - Test loss: 1.04888\n",
      "Epoch 1221 - lr: 0.01000 - Train loss: 0.98150 - Test loss: 1.04886\n",
      "Epoch 1222 - lr: 0.01000 - Train loss: 0.98151 - Test loss: 1.04884\n",
      "Epoch 1223 - lr: 0.01000 - Train loss: 0.98152 - Test loss: 1.04883\n",
      "Epoch 1224 - lr: 0.01000 - Train loss: 0.98153 - Test loss: 1.04881\n",
      "Epoch 1225 - lr: 0.01000 - Train loss: 0.98154 - Test loss: 1.04880\n",
      "Epoch 1226 - lr: 0.01000 - Train loss: 0.98155 - Test loss: 1.04878\n",
      "Epoch 1227 - lr: 0.01000 - Train loss: 0.98156 - Test loss: 1.04876\n",
      "Epoch 1228 - lr: 0.01000 - Train loss: 0.98158 - Test loss: 1.04875\n",
      "Epoch 1229 - lr: 0.01000 - Train loss: 0.98159 - Test loss: 1.04873\n",
      "Epoch 1230 - lr: 0.01000 - Train loss: 0.98160 - Test loss: 1.04872\n",
      "Epoch 1231 - lr: 0.01000 - Train loss: 0.98161 - Test loss: 1.04870\n",
      "Epoch 1232 - lr: 0.01000 - Train loss: 0.98162 - Test loss: 1.04868\n",
      "Epoch 1233 - lr: 0.01000 - Train loss: 0.98163 - Test loss: 1.04867\n",
      "Epoch 1234 - lr: 0.01000 - Train loss: 0.98165 - Test loss: 1.04865\n",
      "Epoch 1235 - lr: 0.01000 - Train loss: 0.98166 - Test loss: 1.04864\n",
      "Epoch 1236 - lr: 0.01000 - Train loss: 0.98167 - Test loss: 1.04862\n",
      "Epoch 1237 - lr: 0.01000 - Train loss: 0.98168 - Test loss: 1.04861\n",
      "Epoch 1238 - lr: 0.01000 - Train loss: 0.98170 - Test loss: 1.04859\n",
      "Epoch 1239 - lr: 0.01000 - Train loss: 0.98171 - Test loss: 1.04857\n",
      "Epoch 1240 - lr: 0.01000 - Train loss: 0.98172 - Test loss: 1.04856\n",
      "Epoch 1241 - lr: 0.01000 - Train loss: 0.98173 - Test loss: 1.04854\n",
      "Epoch 1242 - lr: 0.01000 - Train loss: 0.98175 - Test loss: 1.04853\n",
      "Epoch 1243 - lr: 0.01000 - Train loss: 0.98176 - Test loss: 1.04851\n",
      "Epoch 1244 - lr: 0.01000 - Train loss: 0.98177 - Test loss: 1.04850\n",
      "Epoch 1245 - lr: 0.01000 - Train loss: 0.98178 - Test loss: 1.04848\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1246 - lr: 0.01000 - Train loss: 0.98180 - Test loss: 1.04847\n",
      "Epoch 1247 - lr: 0.01000 - Train loss: 0.98181 - Test loss: 1.04845\n",
      "Epoch 1248 - lr: 0.01000 - Train loss: 0.98182 - Test loss: 1.04844\n",
      "Epoch 1249 - lr: 0.01000 - Train loss: 0.98184 - Test loss: 1.04842\n",
      "Epoch 1250 - lr: 0.01000 - Train loss: 0.98185 - Test loss: 1.04841\n",
      "Epoch 1251 - lr: 0.01000 - Train loss: 0.98186 - Test loss: 1.04839\n",
      "Epoch 1252 - lr: 0.01000 - Train loss: 0.98188 - Test loss: 1.04838\n",
      "Epoch 1253 - lr: 0.01000 - Train loss: 0.98189 - Test loss: 1.04836\n",
      "Epoch 1254 - lr: 0.01000 - Train loss: 0.98190 - Test loss: 1.04835\n",
      "Epoch 1255 - lr: 0.01000 - Train loss: 0.98192 - Test loss: 1.04833\n",
      "Epoch 1256 - lr: 0.01000 - Train loss: 0.98193 - Test loss: 1.04832\n",
      "Epoch 1257 - lr: 0.01000 - Train loss: 0.98195 - Test loss: 1.04830\n",
      "Epoch 1258 - lr: 0.01000 - Train loss: 0.98196 - Test loss: 1.04829\n",
      "Epoch 1259 - lr: 0.01000 - Train loss: 0.98197 - Test loss: 1.04827\n",
      "Epoch 1260 - lr: 0.01000 - Train loss: 0.98199 - Test loss: 1.04826\n",
      "Epoch 1261 - lr: 0.01000 - Train loss: 0.98200 - Test loss: 1.04824\n",
      "Epoch 1262 - lr: 0.01000 - Train loss: 0.98202 - Test loss: 1.04823\n",
      "Epoch 1263 - lr: 0.01000 - Train loss: 0.98203 - Test loss: 1.04821\n",
      "Epoch 1264 - lr: 0.01000 - Train loss: 0.98205 - Test loss: 1.04820\n",
      "Epoch 1265 - lr: 0.01000 - Train loss: 0.98206 - Test loss: 1.04819\n",
      "Epoch 1266 - lr: 0.01000 - Train loss: 0.98207 - Test loss: 1.04817\n",
      "Epoch 1267 - lr: 0.01000 - Train loss: 0.98209 - Test loss: 1.04816\n",
      "Epoch 1268 - lr: 0.01000 - Train loss: 0.98210 - Test loss: 1.04814\n",
      "Epoch 1269 - lr: 0.01000 - Train loss: 0.98212 - Test loss: 1.04813\n",
      "Epoch 1270 - lr: 0.01000 - Train loss: 0.98213 - Test loss: 1.04812\n",
      "Epoch 1271 - lr: 0.01000 - Train loss: 0.98215 - Test loss: 1.04810\n",
      "Epoch 1272 - lr: 0.01000 - Train loss: 0.98216 - Test loss: 1.04809\n",
      "Epoch 1273 - lr: 0.01000 - Train loss: 0.98218 - Test loss: 1.04807\n",
      "Epoch 1274 - lr: 0.01000 - Train loss: 0.98219 - Test loss: 1.04806\n",
      "Epoch 1275 - lr: 0.01000 - Train loss: 0.98221 - Test loss: 1.04805\n",
      "Epoch 1276 - lr: 0.01000 - Train loss: 0.98222 - Test loss: 1.04803\n",
      "Epoch 1277 - lr: 0.01000 - Train loss: 0.98224 - Test loss: 1.04802\n",
      "Epoch 1278 - lr: 0.01000 - Train loss: 0.98226 - Test loss: 1.04801\n",
      "Epoch 1279 - lr: 0.01000 - Train loss: 0.98227 - Test loss: 1.04799\n",
      "Epoch 1280 - lr: 0.01000 - Train loss: 0.98229 - Test loss: 1.04798\n",
      "Epoch 1281 - lr: 0.01000 - Train loss: 0.98230 - Test loss: 1.04797\n",
      "Epoch 1282 - lr: 0.01000 - Train loss: 0.98232 - Test loss: 1.04795\n",
      "Epoch 1283 - lr: 0.01000 - Train loss: 0.98233 - Test loss: 1.04794\n",
      "Epoch 1284 - lr: 0.01000 - Train loss: 0.98235 - Test loss: 1.04793\n",
      "Epoch 1285 - lr: 0.01000 - Train loss: 0.98237 - Test loss: 1.04791\n",
      "Epoch 1286 - lr: 0.01000 - Train loss: 0.98238 - Test loss: 1.04790\n",
      "Epoch 1287 - lr: 0.01000 - Train loss: 0.98240 - Test loss: 1.04789\n",
      "Epoch 1288 - lr: 0.01000 - Train loss: 0.98242 - Test loss: 1.04787\n",
      "Epoch 1289 - lr: 0.01000 - Train loss: 0.98243 - Test loss: 1.04786\n",
      "Epoch 1290 - lr: 0.01000 - Train loss: 0.98245 - Test loss: 1.04785\n",
      "Epoch 1291 - lr: 0.01000 - Train loss: 0.98247 - Test loss: 1.04784\n",
      "Epoch 1292 - lr: 0.01000 - Train loss: 0.98248 - Test loss: 1.04782\n",
      "Epoch 1293 - lr: 0.01000 - Train loss: 0.98250 - Test loss: 1.04781\n",
      "Epoch 1294 - lr: 0.01000 - Train loss: 0.98252 - Test loss: 1.04780\n",
      "Epoch 1295 - lr: 0.01000 - Train loss: 0.98253 - Test loss: 1.04779\n",
      "Epoch 1296 - lr: 0.01000 - Train loss: 0.98255 - Test loss: 1.04777\n",
      "Epoch 1297 - lr: 0.01000 - Train loss: 0.98257 - Test loss: 1.04776\n",
      "Epoch 1298 - lr: 0.01000 - Train loss: 0.98258 - Test loss: 1.04775\n",
      "Epoch 1299 - lr: 0.01000 - Train loss: 0.98260 - Test loss: 1.04774\n",
      "Epoch 1300 - lr: 0.01000 - Train loss: 0.98262 - Test loss: 1.04773\n",
      "Epoch 1301 - lr: 0.01000 - Train loss: 0.98264 - Test loss: 1.04771\n",
      "Epoch 1302 - lr: 0.01000 - Train loss: 0.98265 - Test loss: 1.04770\n",
      "Epoch 1303 - lr: 0.01000 - Train loss: 0.98267 - Test loss: 1.04769\n",
      "Epoch 1304 - lr: 0.01000 - Train loss: 0.98269 - Test loss: 1.04768\n",
      "Epoch 1305 - lr: 0.01000 - Train loss: 0.98271 - Test loss: 1.04767\n",
      "Epoch 1306 - lr: 0.01000 - Train loss: 0.98272 - Test loss: 1.04766\n",
      "Epoch 1307 - lr: 0.01000 - Train loss: 0.98274 - Test loss: 1.04764\n",
      "Epoch 1308 - lr: 0.01000 - Train loss: 0.98276 - Test loss: 1.04763\n",
      "Epoch 1309 - lr: 0.01000 - Train loss: 0.98278 - Test loss: 1.04762\n",
      "Epoch 1310 - lr: 0.01000 - Train loss: 0.98280 - Test loss: 1.04761\n",
      "Epoch 1311 - lr: 0.01000 - Train loss: 0.98281 - Test loss: 1.04760\n",
      "Epoch 1312 - lr: 0.01000 - Train loss: 0.98283 - Test loss: 1.04759\n",
      "Epoch 1313 - lr: 0.01000 - Train loss: 0.98285 - Test loss: 1.04758\n",
      "Epoch 1314 - lr: 0.01000 - Train loss: 0.98287 - Test loss: 1.04757\n",
      "Epoch 1315 - lr: 0.01000 - Train loss: 0.98289 - Test loss: 1.04756\n",
      "Epoch 1316 - lr: 0.01000 - Train loss: 0.98291 - Test loss: 1.04755\n",
      "Epoch 1317 - lr: 0.01000 - Train loss: 0.98293 - Test loss: 1.04753\n",
      "Epoch 1318 - lr: 0.01000 - Train loss: 0.98295 - Test loss: 1.04752\n",
      "Epoch 1319 - lr: 0.01000 - Train loss: 0.98296 - Test loss: 1.04751\n",
      "Epoch 1320 - lr: 0.01000 - Train loss: 0.98298 - Test loss: 1.04750\n",
      "Epoch 1321 - lr: 0.01000 - Train loss: 0.98300 - Test loss: 1.04749\n",
      "Epoch 1322 - lr: 0.01000 - Train loss: 0.98302 - Test loss: 1.04748\n",
      "Epoch 1323 - lr: 0.01000 - Train loss: 0.98304 - Test loss: 1.04747\n",
      "Epoch 1324 - lr: 0.01000 - Train loss: 0.98306 - Test loss: 1.04746\n",
      "Epoch 1325 - lr: 0.01000 - Train loss: 0.98308 - Test loss: 1.04745\n",
      "Epoch 1326 - lr: 0.01000 - Train loss: 0.98310 - Test loss: 1.04744\n",
      "Epoch 1327 - lr: 0.01000 - Train loss: 0.98312 - Test loss: 1.04743\n",
      "Epoch 1328 - lr: 0.01000 - Train loss: 0.98314 - Test loss: 1.04743\n",
      "Epoch 1329 - lr: 0.01000 - Train loss: 0.98316 - Test loss: 1.04742\n",
      "Epoch 1330 - lr: 0.01000 - Train loss: 0.98318 - Test loss: 1.04741\n",
      "Epoch 1331 - lr: 0.01000 - Train loss: 0.98320 - Test loss: 1.04740\n",
      "Epoch 1332 - lr: 0.01000 - Train loss: 0.98322 - Test loss: 1.04739\n",
      "Epoch 1333 - lr: 0.01000 - Train loss: 0.98324 - Test loss: 1.04738\n",
      "Epoch 1334 - lr: 0.01000 - Train loss: 0.98326 - Test loss: 1.04737\n",
      "Epoch 1335 - lr: 0.01000 - Train loss: 0.98328 - Test loss: 1.04736\n",
      "Epoch 1336 - lr: 0.01000 - Train loss: 0.98330 - Test loss: 1.04735\n",
      "Epoch 1337 - lr: 0.01000 - Train loss: 0.98332 - Test loss: 1.04735\n",
      "Epoch 1338 - lr: 0.01000 - Train loss: 0.98335 - Test loss: 1.04734\n",
      "Epoch 1339 - lr: 0.01000 - Train loss: 0.98337 - Test loss: 1.04733\n",
      "Epoch 1340 - lr: 0.01000 - Train loss: 0.98339 - Test loss: 1.04732\n",
      "Epoch 1341 - lr: 0.01000 - Train loss: 0.98341 - Test loss: 1.04731\n",
      "Epoch 1342 - lr: 0.01000 - Train loss: 0.98343 - Test loss: 1.04731\n",
      "Epoch 1343 - lr: 0.01000 - Train loss: 0.98345 - Test loss: 1.04730\n",
      "Epoch 1344 - lr: 0.01000 - Train loss: 0.98347 - Test loss: 1.04729\n",
      "Epoch 1345 - lr: 0.01000 - Train loss: 0.98350 - Test loss: 1.04728\n",
      "Epoch 1346 - lr: 0.01000 - Train loss: 0.98352 - Test loss: 1.04728\n",
      "Epoch 1347 - lr: 0.01000 - Train loss: 0.98354 - Test loss: 1.04727\n",
      "Epoch 1348 - lr: 0.01000 - Train loss: 0.98356 - Test loss: 1.04726\n",
      "Epoch 1349 - lr: 0.01000 - Train loss: 0.98358 - Test loss: 1.04726\n",
      "Epoch 1350 - lr: 0.01000 - Train loss: 0.98361 - Test loss: 1.04725\n",
      "Epoch 1351 - lr: 0.01000 - Train loss: 0.98363 - Test loss: 1.04724\n",
      "Epoch 1352 - lr: 0.01000 - Train loss: 0.98365 - Test loss: 1.04724\n",
      "Epoch 1353 - lr: 0.01000 - Train loss: 0.98368 - Test loss: 1.04723\n",
      "Epoch 1354 - lr: 0.01000 - Train loss: 0.98370 - Test loss: 1.04722\n",
      "Epoch 1355 - lr: 0.01000 - Train loss: 0.98372 - Test loss: 1.04722\n",
      "Epoch 1356 - lr: 0.01000 - Train loss: 0.98375 - Test loss: 1.04721\n",
      "Epoch 1357 - lr: 0.01000 - Train loss: 0.98377 - Test loss: 1.04721\n",
      "Epoch 1358 - lr: 0.01000 - Train loss: 0.98379 - Test loss: 1.04720\n",
      "Epoch 1359 - lr: 0.01000 - Train loss: 0.98382 - Test loss: 1.04720\n",
      "Epoch 1360 - lr: 0.01000 - Train loss: 0.98384 - Test loss: 1.04719\n",
      "Epoch 1361 - lr: 0.01000 - Train loss: 0.98386 - Test loss: 1.04719\n",
      "Epoch 1362 - lr: 0.01000 - Train loss: 0.98389 - Test loss: 1.04718\n",
      "Epoch 1363 - lr: 0.01000 - Train loss: 0.98391 - Test loss: 1.04718\n",
      "Epoch 1364 - lr: 0.01000 - Train loss: 0.98394 - Test loss: 1.04717\n",
      "Epoch 1365 - lr: 0.01000 - Train loss: 0.98396 - Test loss: 1.04717\n",
      "Epoch 1366 - lr: 0.01000 - Train loss: 0.98399 - Test loss: 1.04717\n",
      "Epoch 1367 - lr: 0.01000 - Train loss: 0.98401 - Test loss: 1.04716\n",
      "Epoch 1368 - lr: 0.01000 - Train loss: 0.98404 - Test loss: 1.04716\n",
      "Epoch 1369 - lr: 0.01000 - Train loss: 0.98406 - Test loss: 1.04716\n",
      "Epoch 1370 - lr: 0.01000 - Train loss: 0.98409 - Test loss: 1.04715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1371 - lr: 0.01000 - Train loss: 0.98412 - Test loss: 1.04715\n",
      "Epoch 1372 - lr: 0.01000 - Train loss: 0.98414 - Test loss: 1.04715\n",
      "Epoch 1373 - lr: 0.01000 - Train loss: 0.98417 - Test loss: 1.04715\n",
      "Epoch 1374 - lr: 0.01000 - Train loss: 0.98419 - Test loss: 1.04714\n",
      "Epoch 1375 - lr: 0.01000 - Train loss: 0.98422 - Test loss: 1.04714\n",
      "Epoch 1376 - lr: 0.01000 - Train loss: 0.98425 - Test loss: 1.04714\n",
      "Epoch 1377 - lr: 0.01000 - Train loss: 0.98428 - Test loss: 1.04714\n",
      "Epoch 1378 - lr: 0.01000 - Train loss: 0.98430 - Test loss: 1.04714\n",
      "Epoch 1379 - lr: 0.01000 - Train loss: 0.98433 - Test loss: 1.04714\n",
      "Epoch 1380 - lr: 0.01000 - Train loss: 0.98436 - Test loss: 1.04714\n",
      "Epoch 1381 - lr: 0.01000 - Train loss: 0.98439 - Test loss: 1.04714\n",
      "Epoch 1382 - lr: 0.01000 - Train loss: 0.98442 - Test loss: 1.04714\n",
      "Epoch 1383 - lr: 0.01000 - Train loss: 0.98444 - Test loss: 1.04714\n",
      "Epoch 1384 - lr: 0.01000 - Train loss: 0.98447 - Test loss: 1.04714\n",
      "Epoch 1385 - lr: 0.01000 - Train loss: 0.98450 - Test loss: 1.04714\n",
      "Epoch 1386 - lr: 0.01000 - Train loss: 0.98453 - Test loss: 1.04714\n",
      "Epoch 1387 - lr: 0.01000 - Train loss: 0.98456 - Test loss: 1.04714\n",
      "Epoch 1388 - lr: 0.01000 - Train loss: 0.98459 - Test loss: 1.04715\n",
      "Epoch 1389 - lr: 0.01000 - Train loss: 0.98462 - Test loss: 1.04715\n",
      "Epoch 1390 - lr: 0.01000 - Train loss: 0.98465 - Test loss: 1.04715\n",
      "Epoch 1391 - lr: 0.01000 - Train loss: 0.98468 - Test loss: 1.04715\n",
      "Epoch 1392 - lr: 0.01000 - Train loss: 0.98471 - Test loss: 1.04716\n",
      "Epoch 1393 - lr: 0.01000 - Train loss: 0.98474 - Test loss: 1.04716\n",
      "Epoch 1394 - lr: 0.01000 - Train loss: 0.98478 - Test loss: 1.04717\n",
      "Epoch 1395 - lr: 0.01000 - Train loss: 0.98481 - Test loss: 1.04717\n",
      "Epoch 1396 - lr: 0.01000 - Train loss: 0.98484 - Test loss: 1.04717\n",
      "Epoch 1397 - lr: 0.01000 - Train loss: 0.98487 - Test loss: 1.04718\n",
      "Epoch 1398 - lr: 0.01000 - Train loss: 0.98491 - Test loss: 1.04719\n",
      "Epoch 1399 - lr: 0.01000 - Train loss: 0.98494 - Test loss: 1.04719\n",
      "Epoch 1400 - lr: 0.01000 - Train loss: 0.98497 - Test loss: 1.04720\n",
      "Epoch 1401 - lr: 0.01000 - Train loss: 0.98501 - Test loss: 1.04721\n",
      "Epoch 1402 - lr: 0.01000 - Train loss: 0.98504 - Test loss: 1.04722\n",
      "Epoch 1403 - lr: 0.01000 - Train loss: 0.98508 - Test loss: 1.04722\n",
      "Epoch 1404 - lr: 0.01000 - Train loss: 0.98511 - Test loss: 1.04723\n",
      "Epoch 1405 - lr: 0.01000 - Train loss: 0.98515 - Test loss: 1.04724\n",
      "Epoch 1406 - lr: 0.01000 - Train loss: 0.98519 - Test loss: 1.04725\n",
      "Epoch 1407 - lr: 0.01000 - Train loss: 0.98522 - Test loss: 1.04726\n",
      "Epoch 1408 - lr: 0.01000 - Train loss: 0.98526 - Test loss: 1.04728\n",
      "Epoch 1409 - lr: 0.01000 - Train loss: 0.98530 - Test loss: 1.04729\n",
      "Epoch 1410 - lr: 0.01000 - Train loss: 0.98534 - Test loss: 1.04730\n",
      "Epoch 1411 - lr: 0.01000 - Train loss: 0.98538 - Test loss: 1.04731\n",
      "Epoch 1412 - lr: 0.01000 - Train loss: 0.98542 - Test loss: 1.04733\n",
      "Epoch 1413 - lr: 0.01000 - Train loss: 0.98546 - Test loss: 1.04734\n",
      "Epoch 1414 - lr: 0.01000 - Train loss: 0.98550 - Test loss: 1.04736\n",
      "Epoch 1415 - lr: 0.01000 - Train loss: 0.98554 - Test loss: 1.04738\n",
      "Epoch 1416 - lr: 0.01000 - Train loss: 0.98558 - Test loss: 1.04739\n",
      "Epoch 1417 - lr: 0.01000 - Train loss: 0.98563 - Test loss: 1.04741\n",
      "Epoch 1418 - lr: 0.01000 - Train loss: 0.98567 - Test loss: 1.04743\n",
      "Epoch 1419 - lr: 0.01000 - Train loss: 0.98571 - Test loss: 1.04745\n",
      "Epoch 1420 - lr: 0.01000 - Train loss: 0.98576 - Test loss: 1.04748\n",
      "Epoch 1421 - lr: 0.01000 - Train loss: 0.98581 - Test loss: 1.04750\n",
      "Epoch 1422 - lr: 0.01000 - Train loss: 0.98585 - Test loss: 1.04752\n",
      "Epoch 1423 - lr: 0.01000 - Train loss: 0.98590 - Test loss: 1.04755\n",
      "Epoch 1424 - lr: 0.01000 - Train loss: 0.98595 - Test loss: 1.04758\n",
      "Epoch 1425 - lr: 0.01000 - Train loss: 0.98600 - Test loss: 1.04761\n",
      "Epoch 1426 - lr: 0.01000 - Train loss: 0.98605 - Test loss: 1.04764\n",
      "Epoch 1427 - lr: 0.01000 - Train loss: 0.98611 - Test loss: 1.04767\n",
      "Epoch 1428 - lr: 0.01000 - Train loss: 0.98616 - Test loss: 1.04771\n",
      "Epoch 1429 - lr: 0.01000 - Train loss: 0.98621 - Test loss: 1.04774\n",
      "Epoch 1430 - lr: 0.01000 - Train loss: 0.98627 - Test loss: 1.04778\n",
      "Epoch 1431 - lr: 0.01000 - Train loss: 0.98633 - Test loss: 1.04782\n",
      "Epoch 1432 - lr: 0.01000 - Train loss: 0.98639 - Test loss: 1.04787\n",
      "Epoch 1433 - lr: 0.01000 - Train loss: 0.98645 - Test loss: 1.04791\n",
      "Epoch 1434 - lr: 0.01000 - Train loss: 0.98651 - Test loss: 1.04796\n",
      "Epoch 1435 - lr: 0.01000 - Train loss: 0.98658 - Test loss: 1.04802\n",
      "Epoch 1436 - lr: 0.01000 - Train loss: 0.98665 - Test loss: 1.04807\n",
      "Epoch 1437 - lr: 0.01000 - Train loss: 0.98672 - Test loss: 1.04814\n",
      "Epoch 1438 - lr: 0.01000 - Train loss: 0.98679 - Test loss: 1.04820\n",
      "Epoch 1439 - lr: 0.01000 - Train loss: 0.98687 - Test loss: 1.04828\n",
      "Epoch 1440 - lr: 0.01000 - Train loss: 0.98695 - Test loss: 1.04836\n",
      "Epoch 1441 - lr: 0.01000 - Train loss: 0.98703 - Test loss: 1.04844\n",
      "Epoch 1442 - lr: 0.01000 - Train loss: 0.98711 - Test loss: 1.04854\n",
      "Epoch 1443 - lr: 0.01000 - Train loss: 0.98720 - Test loss: 1.04864\n",
      "Epoch 1444 - lr: 0.01000 - Train loss: 0.98729 - Test loss: 1.04876\n",
      "Epoch 1445 - lr: 0.01000 - Train loss: 0.98739 - Test loss: 1.04890\n",
      "Epoch 1446 - lr: 0.01000 - Train loss: 0.98749 - Test loss: 1.04905\n",
      "Epoch 1447 - lr: 0.01000 - Train loss: 0.98760 - Test loss: 1.04922\n",
      "Epoch 1448 - lr: 0.01000 - Train loss: 0.98771 - Test loss: 1.04943\n",
      "Epoch 1449 - lr: 0.01000 - Train loss: 0.98782 - Test loss: 1.04967\n",
      "Epoch 1450 - lr: 0.01000 - Train loss: 0.98791 - Test loss: 1.04996\n",
      "Epoch 1451 - lr: 0.01000 - Train loss: 0.98798 - Test loss: 1.05031\n",
      "Epoch 1452 - lr: 0.01000 - Train loss: 0.98798 - Test loss: 1.05072\n",
      "Epoch 1453 - lr: 0.01000 - Train loss: 0.98778 - Test loss: 1.05111\n",
      "Epoch 1454 - lr: 0.01000 - Train loss: 0.98708 - Test loss: 1.05108\n",
      "Epoch 1455 - lr: 0.01000 - Train loss: 0.98525 - Test loss: 1.04936\n",
      "Epoch 1456 - lr: 0.01000 - Train loss: 0.98199 - Test loss: 1.04554\n",
      "Epoch 1457 - lr: 0.01000 - Train loss: 0.97748 - Test loss: 1.04123\n",
      "Epoch 1458 - lr: 0.01000 - Train loss: 0.96964 - Test loss: 1.03947\n",
      "Epoch 1459 - lr: 0.01000 - Train loss: 0.92803 - Test loss: 1.04686\n",
      "Epoch 1460 - lr: 0.01000 - Train loss: 0.90661 - Test loss: 1.04122\n",
      "Epoch 1461 - lr: 0.01000 - Train loss: 0.90170 - Test loss: 1.03933\n",
      "Epoch 1462 - lr: 0.01000 - Train loss: 0.90725 - Test loss: 1.04148\n",
      "Epoch 1463 - lr: 0.01000 - Train loss: 0.90931 - Test loss: 1.03448\n",
      "Epoch 1464 - lr: 0.01000 - Train loss: 0.90609 - Test loss: 1.03555\n",
      "Epoch 1465 - lr: 0.01000 - Train loss: 0.90475 - Test loss: 1.02479\n",
      "Epoch 1466 - lr: 0.01000 - Train loss: 0.90651 - Test loss: 1.03739\n",
      "Epoch 1467 - lr: 0.01000 - Train loss: 0.93435 - Test loss: 1.03009\n",
      "Epoch 1468 - lr: 0.01000 - Train loss: 0.91718 - Test loss: 1.02000\n",
      "Epoch 1469 - lr: 0.01000 - Train loss: 0.89920 - Test loss: 1.02854\n",
      "Epoch 1470 - lr: 0.01000 - Train loss: 0.98604 - Test loss: 1.02590\n",
      "Epoch 1471 - lr: 0.01000 - Train loss: 0.90877 - Test loss: 1.01590\n",
      "Epoch 1472 - lr: 0.01000 - Train loss: 0.91340 - Test loss: 1.03312\n",
      "Epoch 1473 - lr: 0.01000 - Train loss: 0.96702 - Test loss: 1.01642\n",
      "Epoch 1474 - lr: 0.01000 - Train loss: 0.91858 - Test loss: 1.03186\n",
      "Epoch 1475 - lr: 0.01000 - Train loss: 1.00428 - Test loss: 1.02477\n",
      "Epoch 1476 - lr: 0.01000 - Train loss: 0.91848 - Test loss: 1.00796\n",
      "Epoch 1477 - lr: 0.01000 - Train loss: 0.89251 - Test loss: 1.00427\n",
      "Epoch 1478 - lr: 0.01000 - Train loss: 0.89534 - Test loss: 1.00131\n",
      "Epoch 1479 - lr: 0.01000 - Train loss: 0.89612 - Test loss: 0.99800\n",
      "Epoch 1480 - lr: 0.01000 - Train loss: 0.89298 - Test loss: 0.99476\n",
      "Epoch 1481 - lr: 0.01000 - Train loss: 0.89157 - Test loss: 0.99183\n",
      "Epoch 1482 - lr: 0.01000 - Train loss: 0.89282 - Test loss: 0.98820\n",
      "Epoch 1483 - lr: 0.01000 - Train loss: 0.88718 - Test loss: 0.98885\n",
      "Epoch 1484 - lr: 0.01000 - Train loss: 0.89960 - Test loss: 0.98349\n",
      "Epoch 1485 - lr: 0.01000 - Train loss: 0.88164 - Test loss: 0.98466\n",
      "Epoch 1486 - lr: 0.01000 - Train loss: 0.96176 - Test loss: 1.00911\n",
      "Epoch 1487 - lr: 0.01000 - Train loss: 0.90613 - Test loss: 0.98394\n",
      "Epoch 1488 - lr: 0.01000 - Train loss: 1.02266 - Test loss: 1.01271\n",
      "Epoch 1489 - lr: 0.01000 - Train loss: 0.88275 - Test loss: 0.98068\n",
      "Epoch 1490 - lr: 0.01000 - Train loss: 0.91608 - Test loss: 0.98396\n",
      "Epoch 1491 - lr: 0.01000 - Train loss: 0.93831 - Test loss: 1.01630\n",
      "Epoch 1492 - lr: 0.01000 - Train loss: 0.89160 - Test loss: 0.98912\n",
      "Epoch 1493 - lr: 0.01000 - Train loss: 1.05058 - Test loss: 0.99701\n",
      "Epoch 1494 - lr: 0.01000 - Train loss: 0.93087 - Test loss: 0.97042\n",
      "Epoch 1495 - lr: 0.01000 - Train loss: 0.86965 - Test loss: 0.98376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1496 - lr: 0.01000 - Train loss: 0.91575 - Test loss: 0.97084\n",
      "Epoch 1497 - lr: 0.01000 - Train loss: 0.89136 - Test loss: 0.99249\n",
      "Epoch 1498 - lr: 0.01000 - Train loss: 0.89355 - Test loss: 0.96511\n",
      "Epoch 1499 - lr: 0.01000 - Train loss: 0.93653 - Test loss: 1.01016\n",
      "Epoch 1500 - lr: 0.01000 - Train loss: 0.88850 - Test loss: 0.96284\n",
      "Epoch 1501 - lr: 0.01000 - Train loss: 0.91120 - Test loss: 0.97058\n",
      "Epoch 1502 - lr: 0.01000 - Train loss: 1.04771 - Test loss: 0.95920\n",
      "Epoch 1503 - lr: 0.01000 - Train loss: 0.87305 - Test loss: 0.95994\n",
      "Epoch 1504 - lr: 0.01000 - Train loss: 0.88772 - Test loss: 0.99481\n",
      "Epoch 1505 - lr: 0.01000 - Train loss: 0.87813 - Test loss: 0.95721\n",
      "Epoch 1506 - lr: 0.01000 - Train loss: 0.90413 - Test loss: 0.95873\n",
      "Epoch 1507 - lr: 0.01000 - Train loss: 0.94223 - Test loss: 1.01306\n",
      "Epoch 1508 - lr: 0.01000 - Train loss: 0.88696 - Test loss: 0.95430\n",
      "Epoch 1509 - lr: 0.01000 - Train loss: 0.92019 - Test loss: 0.94756\n",
      "Epoch 1510 - lr: 0.01000 - Train loss: 0.86619 - Test loss: 0.96358\n",
      "Epoch 1511 - lr: 0.01000 - Train loss: 1.05508 - Test loss: 0.94683\n",
      "Epoch 1512 - lr: 0.01000 - Train loss: 0.88145 - Test loss: 0.95186\n",
      "Epoch 1513 - lr: 0.01000 - Train loss: 0.89936 - Test loss: 1.01449\n",
      "Epoch 1514 - lr: 0.01000 - Train loss: 0.88667 - Test loss: 0.95076\n",
      "Epoch 1515 - lr: 0.01000 - Train loss: 0.91140 - Test loss: 0.95025\n",
      "Epoch 1516 - lr: 0.01000 - Train loss: 0.90175 - Test loss: 0.98362\n",
      "Epoch 1517 - lr: 0.01000 - Train loss: 0.87644 - Test loss: 1.01898\n",
      "Epoch 1518 - lr: 0.01000 - Train loss: 0.87731 - Test loss: 0.99527\n",
      "Epoch 1519 - lr: 0.01000 - Train loss: 0.87464 - Test loss: 1.02890\n",
      "Epoch 1520 - lr: 0.01000 - Train loss: 0.88417 - Test loss: 1.02732\n",
      "Epoch 1521 - lr: 0.01000 - Train loss: 0.88254 - Test loss: 1.03121\n",
      "Epoch 1522 - lr: 0.01000 - Train loss: 0.88319 - Test loss: 1.03026\n",
      "Epoch 1523 - lr: 0.01000 - Train loss: 0.88233 - Test loss: 1.03299\n",
      "Epoch 1524 - lr: 0.01000 - Train loss: 0.88303 - Test loss: 1.03614\n",
      "Epoch 1525 - lr: 0.01000 - Train loss: 0.88434 - Test loss: 1.04545\n",
      "Epoch 1526 - lr: 0.01000 - Train loss: 0.89386 - Test loss: 0.93476\n",
      "Epoch 1527 - lr: 0.01000 - Train loss: 0.87300 - Test loss: 0.95469\n",
      "Epoch 1528 - lr: 0.01000 - Train loss: 0.87559 - Test loss: 0.92376\n",
      "Epoch 1529 - lr: 0.01000 - Train loss: 0.88995 - Test loss: 1.03479\n",
      "Epoch 1530 - lr: 0.01000 - Train loss: 0.89380 - Test loss: 1.04937\n",
      "Epoch 1531 - lr: 0.01000 - Train loss: 0.90857 - Test loss: 0.92333\n",
      "Epoch 1532 - lr: 0.01000 - Train loss: 0.87177 - Test loss: 0.93365\n",
      "Epoch 1533 - lr: 0.01000 - Train loss: 0.93807 - Test loss: 0.91398\n",
      "Epoch 1534 - lr: 0.01000 - Train loss: 0.89464 - Test loss: 1.01729\n",
      "Epoch 1535 - lr: 0.01000 - Train loss: 0.89710 - Test loss: 0.92006\n",
      "Epoch 1536 - lr: 0.01000 - Train loss: 0.96172 - Test loss: 0.91259\n",
      "Epoch 1537 - lr: 0.01000 - Train loss: 1.14031 - Test loss: 0.91779\n",
      "Epoch 1538 - lr: 0.01000 - Train loss: 0.96254 - Test loss: 0.96964\n",
      "Epoch 1539 - lr: 0.01000 - Train loss: 0.87641 - Test loss: 0.92854\n",
      "Epoch 1540 - lr: 0.01000 - Train loss: 1.06291 - Test loss: 0.97963\n",
      "Epoch 1541 - lr: 0.01000 - Train loss: 0.88583 - Test loss: 1.02291\n",
      "Epoch 1542 - lr: 0.01000 - Train loss: 0.89722 - Test loss: 1.03548\n",
      "Epoch 1543 - lr: 0.01000 - Train loss: 0.89437 - Test loss: 0.98717\n",
      "Epoch 1544 - lr: 0.01000 - Train loss: 0.88252 - Test loss: 1.04062\n",
      "Epoch 1545 - lr: 0.01000 - Train loss: 0.90826 - Test loss: 1.02721\n",
      "Epoch 1546 - lr: 0.01000 - Train loss: 0.90487 - Test loss: 1.03891\n",
      "Epoch 1547 - lr: 0.01000 - Train loss: 0.90916 - Test loss: 1.03413\n",
      "Epoch 1548 - lr: 0.01000 - Train loss: 0.88926 - Test loss: 0.92608\n",
      "Epoch 1549 - lr: 0.01000 - Train loss: 0.90146 - Test loss: 1.02804\n",
      "Epoch 1550 - lr: 0.01000 - Train loss: 0.91164 - Test loss: 0.91827\n",
      "Epoch 1551 - lr: 0.01000 - Train loss: 0.91337 - Test loss: 0.99797\n",
      "Epoch 1552 - lr: 0.01000 - Train loss: 0.90515 - Test loss: 1.02282\n",
      "Epoch 1553 - lr: 0.01000 - Train loss: 0.89102 - Test loss: 0.93800\n",
      "Epoch 1554 - lr: 0.01000 - Train loss: 0.89542 - Test loss: 1.01776\n",
      "Epoch 1555 - lr: 0.01000 - Train loss: 0.90725 - Test loss: 1.04101\n",
      "Epoch 1556 - lr: 0.01000 - Train loss: 1.17771 - Test loss: 0.90264\n",
      "Epoch 1557 - lr: 0.01000 - Train loss: 1.20902 - Test loss: 0.92075\n",
      "Epoch 1558 - lr: 0.01000 - Train loss: 1.17019 - Test loss: 0.93439\n",
      "Epoch 1559 - lr: 0.01000 - Train loss: 0.95848 - Test loss: 0.93205\n",
      "Epoch 1560 - lr: 0.01000 - Train loss: 0.90769 - Test loss: 0.97737\n",
      "Epoch 1561 - lr: 0.01000 - Train loss: 0.88230 - Test loss: 1.01533\n",
      "Epoch 1562 - lr: 0.01000 - Train loss: 0.89925 - Test loss: 0.93620\n",
      "Epoch 1563 - lr: 0.01000 - Train loss: 0.89781 - Test loss: 1.01316\n",
      "Epoch 1564 - lr: 0.01000 - Train loss: 0.90193 - Test loss: 0.93168\n",
      "Epoch 1565 - lr: 0.01000 - Train loss: 0.89294 - Test loss: 0.92885\n",
      "Epoch 1566 - lr: 0.01000 - Train loss: 0.90191 - Test loss: 0.92495\n",
      "Epoch 1567 - lr: 0.01000 - Train loss: 0.89162 - Test loss: 0.94140\n",
      "Epoch 1568 - lr: 0.01000 - Train loss: 0.86760 - Test loss: 0.96701\n",
      "Epoch 1569 - lr: 0.01000 - Train loss: 0.87574 - Test loss: 0.99293\n",
      "Epoch 1570 - lr: 0.01000 - Train loss: 0.88446 - Test loss: 0.92155\n",
      "Epoch 1571 - lr: 0.01000 - Train loss: 0.90196 - Test loss: 0.98469\n",
      "Epoch 1572 - lr: 0.01000 - Train loss: 0.88039 - Test loss: 0.91744\n",
      "Epoch 1573 - lr: 0.01000 - Train loss: 0.89003 - Test loss: 0.99741\n",
      "Epoch 1574 - lr: 0.01000 - Train loss: 0.90546 - Test loss: 0.91357\n",
      "Epoch 1575 - lr: 0.01000 - Train loss: 0.86110 - Test loss: 0.91046\n",
      "Epoch 1576 - lr: 0.01000 - Train loss: 0.85204 - Test loss: 0.91000\n",
      "Epoch 1577 - lr: 0.01000 - Train loss: 0.93228 - Test loss: 0.92131\n",
      "Epoch 1578 - lr: 0.01000 - Train loss: 0.86855 - Test loss: 0.99201\n",
      "Epoch 1579 - lr: 0.01000 - Train loss: 0.91495 - Test loss: 0.99672\n",
      "Epoch 1580 - lr: 0.01000 - Train loss: 0.89774 - Test loss: 0.97399\n",
      "Epoch 1581 - lr: 0.01000 - Train loss: 0.87610 - Test loss: 1.01442\n",
      "Epoch 1582 - lr: 0.01000 - Train loss: 0.95316 - Test loss: 0.89802\n",
      "Epoch 1583 - lr: 0.01000 - Train loss: 0.86081 - Test loss: 0.92884\n",
      "Epoch 1584 - lr: 0.01000 - Train loss: 0.86034 - Test loss: 0.90017\n",
      "Epoch 1585 - lr: 0.01000 - Train loss: 0.86040 - Test loss: 0.95579\n",
      "Epoch 1586 - lr: 0.01000 - Train loss: 0.87399 - Test loss: 0.89983\n",
      "Epoch 1587 - lr: 0.01000 - Train loss: 0.87843 - Test loss: 0.99636\n",
      "Epoch 1588 - lr: 0.01000 - Train loss: 0.91413 - Test loss: 0.96164\n",
      "Epoch 1589 - lr: 0.01000 - Train loss: 0.86082 - Test loss: 1.06846\n",
      "Epoch 1590 - lr: 0.01000 - Train loss: 1.22522 - Test loss: 0.89383\n",
      "Epoch 1591 - lr: 0.01000 - Train loss: 1.12624 - Test loss: 0.90863\n",
      "Epoch 1592 - lr: 0.01000 - Train loss: 0.91546 - Test loss: 0.99861\n",
      "Epoch 1593 - lr: 0.01000 - Train loss: 0.91745 - Test loss: 0.90798\n",
      "Epoch 1594 - lr: 0.01000 - Train loss: 0.86067 - Test loss: 0.91396\n",
      "Epoch 1595 - lr: 0.01000 - Train loss: 0.90658 - Test loss: 0.90468\n",
      "Epoch 1596 - lr: 0.01000 - Train loss: 0.85935 - Test loss: 0.91745\n",
      "Epoch 1597 - lr: 0.01000 - Train loss: 0.86528 - Test loss: 0.98757\n",
      "Epoch 1598 - lr: 0.01000 - Train loss: 0.91232 - Test loss: 1.03580\n",
      "Epoch 1599 - lr: 0.01000 - Train loss: 1.23939 - Test loss: 0.90547\n",
      "Epoch 1600 - lr: 0.01000 - Train loss: 0.89815 - Test loss: 1.00107\n",
      "Epoch 1601 - lr: 0.01000 - Train loss: 0.93303 - Test loss: 0.94393\n",
      "Epoch 1602 - lr: 0.01000 - Train loss: 0.87233 - Test loss: 0.93296\n",
      "Epoch 1603 - lr: 0.01000 - Train loss: 0.85695 - Test loss: 0.90915\n",
      "Epoch 1604 - lr: 0.01000 - Train loss: 0.89714 - Test loss: 0.90479\n",
      "Epoch 1605 - lr: 0.01000 - Train loss: 0.85938 - Test loss: 0.96183\n",
      "Epoch 1606 - lr: 0.01000 - Train loss: 0.85602 - Test loss: 0.91007\n",
      "Epoch 1607 - lr: 0.01000 - Train loss: 0.87342 - Test loss: 0.98291\n",
      "Epoch 1608 - lr: 0.01000 - Train loss: 0.92665 - Test loss: 0.96504\n",
      "Epoch 1609 - lr: 0.01000 - Train loss: 0.88444 - Test loss: 0.96892\n",
      "Epoch 1610 - lr: 0.01000 - Train loss: 0.87514 - Test loss: 0.90292\n",
      "Epoch 1611 - lr: 0.01000 - Train loss: 0.89566 - Test loss: 0.89375\n",
      "Epoch 1612 - lr: 0.01000 - Train loss: 0.90405 - Test loss: 0.96433\n",
      "Epoch 1613 - lr: 0.01000 - Train loss: 0.91192 - Test loss: 0.89605\n",
      "Epoch 1614 - lr: 0.01000 - Train loss: 0.88884 - Test loss: 0.93607\n",
      "Epoch 1615 - lr: 0.01000 - Train loss: 0.84913 - Test loss: 0.89823\n",
      "Epoch 1616 - lr: 0.01000 - Train loss: 0.83583 - Test loss: 0.89508\n",
      "Epoch 1617 - lr: 0.01000 - Train loss: 0.89912 - Test loss: 0.93413\n",
      "Epoch 1618 - lr: 0.01000 - Train loss: 0.83959 - Test loss: 0.90329\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1619 - lr: 0.01000 - Train loss: 0.91728 - Test loss: 0.98052\n",
      "Epoch 1620 - lr: 0.01000 - Train loss: 1.26693 - Test loss: 0.88424\n",
      "Epoch 1621 - lr: 0.01000 - Train loss: 0.88108 - Test loss: 0.89233\n",
      "Epoch 1622 - lr: 0.01000 - Train loss: 0.89522 - Test loss: 0.94147\n",
      "Epoch 1623 - lr: 0.01000 - Train loss: 0.84576 - Test loss: 0.90559\n",
      "Epoch 1624 - lr: 0.01000 - Train loss: 0.88472 - Test loss: 0.93088\n",
      "Epoch 1625 - lr: 0.01000 - Train loss: 0.85588 - Test loss: 0.89573\n",
      "Epoch 1626 - lr: 0.01000 - Train loss: 0.84619 - Test loss: 0.89097\n",
      "Epoch 1627 - lr: 0.01000 - Train loss: 0.90604 - Test loss: 1.03010\n",
      "Epoch 1628 - lr: 0.01000 - Train loss: 1.27054 - Test loss: 0.88976\n",
      "Epoch 1629 - lr: 0.01000 - Train loss: 0.89019 - Test loss: 0.88814\n",
      "Epoch 1630 - lr: 0.01000 - Train loss: 0.89567 - Test loss: 0.89278\n",
      "Epoch 1631 - lr: 0.01000 - Train loss: 0.90276 - Test loss: 0.89340\n",
      "Epoch 1632 - lr: 0.01000 - Train loss: 0.90320 - Test loss: 0.89285\n",
      "Epoch 1633 - lr: 0.01000 - Train loss: 0.90064 - Test loss: 0.89224\n",
      "Epoch 1634 - lr: 0.01000 - Train loss: 0.89972 - Test loss: 0.89152\n",
      "Epoch 1635 - lr: 0.01000 - Train loss: 0.89381 - Test loss: 0.89067\n",
      "Epoch 1636 - lr: 0.01000 - Train loss: 0.90669 - Test loss: 0.95851\n",
      "Epoch 1637 - lr: 0.01000 - Train loss: 0.91691 - Test loss: 0.88987\n",
      "Epoch 1638 - lr: 0.01000 - Train loss: 0.89286 - Test loss: 1.01444\n",
      "Epoch 1639 - lr: 0.01000 - Train loss: 0.99879 - Test loss: 0.89192\n",
      "Epoch 1640 - lr: 0.01000 - Train loss: 0.86771 - Test loss: 0.90768\n",
      "Epoch 1641 - lr: 0.01000 - Train loss: 0.90015 - Test loss: 0.92665\n",
      "Epoch 1642 - lr: 0.01000 - Train loss: 0.86225 - Test loss: 1.00093\n",
      "Epoch 1643 - lr: 0.01000 - Train loss: 1.14443 - Test loss: 0.88159\n",
      "Epoch 1644 - lr: 0.01000 - Train loss: 0.90310 - Test loss: 0.92044\n",
      "Epoch 1645 - lr: 0.01000 - Train loss: 0.88295 - Test loss: 0.94313\n",
      "Epoch 1646 - lr: 0.01000 - Train loss: 0.88846 - Test loss: 0.89942\n",
      "Epoch 1647 - lr: 0.01000 - Train loss: 0.90561 - Test loss: 0.89337\n",
      "Epoch 1648 - lr: 0.01000 - Train loss: 0.89735 - Test loss: 0.89191\n",
      "Epoch 1649 - lr: 0.01000 - Train loss: 0.88510 - Test loss: 0.92119\n",
      "Epoch 1650 - lr: 0.01000 - Train loss: 0.84258 - Test loss: 0.89472\n",
      "Epoch 1651 - lr: 0.01000 - Train loss: 0.88829 - Test loss: 0.90411\n",
      "Epoch 1652 - lr: 0.01000 - Train loss: 0.89356 - Test loss: 0.96603\n",
      "Epoch 1653 - lr: 0.01000 - Train loss: 1.27970 - Test loss: 0.88316\n",
      "Epoch 1654 - lr: 0.01000 - Train loss: 0.90557 - Test loss: 0.88854\n",
      "Epoch 1655 - lr: 0.01000 - Train loss: 0.92010 - Test loss: 0.89337\n",
      "Epoch 1656 - lr: 0.01000 - Train loss: 0.84446 - Test loss: 0.89566\n",
      "Epoch 1657 - lr: 0.01000 - Train loss: 0.89954 - Test loss: 0.89331\n",
      "Epoch 1658 - lr: 0.01000 - Train loss: 0.92127 - Test loss: 0.89241\n",
      "Epoch 1659 - lr: 0.01000 - Train loss: 0.85861 - Test loss: 0.89183\n",
      "Epoch 1660 - lr: 0.01000 - Train loss: 0.88754 - Test loss: 0.91920\n",
      "Epoch 1661 - lr: 0.01000 - Train loss: 0.87031 - Test loss: 0.92921\n",
      "Epoch 1662 - lr: 0.01000 - Train loss: 0.87544 - Test loss: 0.93126\n",
      "Epoch 1663 - lr: 0.01000 - Train loss: 0.85047 - Test loss: 0.92988\n",
      "Epoch 1664 - lr: 0.01000 - Train loss: 0.86764 - Test loss: 0.99926\n",
      "Epoch 1665 - lr: 0.01000 - Train loss: 1.36687 - Test loss: 0.88268\n",
      "Epoch 1666 - lr: 0.01000 - Train loss: 0.94611 - Test loss: 0.90921\n",
      "Epoch 1667 - lr: 0.01000 - Train loss: 0.90784 - Test loss: 0.90928\n",
      "Epoch 1668 - lr: 0.01000 - Train loss: 0.86885 - Test loss: 0.90452\n",
      "Epoch 1669 - lr: 0.01000 - Train loss: 0.85756 - Test loss: 0.92677\n",
      "Epoch 1670 - lr: 0.01000 - Train loss: 0.87746 - Test loss: 0.91671\n",
      "Epoch 1671 - lr: 0.01000 - Train loss: 0.84902 - Test loss: 0.89454\n",
      "Epoch 1672 - lr: 0.01000 - Train loss: 0.85580 - Test loss: 0.88783\n",
      "Epoch 1673 - lr: 0.01000 - Train loss: 0.90755 - Test loss: 0.97280\n",
      "Epoch 1674 - lr: 0.01000 - Train loss: 1.02135 - Test loss: 0.91340\n",
      "Epoch 1675 - lr: 0.01000 - Train loss: 0.88469 - Test loss: 0.90059\n",
      "Epoch 1676 - lr: 0.01000 - Train loss: 0.84885 - Test loss: 0.94971\n",
      "Epoch 1677 - lr: 0.01000 - Train loss: 0.93855 - Test loss: 0.98126\n",
      "Epoch 1678 - lr: 0.01000 - Train loss: 1.01046 - Test loss: 0.87848\n",
      "Epoch 1679 - lr: 0.01000 - Train loss: 0.88415 - Test loss: 0.88648\n",
      "Epoch 1680 - lr: 0.01000 - Train loss: 0.86837 - Test loss: 0.88614\n",
      "Epoch 1681 - lr: 0.01000 - Train loss: 0.88893 - Test loss: 0.89177\n",
      "Epoch 1682 - lr: 0.01000 - Train loss: 0.85553 - Test loss: 0.96134\n",
      "Epoch 1683 - lr: 0.01000 - Train loss: 0.94723 - Test loss: 0.89888\n",
      "Epoch 1684 - lr: 0.01000 - Train loss: 0.95077 - Test loss: 0.97602\n",
      "Epoch 1685 - lr: 0.01000 - Train loss: 1.32422 - Test loss: 0.89630\n",
      "Epoch 1686 - lr: 0.01000 - Train loss: 0.87734 - Test loss: 0.95381\n",
      "Epoch 1687 - lr: 0.01000 - Train loss: 0.96004 - Test loss: 0.93833\n",
      "Epoch 1688 - lr: 0.01000 - Train loss: 0.92610 - Test loss: 0.93832\n",
      "Epoch 1689 - lr: 0.01000 - Train loss: 0.91178 - Test loss: 0.90745\n",
      "Epoch 1690 - lr: 0.01000 - Train loss: 0.89810 - Test loss: 0.95901\n",
      "Epoch 1691 - lr: 0.01000 - Train loss: 1.04579 - Test loss: 0.89307\n",
      "Epoch 1692 - lr: 0.01000 - Train loss: 0.86906 - Test loss: 0.89589\n",
      "Epoch 1693 - lr: 0.01000 - Train loss: 0.88663 - Test loss: 0.93620\n",
      "Epoch 1694 - lr: 0.01000 - Train loss: 0.94060 - Test loss: 0.90134\n",
      "Epoch 1695 - lr: 0.01000 - Train loss: 0.90049 - Test loss: 0.92234\n",
      "Epoch 1696 - lr: 0.01000 - Train loss: 0.87766 - Test loss: 0.92579\n",
      "Epoch 1697 - lr: 0.01000 - Train loss: 0.87643 - Test loss: 0.92559\n",
      "Epoch 1698 - lr: 0.01000 - Train loss: 0.87667 - Test loss: 0.92385\n",
      "Epoch 1699 - lr: 0.01000 - Train loss: 0.87702 - Test loss: 0.91976\n",
      "Epoch 1700 - lr: 0.01000 - Train loss: 0.87359 - Test loss: 0.92187\n",
      "Epoch 1701 - lr: 0.01000 - Train loss: 0.87795 - Test loss: 0.91872\n",
      "Epoch 1702 - lr: 0.01000 - Train loss: 0.87469 - Test loss: 0.92594\n",
      "Epoch 1703 - lr: 0.01000 - Train loss: 0.90199 - Test loss: 0.97889\n",
      "Epoch 1704 - lr: 0.01000 - Train loss: 1.35062 - Test loss: 0.89504\n",
      "Epoch 1705 - lr: 0.01000 - Train loss: 0.97737 - Test loss: 0.94508\n",
      "Epoch 1706 - lr: 0.01000 - Train loss: 1.00784 - Test loss: 0.91064\n",
      "Epoch 1707 - lr: 0.01000 - Train loss: 0.86807 - Test loss: 0.95059\n",
      "Epoch 1708 - lr: 0.01000 - Train loss: 0.96191 - Test loss: 0.95013\n",
      "Epoch 1709 - lr: 0.01000 - Train loss: 0.99360 - Test loss: 0.89520\n",
      "Epoch 1710 - lr: 0.01000 - Train loss: 0.95200 - Test loss: 0.89970\n",
      "Epoch 1711 - lr: 0.01000 - Train loss: 0.93321 - Test loss: 0.92938\n",
      "Epoch 1712 - lr: 0.01000 - Train loss: 0.92420 - Test loss: 0.91508\n",
      "Epoch 1713 - lr: 0.01000 - Train loss: 0.89007 - Test loss: 0.90148\n",
      "Epoch 1714 - lr: 0.01000 - Train loss: 0.84828 - Test loss: 0.89958\n",
      "Epoch 1715 - lr: 0.01000 - Train loss: 0.90622 - Test loss: 0.92437\n",
      "Epoch 1716 - lr: 0.01000 - Train loss: 0.88282 - Test loss: 0.89863\n",
      "Epoch 1717 - lr: 0.01000 - Train loss: 0.90264 - Test loss: 0.89624\n",
      "Epoch 1718 - lr: 0.01000 - Train loss: 0.86670 - Test loss: 0.89534\n",
      "Epoch 1719 - lr: 0.01000 - Train loss: 0.91997 - Test loss: 0.89248\n",
      "Epoch 1720 - lr: 0.01000 - Train loss: 0.89456 - Test loss: 0.95437\n",
      "Epoch 1721 - lr: 0.01000 - Train loss: 0.98322 - Test loss: 0.93983\n",
      "Epoch 1722 - lr: 0.01000 - Train loss: 1.29802 - Test loss: 0.89016\n",
      "Epoch 1723 - lr: 0.01000 - Train loss: 0.86700 - Test loss: 0.91223\n",
      "Epoch 1724 - lr: 0.01000 - Train loss: 0.88227 - Test loss: 0.91006\n",
      "Epoch 1725 - lr: 0.01000 - Train loss: 0.96866 - Test loss: 0.92003\n",
      "Epoch 1726 - lr: 0.01000 - Train loss: 0.87895 - Test loss: 0.92275\n",
      "Epoch 1727 - lr: 0.01000 - Train loss: 0.87703 - Test loss: 0.92467\n",
      "Epoch 1728 - lr: 0.01000 - Train loss: 0.85842 - Test loss: 0.92335\n",
      "Epoch 1729 - lr: 0.01000 - Train loss: 0.86673 - Test loss: 0.96243\n",
      "Epoch 1730 - lr: 0.01000 - Train loss: 0.99942 - Test loss: 0.90733\n",
      "Epoch 1731 - lr: 0.01000 - Train loss: 0.88414 - Test loss: 0.90316\n",
      "Epoch 1732 - lr: 0.01000 - Train loss: 0.96398 - Test loss: 0.93106\n",
      "Epoch 1733 - lr: 0.01000 - Train loss: 0.93149 - Test loss: 0.89223\n",
      "Epoch 1734 - lr: 0.01000 - Train loss: 0.90510 - Test loss: 0.91449\n",
      "Epoch 1735 - lr: 0.01000 - Train loss: 0.87494 - Test loss: 0.94319\n",
      "Epoch 1736 - lr: 0.01000 - Train loss: 1.08209 - Test loss: 0.88430\n",
      "Epoch 1737 - lr: 0.01000 - Train loss: 0.96039 - Test loss: 0.88942\n",
      "Epoch 1738 - lr: 0.01000 - Train loss: 0.85235 - Test loss: 0.94138\n",
      "Epoch 1739 - lr: 0.01000 - Train loss: 1.05319 - Test loss: 0.91170\n",
      "Epoch 1740 - lr: 0.01000 - Train loss: 0.87349 - Test loss: 0.89955\n",
      "Epoch 1741 - lr: 0.01000 - Train loss: 0.90317 - Test loss: 0.89632\n",
      "Epoch 1742 - lr: 0.01000 - Train loss: 0.83299 - Test loss: 0.89427\n",
      "Epoch 1743 - lr: 0.01000 - Train loss: 0.85327 - Test loss: 0.89199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1744 - lr: 0.01000 - Train loss: 0.85513 - Test loss: 0.89030\n",
      "Epoch 1745 - lr: 0.01000 - Train loss: 0.85409 - Test loss: 0.88902\n",
      "Epoch 1746 - lr: 0.01000 - Train loss: 0.85648 - Test loss: 0.88764\n",
      "Epoch 1747 - lr: 0.01000 - Train loss: 0.85504 - Test loss: 0.88664\n",
      "Epoch 1748 - lr: 0.01000 - Train loss: 0.86025 - Test loss: 0.88495\n",
      "Epoch 1749 - lr: 0.01000 - Train loss: 0.85354 - Test loss: 0.88463\n",
      "Epoch 1750 - lr: 0.01000 - Train loss: 0.92792 - Test loss: 0.93242\n",
      "Epoch 1751 - lr: 0.01000 - Train loss: 1.02988 - Test loss: 0.87587\n",
      "Epoch 1752 - lr: 0.01000 - Train loss: 0.90670 - Test loss: 0.93805\n",
      "Epoch 1753 - lr: 0.01000 - Train loss: 0.98516 - Test loss: 0.87868\n",
      "Epoch 1754 - lr: 0.01000 - Train loss: 0.87547 - Test loss: 0.88649\n",
      "Epoch 1755 - lr: 0.01000 - Train loss: 0.85799 - Test loss: 0.88612\n",
      "Epoch 1756 - lr: 0.01000 - Train loss: 0.85559 - Test loss: 0.88996\n",
      "Epoch 1757 - lr: 0.01000 - Train loss: 0.95269 - Test loss: 0.94268\n",
      "Epoch 1758 - lr: 0.01000 - Train loss: 1.00392 - Test loss: 0.88081\n",
      "Epoch 1759 - lr: 0.01000 - Train loss: 0.86560 - Test loss: 0.88536\n",
      "Epoch 1760 - lr: 0.01000 - Train loss: 0.83399 - Test loss: 0.88701\n",
      "Epoch 1761 - lr: 0.01000 - Train loss: 0.90234 - Test loss: 0.92642\n",
      "Epoch 1762 - lr: 0.01000 - Train loss: 0.95625 - Test loss: 0.93904\n",
      "Epoch 1763 - lr: 0.01000 - Train loss: 1.04438 - Test loss: 0.89838\n",
      "Epoch 1764 - lr: 0.01000 - Train loss: 0.89734 - Test loss: 0.89147\n",
      "Epoch 1765 - lr: 0.01000 - Train loss: 0.86876 - Test loss: 0.88912\n",
      "Epoch 1766 - lr: 0.01000 - Train loss: 0.92599 - Test loss: 0.93198\n",
      "Epoch 1767 - lr: 0.01000 - Train loss: 1.03688 - Test loss: 0.87968\n",
      "Epoch 1768 - lr: 0.01000 - Train loss: 0.86165 - Test loss: 0.88494\n",
      "Epoch 1769 - lr: 0.01000 - Train loss: 0.91229 - Test loss: 0.89951\n",
      "Epoch 1770 - lr: 0.01000 - Train loss: 0.86557 - Test loss: 0.93914\n",
      "Epoch 1771 - lr: 0.01000 - Train loss: 1.12386 - Test loss: 0.87854\n",
      "Epoch 1772 - lr: 0.01000 - Train loss: 0.83921 - Test loss: 0.90732\n",
      "Epoch 1773 - lr: 0.01000 - Train loss: 0.87466 - Test loss: 0.89542\n",
      "Epoch 1774 - lr: 0.01000 - Train loss: 0.91185 - Test loss: 0.89082\n",
      "Epoch 1775 - lr: 0.01000 - Train loss: 0.90374 - Test loss: 0.88841\n",
      "Epoch 1776 - lr: 0.01000 - Train loss: 0.90666 - Test loss: 0.88723\n",
      "Epoch 1777 - lr: 0.01000 - Train loss: 0.90500 - Test loss: 0.88627\n",
      "Epoch 1778 - lr: 0.01000 - Train loss: 0.90256 - Test loss: 0.88556\n",
      "Epoch 1779 - lr: 0.01000 - Train loss: 0.90408 - Test loss: 0.88495\n",
      "Epoch 1780 - lr: 0.01000 - Train loss: 0.90520 - Test loss: 0.88473\n",
      "Epoch 1781 - lr: 0.01000 - Train loss: 0.91168 - Test loss: 0.88475\n",
      "Epoch 1782 - lr: 0.01000 - Train loss: 0.91233 - Test loss: 0.88214\n",
      "Epoch 1783 - lr: 0.01000 - Train loss: 0.90109 - Test loss: 0.88295\n",
      "Epoch 1784 - lr: 0.01000 - Train loss: 0.91324 - Test loss: 0.88101\n",
      "Epoch 1785 - lr: 0.01000 - Train loss: 0.90094 - Test loss: 0.88116\n",
      "Epoch 1786 - lr: 0.01000 - Train loss: 0.90762 - Test loss: 0.88223\n",
      "Epoch 1787 - lr: 0.01000 - Train loss: 0.91477 - Test loss: 0.88079\n",
      "Epoch 1788 - lr: 0.01000 - Train loss: 0.90264 - Test loss: 0.87884\n",
      "Epoch 1789 - lr: 0.01000 - Train loss: 0.88920 - Test loss: 0.87731\n",
      "Epoch 1790 - lr: 0.01000 - Train loss: 0.87471 - Test loss: 0.87754\n",
      "Epoch 1791 - lr: 0.01000 - Train loss: 0.91264 - Test loss: 0.87666\n",
      "Epoch 1792 - lr: 0.01000 - Train loss: 0.85957 - Test loss: 0.87713\n",
      "Epoch 1793 - lr: 0.01000 - Train loss: 0.90632 - Test loss: 0.87786\n",
      "Epoch 1794 - lr: 0.01000 - Train loss: 0.87510 - Test loss: 0.89790\n",
      "Epoch 1795 - lr: 0.01000 - Train loss: 0.94942 - Test loss: 0.92359\n",
      "Epoch 1796 - lr: 0.01000 - Train loss: 1.16386 - Test loss: 0.86453\n",
      "Epoch 1797 - lr: 0.01000 - Train loss: 0.97011 - Test loss: 0.87432\n",
      "Epoch 1798 - lr: 0.01000 - Train loss: 0.88620 - Test loss: 0.88623\n",
      "Epoch 1799 - lr: 0.01000 - Train loss: 0.92417 - Test loss: 0.88748\n",
      "Epoch 1800 - lr: 0.01000 - Train loss: 0.84893 - Test loss: 0.89011\n",
      "Epoch 1801 - lr: 0.01000 - Train loss: 0.85513 - Test loss: 0.88685\n",
      "Epoch 1802 - lr: 0.01000 - Train loss: 0.89631 - Test loss: 0.88019\n",
      "Epoch 1803 - lr: 0.01000 - Train loss: 0.89273 - Test loss: 0.87490\n",
      "Epoch 1804 - lr: 0.01000 - Train loss: 0.89368 - Test loss: 0.87392\n",
      "Epoch 1805 - lr: 0.01000 - Train loss: 0.87878 - Test loss: 0.87633\n",
      "Epoch 1806 - lr: 0.01000 - Train loss: 0.87631 - Test loss: 0.90405\n",
      "Epoch 1807 - lr: 0.01000 - Train loss: 0.95201 - Test loss: 0.87494\n",
      "Epoch 1808 - lr: 0.01000 - Train loss: 0.87647 - Test loss: 0.89410\n",
      "Epoch 1809 - lr: 0.01000 - Train loss: 0.96375 - Test loss: 0.88054\n",
      "Epoch 1810 - lr: 0.01000 - Train loss: 0.85236 - Test loss: 0.87774\n",
      "Epoch 1811 - lr: 0.01000 - Train loss: 0.89131 - Test loss: 0.87383\n",
      "Epoch 1812 - lr: 0.01000 - Train loss: 0.85051 - Test loss: 0.91292\n",
      "Epoch 1813 - lr: 0.01000 - Train loss: 1.10722 - Test loss: 0.86652\n",
      "Epoch 1814 - lr: 0.01000 - Train loss: 0.89952 - Test loss: 0.87309\n",
      "Epoch 1815 - lr: 0.01000 - Train loss: 0.89408 - Test loss: 0.90148\n",
      "Epoch 1816 - lr: 0.01000 - Train loss: 0.97346 - Test loss: 0.87911\n",
      "Epoch 1817 - lr: 0.01000 - Train loss: 0.88515 - Test loss: 0.87798\n",
      "Epoch 1818 - lr: 0.01000 - Train loss: 0.89357 - Test loss: 0.87589\n",
      "Epoch 1819 - lr: 0.01000 - Train loss: 0.84018 - Test loss: 0.88099\n",
      "Epoch 1820 - lr: 0.01000 - Train loss: 0.90011 - Test loss: 0.88121\n",
      "Epoch 1821 - lr: 0.01000 - Train loss: 0.89201 - Test loss: 0.87423\n",
      "Epoch 1822 - lr: 0.01000 - Train loss: 0.84323 - Test loss: 0.87587\n",
      "Epoch 1823 - lr: 0.01000 - Train loss: 0.84994 - Test loss: 0.87611\n",
      "Epoch 1824 - lr: 0.01000 - Train loss: 0.87701 - Test loss: 0.88861\n",
      "Epoch 1825 - lr: 0.01000 - Train loss: 0.95405 - Test loss: 0.88070\n",
      "Epoch 1826 - lr: 0.01000 - Train loss: 0.87782 - Test loss: 0.88929\n",
      "Epoch 1827 - lr: 0.01000 - Train loss: 0.97644 - Test loss: 0.91856\n",
      "Epoch 1828 - lr: 0.01000 - Train loss: 1.04387 - Test loss: 0.86443\n",
      "Epoch 1829 - lr: 0.01000 - Train loss: 0.87248 - Test loss: 0.88345\n",
      "Epoch 1830 - lr: 0.01000 - Train loss: 0.90391 - Test loss: 0.87741\n",
      "Epoch 1831 - lr: 0.01000 - Train loss: 0.82983 - Test loss: 0.87676\n",
      "Epoch 1832 - lr: 0.01000 - Train loss: 0.85874 - Test loss: 0.87634\n",
      "Epoch 1833 - lr: 0.01000 - Train loss: 0.86892 - Test loss: 0.91545\n",
      "Epoch 1834 - lr: 0.01000 - Train loss: 1.04076 - Test loss: 0.86282\n",
      "Epoch 1835 - lr: 0.01000 - Train loss: 0.85272 - Test loss: 0.90425\n",
      "Epoch 1836 - lr: 0.01000 - Train loss: 1.04625 - Test loss: 0.86850\n",
      "Epoch 1837 - lr: 0.01000 - Train loss: 0.89380 - Test loss: 0.90976\n",
      "Epoch 1838 - lr: 0.01000 - Train loss: 1.00915 - Test loss: 0.87210\n",
      "Epoch 1839 - lr: 0.01000 - Train loss: 0.84983 - Test loss: 0.87732\n",
      "Epoch 1840 - lr: 0.01000 - Train loss: 0.85501 - Test loss: 0.87844\n",
      "Epoch 1841 - lr: 0.01000 - Train loss: 0.85952 - Test loss: 0.87816\n",
      "Epoch 1842 - lr: 0.01000 - Train loss: 0.85337 - Test loss: 0.87670\n",
      "Epoch 1843 - lr: 0.01000 - Train loss: 0.86718 - Test loss: 0.87669\n",
      "Epoch 1844 - lr: 0.01000 - Train loss: 0.86984 - Test loss: 0.87610\n",
      "Epoch 1845 - lr: 0.01000 - Train loss: 0.87367 - Test loss: 0.87554\n",
      "Epoch 1846 - lr: 0.01000 - Train loss: 0.87484 - Test loss: 0.87491\n",
      "Epoch 1847 - lr: 0.01000 - Train loss: 0.87505 - Test loss: 0.87425\n",
      "Epoch 1848 - lr: 0.01000 - Train loss: 0.87624 - Test loss: 0.87366\n",
      "Epoch 1849 - lr: 0.01000 - Train loss: 0.87741 - Test loss: 0.87313\n",
      "Epoch 1850 - lr: 0.01000 - Train loss: 0.87849 - Test loss: 0.87266\n",
      "Epoch 1851 - lr: 0.01000 - Train loss: 0.87936 - Test loss: 0.87222\n",
      "Epoch 1852 - lr: 0.01000 - Train loss: 0.87997 - Test loss: 0.87180\n",
      "Epoch 1853 - lr: 0.01000 - Train loss: 0.88034 - Test loss: 0.87137\n",
      "Epoch 1854 - lr: 0.01000 - Train loss: 0.88056 - Test loss: 0.87090\n",
      "Epoch 1855 - lr: 0.01000 - Train loss: 0.88071 - Test loss: 0.87042\n",
      "Epoch 1856 - lr: 0.01000 - Train loss: 0.88081 - Test loss: 0.86992\n",
      "Epoch 1857 - lr: 0.01000 - Train loss: 0.88088 - Test loss: 0.86941\n",
      "Epoch 1858 - lr: 0.01000 - Train loss: 0.88095 - Test loss: 0.86891\n",
      "Epoch 1859 - lr: 0.01000 - Train loss: 0.88102 - Test loss: 0.86842\n",
      "Epoch 1860 - lr: 0.01000 - Train loss: 0.88111 - Test loss: 0.86794\n",
      "Epoch 1861 - lr: 0.01000 - Train loss: 0.88159 - Test loss: 0.86744\n",
      "Epoch 1862 - lr: 0.01000 - Train loss: 0.88364 - Test loss: 0.86691\n",
      "Epoch 1863 - lr: 0.01000 - Train loss: 0.88080 - Test loss: 0.86635\n",
      "Epoch 1864 - lr: 0.01000 - Train loss: 0.88343 - Test loss: 0.86532\n",
      "Epoch 1865 - lr: 0.01000 - Train loss: 0.87580 - Test loss: 0.86443\n",
      "Epoch 1866 - lr: 0.01000 - Train loss: 0.87688 - Test loss: 0.86408\n",
      "Epoch 1867 - lr: 0.01000 - Train loss: 0.87548 - Test loss: 0.86352\n",
      "Epoch 1868 - lr: 0.01000 - Train loss: 0.87655 - Test loss: 0.86325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1869 - lr: 0.01000 - Train loss: 0.87574 - Test loss: 0.86279\n",
      "Epoch 1870 - lr: 0.01000 - Train loss: 0.87644 - Test loss: 0.86252\n",
      "Epoch 1871 - lr: 0.01000 - Train loss: 0.87685 - Test loss: 0.86225\n",
      "Epoch 1872 - lr: 0.01000 - Train loss: 0.87763 - Test loss: 0.86209\n",
      "Epoch 1873 - lr: 0.01000 - Train loss: 0.87388 - Test loss: 0.86056\n",
      "Epoch 1874 - lr: 0.01000 - Train loss: 0.85701 - Test loss: 0.90013\n",
      "Epoch 1875 - lr: 0.01000 - Train loss: 1.09598 - Test loss: 0.85250\n",
      "Epoch 1876 - lr: 0.01000 - Train loss: 0.87036 - Test loss: 0.86509\n",
      "Epoch 1877 - lr: 0.01000 - Train loss: 0.87712 - Test loss: 0.86355\n",
      "Epoch 1878 - lr: 0.01000 - Train loss: 0.87321 - Test loss: 0.86202\n",
      "Epoch 1879 - lr: 0.01000 - Train loss: 0.86564 - Test loss: 0.86066\n",
      "Epoch 1880 - lr: 0.01000 - Train loss: 0.86604 - Test loss: 0.85998\n",
      "Epoch 1881 - lr: 0.01000 - Train loss: 0.86637 - Test loss: 0.85934\n",
      "Epoch 1882 - lr: 0.01000 - Train loss: 0.92541 - Test loss: 0.86030\n",
      "Epoch 1883 - lr: 0.01000 - Train loss: 0.86089 - Test loss: 0.86391\n",
      "Epoch 1884 - lr: 0.01000 - Train loss: 0.86726 - Test loss: 0.86019\n",
      "Epoch 1885 - lr: 0.01000 - Train loss: 0.90582 - Test loss: 0.90796\n",
      "Epoch 1886 - lr: 0.01000 - Train loss: 1.02430 - Test loss: 0.85582\n",
      "Epoch 1887 - lr: 0.01000 - Train loss: 0.94204 - Test loss: 0.85712\n",
      "Epoch 1888 - lr: 0.01000 - Train loss: 0.95168 - Test loss: 0.86430\n",
      "Epoch 1889 - lr: 0.01000 - Train loss: 0.95045 - Test loss: 0.86498\n",
      "Epoch 1890 - lr: 0.01000 - Train loss: 1.16207 - Test loss: 0.91691\n",
      "Epoch 1891 - lr: 0.01000 - Train loss: 1.01959 - Test loss: 0.86166\n",
      "Epoch 1892 - lr: 0.01000 - Train loss: 0.84156 - Test loss: 0.86156\n",
      "Epoch 1893 - lr: 0.01000 - Train loss: 0.87157 - Test loss: 0.87391\n",
      "Epoch 1894 - lr: 0.01000 - Train loss: 1.00097 - Test loss: 0.85707\n",
      "Epoch 1895 - lr: 0.01000 - Train loss: 0.85872 - Test loss: 0.86298\n",
      "Epoch 1896 - lr: 0.01000 - Train loss: 0.85814 - Test loss: 0.86310\n",
      "Epoch 1897 - lr: 0.01000 - Train loss: 0.84925 - Test loss: 0.86254\n",
      "Epoch 1898 - lr: 0.01000 - Train loss: 0.86973 - Test loss: 0.86541\n",
      "Epoch 1899 - lr: 0.01000 - Train loss: 0.89032 - Test loss: 0.85968\n",
      "Epoch 1900 - lr: 0.01000 - Train loss: 0.88027 - Test loss: 0.85471\n",
      "Epoch 1901 - lr: 0.01000 - Train loss: 0.86878 - Test loss: 0.87777\n",
      "Epoch 1902 - lr: 0.01000 - Train loss: 1.03400 - Test loss: 0.85181\n",
      "Epoch 1903 - lr: 0.01000 - Train loss: 0.86220 - Test loss: 0.88958\n",
      "Epoch 1904 - lr: 0.01000 - Train loss: 1.03764 - Test loss: 0.87853\n",
      "Epoch 1905 - lr: 0.01000 - Train loss: 0.98631 - Test loss: 0.86521\n",
      "Epoch 1906 - lr: 0.01000 - Train loss: 0.97585 - Test loss: 0.86662\n",
      "Epoch 1907 - lr: 0.01000 - Train loss: 1.07642 - Test loss: 0.86786\n",
      "Epoch 1908 - lr: 0.01000 - Train loss: 0.96282 - Test loss: 0.87129\n",
      "Epoch 1909 - lr: 0.01000 - Train loss: 1.07990 - Test loss: 0.86618\n",
      "Epoch 1910 - lr: 0.01000 - Train loss: 0.83146 - Test loss: 0.86659\n",
      "Epoch 1911 - lr: 0.01000 - Train loss: 0.83104 - Test loss: 0.86557\n",
      "Epoch 1912 - lr: 0.01000 - Train loss: 0.85400 - Test loss: 0.87484\n",
      "Epoch 1913 - lr: 0.01000 - Train loss: 0.97570 - Test loss: 0.88005\n",
      "Epoch 1914 - lr: 0.01000 - Train loss: 0.85495 - Test loss: 0.89022\n",
      "Epoch 1915 - lr: 0.01000 - Train loss: 1.01196 - Test loss: 0.87238\n",
      "Epoch 1916 - lr: 0.01000 - Train loss: 1.01343 - Test loss: 0.86103\n",
      "Epoch 1917 - lr: 0.01000 - Train loss: 1.04474 - Test loss: 0.86633\n",
      "Epoch 1918 - lr: 0.01000 - Train loss: 0.90651 - Test loss: 0.86887\n",
      "Epoch 1919 - lr: 0.01000 - Train loss: 1.03710 - Test loss: 0.86857\n",
      "Epoch 1920 - lr: 0.01000 - Train loss: 0.89576 - Test loss: 0.87180\n",
      "Epoch 1921 - lr: 0.01000 - Train loss: 0.89472 - Test loss: 0.88999\n",
      "Epoch 1922 - lr: 0.01000 - Train loss: 0.97352 - Test loss: 0.86867\n",
      "Epoch 1923 - lr: 0.01000 - Train loss: 0.84577 - Test loss: 0.87269\n",
      "Epoch 1924 - lr: 0.01000 - Train loss: 0.91188 - Test loss: 0.87447\n",
      "Epoch 1925 - lr: 0.01000 - Train loss: 0.84238 - Test loss: 0.87660\n",
      "Epoch 1926 - lr: 0.01000 - Train loss: 0.92588 - Test loss: 0.87935\n",
      "Epoch 1927 - lr: 0.01000 - Train loss: 1.01117 - Test loss: 0.87797\n",
      "Epoch 1928 - lr: 0.01000 - Train loss: 0.91307 - Test loss: 0.87139\n",
      "Epoch 1929 - lr: 0.01000 - Train loss: 0.95346 - Test loss: 0.88675\n",
      "Epoch 1930 - lr: 0.01000 - Train loss: 0.96512 - Test loss: 0.87536\n",
      "Epoch 1931 - lr: 0.01000 - Train loss: 1.00498 - Test loss: 0.86762\n",
      "Epoch 1932 - lr: 0.01000 - Train loss: 0.86564 - Test loss: 0.88312\n",
      "Epoch 1933 - lr: 0.01000 - Train loss: 1.01019 - Test loss: 0.86860\n",
      "Epoch 1934 - lr: 0.01000 - Train loss: 0.95093 - Test loss: 0.87388\n",
      "Epoch 1935 - lr: 0.01000 - Train loss: 0.94496 - Test loss: 0.87238\n",
      "Epoch 1936 - lr: 0.01000 - Train loss: 0.91810 - Test loss: 0.88730\n",
      "Epoch 1937 - lr: 0.01000 - Train loss: 0.95607 - Test loss: 0.88546\n",
      "Epoch 1938 - lr: 0.01000 - Train loss: 1.00738 - Test loss: 0.87273\n",
      "Epoch 1939 - lr: 0.01000 - Train loss: 0.96307 - Test loss: 0.87746\n",
      "Epoch 1940 - lr: 0.01000 - Train loss: 0.89541 - Test loss: 0.87967\n",
      "Epoch 1941 - lr: 0.01000 - Train loss: 0.90719 - Test loss: 0.87774\n",
      "Epoch 1942 - lr: 0.01000 - Train loss: 0.93755 - Test loss: 0.87725\n",
      "Epoch 1943 - lr: 0.01000 - Train loss: 0.88215 - Test loss: 0.88814\n",
      "Epoch 1944 - lr: 0.01000 - Train loss: 1.00419 - Test loss: 0.87392\n",
      "Epoch 1945 - lr: 0.01000 - Train loss: 1.06042 - Test loss: 0.87231\n",
      "Epoch 1946 - lr: 0.01000 - Train loss: 0.95810 - Test loss: 0.88411\n",
      "Epoch 1947 - lr: 0.01000 - Train loss: 0.90777 - Test loss: 0.88149\n",
      "Epoch 1948 - lr: 0.01000 - Train loss: 1.00017 - Test loss: 0.87813\n",
      "Epoch 1949 - lr: 0.01000 - Train loss: 0.89811 - Test loss: 0.88426\n",
      "Epoch 1950 - lr: 0.01000 - Train loss: 0.97224 - Test loss: 0.89954\n",
      "Epoch 1951 - lr: 0.01000 - Train loss: 0.85306 - Test loss: 0.88356\n",
      "Epoch 1952 - lr: 0.01000 - Train loss: 0.90359 - Test loss: 0.88090\n",
      "Epoch 1953 - lr: 0.01000 - Train loss: 0.85156 - Test loss: 0.88468\n",
      "Epoch 1954 - lr: 0.01000 - Train loss: 0.95196 - Test loss: 0.88541\n",
      "Epoch 1955 - lr: 0.01000 - Train loss: 0.92520 - Test loss: 0.88164\n",
      "Epoch 1956 - lr: 0.01000 - Train loss: 0.89508 - Test loss: 0.89523\n",
      "Epoch 1957 - lr: 0.01000 - Train loss: 0.86368 - Test loss: 0.87672\n",
      "Epoch 1958 - lr: 0.01000 - Train loss: 1.08136 - Test loss: 0.87299\n",
      "Epoch 1959 - lr: 0.01000 - Train loss: 0.99074 - Test loss: 0.88324\n",
      "Epoch 1960 - lr: 0.01000 - Train loss: 0.91223 - Test loss: 0.88693\n",
      "Epoch 1961 - lr: 0.01000 - Train loss: 0.90780 - Test loss: 0.90143\n",
      "Epoch 1962 - lr: 0.01000 - Train loss: 0.94711 - Test loss: 0.88786\n",
      "Epoch 1963 - lr: 0.01000 - Train loss: 0.97370 - Test loss: 0.90055\n",
      "Epoch 1964 - lr: 0.01000 - Train loss: 0.86630 - Test loss: 0.87893\n",
      "Epoch 1965 - lr: 0.01000 - Train loss: 1.04545 - Test loss: 0.87715\n",
      "Epoch 1966 - lr: 0.01000 - Train loss: 0.86723 - Test loss: 0.87660\n",
      "Epoch 1967 - lr: 0.01000 - Train loss: 0.92038 - Test loss: 0.87701\n",
      "Epoch 1968 - lr: 0.01000 - Train loss: 0.90299 - Test loss: 0.88280\n",
      "Epoch 1969 - lr: 0.01000 - Train loss: 0.90402 - Test loss: 0.88359\n",
      "Epoch 1970 - lr: 0.01000 - Train loss: 0.89858 - Test loss: 0.88177\n",
      "Epoch 1971 - lr: 0.01000 - Train loss: 0.90744 - Test loss: 0.88077\n",
      "Epoch 1972 - lr: 0.01000 - Train loss: 1.06557 - Test loss: 0.87377\n",
      "Epoch 1973 - lr: 0.01000 - Train loss: 0.96796 - Test loss: 0.88139\n",
      "Epoch 1974 - lr: 0.01000 - Train loss: 0.97914 - Test loss: 0.88444\n",
      "Epoch 1975 - lr: 0.01000 - Train loss: 0.87659 - Test loss: 0.87954\n",
      "Epoch 1976 - lr: 0.01000 - Train loss: 1.02198 - Test loss: 0.87936\n",
      "Epoch 1977 - lr: 0.01000 - Train loss: 1.05789 - Test loss: 0.89267\n",
      "Epoch 1978 - lr: 0.01000 - Train loss: 0.90846 - Test loss: 0.88919\n",
      "Epoch 1979 - lr: 0.01000 - Train loss: 0.92370 - Test loss: 0.88222\n",
      "Epoch 1980 - lr: 0.01000 - Train loss: 0.92792 - Test loss: 0.90488\n",
      "Epoch 1981 - lr: 0.01000 - Train loss: 0.87290 - Test loss: 0.88490\n",
      "Epoch 1982 - lr: 0.01000 - Train loss: 0.91154 - Test loss: 0.90538\n",
      "Epoch 1983 - lr: 0.01000 - Train loss: 0.86675 - Test loss: 0.88367\n",
      "Epoch 1984 - lr: 0.01000 - Train loss: 0.90785 - Test loss: 0.88413\n",
      "Epoch 1985 - lr: 0.01000 - Train loss: 0.93569 - Test loss: 0.90140\n",
      "Epoch 1986 - lr: 0.01000 - Train loss: 0.87263 - Test loss: 0.88072\n",
      "Epoch 1987 - lr: 0.01000 - Train loss: 0.96813 - Test loss: 0.88262\n",
      "Epoch 1988 - lr: 0.01000 - Train loss: 0.88220 - Test loss: 0.88199\n",
      "Epoch 1989 - lr: 0.01000 - Train loss: 0.92899 - Test loss: 0.88429\n",
      "Epoch 1990 - lr: 0.01000 - Train loss: 0.88299 - Test loss: 0.87851\n",
      "Epoch 1991 - lr: 0.01000 - Train loss: 1.03344 - Test loss: 0.87920\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1992 - lr: 0.01000 - Train loss: 0.92180 - Test loss: 0.87972\n",
      "Epoch 1993 - lr: 0.01000 - Train loss: 0.90822 - Test loss: 0.88465\n",
      "Epoch 1994 - lr: 0.01000 - Train loss: 0.87353 - Test loss: 0.87999\n",
      "Epoch 1995 - lr: 0.01000 - Train loss: 0.96151 - Test loss: 0.88020\n",
      "Epoch 1996 - lr: 0.01000 - Train loss: 0.86794 - Test loss: 0.87993\n",
      "Epoch 1997 - lr: 0.01000 - Train loss: 0.88071 - Test loss: 0.87885\n",
      "Epoch 1998 - lr: 0.01000 - Train loss: 0.94253 - Test loss: 0.88085\n",
      "Epoch 1999 - lr: 0.01000 - Train loss: 0.92598 - Test loss: 0.90338\n",
      "Epoch 2000 - lr: 0.01000 - Train loss: 0.87552 - Test loss: 0.88353\n",
      "Epoch 2001 - lr: 0.01000 - Train loss: 0.92401 - Test loss: 0.87853\n",
      "Epoch 2002 - lr: 0.01000 - Train loss: 0.92251 - Test loss: 0.88058\n",
      "Epoch 2003 - lr: 0.01000 - Train loss: 0.91614 - Test loss: 0.88601\n",
      "Epoch 2004 - lr: 0.01000 - Train loss: 0.87174 - Test loss: 0.88193\n",
      "Epoch 2005 - lr: 0.01000 - Train loss: 0.87304 - Test loss: 0.88021\n",
      "Epoch 2006 - lr: 0.01000 - Train loss: 0.99888 - Test loss: 0.87967\n",
      "Epoch 2007 - lr: 0.01000 - Train loss: 0.88364 - Test loss: 0.88200\n",
      "Epoch 2008 - lr: 0.01000 - Train loss: 0.95208 - Test loss: 0.88461\n",
      "Epoch 2009 - lr: 0.01000 - Train loss: 0.93113 - Test loss: 0.89876\n",
      "Epoch 2010 - lr: 0.01000 - Train loss: 0.93496 - Test loss: 0.89540\n",
      "Epoch 2011 - lr: 0.01000 - Train loss: 0.95361 - Test loss: 0.88940\n",
      "Epoch 2012 - lr: 0.01000 - Train loss: 0.93317 - Test loss: 0.89038\n",
      "Epoch 2013 - lr: 0.01000 - Train loss: 0.92012 - Test loss: 0.90193\n",
      "Epoch 2014 - lr: 0.01000 - Train loss: 0.87705 - Test loss: 0.88659\n",
      "Epoch 2015 - lr: 0.01000 - Train loss: 0.88898 - Test loss: 0.88303\n",
      "Epoch 2016 - lr: 0.01000 - Train loss: 0.96743 - Test loss: 0.88088\n",
      "Epoch 2017 - lr: 0.01000 - Train loss: 0.89080 - Test loss: 0.88439\n",
      "Epoch 2018 - lr: 0.01000 - Train loss: 1.00027 - Test loss: 0.90617\n",
      "Epoch 2019 - lr: 0.01000 - Train loss: 0.87609 - Test loss: 0.88814\n",
      "Epoch 2020 - lr: 0.01000 - Train loss: 0.98422 - Test loss: 0.89016\n",
      "Epoch 2021 - lr: 0.01000 - Train loss: 0.90592 - Test loss: 0.90783\n",
      "Epoch 2022 - lr: 0.01000 - Train loss: 0.88156 - Test loss: 0.88676\n",
      "Epoch 2023 - lr: 0.01000 - Train loss: 0.90711 - Test loss: 0.90462\n",
      "Epoch 2024 - lr: 0.01000 - Train loss: 0.88009 - Test loss: 0.88397\n",
      "Epoch 2025 - lr: 0.01000 - Train loss: 0.93160 - Test loss: 0.88434\n",
      "Epoch 2026 - lr: 0.01000 - Train loss: 1.02309 - Test loss: 0.87790\n",
      "Epoch 2027 - lr: 0.01000 - Train loss: 0.92368 - Test loss: 0.87925\n",
      "Epoch 2028 - lr: 0.01000 - Train loss: 0.92374 - Test loss: 0.88074\n",
      "Epoch 2029 - lr: 0.01000 - Train loss: 0.86496 - Test loss: 0.88502\n",
      "Epoch 2030 - lr: 0.01000 - Train loss: 0.92715 - Test loss: 0.88315\n",
      "Epoch 2031 - lr: 0.01000 - Train loss: 0.88746 - Test loss: 0.88485\n",
      "Epoch 2032 - lr: 0.01000 - Train loss: 0.91215 - Test loss: 0.88601\n",
      "Epoch 2033 - lr: 0.01000 - Train loss: 0.88967 - Test loss: 0.89018\n",
      "Epoch 2034 - lr: 0.01000 - Train loss: 0.93563 - Test loss: 0.89090\n",
      "Epoch 2035 - lr: 0.01000 - Train loss: 0.99209 - Test loss: 0.88635\n",
      "Epoch 2036 - lr: 0.01000 - Train loss: 0.88254 - Test loss: 0.88810\n",
      "Epoch 2037 - lr: 0.01000 - Train loss: 0.95861 - Test loss: 0.88783\n",
      "Epoch 2038 - lr: 0.01000 - Train loss: 0.95544 - Test loss: 0.89103\n",
      "Epoch 2039 - lr: 0.01000 - Train loss: 0.93697 - Test loss: 0.89561\n",
      "Epoch 2040 - lr: 0.01000 - Train loss: 0.96036 - Test loss: 0.89240\n",
      "Epoch 2041 - lr: 0.01000 - Train loss: 0.93331 - Test loss: 0.89677\n",
      "Epoch 2042 - lr: 0.01000 - Train loss: 0.92679 - Test loss: 0.89783\n",
      "Epoch 2043 - lr: 0.01000 - Train loss: 0.98251 - Test loss: 0.89268\n",
      "Epoch 2044 - lr: 0.01000 - Train loss: 0.92441 - Test loss: 0.90514\n",
      "Epoch 2045 - lr: 0.01000 - Train loss: 0.89795 - Test loss: 0.91676\n",
      "Epoch 2046 - lr: 0.01000 - Train loss: 0.87615 - Test loss: 0.89441\n",
      "Epoch 2047 - lr: 0.01000 - Train loss: 1.01899 - Test loss: 0.89492\n",
      "Epoch 2048 - lr: 0.01000 - Train loss: 0.90922 - Test loss: 0.89517\n",
      "Epoch 2049 - lr: 0.01000 - Train loss: 0.90848 - Test loss: 0.91051\n",
      "Epoch 2050 - lr: 0.01000 - Train loss: 0.91206 - Test loss: 0.92148\n",
      "Epoch 2051 - lr: 0.01000 - Train loss: 0.88629 - Test loss: 0.89315\n",
      "Epoch 2052 - lr: 0.01000 - Train loss: 0.96338 - Test loss: 0.88865\n",
      "Epoch 2053 - lr: 0.01000 - Train loss: 0.91329 - Test loss: 0.89179\n",
      "Epoch 2054 - lr: 0.01000 - Train loss: 0.97401 - Test loss: 0.90576\n",
      "Epoch 2055 - lr: 0.01000 - Train loss: 0.88012 - Test loss: 0.89111\n",
      "Epoch 2056 - lr: 0.01000 - Train loss: 0.99244 - Test loss: 0.91178\n",
      "Epoch 2057 - lr: 0.01000 - Train loss: 0.88803 - Test loss: 0.89172\n",
      "Epoch 2058 - lr: 0.01000 - Train loss: 0.96042 - Test loss: 0.88791\n",
      "Epoch 2059 - lr: 0.01000 - Train loss: 0.95803 - Test loss: 0.88925\n",
      "Epoch 2060 - lr: 0.01000 - Train loss: 0.87970 - Test loss: 0.89355\n",
      "Epoch 2061 - lr: 0.01000 - Train loss: 0.92561 - Test loss: 0.88965\n",
      "Epoch 2062 - lr: 0.01000 - Train loss: 0.88334 - Test loss: 0.89065\n",
      "Epoch 2063 - lr: 0.01000 - Train loss: 0.96538 - Test loss: 0.88924\n",
      "Epoch 2064 - lr: 0.01000 - Train loss: 0.87451 - Test loss: 0.89112\n",
      "Epoch 2065 - lr: 0.01000 - Train loss: 0.97232 - Test loss: 0.89608\n",
      "Epoch 2066 - lr: 0.01000 - Train loss: 0.97070 - Test loss: 0.91061\n",
      "Epoch 2067 - lr: 0.01000 - Train loss: 0.88296 - Test loss: 0.89645\n",
      "Epoch 2068 - lr: 0.01000 - Train loss: 0.95561 - Test loss: 0.89325\n",
      "Epoch 2069 - lr: 0.01000 - Train loss: 0.88616 - Test loss: 0.89940\n",
      "Epoch 2070 - lr: 0.01000 - Train loss: 0.93060 - Test loss: 0.90252\n",
      "Epoch 2071 - lr: 0.01000 - Train loss: 0.96845 - Test loss: 0.91149\n",
      "Epoch 2072 - lr: 0.01000 - Train loss: 0.87644 - Test loss: 0.89694\n",
      "Epoch 2073 - lr: 0.01000 - Train loss: 0.97345 - Test loss: 0.91417\n",
      "Epoch 2074 - lr: 0.01000 - Train loss: 0.87828 - Test loss: 0.89702\n",
      "Epoch 2075 - lr: 0.01000 - Train loss: 0.99872 - Test loss: 0.91365\n",
      "Epoch 2076 - lr: 0.01000 - Train loss: 0.88173 - Test loss: 0.89712\n",
      "Epoch 2077 - lr: 0.01000 - Train loss: 1.01004 - Test loss: 0.90159\n",
      "Epoch 2078 - lr: 0.01000 - Train loss: 0.87968 - Test loss: 0.89627\n",
      "Epoch 2079 - lr: 0.01000 - Train loss: 0.97456 - Test loss: 0.91477\n",
      "Epoch 2080 - lr: 0.01000 - Train loss: 0.87866 - Test loss: 0.89802\n",
      "Epoch 2081 - lr: 0.01000 - Train loss: 0.97290 - Test loss: 0.91451\n",
      "Epoch 2082 - lr: 0.01000 - Train loss: 0.89884 - Test loss: 0.89989\n",
      "Epoch 2083 - lr: 0.01000 - Train loss: 0.97122 - Test loss: 0.91569\n",
      "Epoch 2084 - lr: 0.01000 - Train loss: 0.87914 - Test loss: 0.89794\n",
      "Epoch 2085 - lr: 0.01000 - Train loss: 0.97245 - Test loss: 0.91477\n",
      "Epoch 2086 - lr: 0.01000 - Train loss: 0.87850 - Test loss: 0.89750\n",
      "Epoch 2087 - lr: 0.01000 - Train loss: 0.97299 - Test loss: 0.91416\n",
      "Epoch 2088 - lr: 0.01000 - Train loss: 0.87990 - Test loss: 0.89687\n",
      "Epoch 2089 - lr: 0.01000 - Train loss: 0.95890 - Test loss: 0.91735\n",
      "Epoch 2090 - lr: 0.01000 - Train loss: 0.87727 - Test loss: 0.89554\n",
      "Epoch 2091 - lr: 0.01000 - Train loss: 0.94924 - Test loss: 0.89459\n",
      "Epoch 2092 - lr: 0.01000 - Train loss: 0.91117 - Test loss: 0.89749\n",
      "Epoch 2093 - lr: 0.01000 - Train loss: 0.91584 - Test loss: 0.89629\n",
      "Epoch 2094 - lr: 0.01000 - Train loss: 0.91702 - Test loss: 0.89806\n",
      "Epoch 2095 - lr: 0.01000 - Train loss: 0.97407 - Test loss: 0.90081\n",
      "Epoch 2096 - lr: 0.01000 - Train loss: 0.88419 - Test loss: 0.89829\n",
      "Epoch 2097 - lr: 0.01000 - Train loss: 0.92573 - Test loss: 0.89866\n",
      "Epoch 2098 - lr: 0.01000 - Train loss: 0.96557 - Test loss: 0.89200\n",
      "Epoch 2099 - lr: 0.01000 - Train loss: 0.87471 - Test loss: 0.89390\n",
      "Epoch 2100 - lr: 0.01000 - Train loss: 1.01878 - Test loss: 0.89811\n",
      "Epoch 2101 - lr: 0.01000 - Train loss: 0.99997 - Test loss: 0.89845\n",
      "Epoch 2102 - lr: 0.01000 - Train loss: 0.96690 - Test loss: 0.90418\n",
      "Epoch 2103 - lr: 0.01000 - Train loss: 0.97179 - Test loss: 0.91965\n",
      "Epoch 2104 - lr: 0.01000 - Train loss: 0.88359 - Test loss: 0.92189\n",
      "Epoch 2105 - lr: 0.01000 - Train loss: 0.86950 - Test loss: 0.91464\n",
      "Epoch 2106 - lr: 0.01000 - Train loss: 0.90152 - Test loss: 0.90002\n",
      "Epoch 2107 - lr: 0.01000 - Train loss: 0.93730 - Test loss: 0.90578\n",
      "Epoch 2108 - lr: 0.01000 - Train loss: 0.96411 - Test loss: 0.89587\n",
      "Epoch 2109 - lr: 0.01000 - Train loss: 0.88184 - Test loss: 0.89751\n",
      "Epoch 2110 - lr: 0.01000 - Train loss: 0.99019 - Test loss: 0.89788\n",
      "Epoch 2111 - lr: 0.01000 - Train loss: 0.92507 - Test loss: 0.90263\n",
      "Epoch 2112 - lr: 0.01000 - Train loss: 0.91337 - Test loss: 0.90592\n",
      "Epoch 2113 - lr: 0.01000 - Train loss: 0.91286 - Test loss: 0.90669\n",
      "Epoch 2114 - lr: 0.01000 - Train loss: 0.95380 - Test loss: 0.90048\n",
      "Epoch 2115 - lr: 0.01000 - Train loss: 0.89007 - Test loss: 0.90229\n",
      "Epoch 2116 - lr: 0.01000 - Train loss: 0.98418 - Test loss: 0.90293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2117 - lr: 0.01000 - Train loss: 0.99796 - Test loss: 0.90676\n",
      "Epoch 2118 - lr: 0.01000 - Train loss: 1.01214 - Test loss: 0.91212\n",
      "Epoch 2119 - lr: 0.01000 - Train loss: 0.95892 - Test loss: 0.90932\n",
      "Epoch 2120 - lr: 0.01000 - Train loss: 0.92341 - Test loss: 0.91336\n",
      "Epoch 2121 - lr: 0.01000 - Train loss: 1.00714 - Test loss: 0.91638\n",
      "Epoch 2122 - lr: 0.01000 - Train loss: 0.95399 - Test loss: 0.93163\n",
      "Epoch 2123 - lr: 0.01000 - Train loss: 0.89482 - Test loss: 0.93149\n",
      "Epoch 2124 - lr: 0.01000 - Train loss: 0.88763 - Test loss: 0.92941\n",
      "Epoch 2125 - lr: 0.01000 - Train loss: 0.89318 - Test loss: 0.92770\n",
      "Epoch 2126 - lr: 0.01000 - Train loss: 0.88364 - Test loss: 0.91982\n",
      "Epoch 2127 - lr: 0.01000 - Train loss: 0.92646 - Test loss: 0.90921\n",
      "Epoch 2128 - lr: 0.01000 - Train loss: 0.99583 - Test loss: 0.91370\n",
      "Epoch 2129 - lr: 0.01000 - Train loss: 0.92290 - Test loss: 0.90693\n",
      "Epoch 2130 - lr: 0.01000 - Train loss: 0.94895 - Test loss: 0.90035\n",
      "Epoch 2131 - lr: 0.01000 - Train loss: 0.99352 - Test loss: 0.89990\n",
      "Epoch 2132 - lr: 0.01000 - Train loss: 0.93093 - Test loss: 0.90413\n",
      "Epoch 2133 - lr: 0.01000 - Train loss: 0.91557 - Test loss: 0.90713\n",
      "Epoch 2134 - lr: 0.01000 - Train loss: 0.97138 - Test loss: 0.92070\n",
      "Epoch 2135 - lr: 0.01000 - Train loss: 0.90700 - Test loss: 0.90831\n",
      "Epoch 2136 - lr: 0.01000 - Train loss: 0.97131 - Test loss: 0.92156\n",
      "Epoch 2137 - lr: 0.01000 - Train loss: 0.88387 - Test loss: 0.92327\n",
      "Epoch 2138 - lr: 0.01000 - Train loss: 0.88657 - Test loss: 0.92265\n",
      "Epoch 2139 - lr: 0.01000 - Train loss: 0.91107 - Test loss: 0.90742\n",
      "Epoch 2140 - lr: 0.01000 - Train loss: 0.96612 - Test loss: 0.90465\n",
      "Epoch 2141 - lr: 0.01000 - Train loss: 0.91628 - Test loss: 0.90343\n",
      "Epoch 2142 - lr: 0.01000 - Train loss: 0.95704 - Test loss: 0.89711\n",
      "Epoch 2143 - lr: 0.01000 - Train loss: 0.94842 - Test loss: 0.89896\n",
      "Epoch 2144 - lr: 0.01000 - Train loss: 0.96534 - Test loss: 0.91005\n",
      "Epoch 2145 - lr: 0.01000 - Train loss: 0.90879 - Test loss: 0.90141\n",
      "Epoch 2146 - lr: 0.01000 - Train loss: 0.88093 - Test loss: 0.90485\n",
      "Epoch 2147 - lr: 0.01000 - Train loss: 0.95126 - Test loss: 0.90226\n",
      "Epoch 2148 - lr: 0.01000 - Train loss: 0.98172 - Test loss: 0.91427\n",
      "Epoch 2149 - lr: 0.01000 - Train loss: 0.90731 - Test loss: 0.92662\n",
      "Epoch 2150 - lr: 0.01000 - Train loss: 0.89960 - Test loss: 0.90509\n",
      "Epoch 2151 - lr: 0.01000 - Train loss: 0.97231 - Test loss: 0.91985\n",
      "Epoch 2152 - lr: 0.01000 - Train loss: 0.89550 - Test loss: 0.91195\n",
      "Epoch 2153 - lr: 0.01000 - Train loss: 0.93099 - Test loss: 0.90791\n",
      "Epoch 2154 - lr: 0.01000 - Train loss: 0.97257 - Test loss: 0.91939\n",
      "Epoch 2155 - lr: 0.01000 - Train loss: 0.88850 - Test loss: 0.90078\n",
      "Epoch 2156 - lr: 0.01000 - Train loss: 0.98356 - Test loss: 0.90011\n",
      "Epoch 2157 - lr: 0.01000 - Train loss: 0.92296 - Test loss: 0.89789\n",
      "Epoch 2158 - lr: 0.01000 - Train loss: 0.88613 - Test loss: 0.89880\n",
      "Epoch 2159 - lr: 0.01000 - Train loss: 0.94228 - Test loss: 0.90342\n",
      "Epoch 2160 - lr: 0.01000 - Train loss: 0.93194 - Test loss: 0.90566\n",
      "Epoch 2161 - lr: 0.01000 - Train loss: 0.94380 - Test loss: 0.90003\n",
      "Epoch 2162 - lr: 0.01000 - Train loss: 0.91284 - Test loss: 0.90456\n",
      "Epoch 2163 - lr: 0.01000 - Train loss: 0.97344 - Test loss: 0.91868\n",
      "Epoch 2164 - lr: 0.01000 - Train loss: 0.91057 - Test loss: 0.90817\n",
      "Epoch 2165 - lr: 0.01000 - Train loss: 0.95940 - Test loss: 0.92513\n",
      "Epoch 2166 - lr: 0.01000 - Train loss: 0.87296 - Test loss: 0.91852\n",
      "Epoch 2167 - lr: 0.01000 - Train loss: 0.88112 - Test loss: 0.89905\n",
      "Epoch 2168 - lr: 0.01000 - Train loss: 1.00133 - Test loss: 0.90489\n",
      "Epoch 2169 - lr: 0.01000 - Train loss: 0.92023 - Test loss: 0.92161\n",
      "Epoch 2170 - lr: 0.01000 - Train loss: 0.89339 - Test loss: 0.90954\n",
      "Epoch 2171 - lr: 0.01000 - Train loss: 0.90025 - Test loss: 0.92020\n",
      "Epoch 2172 - lr: 0.01000 - Train loss: 0.87971 - Test loss: 0.89547\n",
      "Epoch 2173 - lr: 0.01000 - Train loss: 0.97842 - Test loss: 0.89240\n",
      "Epoch 2174 - lr: 0.01000 - Train loss: 0.98673 - Test loss: 0.90525\n",
      "Epoch 2175 - lr: 0.01000 - Train loss: 0.86178 - Test loss: 0.89701\n",
      "Epoch 2176 - lr: 0.01000 - Train loss: 0.92276 - Test loss: 0.89252\n",
      "Epoch 2177 - lr: 0.01000 - Train loss: 0.88104 - Test loss: 0.89298\n",
      "Epoch 2178 - lr: 0.01000 - Train loss: 0.98129 - Test loss: 0.89942\n",
      "Epoch 2179 - lr: 0.01000 - Train loss: 0.96840 - Test loss: 0.91491\n",
      "Epoch 2180 - lr: 0.01000 - Train loss: 0.89901 - Test loss: 0.89932\n",
      "Epoch 2181 - lr: 0.01000 - Train loss: 0.97834 - Test loss: 0.91300\n",
      "Epoch 2182 - lr: 0.01000 - Train loss: 0.90761 - Test loss: 0.90179\n",
      "Epoch 2183 - lr: 0.01000 - Train loss: 0.94266 - Test loss: 0.89850\n",
      "Epoch 2184 - lr: 0.01000 - Train loss: 0.90647 - Test loss: 0.91832\n",
      "Epoch 2185 - lr: 0.01000 - Train loss: 0.88138 - Test loss: 0.89642\n",
      "Epoch 2186 - lr: 0.01000 - Train loss: 0.95930 - Test loss: 0.89385\n",
      "Epoch 2187 - lr: 0.01000 - Train loss: 1.02486 - Test loss: 0.89830\n",
      "Epoch 2188 - lr: 0.01000 - Train loss: 0.94201 - Test loss: 0.89781\n",
      "Epoch 2189 - lr: 0.01000 - Train loss: 0.92276 - Test loss: 0.90247\n",
      "Epoch 2190 - lr: 0.01000 - Train loss: 0.96445 - Test loss: 0.89595\n",
      "Epoch 2191 - lr: 0.01000 - Train loss: 0.88588 - Test loss: 0.89820\n",
      "Epoch 2192 - lr: 0.01000 - Train loss: 0.92957 - Test loss: 0.90094\n",
      "Epoch 2193 - lr: 0.01000 - Train loss: 0.93253 - Test loss: 0.90525\n",
      "Epoch 2194 - lr: 0.01000 - Train loss: 0.97098 - Test loss: 0.92022\n",
      "Epoch 2195 - lr: 0.01000 - Train loss: 0.89077 - Test loss: 0.92211\n",
      "Epoch 2196 - lr: 0.01000 - Train loss: 0.87792 - Test loss: 0.91524\n",
      "Epoch 2197 - lr: 0.01000 - Train loss: 0.94348 - Test loss: 0.91045\n",
      "Epoch 2198 - lr: 0.01000 - Train loss: 0.97227 - Test loss: 0.90604\n",
      "Epoch 2199 - lr: 0.01000 - Train loss: 0.92539 - Test loss: 0.90229\n",
      "Epoch 2200 - lr: 0.01000 - Train loss: 0.95790 - Test loss: 0.89665\n",
      "Epoch 2201 - lr: 0.01000 - Train loss: 0.92825 - Test loss: 0.89863\n",
      "Epoch 2202 - lr: 0.01000 - Train loss: 0.91610 - Test loss: 0.90080\n",
      "Epoch 2203 - lr: 0.01000 - Train loss: 0.97171 - Test loss: 0.91772\n",
      "Epoch 2204 - lr: 0.01000 - Train loss: 0.88767 - Test loss: 0.90004\n",
      "Epoch 2205 - lr: 0.01000 - Train loss: 0.94199 - Test loss: 0.91095\n",
      "Epoch 2206 - lr: 0.01000 - Train loss: 0.89522 - Test loss: 0.89915\n",
      "Epoch 2207 - lr: 0.01000 - Train loss: 0.97116 - Test loss: 0.91644\n",
      "Epoch 2208 - lr: 0.01000 - Train loss: 0.88569 - Test loss: 0.89879\n",
      "Epoch 2209 - lr: 0.01000 - Train loss: 0.91429 - Test loss: 0.89855\n",
      "Epoch 2210 - lr: 0.01000 - Train loss: 0.91666 - Test loss: 0.89994\n",
      "Epoch 2211 - lr: 0.01000 - Train loss: 0.96719 - Test loss: 0.90956\n",
      "Epoch 2212 - lr: 0.01000 - Train loss: 0.88856 - Test loss: 0.91625\n",
      "Epoch 2213 - lr: 0.01000 - Train loss: 0.89043 - Test loss: 0.90833\n",
      "Epoch 2214 - lr: 0.01000 - Train loss: 0.90779 - Test loss: 0.92150\n",
      "Epoch 2215 - lr: 0.01000 - Train loss: 0.86928 - Test loss: 0.91237\n",
      "Epoch 2216 - lr: 0.01000 - Train loss: 0.87992 - Test loss: 0.89265\n",
      "Epoch 2217 - lr: 0.01000 - Train loss: 0.98366 - Test loss: 0.90867\n",
      "Epoch 2218 - lr: 0.01000 - Train loss: 0.86387 - Test loss: 0.90509\n",
      "Epoch 2219 - lr: 0.01000 - Train loss: 0.92029 - Test loss: 0.89072\n",
      "Epoch 2220 - lr: 0.01000 - Train loss: 0.87751 - Test loss: 0.89350\n",
      "Epoch 2221 - lr: 0.01000 - Train loss: 0.91595 - Test loss: 0.88916\n",
      "Epoch 2222 - lr: 0.01000 - Train loss: 0.92636 - Test loss: 0.88960\n",
      "Epoch 2223 - lr: 0.01000 - Train loss: 0.88723 - Test loss: 0.89162\n",
      "Epoch 2224 - lr: 0.01000 - Train loss: 0.97895 - Test loss: 0.91109\n",
      "Epoch 2225 - lr: 0.01000 - Train loss: 0.88072 - Test loss: 0.89493\n",
      "Epoch 2226 - lr: 0.01000 - Train loss: 0.93952 - Test loss: 0.89579\n",
      "Epoch 2227 - lr: 0.01000 - Train loss: 0.92139 - Test loss: 0.89800\n",
      "Epoch 2228 - lr: 0.01000 - Train loss: 0.96751 - Test loss: 0.91434\n",
      "Epoch 2229 - lr: 0.01000 - Train loss: 0.88184 - Test loss: 0.89638\n",
      "Epoch 2230 - lr: 0.01000 - Train loss: 0.96802 - Test loss: 0.91382\n",
      "Epoch 2231 - lr: 0.01000 - Train loss: 0.88186 - Test loss: 0.89627\n",
      "Epoch 2232 - lr: 0.01000 - Train loss: 0.97831 - Test loss: 0.91349\n",
      "Epoch 2233 - lr: 0.01000 - Train loss: 0.88274 - Test loss: 0.89649\n",
      "Epoch 2234 - lr: 0.01000 - Train loss: 0.97925 - Test loss: 0.91133\n",
      "Epoch 2235 - lr: 0.01000 - Train loss: 0.90782 - Test loss: 0.90090\n",
      "Epoch 2236 - lr: 0.01000 - Train loss: 0.99040 - Test loss: 0.89431\n",
      "Epoch 2237 - lr: 0.01000 - Train loss: 0.86709 - Test loss: 0.90762\n",
      "Epoch 2238 - lr: 0.01000 - Train loss: 0.89835 - Test loss: 0.90408\n",
      "Epoch 2239 - lr: 0.01000 - Train loss: 0.93631 - Test loss: 0.91598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2240 - lr: 0.01000 - Train loss: 0.86786 - Test loss: 0.90811\n",
      "Epoch 2241 - lr: 0.01000 - Train loss: 0.91886 - Test loss: 0.89907\n",
      "Epoch 2242 - lr: 0.01000 - Train loss: 0.96470 - Test loss: 0.90958\n",
      "Epoch 2243 - lr: 0.01000 - Train loss: 0.88553 - Test loss: 0.89308\n",
      "Epoch 2244 - lr: 0.01000 - Train loss: 0.93865 - Test loss: 0.89221\n",
      "Epoch 2245 - lr: 0.01000 - Train loss: 0.91255 - Test loss: 0.89553\n",
      "Epoch 2246 - lr: 0.01000 - Train loss: 0.96695 - Test loss: 0.91180\n",
      "Epoch 2247 - lr: 0.01000 - Train loss: 0.88043 - Test loss: 0.89385\n",
      "Epoch 2248 - lr: 0.01000 - Train loss: 0.96580 - Test loss: 0.91145\n",
      "Epoch 2249 - lr: 0.01000 - Train loss: 0.88061 - Test loss: 0.89399\n",
      "Epoch 2250 - lr: 0.01000 - Train loss: 0.99617 - Test loss: 0.90502\n",
      "Epoch 2251 - lr: 0.01000 - Train loss: 0.93835 - Test loss: 0.89953\n",
      "Epoch 2252 - lr: 0.01000 - Train loss: 0.93478 - Test loss: 0.89548\n",
      "Epoch 2253 - lr: 0.01000 - Train loss: 0.92705 - Test loss: 0.89777\n",
      "Epoch 2254 - lr: 0.01000 - Train loss: 0.96912 - Test loss: 0.91416\n",
      "Epoch 2255 - lr: 0.01000 - Train loss: 0.88893 - Test loss: 0.89609\n",
      "Epoch 2256 - lr: 0.01000 - Train loss: 0.91015 - Test loss: 0.89409\n",
      "Epoch 2257 - lr: 0.01000 - Train loss: 0.92506 - Test loss: 0.89276\n",
      "Epoch 2258 - lr: 0.01000 - Train loss: 0.88500 - Test loss: 0.89428\n",
      "Epoch 2259 - lr: 0.01000 - Train loss: 0.93058 - Test loss: 0.89642\n",
      "Epoch 2260 - lr: 0.01000 - Train loss: 0.97935 - Test loss: 0.90458\n",
      "Epoch 2261 - lr: 0.01000 - Train loss: 0.89114 - Test loss: 0.89817\n",
      "Epoch 2262 - lr: 0.01000 - Train loss: 0.98024 - Test loss: 0.89676\n",
      "Epoch 2263 - lr: 0.01000 - Train loss: 0.93922 - Test loss: 0.90304\n",
      "Epoch 2264 - lr: 0.01000 - Train loss: 0.94341 - Test loss: 0.90867\n",
      "Epoch 2265 - lr: 0.01000 - Train loss: 0.96826 - Test loss: 0.91859\n",
      "Epoch 2266 - lr: 0.01000 - Train loss: 0.90609 - Test loss: 0.90948\n",
      "Epoch 2267 - lr: 0.01000 - Train loss: 0.88644 - Test loss: 0.90027\n",
      "Epoch 2268 - lr: 0.01000 - Train loss: 0.97178 - Test loss: 0.91839\n",
      "Epoch 2269 - lr: 0.01000 - Train loss: 0.91051 - Test loss: 0.90592\n",
      "Epoch 2270 - lr: 0.01000 - Train loss: 0.92551 - Test loss: 0.90164\n",
      "Epoch 2271 - lr: 0.01000 - Train loss: 0.93732 - Test loss: 0.91196\n",
      "Epoch 2272 - lr: 0.01000 - Train loss: 0.90723 - Test loss: 0.90251\n",
      "Epoch 2273 - lr: 0.01000 - Train loss: 0.91099 - Test loss: 0.92368\n",
      "Epoch 2274 - lr: 0.01000 - Train loss: 0.88535 - Test loss: 0.92036\n",
      "Epoch 2275 - lr: 0.01000 - Train loss: 0.90332 - Test loss: 0.90032\n",
      "Epoch 2276 - lr: 0.01000 - Train loss: 0.99686 - Test loss: 0.89510\n",
      "Epoch 2277 - lr: 0.01000 - Train loss: 0.97192 - Test loss: 0.89632\n",
      "Epoch 2278 - lr: 0.01000 - Train loss: 0.90384 - Test loss: 0.90278\n",
      "Epoch 2279 - lr: 0.01000 - Train loss: 0.90947 - Test loss: 0.91781\n",
      "Epoch 2280 - lr: 0.01000 - Train loss: 0.90919 - Test loss: 0.92789\n",
      "Epoch 2281 - lr: 0.01000 - Train loss: 0.87719 - Test loss: 0.91364\n",
      "Epoch 2282 - lr: 0.01000 - Train loss: 0.90208 - Test loss: 0.92145\n",
      "Epoch 2283 - lr: 0.01000 - Train loss: 0.86576 - Test loss: 0.90854\n",
      "Epoch 2284 - lr: 0.01000 - Train loss: 0.89440 - Test loss: 0.89777\n",
      "Epoch 2285 - lr: 0.01000 - Train loss: 0.93753 - Test loss: 0.89677\n",
      "Epoch 2286 - lr: 0.01000 - Train loss: 1.01757 - Test loss: 0.89431\n",
      "Epoch 2287 - lr: 0.01000 - Train loss: 0.99040 - Test loss: 0.91565\n",
      "Epoch 2288 - lr: 0.01000 - Train loss: 0.90174 - Test loss: 0.91638\n",
      "Epoch 2289 - lr: 0.01000 - Train loss: 0.91453 - Test loss: 0.91509\n",
      "Epoch 2290 - lr: 0.01000 - Train loss: 0.91556 - Test loss: 0.91312\n",
      "Epoch 2291 - lr: 0.01000 - Train loss: 0.88105 - Test loss: 0.89201\n",
      "Epoch 2292 - lr: 0.01000 - Train loss: 0.97401 - Test loss: 0.90946\n",
      "Epoch 2293 - lr: 0.01000 - Train loss: 0.88701 - Test loss: 0.89191\n",
      "Epoch 2294 - lr: 0.01000 - Train loss: 0.96799 - Test loss: 0.90977\n",
      "Epoch 2295 - lr: 0.01000 - Train loss: 0.89900 - Test loss: 0.89300\n",
      "Epoch 2296 - lr: 0.01000 - Train loss: 0.97196 - Test loss: 0.88742\n",
      "Epoch 2297 - lr: 0.01000 - Train loss: 0.88951 - Test loss: 0.89052\n",
      "Epoch 2298 - lr: 0.01000 - Train loss: 1.02843 - Test loss: 0.89711\n",
      "Epoch 2299 - lr: 0.01000 - Train loss: 0.97086 - Test loss: 0.90072\n",
      "Epoch 2300 - lr: 0.01000 - Train loss: 0.92312 - Test loss: 0.90003\n",
      "Epoch 2301 - lr: 0.01000 - Train loss: 0.91181 - Test loss: 0.89734\n",
      "Epoch 2302 - lr: 0.01000 - Train loss: 0.88947 - Test loss: 0.89765\n",
      "Epoch 2303 - lr: 0.01000 - Train loss: 0.92740 - Test loss: 0.90027\n",
      "Epoch 2304 - lr: 0.01000 - Train loss: 0.93072 - Test loss: 0.89844\n",
      "Epoch 2305 - lr: 0.01000 - Train loss: 0.94073 - Test loss: 0.90386\n",
      "Epoch 2306 - lr: 0.01000 - Train loss: 0.97889 - Test loss: 0.91490\n",
      "Epoch 2307 - lr: 0.01000 - Train loss: 0.88186 - Test loss: 0.91853\n",
      "Epoch 2308 - lr: 0.01000 - Train loss: 0.90888 - Test loss: 0.90131\n",
      "Epoch 2309 - lr: 0.01000 - Train loss: 0.99742 - Test loss: 0.89546\n",
      "Epoch 2310 - lr: 0.01000 - Train loss: 0.97393 - Test loss: 0.89635\n",
      "Epoch 2311 - lr: 0.01000 - Train loss: 0.91851 - Test loss: 0.89831\n",
      "Epoch 2312 - lr: 0.01000 - Train loss: 0.88804 - Test loss: 0.91089\n",
      "Epoch 2313 - lr: 0.01000 - Train loss: 0.92295 - Test loss: 0.91826\n",
      "Epoch 2314 - lr: 0.01000 - Train loss: 0.90016 - Test loss: 0.92328\n",
      "Epoch 2315 - lr: 0.01000 - Train loss: 0.89721 - Test loss: 0.92031\n",
      "Epoch 2316 - lr: 0.01000 - Train loss: 0.92152 - Test loss: 0.89674\n",
      "Epoch 2317 - lr: 0.01000 - Train loss: 0.90455 - Test loss: 0.89811\n",
      "Epoch 2318 - lr: 0.01000 - Train loss: 0.97732 - Test loss: 0.91204\n",
      "Epoch 2319 - lr: 0.01000 - Train loss: 0.88856 - Test loss: 0.89831\n",
      "Epoch 2320 - lr: 0.01000 - Train loss: 0.96743 - Test loss: 0.89399\n",
      "Epoch 2321 - lr: 0.01000 - Train loss: 0.90439 - Test loss: 0.90026\n",
      "Epoch 2322 - lr: 0.01000 - Train loss: 0.91890 - Test loss: 0.91592\n",
      "Epoch 2323 - lr: 0.01000 - Train loss: 0.92649 - Test loss: 0.90503\n",
      "Epoch 2324 - lr: 0.01000 - Train loss: 0.97828 - Test loss: 0.91905\n",
      "Epoch 2325 - lr: 0.01000 - Train loss: 0.91799 - Test loss: 0.89852\n",
      "Epoch 2326 - lr: 0.01000 - Train loss: 0.88193 - Test loss: 0.89729\n",
      "Epoch 2327 - lr: 0.01000 - Train loss: 0.96306 - Test loss: 0.89788\n",
      "Epoch 2328 - lr: 0.01000 - Train loss: 0.91541 - Test loss: 0.91531\n",
      "Epoch 2329 - lr: 0.01000 - Train loss: 0.92912 - Test loss: 0.90651\n",
      "Epoch 2330 - lr: 0.01000 - Train loss: 0.97391 - Test loss: 0.92050\n",
      "Epoch 2331 - lr: 0.01000 - Train loss: 0.92179 - Test loss: 0.89988\n",
      "Epoch 2332 - lr: 0.01000 - Train loss: 0.88248 - Test loss: 0.89785\n",
      "Epoch 2333 - lr: 0.01000 - Train loss: 0.97588 - Test loss: 0.90992\n",
      "Epoch 2334 - lr: 0.01000 - Train loss: 0.88839 - Test loss: 0.90415\n",
      "Epoch 2335 - lr: 0.01000 - Train loss: 0.92204 - Test loss: 0.92664\n",
      "Epoch 2336 - lr: 0.01000 - Train loss: 0.90513 - Test loss: 0.90342\n",
      "Epoch 2337 - lr: 0.01000 - Train loss: 0.92079 - Test loss: 0.92527\n",
      "Epoch 2338 - lr: 0.01000 - Train loss: 0.89355 - Test loss: 0.90335\n",
      "Epoch 2339 - lr: 0.01000 - Train loss: 0.92381 - Test loss: 0.90099\n",
      "Epoch 2340 - lr: 0.01000 - Train loss: 0.93911 - Test loss: 0.89745\n",
      "Epoch 2341 - lr: 0.01000 - Train loss: 0.93023 - Test loss: 0.90971\n",
      "Epoch 2342 - lr: 0.01000 - Train loss: 0.93454 - Test loss: 0.90761\n",
      "Epoch 2343 - lr: 0.01000 - Train loss: 1.00226 - Test loss: 0.89914\n",
      "Epoch 2344 - lr: 0.01000 - Train loss: 0.89310 - Test loss: 0.91283\n",
      "Epoch 2345 - lr: 0.01000 - Train loss: 0.94227 - Test loss: 0.91078\n",
      "Epoch 2346 - lr: 0.01000 - Train loss: 0.91219 - Test loss: 0.92344\n",
      "Epoch 2347 - lr: 0.01000 - Train loss: 0.92331 - Test loss: 0.89356\n",
      "Epoch 2348 - lr: 0.01000 - Train loss: 0.89031 - Test loss: 0.89180\n",
      "Epoch 2349 - lr: 0.01000 - Train loss: 0.97470 - Test loss: 0.91278\n",
      "Epoch 2350 - lr: 0.01000 - Train loss: 0.95013 - Test loss: 0.89603\n",
      "Epoch 2351 - lr: 0.01000 - Train loss: 1.02140 - Test loss: 0.89596\n",
      "Epoch 2352 - lr: 0.01000 - Train loss: 0.95469 - Test loss: 0.89781\n",
      "Epoch 2353 - lr: 0.01000 - Train loss: 0.91346 - Test loss: 0.92217\n",
      "Epoch 2354 - lr: 0.01000 - Train loss: 0.88142 - Test loss: 0.92004\n",
      "Epoch 2355 - lr: 0.01000 - Train loss: 0.91042 - Test loss: 0.90558\n",
      "Epoch 2356 - lr: 0.01000 - Train loss: 0.90593 - Test loss: 0.91662\n",
      "Epoch 2357 - lr: 0.01000 - Train loss: 0.95946 - Test loss: 0.89725\n",
      "Epoch 2358 - lr: 0.01000 - Train loss: 0.98313 - Test loss: 0.90410\n",
      "Epoch 2359 - lr: 0.01000 - Train loss: 0.92254 - Test loss: 0.90027\n",
      "Epoch 2360 - lr: 0.01000 - Train loss: 0.97840 - Test loss: 0.91657\n",
      "Epoch 2361 - lr: 0.01000 - Train loss: 0.94428 - Test loss: 0.89914\n",
      "Epoch 2362 - lr: 0.01000 - Train loss: 0.96898 - Test loss: 0.89977\n",
      "Epoch 2363 - lr: 0.01000 - Train loss: 0.96734 - Test loss: 0.89380\n",
      "Epoch 2364 - lr: 0.01000 - Train loss: 0.88186 - Test loss: 0.89615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2365 - lr: 0.01000 - Train loss: 0.96949 - Test loss: 0.89670\n",
      "Epoch 2366 - lr: 0.01000 - Train loss: 0.91000 - Test loss: 0.91864\n",
      "Epoch 2367 - lr: 0.01000 - Train loss: 0.87083 - Test loss: 0.91285\n",
      "Epoch 2368 - lr: 0.01000 - Train loss: 0.92071 - Test loss: 0.90449\n",
      "Epoch 2369 - lr: 0.01000 - Train loss: 0.96936 - Test loss: 0.91425\n",
      "Epoch 2370 - lr: 0.01000 - Train loss: 0.90298 - Test loss: 0.90138\n",
      "Epoch 2371 - lr: 0.01000 - Train loss: 0.92875 - Test loss: 0.89919\n",
      "Epoch 2372 - lr: 0.01000 - Train loss: 0.91135 - Test loss: 0.89930\n",
      "Epoch 2373 - lr: 0.01000 - Train loss: 0.90119 - Test loss: 0.91531\n",
      "Epoch 2374 - lr: 0.01000 - Train loss: 0.92162 - Test loss: 0.92845\n",
      "Epoch 2375 - lr: 0.01000 - Train loss: 0.93369 - Test loss: 0.92038\n",
      "Epoch 2376 - lr: 0.01000 - Train loss: 0.89393 - Test loss: 0.90655\n",
      "Epoch 2377 - lr: 0.01000 - Train loss: 0.90843 - Test loss: 0.92258\n",
      "Epoch 2378 - lr: 0.01000 - Train loss: 0.91488 - Test loss: 0.89531\n",
      "Epoch 2379 - lr: 0.01000 - Train loss: 0.91058 - Test loss: 0.89621\n",
      "Epoch 2380 - lr: 0.01000 - Train loss: 0.97855 - Test loss: 0.91355\n",
      "Epoch 2381 - lr: 0.01000 - Train loss: 0.87638 - Test loss: 0.89889\n",
      "Epoch 2382 - lr: 0.01000 - Train loss: 0.94641 - Test loss: 0.89388\n",
      "Epoch 2383 - lr: 0.01000 - Train loss: 1.00389 - Test loss: 0.91619\n",
      "Epoch 2384 - lr: 0.01000 - Train loss: 0.87510 - Test loss: 0.89994\n",
      "Epoch 2385 - lr: 0.01000 - Train loss: 0.95437 - Test loss: 0.89467\n",
      "Epoch 2386 - lr: 0.01000 - Train loss: 0.93695 - Test loss: 0.89538\n",
      "Epoch 2387 - lr: 0.01000 - Train loss: 0.91036 - Test loss: 0.89927\n",
      "Epoch 2388 - lr: 0.01000 - Train loss: 0.98211 - Test loss: 0.91648\n",
      "Epoch 2389 - lr: 0.01000 - Train loss: 0.91106 - Test loss: 0.89761\n",
      "Epoch 2390 - lr: 0.01000 - Train loss: 0.89746 - Test loss: 0.90020\n",
      "Epoch 2391 - lr: 0.01000 - Train loss: 0.93094 - Test loss: 0.90265\n",
      "Epoch 2392 - lr: 0.01000 - Train loss: 0.97260 - Test loss: 0.91928\n",
      "Epoch 2393 - lr: 0.01000 - Train loss: 0.87736 - Test loss: 0.90340\n",
      "Epoch 2394 - lr: 0.01000 - Train loss: 0.88262 - Test loss: 0.90030\n",
      "Epoch 2395 - lr: 0.01000 - Train loss: 0.91582 - Test loss: 0.90153\n",
      "Epoch 2396 - lr: 0.01000 - Train loss: 0.97152 - Test loss: 0.91862\n",
      "Epoch 2397 - lr: 0.01000 - Train loss: 0.87879 - Test loss: 0.90264\n",
      "Epoch 2398 - lr: 0.01000 - Train loss: 0.93869 - Test loss: 0.92165\n",
      "Epoch 2399 - lr: 0.01000 - Train loss: 0.88112 - Test loss: 0.90327\n",
      "Epoch 2400 - lr: 0.01000 - Train loss: 0.93478 - Test loss: 0.90591\n",
      "Epoch 2401 - lr: 0.01000 - Train loss: 0.98863 - Test loss: 0.91234\n",
      "Epoch 2402 - lr: 0.01000 - Train loss: 0.92414 - Test loss: 0.90111\n",
      "Epoch 2403 - lr: 0.01000 - Train loss: 0.97063 - Test loss: 0.91609\n",
      "Epoch 2404 - lr: 0.01000 - Train loss: 0.97154 - Test loss: 0.89967\n",
      "Epoch 2405 - lr: 0.01000 - Train loss: 1.02092 - Test loss: 0.89729\n",
      "Epoch 2406 - lr: 0.01000 - Train loss: 0.95194 - Test loss: 0.89895\n",
      "Epoch 2407 - lr: 0.01000 - Train loss: 0.90222 - Test loss: 0.91721\n",
      "Epoch 2408 - lr: 0.01000 - Train loss: 0.91138 - Test loss: 0.90497\n",
      "Epoch 2409 - lr: 0.01000 - Train loss: 0.98001 - Test loss: 0.90209\n",
      "Epoch 2410 - lr: 0.01000 - Train loss: 0.98065 - Test loss: 0.91891\n",
      "Epoch 2411 - lr: 0.01000 - Train loss: 0.88442 - Test loss: 0.90337\n",
      "Epoch 2412 - lr: 0.01000 - Train loss: 0.91045 - Test loss: 0.92479\n",
      "Epoch 2413 - lr: 0.01000 - Train loss: 0.92204 - Test loss: 0.89779\n",
      "Epoch 2414 - lr: 0.01000 - Train loss: 0.90363 - Test loss: 0.89855\n",
      "Epoch 2415 - lr: 0.01000 - Train loss: 1.02182 - Test loss: 0.89987\n",
      "Epoch 2416 - lr: 0.01000 - Train loss: 0.97931 - Test loss: 0.89974\n",
      "Epoch 2417 - lr: 0.01000 - Train loss: 0.92738 - Test loss: 0.89914\n",
      "Epoch 2418 - lr: 0.01000 - Train loss: 0.90544 - Test loss: 0.90070\n",
      "Epoch 2419 - lr: 0.01000 - Train loss: 0.94518 - Test loss: 0.91035\n",
      "Epoch 2420 - lr: 0.01000 - Train loss: 0.95680 - Test loss: 0.90395\n",
      "Epoch 2421 - lr: 0.01000 - Train loss: 1.00542 - Test loss: 0.90703\n",
      "Epoch 2422 - lr: 0.01000 - Train loss: 0.93704 - Test loss: 0.90817\n",
      "Epoch 2423 - lr: 0.01000 - Train loss: 0.92911 - Test loss: 0.93410\n",
      "Epoch 2424 - lr: 0.01000 - Train loss: 0.94669 - Test loss: 0.91125\n",
      "Epoch 2425 - lr: 0.01000 - Train loss: 0.95102 - Test loss: 0.90900\n",
      "Epoch 2426 - lr: 0.01000 - Train loss: 0.92694 - Test loss: 0.91361\n",
      "Epoch 2427 - lr: 0.01000 - Train loss: 0.92587 - Test loss: 0.90975\n",
      "Epoch 2428 - lr: 0.01000 - Train loss: 0.93843 - Test loss: 0.93254\n",
      "Epoch 2429 - lr: 0.01000 - Train loss: 0.88017 - Test loss: 0.91461\n",
      "Epoch 2430 - lr: 0.01000 - Train loss: 0.91613 - Test loss: 0.91138\n",
      "Epoch 2431 - lr: 0.01000 - Train loss: 0.92832 - Test loss: 0.90881\n",
      "Epoch 2432 - lr: 0.01000 - Train loss: 0.89767 - Test loss: 0.92459\n",
      "Epoch 2433 - lr: 0.01000 - Train loss: 0.88965 - Test loss: 0.92641\n",
      "Epoch 2434 - lr: 0.01000 - Train loss: 0.88193 - Test loss: 0.92451\n",
      "Epoch 2435 - lr: 0.01000 - Train loss: 0.89951 - Test loss: 0.92440\n",
      "Epoch 2436 - lr: 0.01000 - Train loss: 0.92624 - Test loss: 0.90441\n",
      "Epoch 2437 - lr: 0.01000 - Train loss: 0.88730 - Test loss: 0.89899\n",
      "Epoch 2438 - lr: 0.01000 - Train loss: 1.02063 - Test loss: 0.90482\n",
      "Epoch 2439 - lr: 0.01000 - Train loss: 0.90773 - Test loss: 0.92075\n",
      "Epoch 2440 - lr: 0.01000 - Train loss: 0.88582 - Test loss: 0.90543\n",
      "Epoch 2441 - lr: 0.01000 - Train loss: 0.92138 - Test loss: 0.92738\n",
      "Epoch 2442 - lr: 0.01000 - Train loss: 0.97874 - Test loss: 0.90985\n",
      "Epoch 2443 - lr: 0.01000 - Train loss: 0.91971 - Test loss: 0.92646\n",
      "Epoch 2444 - lr: 0.01000 - Train loss: 0.90178 - Test loss: 0.90252\n",
      "Epoch 2445 - lr: 0.01000 - Train loss: 0.93915 - Test loss: 0.90836\n",
      "Epoch 2446 - lr: 0.01000 - Train loss: 0.86551 - Test loss: 0.91104\n",
      "Epoch 2447 - lr: 0.01000 - Train loss: 0.88447 - Test loss: 0.91596\n",
      "Epoch 2448 - lr: 0.01000 - Train loss: 0.88463 - Test loss: 0.89793\n",
      "Epoch 2449 - lr: 0.01000 - Train loss: 0.91667 - Test loss: 0.92016\n",
      "Epoch 2450 - lr: 0.01000 - Train loss: 0.93516 - Test loss: 0.92095\n",
      "Epoch 2451 - lr: 0.01000 - Train loss: 0.93772 - Test loss: 0.92017\n",
      "Epoch 2452 - lr: 0.01000 - Train loss: 0.92752 - Test loss: 0.91983\n",
      "Epoch 2453 - lr: 0.01000 - Train loss: 0.93385 - Test loss: 0.91847\n",
      "Epoch 2454 - lr: 0.01000 - Train loss: 0.93702 - Test loss: 0.91741\n",
      "Epoch 2455 - lr: 0.01000 - Train loss: 0.90099 - Test loss: 0.91316\n",
      "Epoch 2456 - lr: 0.01000 - Train loss: 0.90751 - Test loss: 0.89224\n",
      "Epoch 2457 - lr: 0.01000 - Train loss: 0.95616 - Test loss: 0.90312\n",
      "Epoch 2458 - lr: 0.01000 - Train loss: 0.92871 - Test loss: 0.89129\n",
      "Epoch 2459 - lr: 0.01000 - Train loss: 1.00322 - Test loss: 0.88178\n",
      "Epoch 2460 - lr: 0.01000 - Train loss: 0.90070 - Test loss: 0.90204\n",
      "Epoch 2461 - lr: 0.01000 - Train loss: 0.87823 - Test loss: 0.89092\n",
      "Epoch 2462 - lr: 0.01000 - Train loss: 0.93228 - Test loss: 0.89161\n",
      "Epoch 2463 - lr: 0.01000 - Train loss: 0.97356 - Test loss: 0.88331\n",
      "Epoch 2464 - lr: 0.01000 - Train loss: 0.89251 - Test loss: 0.88651\n",
      "Epoch 2465 - lr: 0.01000 - Train loss: 0.94360 - Test loss: 0.89043\n",
      "Epoch 2466 - lr: 0.01000 - Train loss: 0.89355 - Test loss: 0.88872\n",
      "Epoch 2467 - lr: 0.01000 - Train loss: 0.97270 - Test loss: 0.90691\n",
      "Epoch 2468 - lr: 0.01000 - Train loss: 0.97255 - Test loss: 0.89429\n",
      "Epoch 2469 - lr: 0.01000 - Train loss: 0.98547 - Test loss: 0.91197\n",
      "Epoch 2470 - lr: 0.01000 - Train loss: 0.95851 - Test loss: 0.89484\n",
      "Epoch 2471 - lr: 0.01000 - Train loss: 0.97035 - Test loss: 0.91134\n",
      "Epoch 2472 - lr: 0.01000 - Train loss: 0.95335 - Test loss: 0.89560\n",
      "Epoch 2473 - lr: 0.01000 - Train loss: 1.02777 - Test loss: 0.89778\n",
      "Epoch 2474 - lr: 0.01000 - Train loss: 0.96835 - Test loss: 0.89395\n",
      "Epoch 2475 - lr: 0.01000 - Train loss: 0.88028 - Test loss: 0.90049\n",
      "Epoch 2476 - lr: 0.01000 - Train loss: 0.91899 - Test loss: 0.89716\n",
      "Epoch 2477 - lr: 0.01000 - Train loss: 0.88649 - Test loss: 0.91723\n",
      "Epoch 2478 - lr: 0.01000 - Train loss: 0.91828 - Test loss: 0.89948\n",
      "Epoch 2479 - lr: 0.01000 - Train loss: 0.90954 - Test loss: 0.90383\n",
      "Epoch 2480 - lr: 0.01000 - Train loss: 1.01470 - Test loss: 0.90175\n",
      "Epoch 2481 - lr: 0.01000 - Train loss: 0.92537 - Test loss: 0.90267\n",
      "Epoch 2482 - lr: 0.01000 - Train loss: 0.94192 - Test loss: 0.92419\n",
      "Epoch 2483 - lr: 0.01000 - Train loss: 0.93217 - Test loss: 0.90261\n",
      "Epoch 2484 - lr: 0.01000 - Train loss: 0.97771 - Test loss: 0.92111\n",
      "Epoch 2485 - lr: 0.01000 - Train loss: 0.87442 - Test loss: 0.91791\n",
      "Epoch 2486 - lr: 0.01000 - Train loss: 0.90805 - Test loss: 0.92415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2487 - lr: 0.01000 - Train loss: 0.95574 - Test loss: 0.90371\n",
      "Epoch 2488 - lr: 0.01000 - Train loss: 0.97427 - Test loss: 0.92087\n",
      "Epoch 2489 - lr: 0.01000 - Train loss: 0.90208 - Test loss: 0.90240\n",
      "Epoch 2490 - lr: 0.01000 - Train loss: 0.91733 - Test loss: 0.90517\n",
      "Epoch 2491 - lr: 0.01000 - Train loss: 0.97887 - Test loss: 0.92152\n",
      "Epoch 2492 - lr: 0.01000 - Train loss: 0.89018 - Test loss: 0.90464\n",
      "Epoch 2493 - lr: 0.01000 - Train loss: 0.92202 - Test loss: 0.92822\n",
      "Epoch 2494 - lr: 0.01000 - Train loss: 0.93376 - Test loss: 0.90497\n",
      "Epoch 2495 - lr: 0.01000 - Train loss: 0.93034 - Test loss: 0.89798\n",
      "Epoch 2496 - lr: 0.01000 - Train loss: 0.97681 - Test loss: 0.89842\n",
      "Epoch 2497 - lr: 0.01000 - Train loss: 0.88538 - Test loss: 0.90145\n",
      "Epoch 2498 - lr: 0.01000 - Train loss: 0.96987 - Test loss: 0.90168\n",
      "Epoch 2499 - lr: 0.01000 - Train loss: 0.92439 - Test loss: 0.90332\n",
      "Epoch 2500 - lr: 0.01000 - Train loss: 0.92370 - Test loss: 0.90417\n",
      "Epoch 2501 - lr: 0.01000 - Train loss: 0.91866 - Test loss: 0.90531\n",
      "Epoch 2502 - lr: 0.01000 - Train loss: 0.92359 - Test loss: 0.90719\n",
      "Epoch 2503 - lr: 0.01000 - Train loss: 0.92066 - Test loss: 0.91138\n",
      "Epoch 2504 - lr: 0.01000 - Train loss: 0.96254 - Test loss: 0.91312\n",
      "Epoch 2505 - lr: 0.01000 - Train loss: 0.96434 - Test loss: 0.91278\n",
      "Epoch 2506 - lr: 0.01000 - Train loss: 0.93558 - Test loss: 0.91501\n",
      "Epoch 2507 - lr: 0.01000 - Train loss: 1.00931 - Test loss: 0.91975\n",
      "Epoch 2508 - lr: 0.01000 - Train loss: 0.96259 - Test loss: 0.94105\n",
      "Epoch 2509 - lr: 0.01000 - Train loss: 0.90814 - Test loss: 0.93859\n",
      "Epoch 2510 - lr: 0.01000 - Train loss: 0.91886 - Test loss: 0.91480\n",
      "Epoch 2511 - lr: 0.01000 - Train loss: 0.92216 - Test loss: 0.91176\n",
      "Epoch 2512 - lr: 0.01000 - Train loss: 0.92114 - Test loss: 0.91466\n",
      "Epoch 2513 - lr: 0.01000 - Train loss: 0.96390 - Test loss: 0.91684\n",
      "Epoch 2514 - lr: 0.01000 - Train loss: 0.94749 - Test loss: 0.91604\n",
      "Epoch 2515 - lr: 0.01000 - Train loss: 0.92764 - Test loss: 0.92949\n",
      "Epoch 2516 - lr: 0.01000 - Train loss: 0.91515 - Test loss: 0.92164\n",
      "Epoch 2517 - lr: 0.01000 - Train loss: 0.94601 - Test loss: 0.91599\n",
      "Epoch 2518 - lr: 0.01000 - Train loss: 0.92253 - Test loss: 0.94027\n",
      "Epoch 2519 - lr: 0.01000 - Train loss: 0.86468 - Test loss: 0.91763\n",
      "Epoch 2520 - lr: 0.01000 - Train loss: 0.91911 - Test loss: 0.91640\n",
      "Epoch 2521 - lr: 0.01000 - Train loss: 0.92372 - Test loss: 0.91428\n",
      "Epoch 2522 - lr: 0.01000 - Train loss: 0.91495 - Test loss: 0.91629\n",
      "Epoch 2523 - lr: 0.01000 - Train loss: 0.95231 - Test loss: 0.91232\n",
      "Epoch 2524 - lr: 0.01000 - Train loss: 0.93800 - Test loss: 0.92093\n",
      "Epoch 2525 - lr: 0.01000 - Train loss: 0.92937 - Test loss: 0.91348\n",
      "Epoch 2526 - lr: 0.01000 - Train loss: 0.92631 - Test loss: 0.93844\n",
      "Epoch 2527 - lr: 0.01000 - Train loss: 0.91805 - Test loss: 0.91353\n",
      "Epoch 2528 - lr: 0.01000 - Train loss: 0.89401 - Test loss: 0.92965\n",
      "Epoch 2529 - lr: 0.01000 - Train loss: 0.88582 - Test loss: 0.91314\n",
      "Epoch 2530 - lr: 0.01000 - Train loss: 0.92526 - Test loss: 0.93588\n",
      "Epoch 2531 - lr: 0.01000 - Train loss: 0.90068 - Test loss: 0.91298\n",
      "Epoch 2532 - lr: 0.01000 - Train loss: 0.92452 - Test loss: 0.93438\n",
      "Epoch 2533 - lr: 0.01000 - Train loss: 0.87522 - Test loss: 0.91235\n",
      "Epoch 2534 - lr: 0.01000 - Train loss: 0.91894 - Test loss: 0.90320\n",
      "Epoch 2535 - lr: 0.01000 - Train loss: 0.93757 - Test loss: 0.90788\n",
      "Epoch 2536 - lr: 0.01000 - Train loss: 0.91920 - Test loss: 0.90905\n",
      "Epoch 2537 - lr: 0.01000 - Train loss: 0.92371 - Test loss: 0.90697\n",
      "Epoch 2538 - lr: 0.01000 - Train loss: 0.91846 - Test loss: 0.90522\n",
      "Epoch 2539 - lr: 0.01000 - Train loss: 0.87257 - Test loss: 0.90709\n",
      "Epoch 2540 - lr: 0.01000 - Train loss: 0.91454 - Test loss: 0.93133\n",
      "Epoch 2541 - lr: 0.01000 - Train loss: 0.92239 - Test loss: 0.90690\n",
      "Epoch 2542 - lr: 0.01000 - Train loss: 0.96564 - Test loss: 0.90545\n",
      "Epoch 2543 - lr: 0.01000 - Train loss: 0.92719 - Test loss: 0.90782\n",
      "Epoch 2544 - lr: 0.01000 - Train loss: 0.93659 - Test loss: 0.91705\n",
      "Epoch 2545 - lr: 0.01000 - Train loss: 0.87899 - Test loss: 0.91024\n",
      "Epoch 2546 - lr: 0.01000 - Train loss: 0.87781 - Test loss: 0.90853\n",
      "Epoch 2547 - lr: 0.01000 - Train loss: 0.89005 - Test loss: 0.90796\n",
      "Epoch 2548 - lr: 0.01000 - Train loss: 0.92220 - Test loss: 0.93110\n",
      "Epoch 2549 - lr: 0.01000 - Train loss: 0.89859 - Test loss: 0.90899\n",
      "Epoch 2550 - lr: 0.01000 - Train loss: 0.90259 - Test loss: 0.92432\n",
      "Epoch 2551 - lr: 0.01000 - Train loss: 0.88891 - Test loss: 0.92384\n",
      "Epoch 2552 - lr: 0.01000 - Train loss: 0.90979 - Test loss: 0.90240\n",
      "Epoch 2553 - lr: 0.01000 - Train loss: 0.93502 - Test loss: 0.90916\n",
      "Epoch 2554 - lr: 0.01000 - Train loss: 1.00798 - Test loss: 0.90208\n",
      "Epoch 2555 - lr: 0.01000 - Train loss: 0.93087 - Test loss: 0.90132\n",
      "Epoch 2556 - lr: 0.01000 - Train loss: 0.90181 - Test loss: 0.91712\n",
      "Epoch 2557 - lr: 0.01000 - Train loss: 0.87848 - Test loss: 0.90487\n",
      "Epoch 2558 - lr: 0.01000 - Train loss: 0.91980 - Test loss: 0.90341\n",
      "Epoch 2559 - lr: 0.01000 - Train loss: 0.97611 - Test loss: 0.90998\n",
      "Epoch 2560 - lr: 0.01000 - Train loss: 0.92086 - Test loss: 0.92795\n",
      "Epoch 2561 - lr: 0.01000 - Train loss: 0.93716 - Test loss: 0.92715\n",
      "Epoch 2562 - lr: 0.01000 - Train loss: 0.93846 - Test loss: 0.92450\n",
      "Epoch 2563 - lr: 0.01000 - Train loss: 0.86942 - Test loss: 0.89928\n",
      "Epoch 2564 - lr: 0.01000 - Train loss: 0.90990 - Test loss: 0.92166\n",
      "Epoch 2565 - lr: 0.01000 - Train loss: 0.87419 - Test loss: 0.90023\n",
      "Epoch 2566 - lr: 0.01000 - Train loss: 0.92379 - Test loss: 0.89164\n",
      "Epoch 2567 - lr: 0.01000 - Train loss: 0.88781 - Test loss: 0.89216\n",
      "Epoch 2568 - lr: 0.01000 - Train loss: 0.92803 - Test loss: 0.89470\n",
      "Epoch 2569 - lr: 0.01000 - Train loss: 0.91838 - Test loss: 0.90455\n",
      "Epoch 2570 - lr: 0.01000 - Train loss: 0.89399 - Test loss: 0.91938\n",
      "Epoch 2571 - lr: 0.01000 - Train loss: 0.88499 - Test loss: 0.91718\n",
      "Epoch 2572 - lr: 0.01000 - Train loss: 0.89490 - Test loss: 0.90043\n",
      "Epoch 2573 - lr: 0.01000 - Train loss: 0.94581 - Test loss: 0.89829\n",
      "Epoch 2574 - lr: 0.01000 - Train loss: 0.95058 - Test loss: 0.89312\n",
      "Epoch 2575 - lr: 0.01000 - Train loss: 0.90608 - Test loss: 0.91790\n",
      "Epoch 2576 - lr: 0.01000 - Train loss: 0.92101 - Test loss: 0.89217\n",
      "Epoch 2577 - lr: 0.01000 - Train loss: 0.89034 - Test loss: 0.89125\n",
      "Epoch 2578 - lr: 0.01000 - Train loss: 0.96796 - Test loss: 0.91260\n",
      "Epoch 2579 - lr: 0.01000 - Train loss: 0.96529 - Test loss: 0.89674\n",
      "Epoch 2580 - lr: 0.01000 - Train loss: 0.91833 - Test loss: 0.89458\n",
      "Epoch 2581 - lr: 0.01000 - Train loss: 0.91510 - Test loss: 0.89749\n",
      "Epoch 2582 - lr: 0.01000 - Train loss: 0.98073 - Test loss: 0.91160\n",
      "Epoch 2583 - lr: 0.01000 - Train loss: 0.86719 - Test loss: 0.91328\n",
      "Epoch 2584 - lr: 0.01000 - Train loss: 0.88909 - Test loss: 0.89607\n",
      "Epoch 2585 - lr: 0.01000 - Train loss: 0.96968 - Test loss: 0.89122\n",
      "Epoch 2586 - lr: 0.01000 - Train loss: 0.88202 - Test loss: 0.89405\n",
      "Epoch 2587 - lr: 0.01000 - Train loss: 0.94427 - Test loss: 0.89742\n",
      "Epoch 2588 - lr: 0.01000 - Train loss: 0.91794 - Test loss: 0.92349\n",
      "Epoch 2589 - lr: 0.01000 - Train loss: 0.89987 - Test loss: 0.90271\n",
      "Epoch 2590 - lr: 0.01000 - Train loss: 0.92806 - Test loss: 0.90228\n",
      "Epoch 2591 - lr: 0.01000 - Train loss: 0.96837 - Test loss: 0.91868\n",
      "Epoch 2592 - lr: 0.01000 - Train loss: 0.95928 - Test loss: 0.90062\n",
      "Epoch 2593 - lr: 0.01000 - Train loss: 0.95348 - Test loss: 0.89640\n",
      "Epoch 2594 - lr: 0.01000 - Train loss: 0.94079 - Test loss: 0.90819\n",
      "Epoch 2595 - lr: 0.01000 - Train loss: 0.90921 - Test loss: 0.90425\n",
      "Epoch 2596 - lr: 0.01000 - Train loss: 0.97354 - Test loss: 0.92628\n",
      "Epoch 2597 - lr: 0.01000 - Train loss: 0.93369 - Test loss: 0.92257\n",
      "Epoch 2598 - lr: 0.01000 - Train loss: 0.89264 - Test loss: 0.92092\n",
      "Epoch 2599 - lr: 0.01000 - Train loss: 0.98004 - Test loss: 0.90556\n",
      "Epoch 2600 - lr: 0.01000 - Train loss: 0.97107 - Test loss: 0.92495\n",
      "Epoch 2601 - lr: 0.01000 - Train loss: 0.93805 - Test loss: 0.92493\n",
      "Epoch 2602 - lr: 0.01000 - Train loss: 0.97990 - Test loss: 0.90585\n",
      "Epoch 2603 - lr: 0.01000 - Train loss: 0.95259 - Test loss: 0.90053\n",
      "Epoch 2604 - lr: 0.01000 - Train loss: 0.97620 - Test loss: 0.91582\n",
      "Epoch 2605 - lr: 0.01000 - Train loss: 0.92062 - Test loss: 0.89640\n",
      "Epoch 2606 - lr: 0.01000 - Train loss: 0.85949 - Test loss: 0.89665\n",
      "Epoch 2607 - lr: 0.01000 - Train loss: 0.93400 - Test loss: 0.89357\n",
      "Epoch 2608 - lr: 0.01000 - Train loss: 0.98121 - Test loss: 0.91422\n",
      "Epoch 2609 - lr: 0.01000 - Train loss: 0.86640 - Test loss: 0.91442\n",
      "Epoch 2610 - lr: 0.01000 - Train loss: 0.89615 - Test loss: 0.91880\n",
      "Epoch 2611 - lr: 0.01000 - Train loss: 0.96638 - Test loss: 0.90057\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2612 - lr: 0.01000 - Train loss: 0.97088 - Test loss: 0.91821\n",
      "Epoch 2613 - lr: 0.01000 - Train loss: 0.94657 - Test loss: 0.90053\n",
      "Epoch 2614 - lr: 0.01000 - Train loss: 0.94147 - Test loss: 0.89977\n",
      "Epoch 2615 - lr: 0.01000 - Train loss: 0.92372 - Test loss: 0.92516\n",
      "Epoch 2616 - lr: 0.01000 - Train loss: 0.93843 - Test loss: 0.92632\n",
      "Epoch 2617 - lr: 0.01000 - Train loss: 0.88029 - Test loss: 0.90259\n",
      "Epoch 2618 - lr: 0.01000 - Train loss: 0.91740 - Test loss: 0.90125\n",
      "Epoch 2619 - lr: 0.01000 - Train loss: 0.91367 - Test loss: 0.89737\n",
      "Epoch 2620 - lr: 0.01000 - Train loss: 0.92390 - Test loss: 0.89495\n",
      "Epoch 2621 - lr: 0.01000 - Train loss: 0.88467 - Test loss: 0.89635\n",
      "Epoch 2622 - lr: 0.01000 - Train loss: 0.93371 - Test loss: 0.89829\n",
      "Epoch 2623 - lr: 0.01000 - Train loss: 0.91895 - Test loss: 0.90302\n",
      "Epoch 2624 - lr: 0.01000 - Train loss: 0.97326 - Test loss: 0.92055\n",
      "Epoch 2625 - lr: 0.01000 - Train loss: 0.87925 - Test loss: 0.90495\n",
      "Epoch 2626 - lr: 0.01000 - Train loss: 0.91492 - Test loss: 0.90347\n",
      "Epoch 2627 - lr: 0.01000 - Train loss: 1.01842 - Test loss: 0.90613\n",
      "Epoch 2628 - lr: 0.01000 - Train loss: 0.88547 - Test loss: 0.92036\n",
      "Epoch 2629 - lr: 0.01000 - Train loss: 0.91810 - Test loss: 0.90046\n",
      "Epoch 2630 - lr: 0.01000 - Train loss: 0.88809 - Test loss: 0.90900\n",
      "Epoch 2631 - lr: 0.01000 - Train loss: 0.91875 - Test loss: 0.92811\n",
      "Epoch 2632 - lr: 0.01000 - Train loss: 0.97559 - Test loss: 0.90662\n",
      "Epoch 2633 - lr: 0.01000 - Train loss: 0.96924 - Test loss: 0.92128\n",
      "Epoch 2634 - lr: 0.01000 - Train loss: 0.88869 - Test loss: 0.90657\n",
      "Epoch 2635 - lr: 0.01000 - Train loss: 0.91486 - Test loss: 0.91605\n",
      "Epoch 2636 - lr: 0.01000 - Train loss: 0.93310 - Test loss: 0.90453\n",
      "Epoch 2637 - lr: 0.01000 - Train loss: 0.91520 - Test loss: 0.89864\n",
      "Epoch 2638 - lr: 0.01000 - Train loss: 0.92028 - Test loss: 0.90066\n",
      "Epoch 2639 - lr: 0.01000 - Train loss: 0.92149 - Test loss: 0.89925\n",
      "Epoch 2640 - lr: 0.01000 - Train loss: 0.92309 - Test loss: 0.92390\n",
      "Epoch 2641 - lr: 0.01000 - Train loss: 0.93162 - Test loss: 0.92580\n",
      "Epoch 2642 - lr: 0.01000 - Train loss: 0.93025 - Test loss: 0.92528\n",
      "Epoch 2643 - lr: 0.01000 - Train loss: 0.92562 - Test loss: 0.92449\n",
      "Epoch 2644 - lr: 0.01000 - Train loss: 0.89691 - Test loss: 0.91709\n",
      "Epoch 2645 - lr: 0.01000 - Train loss: 0.87519 - Test loss: 0.90591\n",
      "Epoch 2646 - lr: 0.01000 - Train loss: 0.93905 - Test loss: 0.89535\n",
      "Epoch 2647 - lr: 0.01000 - Train loss: 0.96917 - Test loss: 0.88633\n",
      "Epoch 2648 - lr: 0.01000 - Train loss: 0.89479 - Test loss: 0.88973\n",
      "Epoch 2649 - lr: 0.01000 - Train loss: 0.96110 - Test loss: 0.88983\n",
      "Epoch 2650 - lr: 0.01000 - Train loss: 0.93006 - Test loss: 0.89296\n",
      "Epoch 2651 - lr: 0.01000 - Train loss: 0.95254 - Test loss: 0.89351\n",
      "Epoch 2652 - lr: 0.01000 - Train loss: 0.96173 - Test loss: 0.89606\n",
      "Epoch 2653 - lr: 0.01000 - Train loss: 0.93440 - Test loss: 0.90664\n",
      "Epoch 2654 - lr: 0.01000 - Train loss: 0.91878 - Test loss: 0.90174\n",
      "Epoch 2655 - lr: 0.01000 - Train loss: 0.91156 - Test loss: 0.89829\n",
      "Epoch 2656 - lr: 0.01000 - Train loss: 0.90581 - Test loss: 0.89617\n",
      "Epoch 2657 - lr: 0.01000 - Train loss: 0.85986 - Test loss: 0.89769\n",
      "Epoch 2658 - lr: 0.01000 - Train loss: 0.86200 - Test loss: 0.89758\n",
      "Epoch 2659 - lr: 0.01000 - Train loss: 0.89588 - Test loss: 0.89990\n",
      "Epoch 2660 - lr: 0.01000 - Train loss: 0.92543 - Test loss: 0.90184\n",
      "Epoch 2661 - lr: 0.01000 - Train loss: 0.99467 - Test loss: 0.89727\n",
      "Epoch 2662 - lr: 0.01000 - Train loss: 0.95303 - Test loss: 0.89650\n",
      "Epoch 2663 - lr: 0.01000 - Train loss: 0.89245 - Test loss: 0.90285\n",
      "Epoch 2664 - lr: 0.01000 - Train loss: 0.92023 - Test loss: 0.92779\n",
      "Epoch 2665 - lr: 0.01000 - Train loss: 0.91098 - Test loss: 0.90647\n",
      "Epoch 2666 - lr: 0.01000 - Train loss: 0.92913 - Test loss: 0.91173\n",
      "Epoch 2667 - lr: 0.01000 - Train loss: 0.86499 - Test loss: 0.91190\n",
      "Epoch 2668 - lr: 0.01000 - Train loss: 0.86023 - Test loss: 0.89947\n",
      "Epoch 2669 - lr: 0.01000 - Train loss: 0.90483 - Test loss: 0.89844\n",
      "Epoch 2670 - lr: 0.01000 - Train loss: 0.91726 - Test loss: 0.92335\n",
      "Epoch 2671 - lr: 0.01000 - Train loss: 0.94695 - Test loss: 0.90003\n",
      "Epoch 2672 - lr: 0.01000 - Train loss: 0.92262 - Test loss: 0.89933\n",
      "Epoch 2673 - lr: 0.01000 - Train loss: 0.92075 - Test loss: 0.92405\n",
      "Epoch 2674 - lr: 0.01000 - Train loss: 0.92409 - Test loss: 0.92631\n",
      "Epoch 2675 - lr: 0.01000 - Train loss: 0.90663 - Test loss: 0.92321\n",
      "Epoch 2676 - lr: 0.01000 - Train loss: 0.87589 - Test loss: 0.91640\n",
      "Epoch 2677 - lr: 0.01000 - Train loss: 0.88993 - Test loss: 0.89379\n",
      "Epoch 2678 - lr: 0.01000 - Train loss: 0.95865 - Test loss: 0.89050\n",
      "Epoch 2679 - lr: 0.01000 - Train loss: 0.97088 - Test loss: 0.91233\n",
      "Epoch 2680 - lr: 0.01000 - Train loss: 0.93521 - Test loss: 0.91653\n",
      "Epoch 2681 - lr: 0.01000 - Train loss: 0.90036 - Test loss: 0.91541\n",
      "Epoch 2682 - lr: 0.01000 - Train loss: 0.93161 - Test loss: 0.91354\n",
      "Epoch 2683 - lr: 0.01000 - Train loss: 0.89027 - Test loss: 0.89300\n",
      "Epoch 2684 - lr: 0.01000 - Train loss: 0.91320 - Test loss: 0.89199\n",
      "Epoch 2685 - lr: 0.01000 - Train loss: 0.88726 - Test loss: 0.89056\n",
      "Epoch 2686 - lr: 0.01000 - Train loss: 0.92923 - Test loss: 0.89272\n",
      "Epoch 2687 - lr: 0.01000 - Train loss: 0.89038 - Test loss: 0.90436\n",
      "Epoch 2688 - lr: 0.01000 - Train loss: 0.94584 - Test loss: 0.89989\n",
      "Epoch 2689 - lr: 0.01000 - Train loss: 0.96085 - Test loss: 0.91493\n",
      "Epoch 2690 - lr: 0.01000 - Train loss: 0.93720 - Test loss: 0.89654\n",
      "Epoch 2691 - lr: 0.01000 - Train loss: 0.93365 - Test loss: 0.91690\n",
      "Epoch 2692 - lr: 0.01000 - Train loss: 0.93542 - Test loss: 0.91751\n",
      "Epoch 2693 - lr: 0.01000 - Train loss: 0.89751 - Test loss: 0.91557\n",
      "Epoch 2694 - lr: 0.01000 - Train loss: 0.93485 - Test loss: 0.91550\n",
      "Epoch 2695 - lr: 0.01000 - Train loss: 0.89910 - Test loss: 0.91375\n",
      "Epoch 2696 - lr: 0.01000 - Train loss: 0.93845 - Test loss: 0.91661\n",
      "Epoch 2697 - lr: 0.01000 - Train loss: 0.90853 - Test loss: 0.89069\n",
      "Epoch 2698 - lr: 0.01000 - Train loss: 0.90863 - Test loss: 0.89289\n",
      "Epoch 2699 - lr: 0.01000 - Train loss: 0.95654 - Test loss: 0.91023\n",
      "Epoch 2700 - lr: 0.01000 - Train loss: 0.86869 - Test loss: 0.89480\n",
      "Epoch 2701 - lr: 0.01000 - Train loss: 0.92434 - Test loss: 0.89427\n",
      "Epoch 2702 - lr: 0.01000 - Train loss: 0.92843 - Test loss: 0.89068\n",
      "Epoch 2703 - lr: 0.01000 - Train loss: 0.92113 - Test loss: 0.91660\n",
      "Epoch 2704 - lr: 0.01000 - Train loss: 0.93147 - Test loss: 0.91896\n",
      "Epoch 2705 - lr: 0.01000 - Train loss: 0.93555 - Test loss: 0.91864\n",
      "Epoch 2706 - lr: 0.01000 - Train loss: 0.91512 - Test loss: 0.90803\n",
      "Epoch 2707 - lr: 0.01000 - Train loss: 0.94157 - Test loss: 0.89386\n",
      "Epoch 2708 - lr: 0.01000 - Train loss: 0.98076 - Test loss: 0.88908\n",
      "Epoch 2709 - lr: 0.01000 - Train loss: 0.96272 - Test loss: 0.90867\n",
      "Epoch 2710 - lr: 0.01000 - Train loss: 0.87504 - Test loss: 0.89359\n",
      "Epoch 2711 - lr: 0.01000 - Train loss: 0.86979 - Test loss: 0.88899\n",
      "Epoch 2712 - lr: 0.01000 - Train loss: 0.89484 - Test loss: 0.91202\n",
      "Epoch 2713 - lr: 0.01000 - Train loss: 0.86405 - Test loss: 0.90705\n",
      "Epoch 2714 - lr: 0.01000 - Train loss: 0.88008 - Test loss: 0.88679\n",
      "Epoch 2715 - lr: 0.01000 - Train loss: 0.97226 - Test loss: 0.88567\n",
      "Epoch 2716 - lr: 0.01000 - Train loss: 0.97595 - Test loss: 0.88571\n",
      "Epoch 2717 - lr: 0.01000 - Train loss: 0.92299 - Test loss: 0.91368\n",
      "Epoch 2718 - lr: 0.01000 - Train loss: 0.93291 - Test loss: 0.91706\n",
      "Epoch 2719 - lr: 0.01000 - Train loss: 0.91598 - Test loss: 0.91773\n",
      "Epoch 2720 - lr: 0.01000 - Train loss: 0.91662 - Test loss: 0.91703\n",
      "Epoch 2721 - lr: 0.01000 - Train loss: 0.93862 - Test loss: 0.89589\n",
      "Epoch 2722 - lr: 0.01000 - Train loss: 0.97620 - Test loss: 0.88248\n",
      "Epoch 2723 - lr: 0.01000 - Train loss: 0.88897 - Test loss: 0.88552\n",
      "Epoch 2724 - lr: 0.01000 - Train loss: 0.99843 - Test loss: 0.88427\n",
      "Epoch 2725 - lr: 0.01000 - Train loss: 0.93578 - Test loss: 0.89213\n",
      "Epoch 2726 - lr: 0.01000 - Train loss: 0.89161 - Test loss: 0.89042\n",
      "Epoch 2727 - lr: 0.01000 - Train loss: 0.97337 - Test loss: 0.88712\n",
      "Epoch 2728 - lr: 0.01000 - Train loss: 0.89071 - Test loss: 0.89087\n",
      "Epoch 2729 - lr: 0.01000 - Train loss: 0.93453 - Test loss: 0.89356\n",
      "Epoch 2730 - lr: 0.01000 - Train loss: 0.92724 - Test loss: 0.91870\n",
      "Epoch 2731 - lr: 0.01000 - Train loss: 0.93477 - Test loss: 0.92378\n",
      "Epoch 2732 - lr: 0.01000 - Train loss: 0.91536 - Test loss: 0.92419\n",
      "Epoch 2733 - lr: 0.01000 - Train loss: 0.87292 - Test loss: 0.89989\n",
      "Epoch 2734 - lr: 0.01000 - Train loss: 0.88645 - Test loss: 0.89181\n",
      "Epoch 2735 - lr: 0.01000 - Train loss: 0.98703 - Test loss: 0.91122\n",
      "Epoch 2736 - lr: 0.01000 - Train loss: 0.92182 - Test loss: 0.89306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2737 - lr: 0.01000 - Train loss: 0.87911 - Test loss: 0.89204\n",
      "Epoch 2738 - lr: 0.01000 - Train loss: 0.96751 - Test loss: 0.91435\n",
      "Epoch 2739 - lr: 0.01000 - Train loss: 0.94044 - Test loss: 0.91158\n",
      "Epoch 2740 - lr: 0.01000 - Train loss: 0.90229 - Test loss: 0.92234\n",
      "Epoch 2741 - lr: 0.01000 - Train loss: 0.87650 - Test loss: 0.91601\n",
      "Epoch 2742 - lr: 0.01000 - Train loss: 0.89046 - Test loss: 0.90537\n",
      "Epoch 2743 - lr: 0.01000 - Train loss: 0.93980 - Test loss: 0.89800\n",
      "Epoch 2744 - lr: 0.01000 - Train loss: 0.95466 - Test loss: 0.91168\n",
      "Epoch 2745 - lr: 0.01000 - Train loss: 0.90913 - Test loss: 0.89067\n",
      "Epoch 2746 - lr: 0.01000 - Train loss: 0.92864 - Test loss: 0.89552\n",
      "Epoch 2747 - lr: 0.01000 - Train loss: 0.96394 - Test loss: 0.91277\n",
      "Epoch 2748 - lr: 0.01000 - Train loss: 0.93643 - Test loss: 0.91947\n",
      "Epoch 2749 - lr: 0.01000 - Train loss: 0.90354 - Test loss: 0.91808\n",
      "Epoch 2750 - lr: 0.01000 - Train loss: 0.89366 - Test loss: 0.91347\n",
      "Epoch 2751 - lr: 0.01000 - Train loss: 0.93828 - Test loss: 0.91500\n",
      "Epoch 2752 - lr: 0.01000 - Train loss: 0.85655 - Test loss: 0.89213\n",
      "Epoch 2753 - lr: 0.01000 - Train loss: 0.89552 - Test loss: 0.88801\n",
      "Epoch 2754 - lr: 0.01000 - Train loss: 0.97028 - Test loss: 0.88714\n",
      "Epoch 2755 - lr: 0.01000 - Train loss: 0.97117 - Test loss: 0.88780\n",
      "Epoch 2756 - lr: 0.01000 - Train loss: 0.97643 - Test loss: 0.90658\n",
      "Epoch 2757 - lr: 0.01000 - Train loss: 0.97623 - Test loss: 0.89564\n",
      "Epoch 2758 - lr: 0.01000 - Train loss: 1.00526 - Test loss: 0.89046\n",
      "Epoch 2759 - lr: 0.01000 - Train loss: 0.96077 - Test loss: 0.89446\n",
      "Epoch 2760 - lr: 0.01000 - Train loss: 0.90610 - Test loss: 0.91870\n",
      "Epoch 2761 - lr: 0.01000 - Train loss: 0.89921 - Test loss: 0.91561\n",
      "Epoch 2762 - lr: 0.01000 - Train loss: 0.88765 - Test loss: 0.91774\n",
      "Epoch 2763 - lr: 0.01000 - Train loss: 0.93152 - Test loss: 0.91083\n",
      "Epoch 2764 - lr: 0.01000 - Train loss: 0.98088 - Test loss: 0.90183\n",
      "Epoch 2765 - lr: 0.01000 - Train loss: 0.99857 - Test loss: 0.89615\n",
      "Epoch 2766 - lr: 0.01000 - Train loss: 0.91705 - Test loss: 0.89877\n",
      "Epoch 2767 - lr: 0.01000 - Train loss: 0.96568 - Test loss: 0.91652\n",
      "Epoch 2768 - lr: 0.01000 - Train loss: 0.93191 - Test loss: 0.92366\n",
      "Epoch 2769 - lr: 0.01000 - Train loss: 0.89608 - Test loss: 0.91784\n",
      "Epoch 2770 - lr: 0.01000 - Train loss: 0.89913 - Test loss: 0.91598\n",
      "Epoch 2771 - lr: 0.01000 - Train loss: 0.91062 - Test loss: 0.91994\n",
      "Epoch 2772 - lr: 0.01000 - Train loss: 0.89484 - Test loss: 0.89414\n",
      "Epoch 2773 - lr: 0.01000 - Train loss: 0.92573 - Test loss: 0.89496\n",
      "Epoch 2774 - lr: 0.01000 - Train loss: 0.96902 - Test loss: 0.91216\n",
      "Epoch 2775 - lr: 0.01000 - Train loss: 0.91401 - Test loss: 0.91931\n",
      "Epoch 2776 - lr: 0.01000 - Train loss: 0.87239 - Test loss: 0.89690\n",
      "Epoch 2777 - lr: 0.01000 - Train loss: 0.88774 - Test loss: 0.88953\n",
      "Epoch 2778 - lr: 0.01000 - Train loss: 0.97085 - Test loss: 0.90286\n",
      "Epoch 2779 - lr: 0.01000 - Train loss: 0.85905 - Test loss: 0.90266\n",
      "Epoch 2780 - lr: 0.01000 - Train loss: 0.92142 - Test loss: 0.88791\n",
      "Epoch 2781 - lr: 0.01000 - Train loss: 0.89035 - Test loss: 0.88865\n",
      "Epoch 2782 - lr: 0.01000 - Train loss: 1.03437 - Test loss: 0.89400\n",
      "Epoch 2783 - lr: 0.01000 - Train loss: 0.99307 - Test loss: 0.89695\n",
      "Epoch 2784 - lr: 0.01000 - Train loss: 0.96792 - Test loss: 0.91550\n",
      "Epoch 2785 - lr: 0.01000 - Train loss: 0.92046 - Test loss: 0.92338\n",
      "Epoch 2786 - lr: 0.01000 - Train loss: 0.90323 - Test loss: 0.92070\n",
      "Epoch 2787 - lr: 0.01000 - Train loss: 0.90230 - Test loss: 0.91498\n",
      "Epoch 2788 - lr: 0.01000 - Train loss: 0.89816 - Test loss: 0.89662\n",
      "Epoch 2789 - lr: 0.01000 - Train loss: 0.94719 - Test loss: 0.89785\n",
      "Epoch 2790 - lr: 0.01000 - Train loss: 0.97476 - Test loss: 0.91370\n",
      "Epoch 2791 - lr: 0.01000 - Train loss: 0.89624 - Test loss: 0.91063\n",
      "Epoch 2792 - lr: 0.01000 - Train loss: 0.91854 - Test loss: 0.89693\n",
      "Epoch 2793 - lr: 0.01000 - Train loss: 0.97510 - Test loss: 0.91245\n",
      "Epoch 2794 - lr: 0.01000 - Train loss: 0.90393 - Test loss: 0.90999\n",
      "Epoch 2795 - lr: 0.01000 - Train loss: 0.92119 - Test loss: 0.89662\n",
      "Epoch 2796 - lr: 0.01000 - Train loss: 0.96193 - Test loss: 0.91194\n",
      "Epoch 2797 - lr: 0.01000 - Train loss: 0.91236 - Test loss: 0.90861\n",
      "Epoch 2798 - lr: 0.01000 - Train loss: 0.93831 - Test loss: 0.89464\n",
      "Epoch 2799 - lr: 0.01000 - Train loss: 0.95421 - Test loss: 0.90991\n",
      "Epoch 2800 - lr: 0.01000 - Train loss: 0.92078 - Test loss: 0.88855\n",
      "Epoch 2801 - lr: 0.01000 - Train loss: 0.91491 - Test loss: 0.90864\n",
      "Epoch 2802 - lr: 0.01000 - Train loss: 0.91538 - Test loss: 0.89468\n",
      "Epoch 2803 - lr: 0.01000 - Train loss: 0.97322 - Test loss: 0.91131\n",
      "Epoch 2804 - lr: 0.01000 - Train loss: 0.91558 - Test loss: 0.89556\n",
      "Epoch 2805 - lr: 0.01000 - Train loss: 0.97804 - Test loss: 0.91155\n",
      "Epoch 2806 - lr: 0.01000 - Train loss: 0.93394 - Test loss: 0.89887\n",
      "Epoch 2807 - lr: 0.01000 - Train loss: 0.98277 - Test loss: 0.91331\n",
      "Epoch 2808 - lr: 0.01000 - Train loss: 0.91494 - Test loss: 0.89731\n",
      "Epoch 2809 - lr: 0.01000 - Train loss: 0.96081 - Test loss: 0.91369\n",
      "Epoch 2810 - lr: 0.01000 - Train loss: 0.91887 - Test loss: 0.92086\n",
      "Epoch 2811 - lr: 0.01000 - Train loss: 0.91606 - Test loss: 0.89749\n",
      "Epoch 2812 - lr: 0.01000 - Train loss: 1.00587 - Test loss: 0.89055\n",
      "Epoch 2813 - lr: 0.01000 - Train loss: 0.91466 - Test loss: 0.89264\n",
      "Epoch 2814 - lr: 0.01000 - Train loss: 0.91114 - Test loss: 0.89118\n",
      "Epoch 2815 - lr: 0.01000 - Train loss: 0.94217 - Test loss: 0.91615\n",
      "Epoch 2816 - lr: 0.01000 - Train loss: 0.87739 - Test loss: 0.89961\n",
      "Epoch 2817 - lr: 0.01000 - Train loss: 0.93869 - Test loss: 0.91922\n",
      "Epoch 2818 - lr: 0.01000 - Train loss: 0.93742 - Test loss: 0.90446\n",
      "Epoch 2819 - lr: 0.01000 - Train loss: 0.96087 - Test loss: 0.89298\n",
      "Epoch 2820 - lr: 0.01000 - Train loss: 0.97264 - Test loss: 0.89742\n",
      "Epoch 2821 - lr: 0.01000 - Train loss: 0.94695 - Test loss: 0.89985\n",
      "Epoch 2822 - lr: 0.01000 - Train loss: 0.93595 - Test loss: 0.90991\n",
      "Epoch 2823 - lr: 0.01000 - Train loss: 0.85697 - Test loss: 0.90037\n",
      "Epoch 2824 - lr: 0.01000 - Train loss: 0.93211 - Test loss: 0.90703\n",
      "Epoch 2825 - lr: 0.01000 - Train loss: 0.92846 - Test loss: 0.90389\n",
      "Epoch 2826 - lr: 0.01000 - Train loss: 0.97735 - Test loss: 0.91961\n",
      "Epoch 2827 - lr: 0.01000 - Train loss: 0.89624 - Test loss: 0.91743\n",
      "Epoch 2828 - lr: 0.01000 - Train loss: 0.91810 - Test loss: 0.89635\n",
      "Epoch 2829 - lr: 0.01000 - Train loss: 0.88558 - Test loss: 0.89569\n",
      "Epoch 2830 - lr: 0.01000 - Train loss: 0.92825 - Test loss: 0.89761\n",
      "Epoch 2831 - lr: 0.01000 - Train loss: 0.90395 - Test loss: 0.89663\n",
      "Epoch 2832 - lr: 0.01000 - Train loss: 0.91787 - Test loss: 0.90246\n",
      "Epoch 2833 - lr: 0.01000 - Train loss: 0.97125 - Test loss: 0.92088\n",
      "Epoch 2834 - lr: 0.01000 - Train loss: 0.92094 - Test loss: 0.92870\n",
      "Epoch 2835 - lr: 0.01000 - Train loss: 0.93169 - Test loss: 0.92896\n",
      "Epoch 2836 - lr: 0.01000 - Train loss: 0.91599 - Test loss: 0.90475\n",
      "Epoch 2837 - lr: 0.01000 - Train loss: 0.97368 - Test loss: 0.91919\n",
      "Epoch 2838 - lr: 0.01000 - Train loss: 0.91574 - Test loss: 0.92578\n",
      "Epoch 2839 - lr: 0.01000 - Train loss: 0.87467 - Test loss: 0.90218\n",
      "Epoch 2840 - lr: 0.01000 - Train loss: 0.92367 - Test loss: 0.89365\n",
      "Epoch 2841 - lr: 0.01000 - Train loss: 0.89336 - Test loss: 0.90400\n",
      "Epoch 2842 - lr: 0.01000 - Train loss: 0.93711 - Test loss: 0.92158\n",
      "Epoch 2843 - lr: 0.01000 - Train loss: 0.91472 - Test loss: 0.91454\n",
      "Epoch 2844 - lr: 0.01000 - Train loss: 0.93604 - Test loss: 0.90272\n",
      "Epoch 2845 - lr: 0.01000 - Train loss: 0.91294 - Test loss: 0.89607\n",
      "Epoch 2846 - lr: 0.01000 - Train loss: 0.90182 - Test loss: 0.91437\n",
      "Epoch 2847 - lr: 0.01000 - Train loss: 0.91978 - Test loss: 0.92267\n",
      "Epoch 2848 - lr: 0.01000 - Train loss: 0.93328 - Test loss: 0.90740\n",
      "Epoch 2849 - lr: 0.01000 - Train loss: 0.87582 - Test loss: 0.89452\n",
      "Epoch 2850 - lr: 0.01000 - Train loss: 0.89973 - Test loss: 0.91503\n",
      "Epoch 2851 - lr: 0.01000 - Train loss: 0.89576 - Test loss: 0.91404\n",
      "Epoch 2852 - lr: 0.01000 - Train loss: 0.92880 - Test loss: 0.89719\n",
      "Epoch 2853 - lr: 0.01000 - Train loss: 0.94735 - Test loss: 0.90939\n",
      "Epoch 2854 - lr: 0.01000 - Train loss: 0.89390 - Test loss: 0.91237\n",
      "Epoch 2855 - lr: 0.01000 - Train loss: 0.92884 - Test loss: 0.90326\n",
      "Epoch 2856 - lr: 0.01000 - Train loss: 0.92380 - Test loss: 0.89454\n",
      "Epoch 2857 - lr: 0.01000 - Train loss: 0.97405 - Test loss: 0.88919\n",
      "Epoch 2858 - lr: 0.01000 - Train loss: 0.93062 - Test loss: 0.88948\n",
      "Epoch 2859 - lr: 0.01000 - Train loss: 0.89524 - Test loss: 0.88945\n",
      "Epoch 2860 - lr: 0.01000 - Train loss: 0.96948 - Test loss: 0.88939\n",
      "Epoch 2861 - lr: 0.01000 - Train loss: 0.97398 - Test loss: 0.88714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2862 - lr: 0.01000 - Train loss: 0.91931 - Test loss: 0.90065\n",
      "Epoch 2863 - lr: 0.01000 - Train loss: 0.89414 - Test loss: 0.91850\n",
      "Epoch 2864 - lr: 0.01000 - Train loss: 0.89860 - Test loss: 0.91605\n",
      "Epoch 2865 - lr: 0.01000 - Train loss: 0.90353 - Test loss: 0.91081\n",
      "Epoch 2866 - lr: 0.01000 - Train loss: 0.92519 - Test loss: 0.89725\n",
      "Epoch 2867 - lr: 0.01000 - Train loss: 0.91454 - Test loss: 0.89147\n",
      "Epoch 2868 - lr: 0.01000 - Train loss: 0.89222 - Test loss: 0.88972\n",
      "Epoch 2869 - lr: 0.01000 - Train loss: 0.96786 - Test loss: 0.88721\n",
      "Epoch 2870 - lr: 0.01000 - Train loss: 0.92057 - Test loss: 0.90954\n",
      "Epoch 2871 - lr: 0.01000 - Train loss: 0.87228 - Test loss: 0.89807\n",
      "Epoch 2872 - lr: 0.01000 - Train loss: 0.88513 - Test loss: 0.89187\n",
      "Epoch 2873 - lr: 0.01000 - Train loss: 0.95038 - Test loss: 0.89496\n",
      "Epoch 2874 - lr: 0.01000 - Train loss: 0.92364 - Test loss: 0.89752\n",
      "Epoch 2875 - lr: 0.01000 - Train loss: 0.97770 - Test loss: 0.91554\n",
      "Epoch 2876 - lr: 0.01000 - Train loss: 0.90732 - Test loss: 0.92150\n",
      "Epoch 2877 - lr: 0.01000 - Train loss: 0.86821 - Test loss: 0.90819\n",
      "Epoch 2878 - lr: 0.01000 - Train loss: 0.90658 - Test loss: 0.89764\n",
      "Epoch 2879 - lr: 0.01000 - Train loss: 0.95554 - Test loss: 0.91292\n",
      "Epoch 2880 - lr: 0.01000 - Train loss: 0.92037 - Test loss: 0.89160\n",
      "Epoch 2881 - lr: 0.01000 - Train loss: 0.88803 - Test loss: 0.89127\n",
      "Epoch 2882 - lr: 0.01000 - Train loss: 0.96248 - Test loss: 0.89344\n",
      "Epoch 2883 - lr: 0.01000 - Train loss: 0.92218 - Test loss: 0.92036\n",
      "Epoch 2884 - lr: 0.01000 - Train loss: 0.92548 - Test loss: 0.90124\n",
      "Epoch 2885 - lr: 0.01000 - Train loss: 0.97180 - Test loss: 0.91613\n",
      "Epoch 2886 - lr: 0.01000 - Train loss: 0.89555 - Test loss: 0.91283\n",
      "Epoch 2887 - lr: 0.01000 - Train loss: 0.93286 - Test loss: 0.92066\n",
      "Epoch 2888 - lr: 0.01000 - Train loss: 0.92005 - Test loss: 0.89871\n",
      "Epoch 2889 - lr: 0.01000 - Train loss: 0.95462 - Test loss: 0.89241\n",
      "Epoch 2890 - lr: 0.01000 - Train loss: 0.90809 - Test loss: 0.89556\n",
      "Epoch 2891 - lr: 0.01000 - Train loss: 0.97837 - Test loss: 0.91394\n",
      "Epoch 2892 - lr: 0.01000 - Train loss: 0.90677 - Test loss: 0.92007\n",
      "Epoch 2893 - lr: 0.01000 - Train loss: 0.90464 - Test loss: 0.90189\n",
      "Epoch 2894 - lr: 0.01000 - Train loss: 0.91688 - Test loss: 0.91137\n",
      "Epoch 2895 - lr: 0.01000 - Train loss: 0.89894 - Test loss: 0.89281\n",
      "Epoch 2896 - lr: 0.01000 - Train loss: 0.92167 - Test loss: 0.91791\n",
      "Epoch 2897 - lr: 0.01000 - Train loss: 0.92510 - Test loss: 0.89782\n",
      "Epoch 2898 - lr: 0.01000 - Train loss: 0.95472 - Test loss: 0.91214\n",
      "Epoch 2899 - lr: 0.01000 - Train loss: 0.92354 - Test loss: 0.89059\n",
      "Epoch 2900 - lr: 0.01000 - Train loss: 0.89189 - Test loss: 0.89052\n",
      "Epoch 2901 - lr: 0.01000 - Train loss: 0.96212 - Test loss: 0.88879\n",
      "Epoch 2902 - lr: 0.01000 - Train loss: 0.98269 - Test loss: 0.89738\n",
      "Epoch 2903 - lr: 0.01000 - Train loss: 0.96030 - Test loss: 0.89490\n",
      "Epoch 2904 - lr: 0.01000 - Train loss: 0.93709 - Test loss: 0.89652\n",
      "Epoch 2905 - lr: 0.01000 - Train loss: 0.91076 - Test loss: 0.90094\n",
      "Epoch 2906 - lr: 0.01000 - Train loss: 0.96879 - Test loss: 0.91956\n",
      "Epoch 2907 - lr: 0.01000 - Train loss: 0.92845 - Test loss: 0.92725\n",
      "Epoch 2908 - lr: 0.01000 - Train loss: 0.93953 - Test loss: 0.90979\n",
      "Epoch 2909 - lr: 0.01000 - Train loss: 0.95847 - Test loss: 0.92484\n",
      "Epoch 2910 - lr: 0.01000 - Train loss: 0.85553 - Test loss: 0.90014\n",
      "Epoch 2911 - lr: 0.01000 - Train loss: 0.89968 - Test loss: 0.89749\n",
      "Epoch 2912 - lr: 0.01000 - Train loss: 0.92009 - Test loss: 0.89879\n",
      "Epoch 2913 - lr: 0.01000 - Train loss: 0.96490 - Test loss: 0.91699\n",
      "Epoch 2914 - lr: 0.01000 - Train loss: 0.90736 - Test loss: 0.92243\n",
      "Epoch 2915 - lr: 0.01000 - Train loss: 0.86524 - Test loss: 0.91002\n",
      "Epoch 2916 - lr: 0.01000 - Train loss: 0.89018 - Test loss: 0.89521\n",
      "Epoch 2917 - lr: 0.01000 - Train loss: 0.91944 - Test loss: 0.89623\n",
      "Epoch 2918 - lr: 0.01000 - Train loss: 0.96330 - Test loss: 0.91436\n",
      "Epoch 2919 - lr: 0.01000 - Train loss: 0.93149 - Test loss: 0.92171\n",
      "Epoch 2920 - lr: 0.01000 - Train loss: 0.91538 - Test loss: 0.89951\n",
      "Epoch 2921 - lr: 0.01000 - Train loss: 0.96153 - Test loss: 0.91470\n",
      "Epoch 2922 - lr: 0.01000 - Train loss: 0.89404 - Test loss: 0.91389\n",
      "Epoch 2923 - lr: 0.01000 - Train loss: 0.90017 - Test loss: 0.91381\n",
      "Epoch 2924 - lr: 0.01000 - Train loss: 0.93047 - Test loss: 0.89797\n",
      "Epoch 2925 - lr: 0.01000 - Train loss: 0.97597 - Test loss: 0.91275\n",
      "Epoch 2926 - lr: 0.01000 - Train loss: 0.92777 - Test loss: 0.89793\n",
      "Epoch 2927 - lr: 0.01000 - Train loss: 0.95777 - Test loss: 0.91264\n",
      "Epoch 2928 - lr: 0.01000 - Train loss: 0.92293 - Test loss: 0.89140\n",
      "Epoch 2929 - lr: 0.01000 - Train loss: 0.88716 - Test loss: 0.89042\n",
      "Epoch 2930 - lr: 0.01000 - Train loss: 1.00487 - Test loss: 0.90154\n",
      "Epoch 2931 - lr: 0.01000 - Train loss: 0.92550 - Test loss: 0.89949\n",
      "Epoch 2932 - lr: 0.01000 - Train loss: 0.97397 - Test loss: 0.91578\n",
      "Epoch 2933 - lr: 0.01000 - Train loss: 0.94015 - Test loss: 0.90586\n",
      "Epoch 2934 - lr: 0.01000 - Train loss: 1.02374 - Test loss: 0.89581\n",
      "Epoch 2935 - lr: 0.01000 - Train loss: 0.90595 - Test loss: 0.89817\n",
      "Epoch 2936 - lr: 0.01000 - Train loss: 0.98354 - Test loss: 0.89882\n",
      "Epoch 2937 - lr: 0.01000 - Train loss: 0.91076 - Test loss: 0.89810\n",
      "Epoch 2938 - lr: 0.01000 - Train loss: 0.91851 - Test loss: 0.91626\n",
      "Epoch 2939 - lr: 0.01000 - Train loss: 0.95154 - Test loss: 0.89976\n",
      "Epoch 2940 - lr: 0.01000 - Train loss: 0.92999 - Test loss: 0.89968\n",
      "Epoch 2941 - lr: 0.01000 - Train loss: 0.91582 - Test loss: 0.90316\n",
      "Epoch 2942 - lr: 0.01000 - Train loss: 0.97032 - Test loss: 0.92128\n",
      "Epoch 2943 - lr: 0.01000 - Train loss: 0.91982 - Test loss: 0.90620\n",
      "Epoch 2944 - lr: 0.01000 - Train loss: 0.97280 - Test loss: 0.92176\n",
      "Epoch 2945 - lr: 0.01000 - Train loss: 0.93867 - Test loss: 0.91240\n",
      "Epoch 2946 - lr: 0.01000 - Train loss: 0.92372 - Test loss: 0.89821\n",
      "Epoch 2947 - lr: 0.01000 - Train loss: 0.89743 - Test loss: 0.91417\n",
      "Epoch 2948 - lr: 0.01000 - Train loss: 0.89455 - Test loss: 0.91571\n",
      "Epoch 2949 - lr: 0.01000 - Train loss: 0.87188 - Test loss: 0.91776\n",
      "Epoch 2950 - lr: 0.01000 - Train loss: 0.89877 - Test loss: 0.91687\n",
      "Epoch 2951 - lr: 0.01000 - Train loss: 0.88441 - Test loss: 0.91946\n",
      "Epoch 2952 - lr: 0.01000 - Train loss: 0.97898 - Test loss: 0.90534\n",
      "Epoch 2953 - lr: 0.01000 - Train loss: 0.93285 - Test loss: 0.90055\n",
      "Epoch 2954 - lr: 0.01000 - Train loss: 0.97554 - Test loss: 0.91511\n",
      "Epoch 2955 - lr: 0.01000 - Train loss: 0.91038 - Test loss: 0.90867\n",
      "Epoch 2956 - lr: 0.01000 - Train loss: 0.93015 - Test loss: 0.90187\n",
      "Epoch 2957 - lr: 0.01000 - Train loss: 0.99893 - Test loss: 0.90695\n",
      "Epoch 2958 - lr: 0.01000 - Train loss: 0.92057 - Test loss: 0.90203\n",
      "Epoch 2959 - lr: 0.01000 - Train loss: 0.97163 - Test loss: 0.91870\n",
      "Epoch 2960 - lr: 0.01000 - Train loss: 0.93984 - Test loss: 0.90823\n",
      "Epoch 2961 - lr: 0.01000 - Train loss: 0.98382 - Test loss: 0.90176\n",
      "Epoch 2962 - lr: 0.01000 - Train loss: 0.97024 - Test loss: 0.91899\n",
      "Epoch 2963 - lr: 0.01000 - Train loss: 0.91540 - Test loss: 0.90338\n",
      "Epoch 2964 - lr: 0.01000 - Train loss: 0.96635 - Test loss: 0.91961\n",
      "Epoch 2965 - lr: 0.01000 - Train loss: 0.91611 - Test loss: 0.90344\n",
      "Epoch 2966 - lr: 0.01000 - Train loss: 0.97132 - Test loss: 0.91937\n",
      "Epoch 2967 - lr: 0.01000 - Train loss: 0.91521 - Test loss: 0.90345\n",
      "Epoch 2968 - lr: 0.01000 - Train loss: 0.96692 - Test loss: 0.91963\n",
      "Epoch 2969 - lr: 0.01000 - Train loss: 0.91505 - Test loss: 0.90346\n",
      "Epoch 2970 - lr: 0.01000 - Train loss: 0.96709 - Test loss: 0.91959\n",
      "Epoch 2971 - lr: 0.01000 - Train loss: 0.91478 - Test loss: 0.90343\n",
      "Epoch 2972 - lr: 0.01000 - Train loss: 0.96663 - Test loss: 0.91956\n",
      "Epoch 2973 - lr: 0.01000 - Train loss: 0.91483 - Test loss: 0.90336\n",
      "Epoch 2974 - lr: 0.01000 - Train loss: 0.96853 - Test loss: 0.91943\n",
      "Epoch 2975 - lr: 0.01000 - Train loss: 0.91443 - Test loss: 0.90339\n",
      "Epoch 2976 - lr: 0.01000 - Train loss: 0.96632 - Test loss: 0.91951\n",
      "Epoch 2977 - lr: 0.01000 - Train loss: 0.91471 - Test loss: 0.90327\n",
      "Epoch 2978 - lr: 0.01000 - Train loss: 0.97204 - Test loss: 0.91916\n",
      "Epoch 2979 - lr: 0.01000 - Train loss: 0.91478 - Test loss: 0.90325\n",
      "Epoch 2980 - lr: 0.01000 - Train loss: 0.97524 - Test loss: 0.91862\n",
      "Epoch 2981 - lr: 0.01000 - Train loss: 0.93542 - Test loss: 0.92592\n",
      "Epoch 2982 - lr: 0.01000 - Train loss: 0.91858 - Test loss: 0.90464\n",
      "Epoch 2983 - lr: 0.01000 - Train loss: 0.92040 - Test loss: 0.89730\n",
      "Epoch 2984 - lr: 0.01000 - Train loss: 0.90775 - Test loss: 0.89991\n",
      "Epoch 2985 - lr: 0.01000 - Train loss: 0.96763 - Test loss: 0.91851\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2986 - lr: 0.01000 - Train loss: 0.91604 - Test loss: 0.90307\n",
      "Epoch 2987 - lr: 0.01000 - Train loss: 0.92911 - Test loss: 0.89822\n",
      "Epoch 2988 - lr: 0.01000 - Train loss: 0.91651 - Test loss: 0.92392\n",
      "Epoch 2989 - lr: 0.01000 - Train loss: 0.92089 - Test loss: 0.92685\n",
      "Epoch 2990 - lr: 0.01000 - Train loss: 0.91841 - Test loss: 0.90348\n",
      "Epoch 2991 - lr: 0.01000 - Train loss: 0.97073 - Test loss: 0.89224\n",
      "Epoch 2992 - lr: 0.01000 - Train loss: 0.87615 - Test loss: 0.89372\n",
      "Epoch 2993 - lr: 0.01000 - Train loss: 1.02144 - Test loss: 0.89793\n",
      "Epoch 2994 - lr: 0.01000 - Train loss: 0.92598 - Test loss: 0.90070\n",
      "Epoch 2995 - lr: 0.01000 - Train loss: 0.97087 - Test loss: 0.90914\n",
      "Epoch 2996 - lr: 0.01000 - Train loss: 0.90338 - Test loss: 0.90104\n",
      "Epoch 2997 - lr: 0.01000 - Train loss: 0.91536 - Test loss: 0.90462\n",
      "Epoch 2998 - lr: 0.01000 - Train loss: 0.97272 - Test loss: 0.92363\n",
      "Epoch 2999 - lr: 0.01000 - Train loss: 0.92940 - Test loss: 0.93177\n",
      "Epoch 3000 - lr: 0.01000 - Train loss: 0.91541 - Test loss: 0.90913\n",
      "Epoch 3001 - lr: 0.01000 - Train loss: 0.97962 - Test loss: 0.90102\n",
      "Epoch 3002 - lr: 0.01000 - Train loss: 0.97372 - Test loss: 0.92240\n",
      "Epoch 3003 - lr: 0.01000 - Train loss: 0.88463 - Test loss: 0.90821\n",
      "Epoch 3004 - lr: 0.01000 - Train loss: 0.92340 - Test loss: 0.93099\n",
      "Epoch 3005 - lr: 0.01000 - Train loss: 0.91498 - Test loss: 0.90908\n",
      "Epoch 3006 - lr: 0.01000 - Train loss: 0.91098 - Test loss: 0.90311\n",
      "Epoch 3007 - lr: 0.01000 - Train loss: 0.92169 - Test loss: 0.89953\n",
      "Epoch 3008 - lr: 0.01000 - Train loss: 0.89993 - Test loss: 0.91885\n",
      "Epoch 3009 - lr: 0.01000 - Train loss: 0.91890 - Test loss: 0.90239\n",
      "Epoch 3010 - lr: 0.01000 - Train loss: 0.86472 - Test loss: 0.91378\n",
      "Epoch 3011 - lr: 0.01000 - Train loss: 0.90466 - Test loss: 0.90528\n",
      "Epoch 3012 - lr: 0.01000 - Train loss: 0.97133 - Test loss: 0.92316\n",
      "Epoch 3013 - lr: 0.01000 - Train loss: 0.92773 - Test loss: 0.93081\n",
      "Epoch 3014 - lr: 0.01000 - Train loss: 0.93169 - Test loss: 0.91057\n",
      "Epoch 3015 - lr: 0.01000 - Train loss: 0.96802 - Test loss: 0.92362\n",
      "Epoch 3016 - lr: 0.01000 - Train loss: 0.91877 - Test loss: 0.92958\n",
      "Epoch 3017 - lr: 0.01000 - Train loss: 0.92808 - Test loss: 0.91936\n",
      "Epoch 3018 - lr: 0.01000 - Train loss: 0.90201 - Test loss: 0.90248\n",
      "Epoch 3019 - lr: 0.01000 - Train loss: 0.96892 - Test loss: 0.91984\n",
      "Epoch 3020 - lr: 0.01000 - Train loss: 0.92846 - Test loss: 0.92705\n",
      "Epoch 3021 - lr: 0.01000 - Train loss: 0.90665 - Test loss: 0.91742\n",
      "Epoch 3022 - lr: 0.01000 - Train loss: 0.93169 - Test loss: 0.90384\n",
      "Epoch 3023 - lr: 0.01000 - Train loss: 0.98928 - Test loss: 0.91536\n",
      "Epoch 3024 - lr: 0.01000 - Train loss: 0.87196 - Test loss: 0.90111\n",
      "Epoch 3025 - lr: 0.01000 - Train loss: 0.85724 - Test loss: 0.89628\n",
      "Epoch 3026 - lr: 0.01000 - Train loss: 0.91221 - Test loss: 0.91428\n",
      "Epoch 3027 - lr: 0.01000 - Train loss: 0.91882 - Test loss: 0.92277\n",
      "Epoch 3028 - lr: 0.01000 - Train loss: 0.91109 - Test loss: 0.92281\n",
      "Epoch 3029 - lr: 0.01000 - Train loss: 0.91448 - Test loss: 0.89417\n",
      "Epoch 3030 - lr: 0.01000 - Train loss: 0.92512 - Test loss: 0.89701\n",
      "Epoch 3031 - lr: 0.01000 - Train loss: 0.94750 - Test loss: 0.89442\n",
      "Epoch 3032 - lr: 0.01000 - Train loss: 0.92152 - Test loss: 0.89724\n",
      "Epoch 3033 - lr: 0.01000 - Train loss: 0.97461 - Test loss: 0.89625\n",
      "Epoch 3034 - lr: 0.01000 - Train loss: 0.95300 - Test loss: 0.89270\n",
      "Epoch 3035 - lr: 0.01000 - Train loss: 0.91238 - Test loss: 0.89843\n",
      "Epoch 3036 - lr: 0.01000 - Train loss: 0.93931 - Test loss: 0.89872\n",
      "Epoch 3037 - lr: 0.01000 - Train loss: 0.88043 - Test loss: 0.89749\n",
      "Epoch 3038 - lr: 0.01000 - Train loss: 0.92425 - Test loss: 0.90047\n",
      "Epoch 3039 - lr: 0.01000 - Train loss: 0.97868 - Test loss: 0.91655\n",
      "Epoch 3040 - lr: 0.01000 - Train loss: 0.92853 - Test loss: 0.92496\n",
      "Epoch 3041 - lr: 0.01000 - Train loss: 0.91724 - Test loss: 0.91383\n",
      "Epoch 3042 - lr: 0.01000 - Train loss: 0.91736 - Test loss: 0.91176\n",
      "Epoch 3043 - lr: 0.01000 - Train loss: 0.91575 - Test loss: 0.92397\n",
      "Epoch 3044 - lr: 0.01000 - Train loss: 0.91924 - Test loss: 0.89960\n",
      "Epoch 3045 - lr: 0.01000 - Train loss: 1.04576 - Test loss: 0.89543\n",
      "Epoch 3046 - lr: 0.01000 - Train loss: 0.96952 - Test loss: 0.90801\n",
      "Epoch 3047 - lr: 0.01000 - Train loss: 0.87816 - Test loss: 0.89412\n",
      "Epoch 3048 - lr: 0.01000 - Train loss: 1.01430 - Test loss: 0.89547\n",
      "Epoch 3049 - lr: 0.01000 - Train loss: 0.96937 - Test loss: 0.91653\n",
      "Epoch 3050 - lr: 0.01000 - Train loss: 0.92716 - Test loss: 0.92505\n",
      "Epoch 3051 - lr: 0.01000 - Train loss: 0.92703 - Test loss: 0.91118\n",
      "Epoch 3052 - lr: 0.01000 - Train loss: 0.91372 - Test loss: 0.90005\n",
      "Epoch 3053 - lr: 0.01000 - Train loss: 0.97791 - Test loss: 0.91477\n",
      "Epoch 3054 - lr: 0.01000 - Train loss: 0.92642 - Test loss: 0.92282\n",
      "Epoch 3055 - lr: 0.01000 - Train loss: 0.90051 - Test loss: 0.91332\n",
      "Epoch 3056 - lr: 0.01000 - Train loss: 0.90525 - Test loss: 0.92275\n",
      "Epoch 3057 - lr: 0.01000 - Train loss: 0.87329 - Test loss: 0.89666\n",
      "Epoch 3058 - lr: 0.01000 - Train loss: 0.87542 - Test loss: 0.89108\n",
      "Epoch 3059 - lr: 0.01000 - Train loss: 0.90433 - Test loss: 0.91493\n",
      "Epoch 3060 - lr: 0.01000 - Train loss: 0.93757 - Test loss: 0.88940\n",
      "Epoch 3061 - lr: 0.01000 - Train loss: 1.04484 - Test loss: 0.89077\n",
      "Epoch 3062 - lr: 0.01000 - Train loss: 0.97224 - Test loss: 0.91757\n",
      "Epoch 3063 - lr: 0.01000 - Train loss: 0.92868 - Test loss: 0.90033\n",
      "Epoch 3064 - lr: 0.01000 - Train loss: 0.97806 - Test loss: 0.91016\n",
      "Epoch 3065 - lr: 0.01000 - Train loss: 0.98056 - Test loss: 0.89975\n",
      "Epoch 3066 - lr: 0.01000 - Train loss: 1.03542 - Test loss: 0.89215\n",
      "Epoch 3067 - lr: 0.01000 - Train loss: 0.97349 - Test loss: 0.91992\n",
      "Epoch 3068 - lr: 0.01000 - Train loss: 0.92469 - Test loss: 0.90338\n",
      "Epoch 3069 - lr: 0.01000 - Train loss: 0.94713 - Test loss: 0.89280\n",
      "Epoch 3070 - lr: 0.01000 - Train loss: 0.92690 - Test loss: 0.92065\n",
      "Epoch 3071 - lr: 0.01000 - Train loss: 0.91604 - Test loss: 0.90222\n",
      "Epoch 3072 - lr: 0.01000 - Train loss: 0.92925 - Test loss: 0.89481\n",
      "Epoch 3073 - lr: 0.01000 - Train loss: 0.90427 - Test loss: 0.89364\n",
      "Epoch 3074 - lr: 0.01000 - Train loss: 0.91488 - Test loss: 0.89463\n",
      "Epoch 3075 - lr: 0.01000 - Train loss: 0.91702 - Test loss: 0.89683\n",
      "Epoch 3076 - lr: 0.01000 - Train loss: 0.93703 - Test loss: 0.89486\n",
      "Epoch 3077 - lr: 0.01000 - Train loss: 0.91418 - Test loss: 0.89914\n",
      "Epoch 3078 - lr: 0.01000 - Train loss: 0.95304 - Test loss: 0.89500\n",
      "Epoch 3079 - lr: 0.01000 - Train loss: 0.97712 - Test loss: 0.90762\n",
      "Epoch 3080 - lr: 0.01000 - Train loss: 0.90893 - Test loss: 0.91264\n",
      "Epoch 3081 - lr: 0.01000 - Train loss: 0.87607 - Test loss: 0.91796\n",
      "Epoch 3082 - lr: 0.01000 - Train loss: 0.94681 - Test loss: 0.90469\n",
      "Epoch 3083 - lr: 0.01000 - Train loss: 0.87257 - Test loss: 0.90020\n",
      "Epoch 3084 - lr: 0.01000 - Train loss: 0.91337 - Test loss: 0.91643\n",
      "Epoch 3085 - lr: 0.01000 - Train loss: 0.92300 - Test loss: 0.90313\n",
      "Epoch 3086 - lr: 0.01000 - Train loss: 0.91343 - Test loss: 0.89523\n",
      "Epoch 3087 - lr: 0.01000 - Train loss: 0.88618 - Test loss: 0.89700\n",
      "Epoch 3088 - lr: 0.01000 - Train loss: 0.92508 - Test loss: 0.92285\n",
      "Epoch 3089 - lr: 0.01000 - Train loss: 0.92588 - Test loss: 0.90506\n",
      "Epoch 3090 - lr: 0.01000 - Train loss: 0.99987 - Test loss: 0.90373\n",
      "Epoch 3091 - lr: 0.01000 - Train loss: 0.90146 - Test loss: 0.92303\n",
      "Epoch 3092 - lr: 0.01000 - Train loss: 0.91134 - Test loss: 0.89691\n",
      "Epoch 3093 - lr: 0.01000 - Train loss: 0.91923 - Test loss: 0.90629\n",
      "Epoch 3094 - lr: 0.01000 - Train loss: 0.93291 - Test loss: 0.90059\n",
      "Epoch 3095 - lr: 0.01000 - Train loss: 0.96145 - Test loss: 0.91656\n",
      "Epoch 3096 - lr: 0.01000 - Train loss: 0.92079 - Test loss: 0.90052\n",
      "Epoch 3097 - lr: 0.01000 - Train loss: 0.96077 - Test loss: 0.89396\n",
      "Epoch 3098 - lr: 0.01000 - Train loss: 0.96437 - Test loss: 0.89100\n",
      "Epoch 3099 - lr: 0.01000 - Train loss: 0.88452 - Test loss: 0.89445\n",
      "Epoch 3100 - lr: 0.01000 - Train loss: 0.97211 - Test loss: 0.91731\n",
      "Epoch 3101 - lr: 0.01000 - Train loss: 0.91607 - Test loss: 0.90416\n",
      "Epoch 3102 - lr: 0.01000 - Train loss: 0.94086 - Test loss: 0.89810\n",
      "Epoch 3103 - lr: 0.01000 - Train loss: 0.92037 - Test loss: 0.92489\n",
      "Epoch 3104 - lr: 0.01000 - Train loss: 0.92405 - Test loss: 0.90652\n",
      "Epoch 3105 - lr: 0.01000 - Train loss: 0.93670 - Test loss: 0.89847\n",
      "Epoch 3106 - lr: 0.01000 - Train loss: 0.90355 - Test loss: 0.91344\n",
      "Epoch 3107 - lr: 0.01000 - Train loss: 0.92240 - Test loss: 0.92756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3108 - lr: 0.01000 - Train loss: 0.91254 - Test loss: 0.90387\n",
      "Epoch 3109 - lr: 0.01000 - Train loss: 0.92205 - Test loss: 0.89543\n",
      "Epoch 3110 - lr: 0.01000 - Train loss: 0.92652 - Test loss: 0.92146\n",
      "Epoch 3111 - lr: 0.01000 - Train loss: 0.91020 - Test loss: 0.90155\n",
      "Epoch 3112 - lr: 0.01000 - Train loss: 0.92440 - Test loss: 0.89287\n",
      "Epoch 3113 - lr: 0.01000 - Train loss: 0.92575 - Test loss: 0.92013\n",
      "Epoch 3114 - lr: 0.01000 - Train loss: 0.90722 - Test loss: 0.90043\n",
      "Epoch 3115 - lr: 0.01000 - Train loss: 0.97284 - Test loss: 0.88949\n",
      "Epoch 3116 - lr: 0.01000 - Train loss: 0.88249 - Test loss: 0.91107\n",
      "Epoch 3117 - lr: 0.01000 - Train loss: 0.93636 - Test loss: 0.90559\n",
      "Epoch 3118 - lr: 0.01000 - Train loss: 0.89963 - Test loss: 0.91413\n",
      "Epoch 3119 - lr: 0.01000 - Train loss: 0.92670 - Test loss: 0.90087\n",
      "Epoch 3120 - lr: 0.01000 - Train loss: 0.94419 - Test loss: 0.89323\n",
      "Epoch 3121 - lr: 0.01000 - Train loss: 0.89912 - Test loss: 0.90883\n",
      "Epoch 3122 - lr: 0.01000 - Train loss: 0.92182 - Test loss: 0.92333\n",
      "Epoch 3123 - lr: 0.01000 - Train loss: 0.90835 - Test loss: 0.89932\n",
      "Epoch 3124 - lr: 0.01000 - Train loss: 0.96476 - Test loss: 0.89066\n",
      "Epoch 3125 - lr: 0.01000 - Train loss: 0.97059 - Test loss: 0.89044\n",
      "Epoch 3126 - lr: 0.01000 - Train loss: 0.96802 - Test loss: 0.89101\n",
      "Epoch 3127 - lr: 0.01000 - Train loss: 0.93611 - Test loss: 0.89296\n",
      "Epoch 3128 - lr: 0.01000 - Train loss: 0.94203 - Test loss: 0.90303\n",
      "Epoch 3129 - lr: 0.01000 - Train loss: 0.94745 - Test loss: 0.89979\n",
      "Epoch 3130 - lr: 0.01000 - Train loss: 0.94944 - Test loss: 0.89443\n",
      "Epoch 3131 - lr: 0.01000 - Train loss: 1.00123 - Test loss: 0.89405\n",
      "Epoch 3132 - lr: 0.01000 - Train loss: 0.94119 - Test loss: 0.89828\n",
      "Epoch 3133 - lr: 0.01000 - Train loss: 0.93091 - Test loss: 0.92667\n",
      "Epoch 3134 - lr: 0.01000 - Train loss: 0.91047 - Test loss: 0.90778\n",
      "Epoch 3135 - lr: 0.01000 - Train loss: 0.96883 - Test loss: 0.92258\n",
      "Epoch 3136 - lr: 0.01000 - Train loss: 0.90976 - Test loss: 0.90657\n",
      "Epoch 3137 - lr: 0.01000 - Train loss: 0.96913 - Test loss: 0.92189\n",
      "Epoch 3138 - lr: 0.01000 - Train loss: 0.91163 - Test loss: 0.90630\n",
      "Epoch 3139 - lr: 0.01000 - Train loss: 0.96946 - Test loss: 0.92147\n",
      "Epoch 3140 - lr: 0.01000 - Train loss: 0.91413 - Test loss: 0.90627\n",
      "Epoch 3141 - lr: 0.01000 - Train loss: 0.96803 - Test loss: 0.92122\n",
      "Epoch 3142 - lr: 0.01000 - Train loss: 0.90946 - Test loss: 0.90517\n",
      "Epoch 3143 - lr: 0.01000 - Train loss: 0.97616 - Test loss: 0.91963\n",
      "Epoch 3144 - lr: 0.01000 - Train loss: 0.92381 - Test loss: 0.90577\n",
      "Epoch 3145 - lr: 0.01000 - Train loss: 0.97203 - Test loss: 0.92066\n",
      "Epoch 3146 - lr: 0.01000 - Train loss: 0.91193 - Test loss: 0.90526\n",
      "Epoch 3147 - lr: 0.01000 - Train loss: 0.97598 - Test loss: 0.91961\n",
      "Epoch 3148 - lr: 0.01000 - Train loss: 0.92695 - Test loss: 0.90651\n",
      "Epoch 3149 - lr: 0.01000 - Train loss: 0.96744 - Test loss: 0.92090\n",
      "Epoch 3150 - lr: 0.01000 - Train loss: 0.90779 - Test loss: 0.90455\n",
      "Epoch 3151 - lr: 0.01000 - Train loss: 0.97555 - Test loss: 0.91940\n",
      "Epoch 3152 - lr: 0.01000 - Train loss: 0.92170 - Test loss: 0.90576\n",
      "Epoch 3153 - lr: 0.01000 - Train loss: 0.97604 - Test loss: 0.91951\n",
      "Epoch 3154 - lr: 0.01000 - Train loss: 0.92690 - Test loss: 0.90619\n",
      "Epoch 3155 - lr: 0.01000 - Train loss: 0.96585 - Test loss: 0.92061\n",
      "Epoch 3156 - lr: 0.01000 - Train loss: 0.90767 - Test loss: 0.90424\n",
      "Epoch 3157 - lr: 0.01000 - Train loss: 0.97456 - Test loss: 0.91605\n",
      "Epoch 3158 - lr: 0.01000 - Train loss: 0.88012 - Test loss: 0.90982\n",
      "Epoch 3159 - lr: 0.01000 - Train loss: 0.91995 - Test loss: 0.90147\n",
      "Epoch 3160 - lr: 0.01000 - Train loss: 1.01686 - Test loss: 0.89908\n",
      "Epoch 3161 - lr: 0.01000 - Train loss: 0.95807 - Test loss: 0.89659\n",
      "Epoch 3162 - lr: 0.01000 - Train loss: 0.92573 - Test loss: 0.89917\n",
      "Epoch 3163 - lr: 0.01000 - Train loss: 0.95210 - Test loss: 0.89974\n",
      "Epoch 3164 - lr: 0.01000 - Train loss: 0.93192 - Test loss: 0.90125\n",
      "Epoch 3165 - lr: 0.01000 - Train loss: 0.91578 - Test loss: 0.90545\n",
      "Epoch 3166 - lr: 0.01000 - Train loss: 1.02142 - Test loss: 0.90795\n",
      "Epoch 3167 - lr: 0.01000 - Train loss: 0.98898 - Test loss: 0.93159\n",
      "Epoch 3168 - lr: 0.01000 - Train loss: 0.91053 - Test loss: 0.91262\n",
      "Epoch 3169 - lr: 0.01000 - Train loss: 0.97925 - Test loss: 0.92621\n",
      "Epoch 3170 - lr: 0.01000 - Train loss: 0.89654 - Test loss: 0.92484\n",
      "Epoch 3171 - lr: 0.01000 - Train loss: 0.89395 - Test loss: 0.92659\n",
      "Epoch 3172 - lr: 0.01000 - Train loss: 0.91897 - Test loss: 0.90955\n",
      "Epoch 3173 - lr: 0.01000 - Train loss: 0.97172 - Test loss: 0.92367\n",
      "Epoch 3174 - lr: 0.01000 - Train loss: 0.90863 - Test loss: 0.90763\n",
      "Epoch 3175 - lr: 0.01000 - Train loss: 0.97012 - Test loss: 0.92297\n",
      "Epoch 3176 - lr: 0.01000 - Train loss: 0.90943 - Test loss: 0.90715\n",
      "Epoch 3177 - lr: 0.01000 - Train loss: 0.97497 - Test loss: 0.91984\n",
      "Epoch 3178 - lr: 0.01000 - Train loss: 0.87821 - Test loss: 0.90224\n",
      "Epoch 3179 - lr: 0.01000 - Train loss: 0.92143 - Test loss: 0.92709\n",
      "Epoch 3180 - lr: 0.01000 - Train loss: 0.91533 - Test loss: 0.90717\n",
      "Epoch 3181 - lr: 0.01000 - Train loss: 0.96593 - Test loss: 0.92116\n",
      "Epoch 3182 - lr: 0.01000 - Train loss: 0.90775 - Test loss: 0.90464\n",
      "Epoch 3183 - lr: 0.01000 - Train loss: 1.01089 - Test loss: 0.90359\n",
      "Epoch 3184 - lr: 0.01000 - Train loss: 0.86520 - Test loss: 0.90860\n",
      "Epoch 3185 - lr: 0.01000 - Train loss: 0.91930 - Test loss: 0.90246\n",
      "Epoch 3186 - lr: 0.01000 - Train loss: 0.95164 - Test loss: 0.89746\n",
      "Epoch 3187 - lr: 0.01000 - Train loss: 0.92297 - Test loss: 0.90207\n",
      "Epoch 3188 - lr: 0.01000 - Train loss: 0.96891 - Test loss: 0.89787\n",
      "Epoch 3189 - lr: 0.01000 - Train loss: 0.88595 - Test loss: 0.89745\n",
      "Epoch 3190 - lr: 0.01000 - Train loss: 0.97070 - Test loss: 0.89901\n",
      "Epoch 3191 - lr: 0.01000 - Train loss: 0.88787 - Test loss: 0.90064\n",
      "Epoch 3192 - lr: 0.01000 - Train loss: 0.93011 - Test loss: 0.92847\n",
      "Epoch 3193 - lr: 0.01000 - Train loss: 0.92781 - Test loss: 0.91351\n",
      "Epoch 3194 - lr: 0.01000 - Train loss: 0.96094 - Test loss: 0.90322\n",
      "Epoch 3195 - lr: 0.01000 - Train loss: 0.92245 - Test loss: 0.92842\n",
      "Epoch 3196 - lr: 0.01000 - Train loss: 0.91528 - Test loss: 0.90862\n",
      "Epoch 3197 - lr: 0.01000 - Train loss: 0.96973 - Test loss: 0.92260\n",
      "Epoch 3198 - lr: 0.01000 - Train loss: 0.91381 - Test loss: 0.90699\n",
      "Epoch 3199 - lr: 0.01000 - Train loss: 0.96983 - Test loss: 0.92181\n",
      "Epoch 3200 - lr: 0.01000 - Train loss: 0.91954 - Test loss: 0.90770\n",
      "Epoch 3201 - lr: 0.01000 - Train loss: 0.97797 - Test loss: 0.92153\n",
      "Epoch 3202 - lr: 0.01000 - Train loss: 0.91336 - Test loss: 0.90626\n",
      "Epoch 3203 - lr: 0.01000 - Train loss: 0.97636 - Test loss: 0.91979\n",
      "Epoch 3204 - lr: 0.01000 - Train loss: 0.92821 - Test loss: 0.92780\n",
      "Epoch 3205 - lr: 0.01000 - Train loss: 0.88347 - Test loss: 0.91899\n",
      "Epoch 3206 - lr: 0.01000 - Train loss: 0.93558 - Test loss: 0.90462\n",
      "Epoch 3207 - lr: 0.01000 - Train loss: 0.97036 - Test loss: 0.89255\n",
      "Epoch 3208 - lr: 0.01000 - Train loss: 0.89182 - Test loss: 0.90724\n",
      "Epoch 3209 - lr: 0.01000 - Train loss: 0.86126 - Test loss: 0.90846\n",
      "Epoch 3210 - lr: 0.01000 - Train loss: 0.92223 - Test loss: 0.89449\n",
      "Epoch 3211 - lr: 0.01000 - Train loss: 0.88281 - Test loss: 0.89443\n",
      "Epoch 3212 - lr: 0.01000 - Train loss: 0.98312 - Test loss: 0.91326\n",
      "Epoch 3213 - lr: 0.01000 - Train loss: 0.88849 - Test loss: 0.92030\n",
      "Epoch 3214 - lr: 0.01000 - Train loss: 0.91754 - Test loss: 0.90474\n",
      "Epoch 3215 - lr: 0.01000 - Train loss: 0.93839 - Test loss: 0.89693\n",
      "Epoch 3216 - lr: 0.01000 - Train loss: 0.90871 - Test loss: 0.92164\n",
      "Epoch 3217 - lr: 0.01000 - Train loss: 0.86092 - Test loss: 0.89889\n",
      "Epoch 3218 - lr: 0.01000 - Train loss: 0.95921 - Test loss: 0.90574\n",
      "Epoch 3219 - lr: 0.01000 - Train loss: 0.92716 - Test loss: 0.92519\n",
      "Epoch 3220 - lr: 0.01000 - Train loss: 0.92183 - Test loss: 0.91019\n",
      "Epoch 3221 - lr: 0.01000 - Train loss: 0.91338 - Test loss: 0.89944\n",
      "Epoch 3222 - lr: 0.01000 - Train loss: 0.98619 - Test loss: 0.89701\n",
      "Epoch 3223 - lr: 0.01000 - Train loss: 0.92904 - Test loss: 0.89880\n",
      "Epoch 3224 - lr: 0.01000 - Train loss: 0.96218 - Test loss: 0.91625\n",
      "Epoch 3225 - lr: 0.01000 - Train loss: 0.91886 - Test loss: 0.90222\n",
      "Epoch 3226 - lr: 0.01000 - Train loss: 0.91349 - Test loss: 0.89557\n",
      "Epoch 3227 - lr: 0.01000 - Train loss: 0.87603 - Test loss: 0.90893\n",
      "Epoch 3228 - lr: 0.01000 - Train loss: 0.86741 - Test loss: 0.90502\n",
      "Epoch 3229 - lr: 0.01000 - Train loss: 0.92809 - Test loss: 0.92235\n",
      "Epoch 3230 - lr: 0.01000 - Train loss: 0.90074 - Test loss: 0.92460\n",
      "Epoch 3231 - lr: 0.01000 - Train loss: 0.92214 - Test loss: 0.89976\n",
      "Epoch 3232 - lr: 0.01000 - Train loss: 0.98126 - Test loss: 0.91054\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3233 - lr: 0.01000 - Train loss: 0.93456 - Test loss: 0.91294\n",
      "Epoch 3234 - lr: 0.01000 - Train loss: 0.90111 - Test loss: 0.91269\n",
      "Epoch 3235 - lr: 0.01000 - Train loss: 0.92218 - Test loss: 0.89853\n",
      "Epoch 3236 - lr: 0.01000 - Train loss: 1.02406 - Test loss: 0.89241\n",
      "Epoch 3237 - lr: 0.01000 - Train loss: 0.94189 - Test loss: 0.89318\n",
      "Epoch 3238 - lr: 0.01000 - Train loss: 0.88833 - Test loss: 0.89039\n",
      "Epoch 3239 - lr: 0.01000 - Train loss: 0.91309 - Test loss: 0.89204\n",
      "Epoch 3240 - lr: 0.01000 - Train loss: 0.88738 - Test loss: 0.89114\n",
      "Epoch 3241 - lr: 0.01000 - Train loss: 0.97167 - Test loss: 0.91377\n",
      "Epoch 3242 - lr: 0.01000 - Train loss: 0.92734 - Test loss: 0.90510\n",
      "Epoch 3243 - lr: 0.01000 - Train loss: 0.96053 - Test loss: 0.92156\n",
      "Epoch 3244 - lr: 0.01000 - Train loss: 0.93162 - Test loss: 0.92295\n",
      "Epoch 3245 - lr: 0.01000 - Train loss: 0.90126 - Test loss: 0.92506\n",
      "Epoch 3246 - lr: 0.01000 - Train loss: 0.92148 - Test loss: 0.90004\n",
      "Epoch 3247 - lr: 0.01000 - Train loss: 0.92105 - Test loss: 0.89109\n",
      "Epoch 3248 - lr: 0.01000 - Train loss: 0.92596 - Test loss: 0.89615\n",
      "Epoch 3249 - lr: 0.01000 - Train loss: 0.91756 - Test loss: 0.89075\n",
      "Epoch 3250 - lr: 0.01000 - Train loss: 0.92455 - Test loss: 0.88765\n",
      "Epoch 3251 - lr: 0.01000 - Train loss: 0.88912 - Test loss: 0.90984\n",
      "Epoch 3252 - lr: 0.01000 - Train loss: 0.92571 - Test loss: 0.90214\n",
      "Epoch 3253 - lr: 0.01000 - Train loss: 0.96998 - Test loss: 0.91315\n",
      "Epoch 3254 - lr: 0.01000 - Train loss: 0.92254 - Test loss: 0.89874\n",
      "Epoch 3255 - lr: 0.01000 - Train loss: 0.91279 - Test loss: 0.89166\n",
      "Epoch 3256 - lr: 0.01000 - Train loss: 0.89037 - Test loss: 0.89018\n",
      "Epoch 3257 - lr: 0.01000 - Train loss: 0.91137 - Test loss: 0.89113\n",
      "Epoch 3258 - lr: 0.01000 - Train loss: 0.90730 - Test loss: 0.89306\n",
      "Epoch 3259 - lr: 0.01000 - Train loss: 1.03127 - Test loss: 0.89735\n",
      "Epoch 3260 - lr: 0.01000 - Train loss: 0.97530 - Test loss: 0.92375\n",
      "Epoch 3261 - lr: 0.01000 - Train loss: 0.89361 - Test loss: 0.92651\n",
      "Epoch 3262 - lr: 0.01000 - Train loss: 0.90815 - Test loss: 0.91381\n",
      "Epoch 3263 - lr: 0.01000 - Train loss: 0.88136 - Test loss: 0.91608\n",
      "Epoch 3264 - lr: 0.01000 - Train loss: 0.93534 - Test loss: 0.90139\n",
      "Epoch 3265 - lr: 0.01000 - Train loss: 0.91846 - Test loss: 0.89320\n",
      "Epoch 3266 - lr: 0.01000 - Train loss: 0.94838 - Test loss: 0.89276\n",
      "Epoch 3267 - lr: 0.01000 - Train loss: 0.99198 - Test loss: 0.89079\n",
      "Epoch 3268 - lr: 0.01000 - Train loss: 0.95270 - Test loss: 0.89266\n",
      "Epoch 3269 - lr: 0.01000 - Train loss: 0.91594 - Test loss: 0.89969\n",
      "Epoch 3270 - lr: 0.01000 - Train loss: 0.98039 - Test loss: 0.91990\n",
      "Epoch 3271 - lr: 0.01000 - Train loss: 0.92375 - Test loss: 0.90803\n",
      "Epoch 3272 - lr: 0.01000 - Train loss: 0.96901 - Test loss: 0.92296\n",
      "Epoch 3273 - lr: 0.01000 - Train loss: 0.90870 - Test loss: 0.90696\n",
      "Epoch 3274 - lr: 0.01000 - Train loss: 0.97385 - Test loss: 0.92183\n",
      "Epoch 3275 - lr: 0.01000 - Train loss: 0.91753 - Test loss: 0.90746\n",
      "Epoch 3276 - lr: 0.01000 - Train loss: 0.97433 - Test loss: 0.91928\n",
      "Epoch 3277 - lr: 0.01000 - Train loss: 0.93992 - Test loss: 0.90840\n",
      "Epoch 3278 - lr: 0.01000 - Train loss: 0.91229 - Test loss: 0.90449\n",
      "Epoch 3279 - lr: 0.01000 - Train loss: 0.97412 - Test loss: 0.92106\n",
      "Epoch 3280 - lr: 0.01000 - Train loss: 0.92400 - Test loss: 0.90798\n",
      "Epoch 3281 - lr: 0.01000 - Train loss: 0.97008 - Test loss: 0.92241\n",
      "Epoch 3282 - lr: 0.01000 - Train loss: 0.90917 - Test loss: 0.90653\n",
      "Epoch 3283 - lr: 0.01000 - Train loss: 0.97493 - Test loss: 0.92096\n",
      "Epoch 3284 - lr: 0.01000 - Train loss: 0.91562 - Test loss: 0.90561\n",
      "Epoch 3285 - lr: 0.01000 - Train loss: 0.97466 - Test loss: 0.91906\n",
      "Epoch 3286 - lr: 0.01000 - Train loss: 0.87188 - Test loss: 0.90161\n",
      "Epoch 3287 - lr: 0.01000 - Train loss: 0.92812 - Test loss: 0.92636\n",
      "Epoch 3288 - lr: 0.01000 - Train loss: 0.90866 - Test loss: 0.90573\n",
      "Epoch 3289 - lr: 0.01000 - Train loss: 0.92775 - Test loss: 0.89812\n",
      "Epoch 3290 - lr: 0.01000 - Train loss: 0.92851 - Test loss: 0.92470\n",
      "Epoch 3291 - lr: 0.01000 - Train loss: 0.92008 - Test loss: 0.90693\n",
      "Epoch 3292 - lr: 0.01000 - Train loss: 0.96307 - Test loss: 0.92006\n",
      "Epoch 3293 - lr: 0.01000 - Train loss: 0.92258 - Test loss: 0.90529\n",
      "Epoch 3294 - lr: 0.01000 - Train loss: 0.98773 - Test loss: 0.89524\n",
      "Epoch 3295 - lr: 0.01000 - Train loss: 1.00368 - Test loss: 0.89619\n",
      "Epoch 3296 - lr: 0.01000 - Train loss: 0.95379 - Test loss: 0.89868\n",
      "Epoch 3297 - lr: 0.01000 - Train loss: 0.92452 - Test loss: 0.90224\n",
      "Epoch 3298 - lr: 0.01000 - Train loss: 0.89145 - Test loss: 0.90207\n",
      "Epoch 3299 - lr: 0.01000 - Train loss: 0.93034 - Test loss: 0.92976\n",
      "Epoch 3300 - lr: 0.01000 - Train loss: 0.90984 - Test loss: 0.91057\n",
      "Epoch 3301 - lr: 0.01000 - Train loss: 0.97348 - Test loss: 0.92453\n",
      "Epoch 3302 - lr: 0.01000 - Train loss: 0.91724 - Test loss: 0.90922\n",
      "Epoch 3303 - lr: 0.01000 - Train loss: 0.99726 - Test loss: 0.90989\n",
      "Epoch 3304 - lr: 0.01000 - Train loss: 0.91846 - Test loss: 0.90795\n",
      "Epoch 3305 - lr: 0.01000 - Train loss: 0.97778 - Test loss: 0.90205\n",
      "Epoch 3306 - lr: 0.01000 - Train loss: 0.97756 - Test loss: 0.90108\n",
      "Epoch 3307 - lr: 0.01000 - Train loss: 0.92498 - Test loss: 0.90608\n",
      "Epoch 3308 - lr: 0.01000 - Train loss: 0.92969 - Test loss: 0.90850\n",
      "Epoch 3309 - lr: 0.01000 - Train loss: 0.90182 - Test loss: 0.92600\n",
      "Epoch 3310 - lr: 0.01000 - Train loss: 0.93603 - Test loss: 0.93261\n",
      "Epoch 3311 - lr: 0.01000 - Train loss: 0.90764 - Test loss: 0.93143\n",
      "Epoch 3312 - lr: 0.01000 - Train loss: 0.91858 - Test loss: 0.91350\n",
      "Epoch 3313 - lr: 0.01000 - Train loss: 0.99382 - Test loss: 0.90535\n",
      "Epoch 3314 - lr: 0.01000 - Train loss: 0.95658 - Test loss: 0.90582\n",
      "Epoch 3315 - lr: 0.01000 - Train loss: 0.88603 - Test loss: 0.91558\n",
      "Epoch 3316 - lr: 0.01000 - Train loss: 0.92516 - Test loss: 0.93560\n",
      "Epoch 3317 - lr: 0.01000 - Train loss: 0.91330 - Test loss: 0.91364\n",
      "Epoch 3318 - lr: 0.01000 - Train loss: 0.96919 - Test loss: 0.92289\n",
      "Epoch 3319 - lr: 0.01000 - Train loss: 0.89202 - Test loss: 0.91782\n",
      "Epoch 3320 - lr: 0.01000 - Train loss: 0.89192 - Test loss: 0.92544\n",
      "Epoch 3321 - lr: 0.01000 - Train loss: 0.91828 - Test loss: 0.90941\n",
      "Epoch 3322 - lr: 0.01000 - Train loss: 0.94806 - Test loss: 0.90244\n",
      "Epoch 3323 - lr: 0.01000 - Train loss: 0.91845 - Test loss: 0.90582\n",
      "Epoch 3324 - lr: 0.01000 - Train loss: 0.92942 - Test loss: 0.90420\n",
      "Epoch 3325 - lr: 0.01000 - Train loss: 0.92175 - Test loss: 0.92963\n",
      "Epoch 3326 - lr: 0.01000 - Train loss: 0.92131 - Test loss: 0.91067\n",
      "Epoch 3327 - lr: 0.01000 - Train loss: 1.00740 - Test loss: 0.90798\n",
      "Epoch 3328 - lr: 0.01000 - Train loss: 0.87505 - Test loss: 0.91963\n",
      "Epoch 3329 - lr: 0.01000 - Train loss: 0.89739 - Test loss: 0.92378\n",
      "Epoch 3330 - lr: 0.01000 - Train loss: 0.92132 - Test loss: 0.90832\n",
      "Epoch 3331 - lr: 0.01000 - Train loss: 1.00011 - Test loss: 0.90005\n",
      "Epoch 3332 - lr: 0.01000 - Train loss: 0.97389 - Test loss: 0.92012\n",
      "Epoch 3333 - lr: 0.01000 - Train loss: 0.91802 - Test loss: 0.90209\n",
      "Epoch 3334 - lr: 0.01000 - Train loss: 0.88974 - Test loss: 0.90102\n",
      "Epoch 3335 - lr: 0.01000 - Train loss: 0.96393 - Test loss: 0.89876\n",
      "Epoch 3336 - lr: 0.01000 - Train loss: 0.91227 - Test loss: 0.90166\n",
      "Epoch 3337 - lr: 0.01000 - Train loss: 0.88690 - Test loss: 0.92205\n",
      "Epoch 3338 - lr: 0.01000 - Train loss: 0.94588 - Test loss: 0.90691\n",
      "Epoch 3339 - lr: 0.01000 - Train loss: 0.97269 - Test loss: 0.92098\n",
      "Epoch 3340 - lr: 0.01000 - Train loss: 0.89395 - Test loss: 0.92786\n",
      "Epoch 3341 - lr: 0.01000 - Train loss: 0.91673 - Test loss: 0.91111\n",
      "Epoch 3342 - lr: 0.01000 - Train loss: 0.99151 - Test loss: 0.90416\n",
      "Epoch 3343 - lr: 0.01000 - Train loss: 0.94911 - Test loss: 0.90349\n",
      "Epoch 3344 - lr: 0.01000 - Train loss: 0.95146 - Test loss: 0.90681\n",
      "Epoch 3345 - lr: 0.01000 - Train loss: 0.99226 - Test loss: 0.91699\n",
      "Epoch 3346 - lr: 0.01000 - Train loss: 0.92144 - Test loss: 0.93814\n",
      "Epoch 3347 - lr: 0.01000 - Train loss: 0.91730 - Test loss: 0.91607\n",
      "Epoch 3348 - lr: 0.01000 - Train loss: 1.00715 - Test loss: 0.91390\n",
      "Epoch 3349 - lr: 0.01000 - Train loss: 0.93526 - Test loss: 0.93609\n",
      "Epoch 3350 - lr: 0.01000 - Train loss: 0.91478 - Test loss: 0.91483\n",
      "Epoch 3351 - lr: 0.01000 - Train loss: 0.96315 - Test loss: 0.92189\n",
      "Epoch 3352 - lr: 0.01000 - Train loss: 0.90099 - Test loss: 0.92761\n",
      "Epoch 3353 - lr: 0.01000 - Train loss: 0.87246 - Test loss: 0.91156\n",
      "Epoch 3354 - lr: 0.01000 - Train loss: 0.91727 - Test loss: 0.90336\n",
      "Epoch 3355 - lr: 0.01000 - Train loss: 0.89930 - Test loss: 0.92265\n",
      "Epoch 3356 - lr: 0.01000 - Train loss: 0.94851 - Test loss: 0.90745\n",
      "Epoch 3357 - lr: 0.01000 - Train loss: 0.97326 - Test loss: 0.92599\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3358 - lr: 0.01000 - Train loss: 0.89770 - Test loss: 0.90993\n",
      "Epoch 3359 - lr: 0.01000 - Train loss: 0.92305 - Test loss: 0.93328\n",
      "Epoch 3360 - lr: 0.01000 - Train loss: 0.93468 - Test loss: 0.91855\n",
      "Epoch 3361 - lr: 0.01000 - Train loss: 0.89480 - Test loss: 0.92513\n",
      "Epoch 3362 - lr: 0.01000 - Train loss: 0.91844 - Test loss: 0.93117\n",
      "Epoch 3363 - lr: 0.01000 - Train loss: 0.92696 - Test loss: 0.92232\n",
      "Epoch 3364 - lr: 0.01000 - Train loss: 0.89092 - Test loss: 0.92309\n",
      "Epoch 3365 - lr: 0.01000 - Train loss: 0.92316 - Test loss: 0.92782\n",
      "Epoch 3366 - lr: 0.01000 - Train loss: 0.90222 - Test loss: 0.92473\n",
      "Epoch 3367 - lr: 0.01000 - Train loss: 0.88007 - Test loss: 0.91992\n",
      "Epoch 3368 - lr: 0.01000 - Train loss: 0.87559 - Test loss: 0.89931\n",
      "Epoch 3369 - lr: 0.01000 - Train loss: 0.89588 - Test loss: 0.90924\n",
      "Epoch 3370 - lr: 0.01000 - Train loss: 0.91210 - Test loss: 0.89752\n",
      "Epoch 3371 - lr: 0.01000 - Train loss: 0.98228 - Test loss: 0.89746\n",
      "Epoch 3372 - lr: 0.01000 - Train loss: 0.97593 - Test loss: 0.89752\n",
      "Epoch 3373 - lr: 0.01000 - Train loss: 0.97768 - Test loss: 0.89729\n",
      "Epoch 3374 - lr: 0.01000 - Train loss: 0.97513 - Test loss: 0.89477\n",
      "Epoch 3375 - lr: 0.01000 - Train loss: 0.91684 - Test loss: 0.92156\n",
      "Epoch 3376 - lr: 0.01000 - Train loss: 0.89258 - Test loss: 0.91711\n",
      "Epoch 3377 - lr: 0.01000 - Train loss: 0.87474 - Test loss: 0.91590\n",
      "Epoch 3378 - lr: 0.01000 - Train loss: 0.87295 - Test loss: 0.89535\n",
      "Epoch 3379 - lr: 0.01000 - Train loss: 0.91364 - Test loss: 0.91891\n",
      "Epoch 3380 - lr: 0.01000 - Train loss: 0.93356 - Test loss: 0.90044\n",
      "Epoch 3381 - lr: 0.01000 - Train loss: 0.95272 - Test loss: 0.89036\n",
      "Epoch 3382 - lr: 0.01000 - Train loss: 0.91814 - Test loss: 0.89109\n",
      "Epoch 3383 - lr: 0.01000 - Train loss: 0.91359 - Test loss: 0.91501\n",
      "Epoch 3384 - lr: 0.01000 - Train loss: 0.91217 - Test loss: 0.89276\n",
      "Epoch 3385 - lr: 0.01000 - Train loss: 0.93039 - Test loss: 0.91584\n",
      "Epoch 3386 - lr: 0.01000 - Train loss: 0.93317 - Test loss: 0.91777\n",
      "Epoch 3387 - lr: 0.01000 - Train loss: 0.87948 - Test loss: 0.89373\n",
      "Epoch 3388 - lr: 0.01000 - Train loss: 0.99119 - Test loss: 0.88932\n",
      "Epoch 3389 - lr: 0.01000 - Train loss: 0.88946 - Test loss: 0.89182\n",
      "Epoch 3390 - lr: 0.01000 - Train loss: 1.01873 - Test loss: 0.89282\n",
      "Epoch 3391 - lr: 0.01000 - Train loss: 0.97323 - Test loss: 0.89557\n",
      "Epoch 3392 - lr: 0.01000 - Train loss: 0.91667 - Test loss: 0.92284\n",
      "Epoch 3393 - lr: 0.01000 - Train loss: 0.92325 - Test loss: 0.92597\n",
      "Epoch 3394 - lr: 0.01000 - Train loss: 0.92424 - Test loss: 0.92545\n",
      "Epoch 3395 - lr: 0.01000 - Train loss: 0.93160 - Test loss: 0.92391\n",
      "Epoch 3396 - lr: 0.01000 - Train loss: 0.93340 - Test loss: 0.92244\n",
      "Epoch 3397 - lr: 0.01000 - Train loss: 0.95364 - Test loss: 0.89603\n",
      "Epoch 3398 - lr: 0.01000 - Train loss: 0.98633 - Test loss: 0.89829\n",
      "Epoch 3399 - lr: 0.01000 - Train loss: 0.96343 - Test loss: 0.89691\n",
      "Epoch 3400 - lr: 0.01000 - Train loss: 0.98876 - Test loss: 0.89212\n",
      "Epoch 3401 - lr: 0.01000 - Train loss: 0.91069 - Test loss: 0.91378\n",
      "Epoch 3402 - lr: 0.01000 - Train loss: 0.93250 - Test loss: 0.91927\n",
      "Epoch 3403 - lr: 0.01000 - Train loss: 0.89419 - Test loss: 0.90458\n",
      "Epoch 3404 - lr: 0.01000 - Train loss: 0.93479 - Test loss: 0.91747\n",
      "Epoch 3405 - lr: 0.01000 - Train loss: 0.90420 - Test loss: 0.91766\n",
      "Epoch 3406 - lr: 0.01000 - Train loss: 0.96284 - Test loss: 0.89693\n",
      "Epoch 3407 - lr: 0.01000 - Train loss: 0.98546 - Test loss: 0.91740\n",
      "Epoch 3408 - lr: 0.01000 - Train loss: 0.92896 - Test loss: 0.89738\n",
      "Epoch 3409 - lr: 0.01000 - Train loss: 0.90153 - Test loss: 0.89758\n",
      "Epoch 3410 - lr: 0.01000 - Train loss: 1.00673 - Test loss: 0.89481\n",
      "Epoch 3411 - lr: 0.01000 - Train loss: 0.89826 - Test loss: 0.89420\n",
      "Epoch 3412 - lr: 0.01000 - Train loss: 0.91831 - Test loss: 0.92165\n",
      "Epoch 3413 - lr: 0.01000 - Train loss: 0.92336 - Test loss: 0.90270\n",
      "Epoch 3414 - lr: 0.01000 - Train loss: 0.95966 - Test loss: 0.89510\n",
      "Epoch 3415 - lr: 0.01000 - Train loss: 0.98941 - Test loss: 0.89849\n",
      "Epoch 3416 - lr: 0.01000 - Train loss: 0.98841 - Test loss: 0.90047\n",
      "Epoch 3417 - lr: 0.01000 - Train loss: 0.93721 - Test loss: 0.89810\n",
      "Epoch 3418 - lr: 0.01000 - Train loss: 0.90744 - Test loss: 0.92271\n",
      "Epoch 3419 - lr: 0.01000 - Train loss: 0.89198 - Test loss: 0.92217\n",
      "Epoch 3420 - lr: 0.01000 - Train loss: 0.93093 - Test loss: 0.92564\n",
      "Epoch 3421 - lr: 0.01000 - Train loss: 0.91540 - Test loss: 0.90310\n",
      "Epoch 3422 - lr: 0.01000 - Train loss: 0.93525 - Test loss: 0.89568\n",
      "Epoch 3423 - lr: 0.01000 - Train loss: 0.91446 - Test loss: 0.89822\n",
      "Epoch 3424 - lr: 0.01000 - Train loss: 0.95921 - Test loss: 0.89503\n",
      "Epoch 3425 - lr: 0.01000 - Train loss: 0.96487 - Test loss: 0.89304\n",
      "Epoch 3426 - lr: 0.01000 - Train loss: 0.87268 - Test loss: 0.91310\n",
      "Epoch 3427 - lr: 0.01000 - Train loss: 0.87162 - Test loss: 0.91736\n",
      "Epoch 3428 - lr: 0.01000 - Train loss: 0.88545 - Test loss: 0.91975\n",
      "Epoch 3429 - lr: 0.01000 - Train loss: 0.93667 - Test loss: 0.90600\n",
      "Epoch 3430 - lr: 0.01000 - Train loss: 0.97382 - Test loss: 0.90094\n",
      "Epoch 3431 - lr: 0.01000 - Train loss: 0.96675 - Test loss: 0.89263\n",
      "Epoch 3432 - lr: 0.01000 - Train loss: 0.87203 - Test loss: 0.91206\n",
      "Epoch 3433 - lr: 0.01000 - Train loss: 0.86071 - Test loss: 0.90992\n",
      "Epoch 3434 - lr: 0.01000 - Train loss: 0.92106 - Test loss: 0.89495\n",
      "Epoch 3435 - lr: 0.01000 - Train loss: 1.01089 - Test loss: 0.91994\n",
      "Epoch 3436 - lr: 0.01000 - Train loss: 0.93722 - Test loss: 0.90694\n",
      "Epoch 3437 - lr: 0.01000 - Train loss: 0.96818 - Test loss: 0.92512\n",
      "Epoch 3438 - lr: 0.01000 - Train loss: 0.92526 - Test loss: 0.90515\n",
      "Epoch 3439 - lr: 0.01000 - Train loss: 0.98782 - Test loss: 0.89369\n",
      "Epoch 3440 - lr: 0.01000 - Train loss: 0.98313 - Test loss: 0.91412\n",
      "Epoch 3441 - lr: 0.01000 - Train loss: 0.94744 - Test loss: 0.90496\n",
      "Epoch 3442 - lr: 0.01000 - Train loss: 0.86547 - Test loss: 0.89839\n",
      "Epoch 3443 - lr: 0.01000 - Train loss: 0.91005 - Test loss: 0.90100\n",
      "Epoch 3444 - lr: 0.01000 - Train loss: 0.96483 - Test loss: 0.89472\n",
      "Epoch 3445 - lr: 0.01000 - Train loss: 0.89377 - Test loss: 0.91647\n",
      "Epoch 3446 - lr: 0.01000 - Train loss: 0.91414 - Test loss: 0.90436\n",
      "Epoch 3447 - lr: 0.01000 - Train loss: 0.93630 - Test loss: 0.89834\n",
      "Epoch 3448 - lr: 0.01000 - Train loss: 0.91981 - Test loss: 0.92559\n",
      "Epoch 3449 - lr: 0.01000 - Train loss: 0.91898 - Test loss: 0.90673\n",
      "Epoch 3450 - lr: 0.01000 - Train loss: 0.91774 - Test loss: 0.89806\n",
      "Epoch 3451 - lr: 0.01000 - Train loss: 0.90838 - Test loss: 0.90152\n",
      "Epoch 3452 - lr: 0.01000 - Train loss: 0.96553 - Test loss: 0.89535\n",
      "Epoch 3453 - lr: 0.01000 - Train loss: 0.88263 - Test loss: 0.91674\n",
      "Epoch 3454 - lr: 0.01000 - Train loss: 0.87163 - Test loss: 0.90482\n",
      "Epoch 3455 - lr: 0.01000 - Train loss: 0.87591 - Test loss: 0.90102\n",
      "Epoch 3456 - lr: 0.01000 - Train loss: 0.89800 - Test loss: 0.92022\n",
      "Epoch 3457 - lr: 0.01000 - Train loss: 0.88573 - Test loss: 0.92094\n",
      "Epoch 3458 - lr: 0.01000 - Train loss: 0.92095 - Test loss: 0.90302\n",
      "Epoch 3459 - lr: 0.01000 - Train loss: 0.96656 - Test loss: 0.89337\n",
      "Epoch 3460 - lr: 0.01000 - Train loss: 0.87122 - Test loss: 0.91225\n",
      "Epoch 3461 - lr: 0.01000 - Train loss: 0.89561 - Test loss: 0.91894\n",
      "Epoch 3462 - lr: 0.01000 - Train loss: 0.92441 - Test loss: 0.90458\n",
      "Epoch 3463 - lr: 0.01000 - Train loss: 0.92763 - Test loss: 0.89693\n",
      "Epoch 3464 - lr: 0.01000 - Train loss: 0.92353 - Test loss: 0.92315\n",
      "Epoch 3465 - lr: 0.01000 - Train loss: 0.92032 - Test loss: 0.90482\n",
      "Epoch 3466 - lr: 0.01000 - Train loss: 0.96326 - Test loss: 0.89317\n",
      "Epoch 3467 - lr: 0.01000 - Train loss: 0.88973 - Test loss: 0.89567\n",
      "Epoch 3468 - lr: 0.01000 - Train loss: 0.91496 - Test loss: 0.92278\n",
      "Epoch 3469 - lr: 0.01000 - Train loss: 0.89290 - Test loss: 0.91862\n",
      "Epoch 3470 - lr: 0.01000 - Train loss: 0.89831 - Test loss: 0.91813\n",
      "Epoch 3471 - lr: 0.01000 - Train loss: 0.92737 - Test loss: 0.90306\n",
      "Epoch 3472 - lr: 0.01000 - Train loss: 0.91345 - Test loss: 0.89541\n",
      "Epoch 3473 - lr: 0.01000 - Train loss: 0.93481 - Test loss: 0.89144\n",
      "Epoch 3474 - lr: 0.01000 - Train loss: 0.95717 - Test loss: 0.89353\n",
      "Epoch 3475 - lr: 0.01000 - Train loss: 0.96927 - Test loss: 0.91674\n",
      "Epoch 3476 - lr: 0.01000 - Train loss: 0.92087 - Test loss: 0.90467\n",
      "Epoch 3477 - lr: 0.01000 - Train loss: 0.96718 - Test loss: 0.89436\n",
      "Epoch 3478 - lr: 0.01000 - Train loss: 0.89926 - Test loss: 0.91458\n",
      "Epoch 3479 - lr: 0.01000 - Train loss: 0.88532 - Test loss: 0.90232\n",
      "Epoch 3480 - lr: 0.01000 - Train loss: 0.92367 - Test loss: 0.92609\n",
      "Epoch 3481 - lr: 0.01000 - Train loss: 0.92122 - Test loss: 0.90689\n",
      "Epoch 3482 - lr: 0.01000 - Train loss: 0.94108 - Test loss: 0.89765\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3483 - lr: 0.01000 - Train loss: 0.91846 - Test loss: 0.92418\n",
      "Epoch 3484 - lr: 0.01000 - Train loss: 0.92540 - Test loss: 0.90647\n",
      "Epoch 3485 - lr: 0.01000 - Train loss: 0.96441 - Test loss: 0.89689\n",
      "Epoch 3486 - lr: 0.01000 - Train loss: 0.88010 - Test loss: 0.89549\n",
      "Epoch 3487 - lr: 0.01000 - Train loss: 1.01306 - Test loss: 0.89999\n",
      "Epoch 3488 - lr: 0.01000 - Train loss: 0.95296 - Test loss: 0.89859\n",
      "Epoch 3489 - lr: 0.01000 - Train loss: 0.97600 - Test loss: 0.91077\n",
      "Epoch 3490 - lr: 0.01000 - Train loss: 0.92174 - Test loss: 0.92830\n",
      "Epoch 3491 - lr: 0.01000 - Train loss: 0.92360 - Test loss: 0.90910\n",
      "Epoch 3492 - lr: 0.01000 - Train loss: 0.97310 - Test loss: 0.92239\n",
      "Epoch 3493 - lr: 0.01000 - Train loss: 0.87495 - Test loss: 0.90593\n",
      "Epoch 3494 - lr: 0.01000 - Train loss: 0.92913 - Test loss: 0.91507\n",
      "Epoch 3495 - lr: 0.01000 - Train loss: 0.88817 - Test loss: 0.90191\n",
      "Epoch 3496 - lr: 0.01000 - Train loss: 0.91913 - Test loss: 0.92701\n",
      "Epoch 3497 - lr: 0.01000 - Train loss: 0.91406 - Test loss: 0.90636\n",
      "Epoch 3498 - lr: 0.01000 - Train loss: 0.95472 - Test loss: 0.89640\n",
      "Epoch 3499 - lr: 0.01000 - Train loss: 0.93616 - Test loss: 0.89985\n",
      "Epoch 3500 - lr: 0.01000 - Train loss: 0.91976 - Test loss: 0.92594\n",
      "Epoch 3501 - lr: 0.01000 - Train loss: 0.93189 - Test loss: 0.92906\n",
      "Epoch 3502 - lr: 0.01000 - Train loss: 0.91357 - Test loss: 0.90617\n",
      "Epoch 3503 - lr: 0.01000 - Train loss: 0.97243 - Test loss: 0.91784\n",
      "Epoch 3504 - lr: 0.01000 - Train loss: 0.92093 - Test loss: 0.92669\n",
      "Epoch 3505 - lr: 0.01000 - Train loss: 0.91510 - Test loss: 0.90411\n",
      "Epoch 3506 - lr: 0.01000 - Train loss: 0.97419 - Test loss: 0.91746\n",
      "Epoch 3507 - lr: 0.01000 - Train loss: 0.92038 - Test loss: 0.91168\n",
      "Epoch 3508 - lr: 0.01000 - Train loss: 0.91120 - Test loss: 0.92592\n",
      "Epoch 3509 - lr: 0.01000 - Train loss: 0.92337 - Test loss: 0.90343\n",
      "Epoch 3510 - lr: 0.01000 - Train loss: 0.95972 - Test loss: 0.89378\n",
      "Epoch 3511 - lr: 0.01000 - Train loss: 0.96843 - Test loss: 0.89048\n",
      "Epoch 3512 - lr: 0.01000 - Train loss: 0.89825 - Test loss: 0.91172\n",
      "Epoch 3513 - lr: 0.01000 - Train loss: 0.87641 - Test loss: 0.89987\n",
      "Epoch 3514 - lr: 0.01000 - Train loss: 0.92112 - Test loss: 0.89950\n",
      "Epoch 3515 - lr: 0.01000 - Train loss: 0.95754 - Test loss: 0.89560\n",
      "Epoch 3516 - lr: 0.01000 - Train loss: 0.97136 - Test loss: 0.91727\n",
      "Epoch 3517 - lr: 0.01000 - Train loss: 0.93683 - Test loss: 0.90814\n",
      "Epoch 3518 - lr: 0.01000 - Train loss: 0.97005 - Test loss: 0.92671\n",
      "Epoch 3519 - lr: 0.01000 - Train loss: 0.92295 - Test loss: 0.90599\n",
      "Epoch 3520 - lr: 0.01000 - Train loss: 0.92220 - Test loss: 0.89635\n",
      "Epoch 3521 - lr: 0.01000 - Train loss: 0.91804 - Test loss: 0.92300\n",
      "Epoch 3522 - lr: 0.01000 - Train loss: 0.92560 - Test loss: 0.90548\n",
      "Epoch 3523 - lr: 0.01000 - Train loss: 0.93830 - Test loss: 0.89668\n",
      "Epoch 3524 - lr: 0.01000 - Train loss: 0.90837 - Test loss: 0.91162\n",
      "Epoch 3525 - lr: 0.01000 - Train loss: 0.92565 - Test loss: 0.90192\n",
      "Epoch 3526 - lr: 0.01000 - Train loss: 0.98674 - Test loss: 0.89221\n",
      "Epoch 3527 - lr: 0.01000 - Train loss: 0.98527 - Test loss: 0.90554\n",
      "Epoch 3528 - lr: 0.01000 - Train loss: 0.92301 - Test loss: 0.92677\n",
      "Epoch 3529 - lr: 0.01000 - Train loss: 0.91148 - Test loss: 0.90541\n",
      "Epoch 3530 - lr: 0.01000 - Train loss: 0.96500 - Test loss: 0.89367\n",
      "Epoch 3531 - lr: 0.01000 - Train loss: 0.88012 - Test loss: 0.89520\n",
      "Epoch 3532 - lr: 0.01000 - Train loss: 0.97414 - Test loss: 0.91769\n",
      "Epoch 3533 - lr: 0.01000 - Train loss: 0.93650 - Test loss: 0.91060\n",
      "Epoch 3534 - lr: 0.01000 - Train loss: 0.94521 - Test loss: 0.89990\n",
      "Epoch 3535 - lr: 0.01000 - Train loss: 0.93846 - Test loss: 0.90148\n",
      "Epoch 3536 - lr: 0.01000 - Train loss: 0.91186 - Test loss: 0.89779\n",
      "Epoch 3537 - lr: 0.01000 - Train loss: 0.87204 - Test loss: 0.90022\n",
      "Epoch 3538 - lr: 0.01000 - Train loss: 0.92179 - Test loss: 0.92734\n",
      "Epoch 3539 - lr: 0.01000 - Train loss: 0.92081 - Test loss: 0.90925\n",
      "Epoch 3540 - lr: 0.01000 - Train loss: 0.95850 - Test loss: 0.89799\n",
      "Epoch 3541 - lr: 0.01000 - Train loss: 0.90828 - Test loss: 0.89828\n",
      "Epoch 3542 - lr: 0.01000 - Train loss: 0.87721 - Test loss: 0.90304\n",
      "Epoch 3543 - lr: 0.01000 - Train loss: 0.96110 - Test loss: 0.90334\n",
      "Epoch 3544 - lr: 0.01000 - Train loss: 0.92594 - Test loss: 0.90271\n",
      "Epoch 3545 - lr: 0.01000 - Train loss: 0.92401 - Test loss: 0.93015\n",
      "Epoch 3546 - lr: 0.01000 - Train loss: 0.91467 - Test loss: 0.91079\n",
      "Epoch 3547 - lr: 0.01000 - Train loss: 0.96138 - Test loss: 0.90047\n",
      "Epoch 3548 - lr: 0.01000 - Train loss: 0.89429 - Test loss: 0.91695\n",
      "Epoch 3549 - lr: 0.01000 - Train loss: 0.89636 - Test loss: 0.92515\n",
      "Epoch 3550 - lr: 0.01000 - Train loss: 0.92870 - Test loss: 0.91110\n",
      "Epoch 3551 - lr: 0.01000 - Train loss: 0.96727 - Test loss: 0.92055\n",
      "Epoch 3552 - lr: 0.01000 - Train loss: 0.89523 - Test loss: 0.92513\n",
      "Epoch 3553 - lr: 0.01000 - Train loss: 0.93641 - Test loss: 0.91279\n",
      "Epoch 3554 - lr: 0.01000 - Train loss: 0.96188 - Test loss: 0.91777\n",
      "Epoch 3555 - lr: 0.01000 - Train loss: 0.90920 - Test loss: 0.92993\n",
      "Epoch 3556 - lr: 0.01000 - Train loss: 0.88897 - Test loss: 0.90582\n",
      "Epoch 3557 - lr: 0.01000 - Train loss: 0.92059 - Test loss: 0.91152\n",
      "Epoch 3558 - lr: 0.01000 - Train loss: 0.90862 - Test loss: 0.92640\n",
      "Epoch 3559 - lr: 0.01000 - Train loss: 0.92704 - Test loss: 0.90539\n",
      "Epoch 3560 - lr: 0.01000 - Train loss: 0.93741 - Test loss: 0.89553\n",
      "Epoch 3561 - lr: 0.01000 - Train loss: 0.93814 - Test loss: 0.90240\n",
      "Epoch 3562 - lr: 0.01000 - Train loss: 0.94244 - Test loss: 0.89771\n",
      "Epoch 3563 - lr: 0.01000 - Train loss: 0.87228 - Test loss: 0.89849\n",
      "Epoch 3564 - lr: 0.01000 - Train loss: 0.94365 - Test loss: 0.90174\n",
      "Epoch 3565 - lr: 0.01000 - Train loss: 0.91626 - Test loss: 0.90036\n",
      "Epoch 3566 - lr: 0.01000 - Train loss: 0.95720 - Test loss: 0.89551\n",
      "Epoch 3567 - lr: 0.01000 - Train loss: 0.96884 - Test loss: 0.91765\n",
      "Epoch 3568 - lr: 0.01000 - Train loss: 0.91932 - Test loss: 0.90495\n",
      "Epoch 3569 - lr: 0.01000 - Train loss: 0.94320 - Test loss: 0.91401\n",
      "Epoch 3570 - lr: 0.01000 - Train loss: 0.89662 - Test loss: 0.91923\n",
      "Epoch 3571 - lr: 0.01000 - Train loss: 0.92462 - Test loss: 0.90493\n",
      "Epoch 3572 - lr: 0.01000 - Train loss: 0.95119 - Test loss: 0.89510\n",
      "Epoch 3573 - lr: 0.01000 - Train loss: 0.88537 - Test loss: 0.91311\n",
      "Epoch 3574 - lr: 0.01000 - Train loss: 0.85436 - Test loss: 0.89869\n",
      "Epoch 3575 - lr: 0.01000 - Train loss: 0.91481 - Test loss: 0.91492\n",
      "Epoch 3576 - lr: 0.01000 - Train loss: 0.89284 - Test loss: 0.91334\n",
      "Epoch 3577 - lr: 0.01000 - Train loss: 0.96012 - Test loss: 0.89654\n",
      "Epoch 3578 - lr: 0.01000 - Train loss: 0.97811 - Test loss: 0.89326\n",
      "Epoch 3579 - lr: 0.01000 - Train loss: 0.88849 - Test loss: 0.89372\n",
      "Epoch 3580 - lr: 0.01000 - Train loss: 0.96760 - Test loss: 0.89528\n",
      "Epoch 3581 - lr: 0.01000 - Train loss: 0.97073 - Test loss: 0.91770\n",
      "Epoch 3582 - lr: 0.01000 - Train loss: 0.91746 - Test loss: 0.90483\n",
      "Epoch 3583 - lr: 0.01000 - Train loss: 0.96631 - Test loss: 0.89506\n",
      "Epoch 3584 - lr: 0.01000 - Train loss: 0.89780 - Test loss: 0.91407\n",
      "Epoch 3585 - lr: 0.01000 - Train loss: 0.89455 - Test loss: 0.92192\n",
      "Epoch 3586 - lr: 0.01000 - Train loss: 0.89883 - Test loss: 0.91658\n",
      "Epoch 3587 - lr: 0.01000 - Train loss: 0.91736 - Test loss: 0.92805\n",
      "Epoch 3588 - lr: 0.01000 - Train loss: 0.92324 - Test loss: 0.90559\n",
      "Epoch 3589 - lr: 0.01000 - Train loss: 0.91446 - Test loss: 0.89634\n",
      "Epoch 3590 - lr: 0.01000 - Train loss: 0.92063 - Test loss: 0.89261\n",
      "Epoch 3591 - lr: 0.01000 - Train loss: 0.88006 - Test loss: 0.89409\n",
      "Epoch 3592 - lr: 0.01000 - Train loss: 0.97047 - Test loss: 0.91682\n",
      "Epoch 3593 - lr: 0.01000 - Train loss: 0.91361 - Test loss: 0.90335\n",
      "Epoch 3594 - lr: 0.01000 - Train loss: 0.96212 - Test loss: 0.89483\n",
      "Epoch 3595 - lr: 0.01000 - Train loss: 0.89153 - Test loss: 0.91576\n",
      "Epoch 3596 - lr: 0.01000 - Train loss: 0.85627 - Test loss: 0.90086\n",
      "Epoch 3597 - lr: 0.01000 - Train loss: 0.89390 - Test loss: 0.89711\n",
      "Epoch 3598 - lr: 0.01000 - Train loss: 0.92297 - Test loss: 0.92416\n",
      "Epoch 3599 - lr: 0.01000 - Train loss: 0.92288 - Test loss: 0.90627\n",
      "Epoch 3600 - lr: 0.01000 - Train loss: 0.96580 - Test loss: 0.89452\n",
      "Epoch 3601 - lr: 0.01000 - Train loss: 0.88285 - Test loss: 0.91543\n",
      "Epoch 3602 - lr: 0.01000 - Train loss: 0.95820 - Test loss: 0.90078\n",
      "Epoch 3603 - lr: 0.01000 - Train loss: 0.95302 - Test loss: 0.89739\n",
      "Epoch 3604 - lr: 0.01000 - Train loss: 0.98200 - Test loss: 0.89760\n",
      "Epoch 3605 - lr: 0.01000 - Train loss: 1.00884 - Test loss: 0.90156\n",
      "Epoch 3606 - lr: 0.01000 - Train loss: 0.97343 - Test loss: 0.92346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3607 - lr: 0.01000 - Train loss: 0.91449 - Test loss: 0.90600\n",
      "Epoch 3608 - lr: 0.01000 - Train loss: 0.87912 - Test loss: 0.91548\n",
      "Epoch 3609 - lr: 0.01000 - Train loss: 0.89668 - Test loss: 0.93014\n",
      "Epoch 3610 - lr: 0.01000 - Train loss: 0.90097 - Test loss: 0.92577\n",
      "Epoch 3611 - lr: 0.01000 - Train loss: 0.90810 - Test loss: 0.90381\n",
      "Epoch 3612 - lr: 0.01000 - Train loss: 0.91616 - Test loss: 0.90584\n",
      "Epoch 3613 - lr: 0.01000 - Train loss: 0.94250 - Test loss: 0.90020\n",
      "Epoch 3614 - lr: 0.01000 - Train loss: 0.89666 - Test loss: 0.91727\n",
      "Epoch 3615 - lr: 0.01000 - Train loss: 0.91938 - Test loss: 0.93181\n",
      "Epoch 3616 - lr: 0.01000 - Train loss: 0.91564 - Test loss: 0.90809\n",
      "Epoch 3617 - lr: 0.01000 - Train loss: 0.95630 - Test loss: 0.89709\n",
      "Epoch 3618 - lr: 0.01000 - Train loss: 0.91995 - Test loss: 0.89790\n",
      "Epoch 3619 - lr: 0.01000 - Train loss: 0.88315 - Test loss: 0.89948\n",
      "Epoch 3620 - lr: 0.01000 - Train loss: 0.93303 - Test loss: 0.90156\n",
      "Epoch 3621 - lr: 0.01000 - Train loss: 0.92157 - Test loss: 0.92881\n",
      "Epoch 3622 - lr: 0.01000 - Train loss: 0.91917 - Test loss: 0.90942\n",
      "Epoch 3623 - lr: 0.01000 - Train loss: 0.94145 - Test loss: 0.90168\n",
      "Epoch 3624 - lr: 0.01000 - Train loss: 0.92047 - Test loss: 0.92853\n",
      "Epoch 3625 - lr: 0.01000 - Train loss: 0.91388 - Test loss: 0.90878\n",
      "Epoch 3626 - lr: 0.01000 - Train loss: 0.93974 - Test loss: 0.90129\n",
      "Epoch 3627 - lr: 0.01000 - Train loss: 0.92381 - Test loss: 0.92800\n",
      "Epoch 3628 - lr: 0.01000 - Train loss: 0.91688 - Test loss: 0.90879\n",
      "Epoch 3629 - lr: 0.01000 - Train loss: 0.96025 - Test loss: 0.89792\n",
      "Epoch 3630 - lr: 0.01000 - Train loss: 0.88459 - Test loss: 0.89945\n",
      "Epoch 3631 - lr: 0.01000 - Train loss: 0.96294 - Test loss: 0.89823\n",
      "Epoch 3632 - lr: 0.01000 - Train loss: 0.88638 - Test loss: 0.90112\n",
      "Epoch 3633 - lr: 0.01000 - Train loss: 0.94227 - Test loss: 0.90230\n",
      "Epoch 3634 - lr: 0.01000 - Train loss: 0.93191 - Test loss: 0.91404\n",
      "Epoch 3635 - lr: 0.01000 - Train loss: 0.92442 - Test loss: 0.91082\n",
      "Epoch 3636 - lr: 0.01000 - Train loss: 0.97294 - Test loss: 0.92660\n",
      "Epoch 3637 - lr: 0.01000 - Train loss: 0.92616 - Test loss: 0.93398\n",
      "Epoch 3638 - lr: 0.01000 - Train loss: 0.91569 - Test loss: 0.91195\n",
      "Epoch 3639 - lr: 0.01000 - Train loss: 0.91880 - Test loss: 0.90561\n",
      "Epoch 3640 - lr: 0.01000 - Train loss: 0.92137 - Test loss: 0.93024\n",
      "Epoch 3641 - lr: 0.01000 - Train loss: 0.91924 - Test loss: 0.91070\n",
      "Epoch 3642 - lr: 0.01000 - Train loss: 0.92822 - Test loss: 0.90271\n",
      "Epoch 3643 - lr: 0.01000 - Train loss: 0.92241 - Test loss: 0.92860\n",
      "Epoch 3644 - lr: 0.01000 - Train loss: 0.92071 - Test loss: 0.90987\n",
      "Epoch 3645 - lr: 0.01000 - Train loss: 0.91823 - Test loss: 0.90150\n",
      "Epoch 3646 - lr: 0.01000 - Train loss: 0.90825 - Test loss: 0.90405\n",
      "Epoch 3647 - lr: 0.01000 - Train loss: 0.98693 - Test loss: 0.89954\n",
      "Epoch 3648 - lr: 0.01000 - Train loss: 0.90851 - Test loss: 0.90305\n",
      "Epoch 3649 - lr: 0.01000 - Train loss: 0.91856 - Test loss: 0.90137\n",
      "Epoch 3650 - lr: 0.01000 - Train loss: 0.89840 - Test loss: 0.90384\n",
      "Epoch 3651 - lr: 0.01000 - Train loss: 0.97519 - Test loss: 0.92346\n",
      "Epoch 3652 - lr: 0.01000 - Train loss: 0.85565 - Test loss: 0.90845\n",
      "Epoch 3653 - lr: 0.01000 - Train loss: 0.91604 - Test loss: 0.90289\n",
      "Epoch 3654 - lr: 0.01000 - Train loss: 0.89334 - Test loss: 0.92318\n",
      "Epoch 3655 - lr: 0.01000 - Train loss: 0.91942 - Test loss: 0.93286\n",
      "Epoch 3656 - lr: 0.01000 - Train loss: 0.93422 - Test loss: 0.93241\n",
      "Epoch 3657 - lr: 0.01000 - Train loss: 0.92631 - Test loss: 0.90731\n",
      "Epoch 3658 - lr: 0.01000 - Train loss: 0.89699 - Test loss: 0.92106\n",
      "Epoch 3659 - lr: 0.01000 - Train loss: 0.93911 - Test loss: 0.90463\n",
      "Epoch 3660 - lr: 0.01000 - Train loss: 0.96248 - Test loss: 0.92815\n",
      "Epoch 3661 - lr: 0.01000 - Train loss: 0.91647 - Test loss: 0.90608\n",
      "Epoch 3662 - lr: 0.01000 - Train loss: 0.87433 - Test loss: 0.90356\n",
      "Epoch 3663 - lr: 0.01000 - Train loss: 0.88012 - Test loss: 0.90397\n",
      "Epoch 3664 - lr: 0.01000 - Train loss: 0.90031 - Test loss: 0.92533\n",
      "Epoch 3665 - lr: 0.01000 - Train loss: 0.89212 - Test loss: 0.92346\n",
      "Epoch 3666 - lr: 0.01000 - Train loss: 0.94707 - Test loss: 0.90060\n",
      "Epoch 3667 - lr: 0.01000 - Train loss: 0.91606 - Test loss: 0.89989\n",
      "Epoch 3668 - lr: 0.01000 - Train loss: 0.89751 - Test loss: 0.91467\n",
      "Epoch 3669 - lr: 0.01000 - Train loss: 0.91882 - Test loss: 0.90402\n",
      "Epoch 3670 - lr: 0.01000 - Train loss: 0.94722 - Test loss: 0.89830\n",
      "Epoch 3671 - lr: 0.01000 - Train loss: 0.90252 - Test loss: 0.92091\n",
      "Epoch 3672 - lr: 0.01000 - Train loss: 0.88088 - Test loss: 0.91992\n",
      "Epoch 3673 - lr: 0.01000 - Train loss: 0.87029 - Test loss: 0.90124\n",
      "Epoch 3674 - lr: 0.01000 - Train loss: 0.87720 - Test loss: 0.89581\n",
      "Epoch 3675 - lr: 0.01000 - Train loss: 0.89822 - Test loss: 0.91028\n",
      "Epoch 3676 - lr: 0.01000 - Train loss: 0.91529 - Test loss: 0.92392\n",
      "Epoch 3677 - lr: 0.01000 - Train loss: 0.91921 - Test loss: 0.90045\n",
      "Epoch 3678 - lr: 0.01000 - Train loss: 0.99053 - Test loss: 0.89016\n",
      "Epoch 3679 - lr: 0.01000 - Train loss: 0.91168 - Test loss: 0.90660\n",
      "Epoch 3680 - lr: 0.01000 - Train loss: 0.89243 - Test loss: 0.89485\n",
      "Epoch 3681 - lr: 0.01000 - Train loss: 0.91630 - Test loss: 0.89350\n",
      "Epoch 3682 - lr: 0.01000 - Train loss: 0.93534 - Test loss: 0.91429\n",
      "Epoch 3683 - lr: 0.01000 - Train loss: 0.89762 - Test loss: 0.89765\n",
      "Epoch 3684 - lr: 0.01000 - Train loss: 0.98462 - Test loss: 0.89288\n",
      "Epoch 3685 - lr: 0.01000 - Train loss: 0.96921 - Test loss: 0.91595\n",
      "Epoch 3686 - lr: 0.01000 - Train loss: 0.93240 - Test loss: 0.92179\n",
      "Epoch 3687 - lr: 0.01000 - Train loss: 0.89171 - Test loss: 0.90790\n",
      "Epoch 3688 - lr: 0.01000 - Train loss: 0.88141 - Test loss: 0.91537\n",
      "Epoch 3689 - lr: 0.01000 - Train loss: 0.89823 - Test loss: 0.91755\n",
      "Epoch 3690 - lr: 0.01000 - Train loss: 0.92565 - Test loss: 0.92143\n",
      "Epoch 3691 - lr: 0.01000 - Train loss: 0.92910 - Test loss: 0.92118\n",
      "Epoch 3692 - lr: 0.01000 - Train loss: 0.93021 - Test loss: 0.92028\n",
      "Epoch 3693 - lr: 0.01000 - Train loss: 0.93326 - Test loss: 0.91852\n",
      "Epoch 3694 - lr: 0.01000 - Train loss: 0.88242 - Test loss: 0.89483\n",
      "Epoch 3695 - lr: 0.01000 - Train loss: 0.90847 - Test loss: 0.91660\n",
      "Epoch 3696 - lr: 0.01000 - Train loss: 0.92719 - Test loss: 0.91079\n",
      "Epoch 3697 - lr: 0.01000 - Train loss: 0.87917 - Test loss: 0.88941\n",
      "Epoch 3698 - lr: 0.01000 - Train loss: 0.96769 - Test loss: 0.91377\n",
      "Epoch 3699 - lr: 0.01000 - Train loss: 0.92586 - Test loss: 0.91549\n",
      "Epoch 3700 - lr: 0.01000 - Train loss: 0.92497 - Test loss: 0.89039\n",
      "Epoch 3701 - lr: 0.01000 - Train loss: 0.85818 - Test loss: 0.88724\n",
      "Epoch 3702 - lr: 0.01000 - Train loss: 0.87092 - Test loss: 0.89957\n",
      "Epoch 3703 - lr: 0.01000 - Train loss: 0.87161 - Test loss: 0.88539\n",
      "Epoch 3704 - lr: 0.01000 - Train loss: 0.97282 - Test loss: 0.88865\n",
      "Epoch 3705 - lr: 0.01000 - Train loss: 0.95782 - Test loss: 0.91205\n",
      "Epoch 3706 - lr: 0.01000 - Train loss: 0.90263 - Test loss: 0.88820\n",
      "Epoch 3707 - lr: 0.01000 - Train loss: 0.91145 - Test loss: 0.91306\n",
      "Epoch 3708 - lr: 0.01000 - Train loss: 0.91174 - Test loss: 0.89159\n",
      "Epoch 3709 - lr: 0.01000 - Train loss: 0.98566 - Test loss: 0.89076\n",
      "Epoch 3710 - lr: 0.01000 - Train loss: 0.98288 - Test loss: 0.88856\n",
      "Epoch 3711 - lr: 0.01000 - Train loss: 0.97545 - Test loss: 0.91347\n",
      "Epoch 3712 - lr: 0.01000 - Train loss: 0.91485 - Test loss: 0.89344\n",
      "Epoch 3713 - lr: 0.01000 - Train loss: 1.00643 - Test loss: 0.89959\n",
      "Epoch 3714 - lr: 0.01000 - Train loss: 0.91581 - Test loss: 0.91681\n",
      "Epoch 3715 - lr: 0.01000 - Train loss: 0.92442 - Test loss: 0.89522\n",
      "Epoch 3716 - lr: 0.01000 - Train loss: 0.98989 - Test loss: 0.88458\n",
      "Epoch 3717 - lr: 0.01000 - Train loss: 0.96571 - Test loss: 0.89558\n",
      "Epoch 3718 - lr: 0.01000 - Train loss: 0.90375 - Test loss: 0.89279\n",
      "Epoch 3719 - lr: 0.01000 - Train loss: 1.02887 - Test loss: 0.88915\n",
      "Epoch 3720 - lr: 0.01000 - Train loss: 0.96097 - Test loss: 0.91566\n",
      "Epoch 3721 - lr: 0.01000 - Train loss: 0.95284 - Test loss: 0.89306\n",
      "Epoch 3722 - lr: 0.01000 - Train loss: 0.96446 - Test loss: 0.89091\n",
      "Epoch 3723 - lr: 0.01000 - Train loss: 0.99224 - Test loss: 0.88955\n",
      "Epoch 3724 - lr: 0.01000 - Train loss: 0.87643 - Test loss: 0.89162\n",
      "Epoch 3725 - lr: 0.01000 - Train loss: 1.01012 - Test loss: 0.89551\n",
      "Epoch 3726 - lr: 0.01000 - Train loss: 0.96904 - Test loss: 0.91776\n",
      "Epoch 3727 - lr: 0.01000 - Train loss: 0.93512 - Test loss: 0.91002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3728 - lr: 0.01000 - Train loss: 0.91091 - Test loss: 0.89655\n",
      "Epoch 3729 - lr: 0.01000 - Train loss: 0.89181 - Test loss: 0.89952\n",
      "Epoch 3730 - lr: 0.01000 - Train loss: 0.89959 - Test loss: 0.92063\n",
      "Epoch 3731 - lr: 0.01000 - Train loss: 0.89126 - Test loss: 0.92087\n",
      "Epoch 3732 - lr: 0.01000 - Train loss: 0.91710 - Test loss: 0.90274\n",
      "Epoch 3733 - lr: 0.01000 - Train loss: 0.96575 - Test loss: 0.89581\n",
      "Epoch 3734 - lr: 0.01000 - Train loss: 0.97610 - Test loss: 0.91551\n",
      "Epoch 3735 - lr: 0.01000 - Train loss: 0.91787 - Test loss: 0.90266\n",
      "Epoch 3736 - lr: 0.01000 - Train loss: 1.01414 - Test loss: 0.89978\n",
      "Epoch 3737 - lr: 0.01000 - Train loss: 1.00056 - Test loss: 0.91702\n",
      "Epoch 3738 - lr: 0.01000 - Train loss: 0.89081 - Test loss: 0.92145\n",
      "Epoch 3739 - lr: 0.01000 - Train loss: 0.92360 - Test loss: 0.90646\n",
      "Epoch 3740 - lr: 0.01000 - Train loss: 1.00835 - Test loss: 0.89954\n",
      "Epoch 3741 - lr: 0.01000 - Train loss: 0.97706 - Test loss: 0.91982\n",
      "Epoch 3742 - lr: 0.01000 - Train loss: 0.92851 - Test loss: 0.91332\n",
      "Epoch 3743 - lr: 0.01000 - Train loss: 0.91195 - Test loss: 0.89956\n",
      "Epoch 3744 - lr: 0.01000 - Train loss: 0.92843 - Test loss: 0.92344\n",
      "Epoch 3745 - lr: 0.01000 - Train loss: 0.91315 - Test loss: 0.90588\n",
      "Epoch 3746 - lr: 0.01000 - Train loss: 0.95628 - Test loss: 0.89648\n",
      "Epoch 3747 - lr: 0.01000 - Train loss: 0.91950 - Test loss: 0.89754\n",
      "Epoch 3748 - lr: 0.01000 - Train loss: 0.88495 - Test loss: 0.89926\n",
      "Epoch 3749 - lr: 0.01000 - Train loss: 0.95761 - Test loss: 0.89854\n",
      "Epoch 3750 - lr: 0.01000 - Train loss: 0.90152 - Test loss: 0.90264\n",
      "Epoch 3751 - lr: 0.01000 - Train loss: 0.97226 - Test loss: 0.92355\n",
      "Epoch 3752 - lr: 0.01000 - Train loss: 0.87198 - Test loss: 0.90983\n",
      "Epoch 3753 - lr: 0.01000 - Train loss: 0.91542 - Test loss: 0.90255\n",
      "Epoch 3754 - lr: 0.01000 - Train loss: 0.90466 - Test loss: 0.90597\n",
      "Epoch 3755 - lr: 0.01000 - Train loss: 0.90862 - Test loss: 0.90590\n",
      "Epoch 3756 - lr: 0.01000 - Train loss: 0.90825 - Test loss: 0.90492\n",
      "Epoch 3757 - lr: 0.01000 - Train loss: 0.91342 - Test loss: 0.90894\n",
      "Epoch 3758 - lr: 0.01000 - Train loss: 0.95131 - Test loss: 0.90644\n",
      "Epoch 3759 - lr: 0.01000 - Train loss: 0.87029 - Test loss: 0.90709\n",
      "Epoch 3760 - lr: 0.01000 - Train loss: 0.90971 - Test loss: 0.93196\n",
      "Epoch 3761 - lr: 0.01000 - Train loss: 0.91085 - Test loss: 0.90703\n",
      "Epoch 3762 - lr: 0.01000 - Train loss: 0.85914 - Test loss: 0.90616\n",
      "Epoch 3763 - lr: 0.01000 - Train loss: 0.87723 - Test loss: 0.90728\n",
      "Epoch 3764 - lr: 0.01000 - Train loss: 0.93448 - Test loss: 0.91488\n",
      "Epoch 3765 - lr: 0.01000 - Train loss: 0.88667 - Test loss: 0.92534\n",
      "Epoch 3766 - lr: 0.01000 - Train loss: 0.87487 - Test loss: 0.90848\n",
      "Epoch 3767 - lr: 0.01000 - Train loss: 0.89695 - Test loss: 0.92101\n",
      "Epoch 3768 - lr: 0.01000 - Train loss: 0.89110 - Test loss: 0.92401\n",
      "Epoch 3769 - lr: 0.01000 - Train loss: 0.94247 - Test loss: 0.90361\n",
      "Epoch 3770 - lr: 0.01000 - Train loss: 0.97968 - Test loss: 0.89831\n",
      "Epoch 3771 - lr: 0.01000 - Train loss: 1.00839 - Test loss: 0.90588\n",
      "Epoch 3772 - lr: 0.01000 - Train loss: 0.96163 - Test loss: 0.93044\n",
      "Epoch 3773 - lr: 0.01000 - Train loss: 0.90818 - Test loss: 0.90636\n",
      "Epoch 3774 - lr: 0.01000 - Train loss: 0.93172 - Test loss: 0.91733\n",
      "Epoch 3775 - lr: 0.01000 - Train loss: 0.91264 - Test loss: 0.90974\n",
      "Epoch 3776 - lr: 0.01000 - Train loss: 0.96113 - Test loss: 0.92273\n",
      "Epoch 3777 - lr: 0.01000 - Train loss: 0.90934 - Test loss: 0.92435\n",
      "Epoch 3778 - lr: 0.01000 - Train loss: 0.88656 - Test loss: 0.92488\n",
      "Epoch 3779 - lr: 0.01000 - Train loss: 0.90105 - Test loss: 0.90268\n",
      "Epoch 3780 - lr: 0.01000 - Train loss: 0.93909 - Test loss: 0.89918\n",
      "Epoch 3781 - lr: 0.01000 - Train loss: 0.98158 - Test loss: 0.89906\n",
      "Epoch 3782 - lr: 0.01000 - Train loss: 0.88419 - Test loss: 0.90119\n",
      "Epoch 3783 - lr: 0.01000 - Train loss: 0.97939 - Test loss: 0.90097\n",
      "Epoch 3784 - lr: 0.01000 - Train loss: 0.90569 - Test loss: 0.90717\n",
      "Epoch 3785 - lr: 0.01000 - Train loss: 0.96132 - Test loss: 0.90887\n",
      "Epoch 3786 - lr: 0.01000 - Train loss: 0.94435 - Test loss: 0.90449\n",
      "Epoch 3787 - lr: 0.01000 - Train loss: 0.91357 - Test loss: 0.90913\n",
      "Epoch 3788 - lr: 0.01000 - Train loss: 0.95488 - Test loss: 0.90450\n",
      "Epoch 3789 - lr: 0.01000 - Train loss: 0.88201 - Test loss: 0.92418\n",
      "Epoch 3790 - lr: 0.01000 - Train loss: 0.89659 - Test loss: 0.92977\n",
      "Epoch 3791 - lr: 0.01000 - Train loss: 0.88515 - Test loss: 0.91116\n",
      "Epoch 3792 - lr: 0.01000 - Train loss: 0.91523 - Test loss: 0.93240\n",
      "Epoch 3793 - lr: 0.01000 - Train loss: 0.87246 - Test loss: 0.91000\n",
      "Epoch 3794 - lr: 0.01000 - Train loss: 0.87201 - Test loss: 0.90652\n",
      "Epoch 3795 - lr: 0.01000 - Train loss: 0.91281 - Test loss: 0.90152\n",
      "Epoch 3796 - lr: 0.01000 - Train loss: 0.90699 - Test loss: 0.90125\n",
      "Epoch 3797 - lr: 0.01000 - Train loss: 0.88588 - Test loss: 0.90573\n",
      "Epoch 3798 - lr: 0.01000 - Train loss: 0.91217 - Test loss: 0.92959\n",
      "Epoch 3799 - lr: 0.01000 - Train loss: 0.89002 - Test loss: 0.90647\n",
      "Epoch 3800 - lr: 0.01000 - Train loss: 0.91120 - Test loss: 0.92875\n",
      "Epoch 3801 - lr: 0.01000 - Train loss: 0.90459 - Test loss: 0.90334\n",
      "Epoch 3802 - lr: 0.01000 - Train loss: 0.92073 - Test loss: 0.91372\n",
      "Epoch 3803 - lr: 0.01000 - Train loss: 0.90226 - Test loss: 0.92836\n",
      "Epoch 3804 - lr: 0.01000 - Train loss: 0.94251 - Test loss: 0.90874\n",
      "Epoch 3805 - lr: 0.01000 - Train loss: 0.89892 - Test loss: 0.90115\n",
      "Epoch 3806 - lr: 0.01000 - Train loss: 0.90415 - Test loss: 0.92324\n",
      "Epoch 3807 - lr: 0.01000 - Train loss: 0.90584 - Test loss: 0.90626\n",
      "Epoch 3808 - lr: 0.01000 - Train loss: 0.90831 - Test loss: 0.91290\n",
      "Epoch 3809 - lr: 0.01000 - Train loss: 0.88879 - Test loss: 0.89806\n",
      "Epoch 3810 - lr: 0.01000 - Train loss: 0.96351 - Test loss: 0.89529\n",
      "Epoch 3811 - lr: 0.01000 - Train loss: 0.97094 - Test loss: 0.89883\n",
      "Epoch 3812 - lr: 0.01000 - Train loss: 0.91053 - Test loss: 0.89750\n",
      "Epoch 3813 - lr: 0.01000 - Train loss: 0.91728 - Test loss: 0.90062\n",
      "Epoch 3814 - lr: 0.01000 - Train loss: 0.97551 - Test loss: 0.89749\n",
      "Epoch 3815 - lr: 0.01000 - Train loss: 0.88509 - Test loss: 0.89795\n",
      "Epoch 3816 - lr: 0.01000 - Train loss: 0.96377 - Test loss: 0.89854\n",
      "Epoch 3817 - lr: 0.01000 - Train loss: 0.93862 - Test loss: 0.91333\n",
      "Epoch 3818 - lr: 0.01000 - Train loss: 0.88078 - Test loss: 0.92091\n",
      "Epoch 3819 - lr: 0.01000 - Train loss: 0.90191 - Test loss: 0.90025\n",
      "Epoch 3820 - lr: 0.01000 - Train loss: 0.91683 - Test loss: 0.90327\n",
      "Epoch 3821 - lr: 0.01000 - Train loss: 0.95894 - Test loss: 0.89719\n",
      "Epoch 3822 - lr: 0.01000 - Train loss: 0.88806 - Test loss: 0.91723\n",
      "Epoch 3823 - lr: 0.01000 - Train loss: 0.88280 - Test loss: 0.92212\n",
      "Epoch 3824 - lr: 0.01000 - Train loss: 0.90450 - Test loss: 0.90014\n",
      "Epoch 3825 - lr: 0.01000 - Train loss: 0.91245 - Test loss: 0.91321\n",
      "Epoch 3826 - lr: 0.01000 - Train loss: 0.92485 - Test loss: 0.90570\n",
      "Epoch 3827 - lr: 0.01000 - Train loss: 0.98959 - Test loss: 0.89774\n",
      "Epoch 3828 - lr: 0.01000 - Train loss: 0.96036 - Test loss: 0.89682\n",
      "Epoch 3829 - lr: 0.01000 - Train loss: 0.87329 - Test loss: 0.91608\n",
      "Epoch 3830 - lr: 0.01000 - Train loss: 0.89618 - Test loss: 0.91982\n",
      "Epoch 3831 - lr: 0.01000 - Train loss: 0.89286 - Test loss: 0.92313\n",
      "Epoch 3832 - lr: 0.01000 - Train loss: 0.92687 - Test loss: 0.92104\n",
      "Epoch 3833 - lr: 0.01000 - Train loss: 0.88651 - Test loss: 0.92148\n",
      "Epoch 3834 - lr: 0.01000 - Train loss: 0.93108 - Test loss: 0.90109\n",
      "Epoch 3835 - lr: 0.01000 - Train loss: 0.95342 - Test loss: 0.90142\n",
      "Epoch 3836 - lr: 0.01000 - Train loss: 0.91193 - Test loss: 0.90124\n",
      "Epoch 3837 - lr: 0.01000 - Train loss: 0.96857 - Test loss: 0.90214\n",
      "Epoch 3838 - lr: 0.01000 - Train loss: 0.91861 - Test loss: 0.90068\n",
      "Epoch 3839 - lr: 0.01000 - Train loss: 0.91267 - Test loss: 0.92528\n",
      "Epoch 3840 - lr: 0.01000 - Train loss: 0.92506 - Test loss: 0.91803\n",
      "Epoch 3841 - lr: 0.01000 - Train loss: 0.89219 - Test loss: 0.91977\n",
      "Epoch 3842 - lr: 0.01000 - Train loss: 0.94401 - Test loss: 0.89928\n",
      "Epoch 3843 - lr: 0.01000 - Train loss: 0.97716 - Test loss: 0.89963\n",
      "Epoch 3844 - lr: 0.01000 - Train loss: 0.97978 - Test loss: 0.89596\n",
      "Epoch 3845 - lr: 0.01000 - Train loss: 0.88478 - Test loss: 0.89962\n",
      "Epoch 3846 - lr: 0.01000 - Train loss: 0.91570 - Test loss: 0.92539\n",
      "Epoch 3847 - lr: 0.01000 - Train loss: 0.96893 - Test loss: 0.90576\n",
      "Epoch 3848 - lr: 0.01000 - Train loss: 0.99149 - Test loss: 0.89957\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3849 - lr: 0.01000 - Train loss: 0.93669 - Test loss: 0.91326\n",
      "Epoch 3850 - lr: 0.01000 - Train loss: 0.89671 - Test loss: 0.92132\n",
      "Epoch 3851 - lr: 0.01000 - Train loss: 0.91035 - Test loss: 0.90179\n",
      "Epoch 3852 - lr: 0.01000 - Train loss: 0.93943 - Test loss: 0.91361\n",
      "Epoch 3853 - lr: 0.01000 - Train loss: 0.89927 - Test loss: 0.92635\n",
      "Epoch 3854 - lr: 0.01000 - Train loss: 0.89773 - Test loss: 0.92197\n",
      "Epoch 3855 - lr: 0.01000 - Train loss: 0.90948 - Test loss: 0.89960\n",
      "Epoch 3856 - lr: 0.01000 - Train loss: 0.93373 - Test loss: 0.90393\n",
      "Epoch 3857 - lr: 0.01000 - Train loss: 0.90933 - Test loss: 0.89909\n",
      "Epoch 3858 - lr: 0.01000 - Train loss: 0.93374 - Test loss: 0.91136\n",
      "Epoch 3859 - lr: 0.01000 - Train loss: 0.90339 - Test loss: 0.91640\n",
      "Epoch 3860 - lr: 0.01000 - Train loss: 0.91944 - Test loss: 0.90430\n",
      "Epoch 3861 - lr: 0.01000 - Train loss: 1.01066 - Test loss: 0.89861\n",
      "Epoch 3862 - lr: 0.01000 - Train loss: 0.91155 - Test loss: 0.89781\n",
      "Epoch 3863 - lr: 0.01000 - Train loss: 0.93161 - Test loss: 0.92151\n",
      "Epoch 3864 - lr: 0.01000 - Train loss: 0.89439 - Test loss: 0.90384\n",
      "Epoch 3865 - lr: 0.01000 - Train loss: 0.93118 - Test loss: 0.91036\n",
      "Epoch 3866 - lr: 0.01000 - Train loss: 0.87028 - Test loss: 0.91436\n",
      "Epoch 3867 - lr: 0.01000 - Train loss: 0.87538 - Test loss: 0.91746\n",
      "Epoch 3868 - lr: 0.01000 - Train loss: 0.89467 - Test loss: 0.91887\n",
      "Epoch 3869 - lr: 0.01000 - Train loss: 0.86975 - Test loss: 0.90045\n",
      "Epoch 3870 - lr: 0.01000 - Train loss: 0.86075 - Test loss: 0.89460\n",
      "Epoch 3871 - lr: 0.01000 - Train loss: 0.92705 - Test loss: 0.90873\n",
      "Epoch 3872 - lr: 0.01000 - Train loss: 0.88196 - Test loss: 0.91479\n",
      "Epoch 3873 - lr: 0.01000 - Train loss: 0.90855 - Test loss: 0.89451\n",
      "Epoch 3874 - lr: 0.01000 - Train loss: 0.90526 - Test loss: 0.89627\n",
      "Epoch 3875 - lr: 0.01000 - Train loss: 0.95180 - Test loss: 0.89209\n",
      "Epoch 3876 - lr: 0.01000 - Train loss: 0.96747 - Test loss: 0.91330\n",
      "Epoch 3877 - lr: 0.01000 - Train loss: 0.90820 - Test loss: 0.89631\n",
      "Epoch 3878 - lr: 0.01000 - Train loss: 0.90650 - Test loss: 0.89853\n",
      "Epoch 3879 - lr: 0.01000 - Train loss: 0.95892 - Test loss: 0.89249\n",
      "Epoch 3880 - lr: 0.01000 - Train loss: 0.90909 - Test loss: 0.89422\n",
      "Epoch 3881 - lr: 0.01000 - Train loss: 0.94539 - Test loss: 0.89741\n",
      "Epoch 3882 - lr: 0.01000 - Train loss: 0.94578 - Test loss: 0.89815\n",
      "Epoch 3883 - lr: 0.01000 - Train loss: 0.90810 - Test loss: 0.90386\n",
      "Epoch 3884 - lr: 0.01000 - Train loss: 0.95177 - Test loss: 0.89955\n",
      "Epoch 3885 - lr: 0.01000 - Train loss: 0.87683 - Test loss: 0.90421\n",
      "Epoch 3886 - lr: 0.01000 - Train loss: 0.91246 - Test loss: 0.92496\n",
      "Epoch 3887 - lr: 0.01000 - Train loss: 0.87756 - Test loss: 0.92332\n",
      "Epoch 3888 - lr: 0.01000 - Train loss: 0.88571 - Test loss: 0.92288\n",
      "Epoch 3889 - lr: 0.01000 - Train loss: 0.85830 - Test loss: 0.90267\n",
      "Epoch 3890 - lr: 0.01000 - Train loss: 0.91910 - Test loss: 0.90009\n",
      "Epoch 3891 - lr: 0.01000 - Train loss: 0.88407 - Test loss: 0.90018\n",
      "Epoch 3892 - lr: 0.01000 - Train loss: 0.92709 - Test loss: 0.90395\n",
      "Epoch 3893 - lr: 0.01000 - Train loss: 0.95736 - Test loss: 0.89707\n",
      "Epoch 3894 - lr: 0.01000 - Train loss: 0.90912 - Test loss: 0.91410\n",
      "Epoch 3895 - lr: 0.01000 - Train loss: 0.86689 - Test loss: 0.91271\n",
      "Epoch 3896 - lr: 0.01000 - Train loss: 0.90636 - Test loss: 0.91607\n",
      "Epoch 3897 - lr: 0.01000 - Train loss: 0.88863 - Test loss: 0.92006\n",
      "Epoch 3898 - lr: 0.01000 - Train loss: 0.88627 - Test loss: 0.91828\n",
      "Epoch 3899 - lr: 0.01000 - Train loss: 0.90740 - Test loss: 0.89562\n",
      "Epoch 3900 - lr: 0.01000 - Train loss: 0.91206 - Test loss: 0.89746\n",
      "Epoch 3901 - lr: 0.01000 - Train loss: 0.96742 - Test loss: 0.90938\n",
      "Epoch 3902 - lr: 0.01000 - Train loss: 0.87417 - Test loss: 0.91531\n",
      "Epoch 3903 - lr: 0.01000 - Train loss: 0.88220 - Test loss: 0.91628\n",
      "Epoch 3904 - lr: 0.01000 - Train loss: 0.89236 - Test loss: 0.91624\n",
      "Epoch 3905 - lr: 0.01000 - Train loss: 0.90944 - Test loss: 0.89330\n",
      "Epoch 3906 - lr: 0.01000 - Train loss: 0.91834 - Test loss: 0.89613\n",
      "Epoch 3907 - lr: 0.01000 - Train loss: 0.94088 - Test loss: 0.89497\n",
      "Epoch 3908 - lr: 0.01000 - Train loss: 0.93367 - Test loss: 0.90007\n",
      "Epoch 3909 - lr: 0.01000 - Train loss: 0.95846 - Test loss: 0.89294\n",
      "Epoch 3910 - lr: 0.01000 - Train loss: 0.91120 - Test loss: 0.89464\n",
      "Epoch 3911 - lr: 0.01000 - Train loss: 0.85991 - Test loss: 0.89495\n",
      "Epoch 3912 - lr: 0.01000 - Train loss: 0.87406 - Test loss: 0.89690\n",
      "Epoch 3913 - lr: 0.01000 - Train loss: 0.88633 - Test loss: 0.89768\n",
      "Epoch 3914 - lr: 0.01000 - Train loss: 0.89835 - Test loss: 0.91849\n",
      "Epoch 3915 - lr: 0.01000 - Train loss: 0.89202 - Test loss: 0.91708\n",
      "Epoch 3916 - lr: 0.01000 - Train loss: 0.90608 - Test loss: 0.89605\n",
      "Epoch 3917 - lr: 0.01000 - Train loss: 0.89292 - Test loss: 0.91693\n",
      "Epoch 3918 - lr: 0.01000 - Train loss: 0.88617 - Test loss: 0.91539\n",
      "Epoch 3919 - lr: 0.01000 - Train loss: 0.90595 - Test loss: 0.89419\n",
      "Epoch 3920 - lr: 0.01000 - Train loss: 0.90499 - Test loss: 0.91758\n",
      "Epoch 3921 - lr: 0.01000 - Train loss: 0.89267 - Test loss: 0.89658\n",
      "Epoch 3922 - lr: 0.01000 - Train loss: 0.93912 - Test loss: 0.89684\n",
      "Epoch 3923 - lr: 0.01000 - Train loss: 0.99283 - Test loss: 0.88844\n",
      "Epoch 3924 - lr: 0.01000 - Train loss: 0.95995 - Test loss: 0.89122\n",
      "Epoch 3925 - lr: 0.01000 - Train loss: 0.96831 - Test loss: 0.91347\n",
      "Epoch 3926 - lr: 0.01000 - Train loss: 0.87326 - Test loss: 0.90023\n",
      "Epoch 3927 - lr: 0.01000 - Train loss: 0.93619 - Test loss: 0.90354\n",
      "Epoch 3928 - lr: 0.01000 - Train loss: 0.92846 - Test loss: 0.91885\n",
      "Epoch 3929 - lr: 0.01000 - Train loss: 0.87238 - Test loss: 0.89903\n",
      "Epoch 3930 - lr: 0.01000 - Train loss: 0.87883 - Test loss: 0.89560\n",
      "Epoch 3931 - lr: 0.01000 - Train loss: 0.90591 - Test loss: 0.89689\n",
      "Epoch 3932 - lr: 0.01000 - Train loss: 0.94812 - Test loss: 0.91056\n",
      "Epoch 3933 - lr: 0.01000 - Train loss: 0.87842 - Test loss: 0.90645\n",
      "Epoch 3934 - lr: 0.01000 - Train loss: 0.93624 - Test loss: 0.89988\n",
      "Epoch 3935 - lr: 0.01000 - Train loss: 0.94440 - Test loss: 0.89306\n",
      "Epoch 3936 - lr: 0.01000 - Train loss: 0.89455 - Test loss: 0.91407\n",
      "Epoch 3937 - lr: 0.01000 - Train loss: 0.88488 - Test loss: 0.91522\n",
      "Epoch 3938 - lr: 0.01000 - Train loss: 0.93343 - Test loss: 0.91846\n",
      "Epoch 3939 - lr: 0.01000 - Train loss: 0.87785 - Test loss: 0.89560\n",
      "Epoch 3940 - lr: 0.01000 - Train loss: 0.90797 - Test loss: 0.89461\n",
      "Epoch 3941 - lr: 0.01000 - Train loss: 0.91693 - Test loss: 0.88980\n",
      "Epoch 3942 - lr: 0.01000 - Train loss: 0.91023 - Test loss: 0.91314\n",
      "Epoch 3943 - lr: 0.01000 - Train loss: 0.92730 - Test loss: 0.89114\n",
      "Epoch 3944 - lr: 0.01000 - Train loss: 0.91568 - Test loss: 0.89181\n",
      "Epoch 3945 - lr: 0.01000 - Train loss: 0.88727 - Test loss: 0.89648\n",
      "Epoch 3946 - lr: 0.01000 - Train loss: 0.92939 - Test loss: 0.89905\n",
      "Epoch 3947 - lr: 0.01000 - Train loss: 0.98564 - Test loss: 0.89229\n",
      "Epoch 3948 - lr: 0.01000 - Train loss: 0.96800 - Test loss: 0.91444\n",
      "Epoch 3949 - lr: 0.01000 - Train loss: 0.89926 - Test loss: 0.89962\n",
      "Epoch 3950 - lr: 0.01000 - Train loss: 0.90228 - Test loss: 0.92084\n",
      "Epoch 3951 - lr: 0.01000 - Train loss: 0.88068 - Test loss: 0.91824\n",
      "Epoch 3952 - lr: 0.01000 - Train loss: 0.85342 - Test loss: 0.89763\n",
      "Epoch 3953 - lr: 0.01000 - Train loss: 0.88460 - Test loss: 0.89452\n",
      "Epoch 3954 - lr: 0.01000 - Train loss: 0.91896 - Test loss: 0.89680\n",
      "Epoch 3955 - lr: 0.01000 - Train loss: 0.96519 - Test loss: 0.89067\n",
      "Epoch 3956 - lr: 0.01000 - Train loss: 0.88231 - Test loss: 0.89321\n",
      "Epoch 3957 - lr: 0.01000 - Train loss: 0.96588 - Test loss: 0.91544\n",
      "Epoch 3958 - lr: 0.01000 - Train loss: 0.87344 - Test loss: 0.90195\n",
      "Epoch 3959 - lr: 0.01000 - Train loss: 0.92384 - Test loss: 0.90061\n",
      "Epoch 3960 - lr: 0.01000 - Train loss: 0.95822 - Test loss: 0.89639\n",
      "Epoch 3961 - lr: 0.01000 - Train loss: 0.86636 - Test loss: 0.89724\n",
      "Epoch 3962 - lr: 0.01000 - Train loss: 0.90723 - Test loss: 0.90073\n",
      "Epoch 3963 - lr: 0.01000 - Train loss: 0.94707 - Test loss: 0.89678\n",
      "Epoch 3964 - lr: 0.01000 - Train loss: 0.92970 - Test loss: 0.90660\n",
      "Epoch 3965 - lr: 0.01000 - Train loss: 0.97393 - Test loss: 0.90484\n",
      "Epoch 3966 - lr: 0.01000 - Train loss: 0.93868 - Test loss: 0.90123\n",
      "Epoch 3967 - lr: 0.01000 - Train loss: 0.91611 - Test loss: 0.92617\n",
      "Epoch 3968 - lr: 0.01000 - Train loss: 0.92804 - Test loss: 0.92807\n",
      "Epoch 3969 - lr: 0.01000 - Train loss: 0.93125 - Test loss: 0.92701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3970 - lr: 0.01000 - Train loss: 0.96147 - Test loss: 0.90214\n",
      "Epoch 3971 - lr: 0.01000 - Train loss: 0.93860 - Test loss: 0.89825\n",
      "Epoch 3972 - lr: 0.01000 - Train loss: 0.90887 - Test loss: 0.92318\n",
      "Epoch 3973 - lr: 0.01000 - Train loss: 0.88214 - Test loss: 0.90149\n",
      "Epoch 3974 - lr: 0.01000 - Train loss: 0.93690 - Test loss: 0.90629\n",
      "Epoch 3975 - lr: 0.01000 - Train loss: 0.97856 - Test loss: 0.90221\n",
      "Epoch 3976 - lr: 0.01000 - Train loss: 0.97727 - Test loss: 0.90846\n",
      "Epoch 3977 - lr: 0.01000 - Train loss: 0.90674 - Test loss: 0.92589\n",
      "Epoch 3978 - lr: 0.01000 - Train loss: 0.91284 - Test loss: 0.89831\n",
      "Epoch 3979 - lr: 0.01000 - Train loss: 0.87815 - Test loss: 0.89974\n",
      "Epoch 3980 - lr: 0.01000 - Train loss: 0.90961 - Test loss: 0.90086\n",
      "Epoch 3981 - lr: 0.01000 - Train loss: 0.96304 - Test loss: 0.89417\n",
      "Epoch 3982 - lr: 0.01000 - Train loss: 0.88674 - Test loss: 0.89645\n",
      "Epoch 3983 - lr: 0.01000 - Train loss: 0.95730 - Test loss: 0.89588\n",
      "Epoch 3984 - lr: 0.01000 - Train loss: 0.88036 - Test loss: 0.90014\n",
      "Epoch 3985 - lr: 0.01000 - Train loss: 0.91630 - Test loss: 0.92664\n",
      "Epoch 3986 - lr: 0.01000 - Train loss: 0.93340 - Test loss: 0.92793\n",
      "Epoch 3987 - lr: 0.01000 - Train loss: 0.90521 - Test loss: 0.90234\n",
      "Epoch 3988 - lr: 0.01000 - Train loss: 0.92432 - Test loss: 0.90995\n",
      "Epoch 3989 - lr: 0.01000 - Train loss: 0.89166 - Test loss: 0.91956\n",
      "Epoch 3990 - lr: 0.01000 - Train loss: 0.92918 - Test loss: 0.92582\n",
      "Epoch 3991 - lr: 0.01000 - Train loss: 0.92008 - Test loss: 0.92665\n",
      "Epoch 3992 - lr: 0.01000 - Train loss: 0.92467 - Test loss: 0.92544\n",
      "Epoch 3993 - lr: 0.01000 - Train loss: 0.93243 - Test loss: 0.92282\n",
      "Epoch 3994 - lr: 0.01000 - Train loss: 0.90655 - Test loss: 0.89645\n",
      "Epoch 3995 - lr: 0.01000 - Train loss: 0.88701 - Test loss: 0.90980\n",
      "Epoch 3996 - lr: 0.01000 - Train loss: 0.89455 - Test loss: 0.92103\n",
      "Epoch 3997 - lr: 0.01000 - Train loss: 0.93354 - Test loss: 0.89180\n",
      "Epoch 3998 - lr: 0.01000 - Train loss: 1.03123 - Test loss: 0.89376\n",
      "Epoch 3999 - lr: 0.01000 - Train loss: 0.96090 - Test loss: 0.91893\n",
      "Epoch 4000 - lr: 0.01000 - Train loss: 0.87984 - Test loss: 0.89809\n",
      "Epoch 4001 - lr: 0.01000 - Train loss: 0.89741 - Test loss: 0.91870\n",
      "Epoch 4002 - lr: 0.01000 - Train loss: 0.91614 - Test loss: 0.89326\n",
      "Epoch 4003 - lr: 0.01000 - Train loss: 0.92289 - Test loss: 0.89529\n",
      "Epoch 4004 - lr: 0.01000 - Train loss: 0.95967 - Test loss: 0.89298\n",
      "Epoch 4005 - lr: 0.01000 - Train loss: 0.93289 - Test loss: 0.89927\n",
      "Epoch 4006 - lr: 0.01000 - Train loss: 0.95526 - Test loss: 0.89156\n",
      "Epoch 4007 - lr: 0.01000 - Train loss: 0.92994 - Test loss: 0.89868\n",
      "Epoch 4008 - lr: 0.01000 - Train loss: 0.98668 - Test loss: 0.89458\n",
      "Epoch 4009 - lr: 0.01000 - Train loss: 0.99158 - Test loss: 0.89653\n",
      "Epoch 4010 - lr: 0.01000 - Train loss: 0.97156 - Test loss: 0.91800\n",
      "Epoch 4011 - lr: 0.01000 - Train loss: 0.88007 - Test loss: 0.90486\n",
      "Epoch 4012 - lr: 0.01000 - Train loss: 0.89354 - Test loss: 0.92351\n",
      "Epoch 4013 - lr: 0.01000 - Train loss: 0.89649 - Test loss: 0.92181\n",
      "Epoch 4014 - lr: 0.01000 - Train loss: 0.86992 - Test loss: 0.90387\n",
      "Epoch 4015 - lr: 0.01000 - Train loss: 0.90129 - Test loss: 0.89658\n",
      "Epoch 4016 - lr: 0.01000 - Train loss: 0.90851 - Test loss: 0.90074\n",
      "Epoch 4017 - lr: 0.01000 - Train loss: 0.91054 - Test loss: 0.89788\n",
      "Epoch 4018 - lr: 0.01000 - Train loss: 0.93694 - Test loss: 0.90622\n",
      "Epoch 4019 - lr: 0.01000 - Train loss: 0.95263 - Test loss: 0.91055\n",
      "Epoch 4020 - lr: 0.01000 - Train loss: 0.89242 - Test loss: 0.92129\n",
      "Epoch 4021 - lr: 0.01000 - Train loss: 0.88157 - Test loss: 0.91954\n",
      "Epoch 4022 - lr: 0.01000 - Train loss: 0.90403 - Test loss: 0.89797\n",
      "Epoch 4023 - lr: 0.01000 - Train loss: 0.90543 - Test loss: 0.92121\n",
      "Epoch 4024 - lr: 0.01000 - Train loss: 0.88669 - Test loss: 0.89966\n",
      "Epoch 4025 - lr: 0.01000 - Train loss: 0.93515 - Test loss: 0.90278\n",
      "Epoch 4026 - lr: 0.01000 - Train loss: 0.97865 - Test loss: 0.90630\n",
      "Epoch 4027 - lr: 0.01000 - Train loss: 0.89153 - Test loss: 0.89483\n",
      "Epoch 4028 - lr: 0.01000 - Train loss: 0.90009 - Test loss: 0.91718\n",
      "Epoch 4029 - lr: 0.01000 - Train loss: 0.87494 - Test loss: 0.91499\n",
      "Epoch 4030 - lr: 0.01000 - Train loss: 0.94849 - Test loss: 0.89906\n",
      "Epoch 4031 - lr: 0.01000 - Train loss: 0.88478 - Test loss: 0.91120\n",
      "Epoch 4032 - lr: 0.01000 - Train loss: 0.85473 - Test loss: 0.89386\n",
      "Epoch 4033 - lr: 0.01000 - Train loss: 0.89791 - Test loss: 0.91003\n",
      "Epoch 4034 - lr: 0.01000 - Train loss: 0.92593 - Test loss: 0.90658\n",
      "Epoch 4035 - lr: 0.01000 - Train loss: 0.90068 - Test loss: 0.89052\n",
      "Epoch 4036 - lr: 0.01000 - Train loss: 0.91029 - Test loss: 0.91570\n",
      "Epoch 4037 - lr: 0.01000 - Train loss: 0.93214 - Test loss: 0.91537\n",
      "Epoch 4038 - lr: 0.01000 - Train loss: 0.90994 - Test loss: 0.91198\n",
      "Epoch 4039 - lr: 0.01000 - Train loss: 0.90560 - Test loss: 0.89077\n",
      "Epoch 4040 - lr: 0.01000 - Train loss: 0.90695 - Test loss: 0.91451\n",
      "Epoch 4041 - lr: 0.01000 - Train loss: 0.94572 - Test loss: 0.89048\n",
      "Epoch 4042 - lr: 0.01000 - Train loss: 0.98087 - Test loss: 0.88805\n",
      "Epoch 4043 - lr: 0.01000 - Train loss: 0.88623 - Test loss: 0.91000\n",
      "Epoch 4044 - lr: 0.01000 - Train loss: 0.88764 - Test loss: 0.89097\n",
      "Epoch 4045 - lr: 0.01000 - Train loss: 0.93094 - Test loss: 0.88950\n",
      "Epoch 4046 - lr: 0.01000 - Train loss: 0.93420 - Test loss: 0.91043\n",
      "Epoch 4047 - lr: 0.01000 - Train loss: 0.90212 - Test loss: 0.91255\n",
      "Epoch 4048 - lr: 0.01000 - Train loss: 0.90674 - Test loss: 0.89140\n",
      "Epoch 4049 - lr: 0.01000 - Train loss: 0.90143 - Test loss: 0.91507\n",
      "Epoch 4050 - lr: 0.01000 - Train loss: 0.90526 - Test loss: 0.89163\n",
      "Epoch 4051 - lr: 0.01000 - Train loss: 0.90618 - Test loss: 0.91503\n",
      "Epoch 4052 - lr: 0.01000 - Train loss: 0.86980 - Test loss: 0.89390\n",
      "Epoch 4053 - lr: 0.01000 - Train loss: 0.90920 - Test loss: 0.90729\n",
      "Epoch 4054 - lr: 0.01000 - Train loss: 0.90732 - Test loss: 0.88908\n",
      "Epoch 4055 - lr: 0.01000 - Train loss: 0.89559 - Test loss: 0.91239\n",
      "Epoch 4056 - lr: 0.01000 - Train loss: 0.89372 - Test loss: 0.91078\n",
      "Epoch 4057 - lr: 0.01000 - Train loss: 0.90601 - Test loss: 0.88866\n",
      "Epoch 4058 - lr: 0.01000 - Train loss: 0.90467 - Test loss: 0.91247\n",
      "Epoch 4059 - lr: 0.01000 - Train loss: 0.87450 - Test loss: 0.89057\n",
      "Epoch 4060 - lr: 0.01000 - Train loss: 0.89828 - Test loss: 0.90684\n",
      "Epoch 4061 - lr: 0.01000 - Train loss: 0.87812 - Test loss: 0.88612\n",
      "Epoch 4062 - lr: 0.01000 - Train loss: 1.01318 - Test loss: 0.90931\n",
      "Epoch 4063 - lr: 0.01000 - Train loss: 0.97865 - Test loss: 0.89298\n",
      "Epoch 4064 - lr: 0.01000 - Train loss: 0.96732 - Test loss: 0.91392\n",
      "Epoch 4065 - lr: 0.01000 - Train loss: 0.93296 - Test loss: 0.91373\n",
      "Epoch 4066 - lr: 0.01000 - Train loss: 0.87918 - Test loss: 0.89030\n",
      "Epoch 4067 - lr: 0.01000 - Train loss: 0.87909 - Test loss: 0.90209\n",
      "Epoch 4068 - lr: 0.01000 - Train loss: 0.93214 - Test loss: 0.89364\n",
      "Epoch 4069 - lr: 0.01000 - Train loss: 0.96614 - Test loss: 0.88466\n",
      "Epoch 4070 - lr: 0.01000 - Train loss: 1.00613 - Test loss: 0.89129\n",
      "Epoch 4071 - lr: 0.01000 - Train loss: 0.99735 - Test loss: 0.89359\n",
      "Epoch 4072 - lr: 0.01000 - Train loss: 1.00235 - Test loss: 0.88534\n",
      "Epoch 4073 - lr: 0.01000 - Train loss: 0.98816 - Test loss: 0.91139\n",
      "Epoch 4074 - lr: 0.01000 - Train loss: 0.91861 - Test loss: 0.91839\n",
      "Epoch 4075 - lr: 0.01000 - Train loss: 0.89754 - Test loss: 0.91590\n",
      "Epoch 4076 - lr: 0.01000 - Train loss: 0.87830 - Test loss: 0.91195\n",
      "Epoch 4077 - lr: 0.01000 - Train loss: 0.97118 - Test loss: 0.89206\n",
      "Epoch 4078 - lr: 0.01000 - Train loss: 0.91321 - Test loss: 0.88776\n",
      "Epoch 4079 - lr: 0.01000 - Train loss: 0.93301 - Test loss: 0.88554\n",
      "Epoch 4080 - lr: 0.01000 - Train loss: 0.95520 - Test loss: 0.88684\n",
      "Epoch 4081 - lr: 0.01000 - Train loss: 0.91991 - Test loss: 0.89501\n",
      "Epoch 4082 - lr: 0.01000 - Train loss: 0.96758 - Test loss: 0.89430\n",
      "Epoch 4083 - lr: 0.01000 - Train loss: 0.90373 - Test loss: 0.90981\n",
      "Epoch 4084 - lr: 0.01000 - Train loss: 0.92727 - Test loss: 0.90019\n",
      "Epoch 4085 - lr: 0.01000 - Train loss: 0.95770 - Test loss: 0.89239\n",
      "Epoch 4086 - lr: 0.01000 - Train loss: 0.99355 - Test loss: 0.89174\n",
      "Epoch 4087 - lr: 0.01000 - Train loss: 0.95607 - Test loss: 0.89399\n",
      "Epoch 4088 - lr: 0.01000 - Train loss: 0.97398 - Test loss: 0.91683\n",
      "Epoch 4089 - lr: 0.01000 - Train loss: 0.89775 - Test loss: 0.92168\n",
      "Epoch 4090 - lr: 0.01000 - Train loss: 0.89701 - Test loss: 0.92017\n",
      "Epoch 4091 - lr: 0.01000 - Train loss: 0.93196 - Test loss: 0.90844\n",
      "Epoch 4092 - lr: 0.01000 - Train loss: 0.89705 - Test loss: 0.91559\n",
      "Epoch 4093 - lr: 0.01000 - Train loss: 0.92761 - Test loss: 0.91614\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4094 - lr: 0.01000 - Train loss: 0.88058 - Test loss: 0.91656\n",
      "Epoch 4095 - lr: 0.01000 - Train loss: 0.87567 - Test loss: 0.89840\n",
      "Epoch 4096 - lr: 0.01000 - Train loss: 0.90522 - Test loss: 0.89605\n",
      "Epoch 4097 - lr: 0.01000 - Train loss: 0.95494 - Test loss: 0.89095\n",
      "Epoch 4098 - lr: 0.01000 - Train loss: 0.97191 - Test loss: 0.91246\n",
      "Epoch 4099 - lr: 0.01000 - Train loss: 0.92918 - Test loss: 0.92107\n",
      "Epoch 4100 - lr: 0.01000 - Train loss: 0.90320 - Test loss: 0.92084\n",
      "Epoch 4101 - lr: 0.01000 - Train loss: 0.89236 - Test loss: 0.91660\n",
      "Epoch 4102 - lr: 0.01000 - Train loss: 0.92548 - Test loss: 0.91918\n",
      "Epoch 4103 - lr: 0.01000 - Train loss: 0.93272 - Test loss: 0.91759\n",
      "Epoch 4104 - lr: 0.01000 - Train loss: 0.90186 - Test loss: 0.89316\n",
      "Epoch 4105 - lr: 0.01000 - Train loss: 0.92349 - Test loss: 0.90102\n",
      "Epoch 4106 - lr: 0.01000 - Train loss: 0.91807 - Test loss: 0.91484\n",
      "Epoch 4107 - lr: 0.01000 - Train loss: 0.92509 - Test loss: 0.90622\n",
      "Epoch 4108 - lr: 0.01000 - Train loss: 0.92348 - Test loss: 0.88770\n",
      "Epoch 4109 - lr: 0.01000 - Train loss: 1.00788 - Test loss: 0.89055\n",
      "Epoch 4110 - lr: 0.01000 - Train loss: 0.87111 - Test loss: 0.89205\n",
      "Epoch 4111 - lr: 0.01000 - Train loss: 0.89856 - Test loss: 0.90950\n",
      "Epoch 4112 - lr: 0.01000 - Train loss: 0.92800 - Test loss: 0.91700\n",
      "Epoch 4113 - lr: 0.01000 - Train loss: 0.92530 - Test loss: 0.91792\n",
      "Epoch 4114 - lr: 0.01000 - Train loss: 0.92901 - Test loss: 0.91266\n",
      "Epoch 4115 - lr: 0.01000 - Train loss: 0.86123 - Test loss: 0.90728\n",
      "Epoch 4116 - lr: 0.01000 - Train loss: 0.89551 - Test loss: 0.88958\n",
      "Epoch 4117 - lr: 0.01000 - Train loss: 0.91259 - Test loss: 0.88628\n",
      "Epoch 4118 - lr: 0.01000 - Train loss: 0.89219 - Test loss: 0.88568\n",
      "Epoch 4119 - lr: 0.01000 - Train loss: 0.97148 - Test loss: 0.89032\n",
      "Epoch 4120 - lr: 0.01000 - Train loss: 1.00387 - Test loss: 0.88603\n",
      "Epoch 4121 - lr: 0.01000 - Train loss: 1.01773 - Test loss: 0.89316\n",
      "Epoch 4122 - lr: 0.01000 - Train loss: 0.98356 - Test loss: 0.89759\n",
      "Epoch 4123 - lr: 0.01000 - Train loss: 0.94527 - Test loss: 0.90383\n",
      "Epoch 4124 - lr: 0.01000 - Train loss: 0.90537 - Test loss: 0.92180\n",
      "Epoch 4125 - lr: 0.01000 - Train loss: 0.86993 - Test loss: 0.89891\n",
      "Epoch 4126 - lr: 0.01000 - Train loss: 0.90114 - Test loss: 0.91283\n",
      "Epoch 4127 - lr: 0.01000 - Train loss: 0.91477 - Test loss: 0.89765\n",
      "Epoch 4128 - lr: 0.01000 - Train loss: 0.93839 - Test loss: 0.89204\n",
      "Epoch 4129 - lr: 0.01000 - Train loss: 0.89686 - Test loss: 0.91419\n",
      "Epoch 4130 - lr: 0.01000 - Train loss: 0.89961 - Test loss: 0.91314\n",
      "Epoch 4131 - lr: 0.01000 - Train loss: 0.96902 - Test loss: 0.90106\n",
      "Epoch 4132 - lr: 0.01000 - Train loss: 0.88067 - Test loss: 0.89221\n",
      "Epoch 4133 - lr: 0.01000 - Train loss: 0.92365 - Test loss: 0.89496\n",
      "Epoch 4134 - lr: 0.01000 - Train loss: 0.96879 - Test loss: 0.88795\n",
      "Epoch 4135 - lr: 0.01000 - Train loss: 0.88191 - Test loss: 0.89050\n",
      "Epoch 4136 - lr: 0.01000 - Train loss: 0.96520 - Test loss: 0.91363\n",
      "Epoch 4137 - lr: 0.01000 - Train loss: 0.91447 - Test loss: 0.90037\n",
      "Epoch 4138 - lr: 0.01000 - Train loss: 0.93170 - Test loss: 0.89499\n",
      "Epoch 4139 - lr: 0.01000 - Train loss: 0.91363 - Test loss: 0.92120\n",
      "Epoch 4140 - lr: 0.01000 - Train loss: 0.91449 - Test loss: 0.90125\n",
      "Epoch 4141 - lr: 0.01000 - Train loss: 0.93093 - Test loss: 0.89481\n",
      "Epoch 4142 - lr: 0.01000 - Train loss: 0.91721 - Test loss: 0.92127\n",
      "Epoch 4143 - lr: 0.01000 - Train loss: 0.92149 - Test loss: 0.90355\n",
      "Epoch 4144 - lr: 0.01000 - Train loss: 0.97435 - Test loss: 0.89377\n",
      "Epoch 4145 - lr: 0.01000 - Train loss: 0.97110 - Test loss: 0.89588\n",
      "Epoch 4146 - lr: 0.01000 - Train loss: 0.96791 - Test loss: 0.91730\n",
      "Epoch 4147 - lr: 0.01000 - Train loss: 0.91350 - Test loss: 0.90334\n",
      "Epoch 4148 - lr: 0.01000 - Train loss: 0.92772 - Test loss: 0.89791\n",
      "Epoch 4149 - lr: 0.01000 - Train loss: 0.92111 - Test loss: 0.92430\n",
      "Epoch 4150 - lr: 0.01000 - Train loss: 0.92208 - Test loss: 0.90649\n",
      "Epoch 4151 - lr: 0.01000 - Train loss: 0.91098 - Test loss: 0.89884\n",
      "Epoch 4152 - lr: 0.01000 - Train loss: 0.91713 - Test loss: 0.89465\n",
      "Epoch 4153 - lr: 0.01000 - Train loss: 0.91305 - Test loss: 0.90100\n",
      "Epoch 4154 - lr: 0.01000 - Train loss: 0.97419 - Test loss: 0.92427\n",
      "Epoch 4155 - lr: 0.01000 - Train loss: 0.92164 - Test loss: 0.90669\n",
      "Epoch 4156 - lr: 0.01000 - Train loss: 1.00910 - Test loss: 0.90031\n",
      "Epoch 4157 - lr: 0.01000 - Train loss: 0.97249 - Test loss: 0.91925\n",
      "Epoch 4158 - lr: 0.01000 - Train loss: 0.88278 - Test loss: 0.90410\n",
      "Epoch 4159 - lr: 0.01000 - Train loss: 0.91837 - Test loss: 0.92752\n",
      "Epoch 4160 - lr: 0.01000 - Train loss: 0.91953 - Test loss: 0.90818\n",
      "Epoch 4161 - lr: 0.01000 - Train loss: 0.91502 - Test loss: 0.89962\n",
      "Epoch 4162 - lr: 0.01000 - Train loss: 0.93523 - Test loss: 0.92188\n",
      "Epoch 4163 - lr: 0.01000 - Train loss: 0.90867 - Test loss: 0.92648\n",
      "Epoch 4164 - lr: 0.01000 - Train loss: 0.90126 - Test loss: 0.89903\n",
      "Epoch 4165 - lr: 0.01000 - Train loss: 0.90693 - Test loss: 0.90069\n",
      "Epoch 4166 - lr: 0.01000 - Train loss: 0.91534 - Test loss: 0.89649\n",
      "Epoch 4167 - lr: 0.01000 - Train loss: 0.91691 - Test loss: 0.90106\n",
      "Epoch 4168 - lr: 0.01000 - Train loss: 0.91021 - Test loss: 0.89799\n",
      "Epoch 4169 - lr: 0.01000 - Train loss: 0.93491 - Test loss: 0.92126\n",
      "Epoch 4170 - lr: 0.01000 - Train loss: 0.92569 - Test loss: 0.92717\n",
      "Epoch 4171 - lr: 0.01000 - Train loss: 0.91573 - Test loss: 0.90474\n",
      "Epoch 4172 - lr: 0.01000 - Train loss: 0.97518 - Test loss: 0.89950\n",
      "Epoch 4173 - lr: 0.01000 - Train loss: 0.95289 - Test loss: 0.89569\n",
      "Epoch 4174 - lr: 0.01000 - Train loss: 0.91862 - Test loss: 0.92157\n",
      "Epoch 4175 - lr: 0.01000 - Train loss: 0.87044 - Test loss: 0.90481\n",
      "Epoch 4176 - lr: 0.01000 - Train loss: 0.90974 - Test loss: 0.89785\n",
      "Epoch 4177 - lr: 0.01000 - Train loss: 0.93307 - Test loss: 0.90404\n",
      "Epoch 4178 - lr: 0.01000 - Train loss: 0.96680 - Test loss: 0.90857\n",
      "Epoch 4179 - lr: 0.01000 - Train loss: 0.91528 - Test loss: 0.90514\n",
      "Epoch 4180 - lr: 0.01000 - Train loss: 0.95975 - Test loss: 0.89948\n",
      "Epoch 4181 - lr: 0.01000 - Train loss: 0.97411 - Test loss: 0.90180\n",
      "Epoch 4182 - lr: 0.01000 - Train loss: 0.95100 - Test loss: 0.89928\n",
      "Epoch 4183 - lr: 0.01000 - Train loss: 0.95070 - Test loss: 0.90242\n",
      "Epoch 4184 - lr: 0.01000 - Train loss: 0.99000 - Test loss: 0.90380\n",
      "Epoch 4185 - lr: 0.01000 - Train loss: 0.95681 - Test loss: 0.90341\n",
      "Epoch 4186 - lr: 0.01000 - Train loss: 0.89632 - Test loss: 0.92477\n",
      "Epoch 4187 - lr: 0.01000 - Train loss: 0.87200 - Test loss: 0.91295\n",
      "Epoch 4188 - lr: 0.01000 - Train loss: 0.89724 - Test loss: 0.90881\n",
      "Epoch 4189 - lr: 0.01000 - Train loss: 0.91573 - Test loss: 0.93326\n",
      "Epoch 4190 - lr: 0.01000 - Train loss: 0.92040 - Test loss: 0.91086\n",
      "Epoch 4191 - lr: 0.01000 - Train loss: 0.87272 - Test loss: 0.92053\n",
      "Epoch 4192 - lr: 0.01000 - Train loss: 0.89052 - Test loss: 0.92599\n",
      "Epoch 4193 - lr: 0.01000 - Train loss: 0.93862 - Test loss: 0.90649\n",
      "Epoch 4194 - lr: 0.01000 - Train loss: 0.96808 - Test loss: 0.90735\n",
      "Epoch 4195 - lr: 0.01000 - Train loss: 0.95725 - Test loss: 0.90131\n",
      "Epoch 4196 - lr: 0.01000 - Train loss: 0.89064 - Test loss: 0.92216\n",
      "Epoch 4197 - lr: 0.01000 - Train loss: 0.87336 - Test loss: 0.90923\n",
      "Epoch 4198 - lr: 0.01000 - Train loss: 0.92701 - Test loss: 0.91847\n",
      "Epoch 4199 - lr: 0.01000 - Train loss: 0.90637 - Test loss: 0.90385\n",
      "Epoch 4200 - lr: 0.01000 - Train loss: 0.88046 - Test loss: 0.90566\n",
      "Epoch 4201 - lr: 0.01000 - Train loss: 0.92892 - Test loss: 0.91247\n",
      "Epoch 4202 - lr: 0.01000 - Train loss: 0.97188 - Test loss: 0.90946\n",
      "Epoch 4203 - lr: 0.01000 - Train loss: 0.91666 - Test loss: 0.90383\n",
      "Epoch 4204 - lr: 0.01000 - Train loss: 0.89034 - Test loss: 0.92306\n",
      "Epoch 4205 - lr: 0.01000 - Train loss: 0.89745 - Test loss: 0.92461\n",
      "Epoch 4206 - lr: 0.01000 - Train loss: 0.87504 - Test loss: 0.90637\n",
      "Epoch 4207 - lr: 0.01000 - Train loss: 0.91219 - Test loss: 0.90458\n",
      "Epoch 4208 - lr: 0.01000 - Train loss: 0.96588 - Test loss: 0.91582\n",
      "Epoch 4209 - lr: 0.01000 - Train loss: 0.89396 - Test loss: 0.92236\n",
      "Epoch 4210 - lr: 0.01000 - Train loss: 0.93385 - Test loss: 0.90380\n",
      "Epoch 4211 - lr: 0.01000 - Train loss: 0.94511 - Test loss: 0.90509\n",
      "Epoch 4212 - lr: 0.01000 - Train loss: 0.92963 - Test loss: 0.90032\n",
      "Epoch 4213 - lr: 0.01000 - Train loss: 0.91610 - Test loss: 0.92668\n",
      "Epoch 4214 - lr: 0.01000 - Train loss: 0.93178 - Test loss: 0.92868\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4215 - lr: 0.01000 - Train loss: 0.93491 - Test loss: 0.91648\n",
      "Epoch 4216 - lr: 0.01000 - Train loss: 0.91214 - Test loss: 0.92894\n",
      "Epoch 4217 - lr: 0.01000 - Train loss: 0.89875 - Test loss: 0.92377\n",
      "Epoch 4218 - lr: 0.01000 - Train loss: 0.89435 - Test loss: 0.91920\n",
      "Epoch 4219 - lr: 0.01000 - Train loss: 0.92864 - Test loss: 0.92255\n",
      "Epoch 4220 - lr: 0.01000 - Train loss: 0.92690 - Test loss: 0.92248\n",
      "Epoch 4221 - lr: 0.01000 - Train loss: 0.93114 - Test loss: 0.91934\n",
      "Epoch 4222 - lr: 0.01000 - Train loss: 0.90915 - Test loss: 0.91496\n",
      "Epoch 4223 - lr: 0.01000 - Train loss: 0.88920 - Test loss: 0.89457\n",
      "Epoch 4224 - lr: 0.01000 - Train loss: 0.91838 - Test loss: 0.89476\n",
      "Epoch 4225 - lr: 0.01000 - Train loss: 0.96003 - Test loss: 0.89326\n",
      "Epoch 4226 - lr: 0.01000 - Train loss: 0.91949 - Test loss: 0.89568\n",
      "Epoch 4227 - lr: 0.01000 - Train loss: 0.96560 - Test loss: 0.89390\n",
      "Epoch 4228 - lr: 0.01000 - Train loss: 0.93658 - Test loss: 0.90157\n",
      "Epoch 4229 - lr: 0.01000 - Train loss: 0.97525 - Test loss: 0.89953\n",
      "Epoch 4230 - lr: 0.01000 - Train loss: 0.91448 - Test loss: 0.90816\n",
      "Epoch 4231 - lr: 0.01000 - Train loss: 0.91879 - Test loss: 0.92345\n",
      "Epoch 4232 - lr: 0.01000 - Train loss: 0.91817 - Test loss: 0.90216\n",
      "Epoch 4233 - lr: 0.01000 - Train loss: 0.95231 - Test loss: 0.88961\n",
      "Epoch 4234 - lr: 0.01000 - Train loss: 0.91548 - Test loss: 0.91801\n",
      "Epoch 4235 - lr: 0.01000 - Train loss: 0.91741 - Test loss: 0.90111\n",
      "Epoch 4236 - lr: 0.01000 - Train loss: 0.96534 - Test loss: 0.88890\n",
      "Epoch 4237 - lr: 0.01000 - Train loss: 0.91309 - Test loss: 0.89052\n",
      "Epoch 4238 - lr: 0.01000 - Train loss: 0.91729 - Test loss: 0.89703\n",
      "Epoch 4239 - lr: 0.01000 - Train loss: 0.97418 - Test loss: 0.91074\n",
      "Epoch 4240 - lr: 0.01000 - Train loss: 0.90998 - Test loss: 0.91461\n",
      "Epoch 4241 - lr: 0.01000 - Train loss: 0.89113 - Test loss: 0.91807\n",
      "Epoch 4242 - lr: 0.01000 - Train loss: 0.92611 - Test loss: 0.90308\n",
      "Epoch 4243 - lr: 0.01000 - Train loss: 0.94105 - Test loss: 0.89375\n",
      "Epoch 4244 - lr: 0.01000 - Train loss: 0.91184 - Test loss: 0.92005\n",
      "Epoch 4245 - lr: 0.01000 - Train loss: 0.91405 - Test loss: 0.89991\n",
      "Epoch 4246 - lr: 0.01000 - Train loss: 0.91177 - Test loss: 0.89306\n",
      "Epoch 4247 - lr: 0.01000 - Train loss: 0.87693 - Test loss: 0.89385\n",
      "Epoch 4248 - lr: 0.01000 - Train loss: 0.90273 - Test loss: 0.91765\n",
      "Epoch 4249 - lr: 0.01000 - Train loss: 0.90695 - Test loss: 0.91227\n",
      "Epoch 4250 - lr: 0.01000 - Train loss: 0.89953 - Test loss: 0.91297\n",
      "Epoch 4251 - lr: 0.01000 - Train loss: 0.90208 - Test loss: 0.89397\n",
      "Epoch 4252 - lr: 0.01000 - Train loss: 0.91845 - Test loss: 0.91902\n",
      "Epoch 4253 - lr: 0.01000 - Train loss: 0.91163 - Test loss: 0.89933\n",
      "Epoch 4254 - lr: 0.01000 - Train loss: 0.97007 - Test loss: 0.88744\n",
      "Epoch 4255 - lr: 0.01000 - Train loss: 0.88262 - Test loss: 0.88948\n",
      "Epoch 4256 - lr: 0.01000 - Train loss: 0.97351 - Test loss: 0.91241\n",
      "Epoch 4257 - lr: 0.01000 - Train loss: 0.92105 - Test loss: 0.90167\n",
      "Epoch 4258 - lr: 0.01000 - Train loss: 0.94243 - Test loss: 0.89353\n",
      "Epoch 4259 - lr: 0.01000 - Train loss: 0.91947 - Test loss: 0.92088\n",
      "Epoch 4260 - lr: 0.01000 - Train loss: 0.91959 - Test loss: 0.90404\n",
      "Epoch 4261 - lr: 0.01000 - Train loss: 0.98771 - Test loss: 0.89108\n",
      "Epoch 4262 - lr: 0.01000 - Train loss: 0.97017 - Test loss: 0.91502\n",
      "Epoch 4263 - lr: 0.01000 - Train loss: 0.91132 - Test loss: 0.90282\n",
      "Epoch 4264 - lr: 0.01000 - Train loss: 0.96629 - Test loss: 0.89275\n",
      "Epoch 4265 - lr: 0.01000 - Train loss: 0.88423 - Test loss: 0.91429\n",
      "Epoch 4266 - lr: 0.01000 - Train loss: 0.90716 - Test loss: 0.91308\n",
      "Epoch 4267 - lr: 0.01000 - Train loss: 0.91380 - Test loss: 0.91196\n",
      "Epoch 4268 - lr: 0.01000 - Train loss: 0.92218 - Test loss: 0.92533\n",
      "Epoch 4269 - lr: 0.01000 - Train loss: 0.88967 - Test loss: 0.92392\n",
      "Epoch 4270 - lr: 0.01000 - Train loss: 0.91733 - Test loss: 0.89251\n",
      "Epoch 4271 - lr: 0.01000 - Train loss: 0.91167 - Test loss: 0.90360\n",
      "Epoch 4272 - lr: 0.01000 - Train loss: 0.91931 - Test loss: 0.92050\n",
      "Epoch 4273 - lr: 0.01000 - Train loss: 0.90456 - Test loss: 0.92297\n",
      "Epoch 4274 - lr: 0.01000 - Train loss: 0.90442 - Test loss: 0.92216\n",
      "Epoch 4275 - lr: 0.01000 - Train loss: 0.90921 - Test loss: 0.92067\n",
      "Epoch 4276 - lr: 0.01000 - Train loss: 0.91153 - Test loss: 0.91909\n",
      "Epoch 4277 - lr: 0.01000 - Train loss: 0.91206 - Test loss: 0.91759\n",
      "Epoch 4278 - lr: 0.01000 - Train loss: 0.91355 - Test loss: 0.91618\n",
      "Epoch 4279 - lr: 0.01000 - Train loss: 0.91568 - Test loss: 0.91489\n",
      "Epoch 4280 - lr: 0.01000 - Train loss: 0.91878 - Test loss: 0.91372\n",
      "Epoch 4281 - lr: 0.01000 - Train loss: 0.92121 - Test loss: 0.91251\n",
      "Epoch 4282 - lr: 0.01000 - Train loss: 0.92100 - Test loss: 0.91145\n",
      "Epoch 4283 - lr: 0.01000 - Train loss: 0.92171 - Test loss: 0.91039\n",
      "Epoch 4284 - lr: 0.01000 - Train loss: 0.92158 - Test loss: 0.90942\n",
      "Epoch 4285 - lr: 0.01000 - Train loss: 0.92215 - Test loss: 0.90844\n",
      "Epoch 4286 - lr: 0.01000 - Train loss: 0.92195 - Test loss: 0.90756\n",
      "Epoch 4287 - lr: 0.01000 - Train loss: 0.92255 - Test loss: 0.90664\n",
      "Epoch 4288 - lr: 0.01000 - Train loss: 0.92218 - Test loss: 0.90585\n",
      "Epoch 4289 - lr: 0.01000 - Train loss: 0.92296 - Test loss: 0.90498\n",
      "Epoch 4290 - lr: 0.01000 - Train loss: 0.92224 - Test loss: 0.90430\n",
      "Epoch 4291 - lr: 0.01000 - Train loss: 0.92346 - Test loss: 0.90344\n",
      "Epoch 4292 - lr: 0.01000 - Train loss: 0.92203 - Test loss: 0.90289\n",
      "Epoch 4293 - lr: 0.01000 - Train loss: 0.92409 - Test loss: 0.90201\n",
      "Epoch 4294 - lr: 0.01000 - Train loss: 0.92162 - Test loss: 0.90160\n",
      "Epoch 4295 - lr: 0.01000 - Train loss: 0.92461 - Test loss: 0.90073\n",
      "Epoch 4296 - lr: 0.01000 - Train loss: 0.92213 - Test loss: 0.90032\n",
      "Epoch 4297 - lr: 0.01000 - Train loss: 0.92478 - Test loss: 0.89949\n",
      "Epoch 4298 - lr: 0.01000 - Train loss: 0.92217 - Test loss: 0.89917\n",
      "Epoch 4299 - lr: 0.01000 - Train loss: 0.92504 - Test loss: 0.89840\n",
      "Epoch 4300 - lr: 0.01000 - Train loss: 0.92284 - Test loss: 0.89802\n",
      "Epoch 4301 - lr: 0.01000 - Train loss: 0.92508 - Test loss: 0.89728\n",
      "Epoch 4302 - lr: 0.01000 - Train loss: 0.92236 - Test loss: 0.89709\n",
      "Epoch 4303 - lr: 0.01000 - Train loss: 0.92536 - Test loss: 0.89646\n",
      "Epoch 4304 - lr: 0.01000 - Train loss: 0.92417 - Test loss: 0.89591\n",
      "Epoch 4305 - lr: 0.01000 - Train loss: 0.92438 - Test loss: 0.89544\n",
      "Epoch 4306 - lr: 0.01000 - Train loss: 0.92417 - Test loss: 0.89507\n",
      "Epoch 4307 - lr: 0.01000 - Train loss: 0.92470 - Test loss: 0.89460\n",
      "Epoch 4308 - lr: 0.01000 - Train loss: 0.92394 - Test loss: 0.89434\n",
      "Epoch 4309 - lr: 0.01000 - Train loss: 0.92523 - Test loss: 0.89382\n",
      "Epoch 4310 - lr: 0.01000 - Train loss: 0.92354 - Test loss: 0.89367\n",
      "Epoch 4311 - lr: 0.01000 - Train loss: 0.92561 - Test loss: 0.89319\n",
      "Epoch 4312 - lr: 0.01000 - Train loss: 0.92442 - Test loss: 0.89286\n",
      "Epoch 4313 - lr: 0.01000 - Train loss: 0.92521 - Test loss: 0.89244\n",
      "Epoch 4314 - lr: 0.01000 - Train loss: 0.92430 - Test loss: 0.89223\n",
      "Epoch 4315 - lr: 0.01000 - Train loss: 0.92548 - Test loss: 0.89183\n",
      "Epoch 4316 - lr: 0.01000 - Train loss: 0.92449 - Test loss: 0.89161\n",
      "Epoch 4317 - lr: 0.01000 - Train loss: 0.92547 - Test loss: 0.89125\n",
      "Epoch 4318 - lr: 0.01000 - Train loss: 0.92476 - Test loss: 0.89101\n",
      "Epoch 4319 - lr: 0.01000 - Train loss: 0.92540 - Test loss: 0.89070\n",
      "Epoch 4320 - lr: 0.01000 - Train loss: 0.92501 - Test loss: 0.89046\n",
      "Epoch 4321 - lr: 0.01000 - Train loss: 0.92534 - Test loss: 0.89019\n",
      "Epoch 4322 - lr: 0.01000 - Train loss: 0.92521 - Test loss: 0.88995\n",
      "Epoch 4323 - lr: 0.01000 - Train loss: 0.92534 - Test loss: 0.88971\n",
      "Epoch 4324 - lr: 0.01000 - Train loss: 0.92535 - Test loss: 0.88947\n",
      "Epoch 4325 - lr: 0.01000 - Train loss: 0.92541 - Test loss: 0.88925\n",
      "Epoch 4326 - lr: 0.01000 - Train loss: 0.92545 - Test loss: 0.88903\n",
      "Epoch 4327 - lr: 0.01000 - Train loss: 0.92550 - Test loss: 0.88882\n",
      "Epoch 4328 - lr: 0.01000 - Train loss: 0.92555 - Test loss: 0.88861\n",
      "Epoch 4329 - lr: 0.01000 - Train loss: 0.92559 - Test loss: 0.88841\n",
      "Epoch 4330 - lr: 0.01000 - Train loss: 0.92564 - Test loss: 0.88822\n",
      "Epoch 4331 - lr: 0.01000 - Train loss: 0.92569 - Test loss: 0.88803\n",
      "Epoch 4332 - lr: 0.01000 - Train loss: 0.92573 - Test loss: 0.88786\n",
      "Epoch 4333 - lr: 0.01000 - Train loss: 0.92576 - Test loss: 0.88769\n",
      "Epoch 4334 - lr: 0.01000 - Train loss: 0.92577 - Test loss: 0.88754\n",
      "Epoch 4335 - lr: 0.01000 - Train loss: 0.92572 - Test loss: 0.88740\n",
      "Epoch 4336 - lr: 0.01000 - Train loss: 0.92554 - Test loss: 0.88728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4337 - lr: 0.01000 - Train loss: 0.92524 - Test loss: 0.88712\n",
      "Epoch 4338 - lr: 0.01000 - Train loss: 0.92539 - Test loss: 0.88692\n",
      "Epoch 4339 - lr: 0.01000 - Train loss: 0.92520 - Test loss: 0.88670\n",
      "Epoch 4340 - lr: 0.01000 - Train loss: 0.92575 - Test loss: 0.88652\n",
      "Epoch 4341 - lr: 0.01000 - Train loss: 0.92433 - Test loss: 0.88599\n",
      "Epoch 4342 - lr: 0.01000 - Train loss: 0.92738 - Test loss: 0.88522\n",
      "Epoch 4343 - lr: 0.01000 - Train loss: 0.92593 - Test loss: 0.88527\n",
      "Epoch 4344 - lr: 0.01000 - Train loss: 0.92742 - Test loss: 0.88525\n",
      "Epoch 4345 - lr: 0.01000 - Train loss: 0.92738 - Test loss: 0.88507\n",
      "Epoch 4346 - lr: 0.01000 - Train loss: 0.92712 - Test loss: 0.88490\n",
      "Epoch 4347 - lr: 0.01000 - Train loss: 0.92681 - Test loss: 0.88483\n",
      "Epoch 4348 - lr: 0.01000 - Train loss: 0.92684 - Test loss: 0.88480\n",
      "Epoch 4349 - lr: 0.01000 - Train loss: 0.92681 - Test loss: 0.88478\n",
      "Epoch 4350 - lr: 0.01000 - Train loss: 0.92675 - Test loss: 0.88479\n",
      "Epoch 4351 - lr: 0.01000 - Train loss: 0.92650 - Test loss: 0.88486\n",
      "Epoch 4352 - lr: 0.01000 - Train loss: 0.92549 - Test loss: 0.88485\n",
      "Epoch 4353 - lr: 0.01000 - Train loss: 0.92590 - Test loss: 0.88476\n",
      "Epoch 4354 - lr: 0.01000 - Train loss: 0.92470 - Test loss: 0.88429\n",
      "Epoch 4355 - lr: 0.01000 - Train loss: 0.92716 - Test loss: 0.88379\n",
      "Epoch 4356 - lr: 0.01000 - Train loss: 0.92735 - Test loss: 0.88369\n",
      "Epoch 4357 - lr: 0.01000 - Train loss: 0.92723 - Test loss: 0.88369\n",
      "Epoch 4358 - lr: 0.01000 - Train loss: 0.92715 - Test loss: 0.88374\n",
      "Epoch 4359 - lr: 0.01000 - Train loss: 0.92688 - Test loss: 0.88388\n",
      "Epoch 4360 - lr: 0.01000 - Train loss: 0.92559 - Test loss: 0.88391\n",
      "Epoch 4361 - lr: 0.01000 - Train loss: 0.92631 - Test loss: 0.88388\n",
      "Epoch 4362 - lr: 0.01000 - Train loss: 0.92321 - Test loss: 0.88263\n",
      "Epoch 4363 - lr: 0.01000 - Train loss: 0.87533 - Test loss: 0.85533\n",
      "Epoch 4364 - lr: 0.01000 - Train loss: 0.91124 - Test loss: 0.87045\n",
      "Epoch 4365 - lr: 0.01000 - Train loss: 0.84576 - Test loss: 0.87233\n",
      "Epoch 4366 - lr: 0.01000 - Train loss: 0.88093 - Test loss: 0.87849\n",
      "Epoch 4367 - lr: 0.01000 - Train loss: 0.84261 - Test loss: 0.86505\n",
      "Epoch 4368 - lr: 0.01000 - Train loss: 0.91552 - Test loss: 0.87596\n",
      "Epoch 4369 - lr: 0.01000 - Train loss: 0.90707 - Test loss: 0.85354\n",
      "Epoch 4370 - lr: 0.01000 - Train loss: 0.98084 - Test loss: 0.84217\n",
      "Epoch 4371 - lr: 0.01000 - Train loss: 0.92173 - Test loss: 0.85033\n",
      "Epoch 4372 - lr: 0.01000 - Train loss: 1.06582 - Test loss: 0.85002\n",
      "Epoch 4373 - lr: 0.01000 - Train loss: 0.90076 - Test loss: 0.87884\n",
      "Epoch 4374 - lr: 0.01000 - Train loss: 0.93260 - Test loss: 0.88431\n",
      "Epoch 4375 - lr: 0.01000 - Train loss: 0.91494 - Test loss: 0.87203\n",
      "Epoch 4376 - lr: 0.01000 - Train loss: 0.91283 - Test loss: 0.87837\n",
      "Epoch 4377 - lr: 0.01000 - Train loss: 0.88778 - Test loss: 0.85199\n",
      "Epoch 4378 - lr: 0.01000 - Train loss: 1.07239 - Test loss: 0.84647\n",
      "Epoch 4379 - lr: 0.01000 - Train loss: 1.01954 - Test loss: 0.84833\n",
      "Epoch 4380 - lr: 0.01000 - Train loss: 0.90821 - Test loss: 0.87812\n",
      "Epoch 4381 - lr: 0.01000 - Train loss: 0.94810 - Test loss: 0.86772\n",
      "Epoch 4382 - lr: 0.01000 - Train loss: 1.06195 - Test loss: 0.85749\n",
      "Epoch 4383 - lr: 0.01000 - Train loss: 0.90468 - Test loss: 0.87954\n",
      "Epoch 4384 - lr: 0.01000 - Train loss: 0.92044 - Test loss: 0.86839\n",
      "Epoch 4385 - lr: 0.01000 - Train loss: 1.00270 - Test loss: 0.87173\n",
      "Epoch 4386 - lr: 0.01000 - Train loss: 0.91923 - Test loss: 0.88435\n",
      "Epoch 4387 - lr: 0.01000 - Train loss: 0.93705 - Test loss: 0.86149\n",
      "Epoch 4388 - lr: 0.01000 - Train loss: 0.94042 - Test loss: 0.87650\n",
      "Epoch 4389 - lr: 0.01000 - Train loss: 0.89078 - Test loss: 0.86600\n",
      "Epoch 4390 - lr: 0.01000 - Train loss: 0.94995 - Test loss: 0.87915\n",
      "Epoch 4391 - lr: 0.01000 - Train loss: 0.85836 - Test loss: 0.87087\n",
      "Epoch 4392 - lr: 0.01000 - Train loss: 0.90068 - Test loss: 0.88873\n",
      "Epoch 4393 - lr: 0.01000 - Train loss: 0.92722 - Test loss: 0.87651\n",
      "Epoch 4394 - lr: 0.01000 - Train loss: 0.94863 - Test loss: 0.88922\n",
      "Epoch 4395 - lr: 0.01000 - Train loss: 0.90653 - Test loss: 0.89358\n",
      "Epoch 4396 - lr: 0.01000 - Train loss: 0.92399 - Test loss: 0.90471\n",
      "Epoch 4397 - lr: 0.01000 - Train loss: 0.90097 - Test loss: 0.90077\n",
      "Epoch 4398 - lr: 0.01000 - Train loss: 0.95322 - Test loss: 0.87518\n",
      "Epoch 4399 - lr: 0.01000 - Train loss: 0.89044 - Test loss: 0.86794\n",
      "Epoch 4400 - lr: 0.01000 - Train loss: 0.93479 - Test loss: 0.87429\n",
      "Epoch 4401 - lr: 0.01000 - Train loss: 0.97556 - Test loss: 0.87174\n",
      "Epoch 4402 - lr: 0.01000 - Train loss: 0.97343 - Test loss: 0.87237\n",
      "Epoch 4403 - lr: 0.01000 - Train loss: 0.97602 - Test loss: 0.89619\n",
      "Epoch 4404 - lr: 0.01000 - Train loss: 0.92305 - Test loss: 0.89607\n",
      "Epoch 4405 - lr: 0.01000 - Train loss: 0.92200 - Test loss: 0.88142\n",
      "Epoch 4406 - lr: 0.01000 - Train loss: 0.99258 - Test loss: 0.88008\n",
      "Epoch 4407 - lr: 0.01000 - Train loss: 0.96418 - Test loss: 0.90100\n",
      "Epoch 4408 - lr: 0.01000 - Train loss: 0.92051 - Test loss: 0.90159\n",
      "Epoch 4409 - lr: 0.01000 - Train loss: 0.90800 - Test loss: 0.90322\n",
      "Epoch 4410 - lr: 0.01000 - Train loss: 0.93202 - Test loss: 0.91069\n",
      "Epoch 4411 - lr: 0.01000 - Train loss: 0.92838 - Test loss: 0.89024\n",
      "Epoch 4412 - lr: 0.01000 - Train loss: 0.92221 - Test loss: 0.88103\n",
      "Epoch 4413 - lr: 0.01000 - Train loss: 0.94107 - Test loss: 0.87686\n",
      "Epoch 4414 - lr: 0.01000 - Train loss: 0.96378 - Test loss: 0.88037\n",
      "Epoch 4415 - lr: 0.01000 - Train loss: 0.93330 - Test loss: 0.90978\n",
      "Epoch 4416 - lr: 0.01000 - Train loss: 0.92638 - Test loss: 0.91140\n",
      "Epoch 4417 - lr: 0.01000 - Train loss: 0.83950 - Test loss: 0.88498\n",
      "Epoch 4418 - lr: 0.01000 - Train loss: 0.91619 - Test loss: 0.88019\n",
      "Epoch 4419 - lr: 0.01000 - Train loss: 0.92225 - Test loss: 0.89800\n",
      "Epoch 4420 - lr: 0.01000 - Train loss: 0.92320 - Test loss: 0.88213\n",
      "Epoch 4421 - lr: 0.01000 - Train loss: 0.94514 - Test loss: 0.89229\n",
      "Epoch 4422 - lr: 0.01000 - Train loss: 0.97738 - Test loss: 0.90462\n",
      "Epoch 4423 - lr: 0.01000 - Train loss: 0.83593 - Test loss: 0.88719\n",
      "Epoch 4424 - lr: 0.01000 - Train loss: 0.83656 - Test loss: 0.88353\n",
      "Epoch 4425 - lr: 0.01000 - Train loss: 0.88208 - Test loss: 0.90327\n",
      "Epoch 4426 - lr: 0.01000 - Train loss: 0.88485 - Test loss: 0.91341\n",
      "Epoch 4427 - lr: 0.01000 - Train loss: 0.91376 - Test loss: 0.88315\n",
      "Epoch 4428 - lr: 0.01000 - Train loss: 0.94193 - Test loss: 0.89433\n",
      "Epoch 4429 - lr: 0.01000 - Train loss: 0.89347 - Test loss: 0.90633\n",
      "Epoch 4430 - lr: 0.01000 - Train loss: 0.92458 - Test loss: 0.91757\n",
      "Epoch 4431 - lr: 0.01000 - Train loss: 0.92063 - Test loss: 0.89995\n",
      "Epoch 4432 - lr: 0.01000 - Train loss: 0.91053 - Test loss: 0.88794\n",
      "Epoch 4433 - lr: 0.01000 - Train loss: 0.97134 - Test loss: 0.90457\n",
      "Epoch 4434 - lr: 0.01000 - Train loss: 0.91920 - Test loss: 0.90387\n",
      "Epoch 4435 - lr: 0.01000 - Train loss: 0.91471 - Test loss: 0.90453\n",
      "Epoch 4436 - lr: 0.01000 - Train loss: 0.91598 - Test loss: 0.91498\n",
      "Epoch 4437 - lr: 0.01000 - Train loss: 0.92786 - Test loss: 0.89063\n",
      "Epoch 4438 - lr: 0.01000 - Train loss: 0.99110 - Test loss: 0.88510\n",
      "Epoch 4439 - lr: 0.01000 - Train loss: 1.02168 - Test loss: 0.87872\n",
      "Epoch 4440 - lr: 0.01000 - Train loss: 0.95062 - Test loss: 0.88339\n",
      "Epoch 4441 - lr: 0.01000 - Train loss: 0.93705 - Test loss: 0.89916\n",
      "Epoch 4442 - lr: 0.01000 - Train loss: 0.92765 - Test loss: 0.91176\n",
      "Epoch 4443 - lr: 0.01000 - Train loss: 0.83665 - Test loss: 0.88892\n",
      "Epoch 4444 - lr: 0.01000 - Train loss: 0.84136 - Test loss: 0.88404\n",
      "Epoch 4445 - lr: 0.01000 - Train loss: 0.92493 - Test loss: 0.89008\n",
      "Epoch 4446 - lr: 0.01000 - Train loss: 1.04323 - Test loss: 0.88982\n",
      "Epoch 4447 - lr: 0.01000 - Train loss: 0.98627 - Test loss: 0.89331\n",
      "Epoch 4448 - lr: 0.01000 - Train loss: 0.93352 - Test loss: 0.89096\n",
      "Epoch 4449 - lr: 0.01000 - Train loss: 0.90177 - Test loss: 0.88793\n",
      "Epoch 4450 - lr: 0.01000 - Train loss: 0.91556 - Test loss: 0.91853\n",
      "Epoch 4451 - lr: 0.01000 - Train loss: 0.92707 - Test loss: 0.92590\n",
      "Epoch 4452 - lr: 0.01000 - Train loss: 0.92085 - Test loss: 0.92645\n",
      "Epoch 4453 - lr: 0.01000 - Train loss: 0.88864 - Test loss: 0.91765\n",
      "Epoch 4454 - lr: 0.01000 - Train loss: 0.83744 - Test loss: 0.89008\n",
      "Epoch 4455 - lr: 0.01000 - Train loss: 0.86730 - Test loss: 0.88417\n",
      "Epoch 4456 - lr: 0.01000 - Train loss: 0.92538 - Test loss: 0.90122\n",
      "Epoch 4457 - lr: 0.01000 - Train loss: 0.92175 - Test loss: 0.88507\n",
      "Epoch 4458 - lr: 0.01000 - Train loss: 0.93723 - Test loss: 0.89218\n",
      "Epoch 4459 - lr: 0.01000 - Train loss: 0.99158 - Test loss: 0.91006\n",
      "Epoch 4460 - lr: 0.01000 - Train loss: 0.92347 - Test loss: 0.91341\n",
      "Epoch 4461 - lr: 0.01000 - Train loss: 0.84649 - Test loss: 0.88962\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4462 - lr: 0.01000 - Train loss: 0.91616 - Test loss: 0.91590\n",
      "Epoch 4463 - lr: 0.01000 - Train loss: 0.89749 - Test loss: 0.91836\n",
      "Epoch 4464 - lr: 0.01000 - Train loss: 0.92754 - Test loss: 0.88567\n",
      "Epoch 4465 - lr: 0.01000 - Train loss: 0.93160 - Test loss: 0.88187\n",
      "Epoch 4466 - lr: 0.01000 - Train loss: 0.89345 - Test loss: 0.90529\n",
      "Epoch 4467 - lr: 0.01000 - Train loss: 0.93158 - Test loss: 0.91918\n",
      "Epoch 4468 - lr: 0.01000 - Train loss: 0.92115 - Test loss: 0.90969\n",
      "Epoch 4469 - lr: 0.01000 - Train loss: 0.85374 - Test loss: 0.88892\n",
      "Epoch 4470 - lr: 0.01000 - Train loss: 0.83425 - Test loss: 0.88523\n",
      "Epoch 4471 - lr: 0.01000 - Train loss: 0.85775 - Test loss: 0.88344\n",
      "Epoch 4472 - lr: 0.01000 - Train loss: 0.93461 - Test loss: 0.90962\n",
      "Epoch 4473 - lr: 0.01000 - Train loss: 0.91721 - Test loss: 0.88625\n",
      "Epoch 4474 - lr: 0.01000 - Train loss: 0.91137 - Test loss: 0.91416\n",
      "Epoch 4475 - lr: 0.01000 - Train loss: 0.90131 - Test loss: 0.91844\n",
      "Epoch 4476 - lr: 0.01000 - Train loss: 0.92507 - Test loss: 0.90292\n",
      "Epoch 4477 - lr: 0.01000 - Train loss: 0.91925 - Test loss: 0.91566\n",
      "Epoch 4478 - lr: 0.01000 - Train loss: 0.90708 - Test loss: 0.91759\n",
      "Epoch 4479 - lr: 0.01000 - Train loss: 0.92925 - Test loss: 0.91251\n",
      "Epoch 4480 - lr: 0.01000 - Train loss: 0.85276 - Test loss: 0.88473\n",
      "Epoch 4481 - lr: 0.01000 - Train loss: 0.83649 - Test loss: 0.87919\n",
      "Epoch 4482 - lr: 0.01000 - Train loss: 0.83607 - Test loss: 0.87872\n",
      "Epoch 4483 - lr: 0.01000 - Train loss: 0.83788 - Test loss: 0.87828\n",
      "Epoch 4484 - lr: 0.01000 - Train loss: 0.91317 - Test loss: 0.88227\n",
      "Epoch 4485 - lr: 0.01000 - Train loss: 1.00890 - Test loss: 0.89179\n",
      "Epoch 4486 - lr: 0.01000 - Train loss: 0.93693 - Test loss: 0.91040\n",
      "Epoch 4487 - lr: 0.01000 - Train loss: 0.91271 - Test loss: 0.89072\n",
      "Epoch 4488 - lr: 0.01000 - Train loss: 0.98583 - Test loss: 0.90645\n",
      "Epoch 4489 - lr: 0.01000 - Train loss: 0.90293 - Test loss: 0.91563\n",
      "Epoch 4490 - lr: 0.01000 - Train loss: 0.90699 - Test loss: 0.91773\n",
      "Epoch 4491 - lr: 0.01000 - Train loss: 0.94158 - Test loss: 0.89333\n",
      "Epoch 4492 - lr: 0.01000 - Train loss: 0.98584 - Test loss: 0.90453\n",
      "Epoch 4493 - lr: 0.01000 - Train loss: 0.93231 - Test loss: 0.91243\n",
      "Epoch 4494 - lr: 0.01000 - Train loss: 0.92317 - Test loss: 0.89933\n",
      "Epoch 4495 - lr: 0.01000 - Train loss: 0.92226 - Test loss: 0.89877\n",
      "Epoch 4496 - lr: 0.01000 - Train loss: 0.92792 - Test loss: 0.88558\n",
      "Epoch 4497 - lr: 0.01000 - Train loss: 0.95975 - Test loss: 0.88997\n",
      "Epoch 4498 - lr: 0.01000 - Train loss: 0.90841 - Test loss: 0.91059\n",
      "Epoch 4499 - lr: 0.01000 - Train loss: 0.92974 - Test loss: 0.91043\n",
      "Epoch 4500 - lr: 0.01000 - Train loss: 0.89751 - Test loss: 0.87950\n",
      "Epoch 4501 - lr: 0.01000 - Train loss: 0.93271 - Test loss: 0.90643\n",
      "Epoch 4502 - lr: 0.01000 - Train loss: 0.92185 - Test loss: 0.90396\n",
      "Epoch 4503 - lr: 0.01000 - Train loss: 0.88396 - Test loss: 0.87838\n",
      "Epoch 4504 - lr: 0.01000 - Train loss: 0.92797 - Test loss: 0.90285\n",
      "Epoch 4505 - lr: 0.01000 - Train loss: 0.88877 - Test loss: 0.88096\n",
      "Epoch 4506 - lr: 0.01000 - Train loss: 0.92210 - Test loss: 0.90024\n",
      "Epoch 4507 - lr: 0.01000 - Train loss: 0.86954 - Test loss: 0.87927\n",
      "Epoch 4508 - lr: 0.01000 - Train loss: 0.93061 - Test loss: 0.90665\n",
      "Epoch 4509 - lr: 0.01000 - Train loss: 0.91929 - Test loss: 0.91372\n",
      "Epoch 4510 - lr: 0.01000 - Train loss: 0.90292 - Test loss: 0.90431\n",
      "Epoch 4511 - lr: 0.01000 - Train loss: 0.92057 - Test loss: 0.87502\n",
      "Epoch 4512 - lr: 0.01000 - Train loss: 0.91210 - Test loss: 0.89125\n",
      "Epoch 4513 - lr: 0.01000 - Train loss: 0.92490 - Test loss: 0.88291\n",
      "Epoch 4514 - lr: 0.01000 - Train loss: 0.99373 - Test loss: 0.88033\n",
      "Epoch 4515 - lr: 0.01000 - Train loss: 0.95443 - Test loss: 0.87831\n",
      "Epoch 4516 - lr: 0.01000 - Train loss: 0.93289 - Test loss: 0.90357\n",
      "Epoch 4517 - lr: 0.01000 - Train loss: 0.85195 - Test loss: 0.88351\n",
      "Epoch 4518 - lr: 0.01000 - Train loss: 0.88128 - Test loss: 0.89977\n",
      "Epoch 4519 - lr: 0.01000 - Train loss: 0.92103 - Test loss: 0.89241\n",
      "Epoch 4520 - lr: 0.01000 - Train loss: 0.92175 - Test loss: 0.88638\n",
      "Epoch 4521 - lr: 0.01000 - Train loss: 0.93065 - Test loss: 0.90923\n",
      "Epoch 4522 - lr: 0.01000 - Train loss: 0.93104 - Test loss: 0.91336\n",
      "Epoch 4523 - lr: 0.01000 - Train loss: 0.92470 - Test loss: 0.90000\n",
      "Epoch 4524 - lr: 0.01000 - Train loss: 0.91456 - Test loss: 0.89575\n",
      "Epoch 4525 - lr: 0.01000 - Train loss: 0.92259 - Test loss: 0.90045\n",
      "Epoch 4526 - lr: 0.01000 - Train loss: 0.86255 - Test loss: 0.87749\n",
      "Epoch 4527 - lr: 0.01000 - Train loss: 0.90756 - Test loss: 0.90429\n",
      "Epoch 4528 - lr: 0.01000 - Train loss: 0.92509 - Test loss: 0.90329\n",
      "Epoch 4529 - lr: 0.01000 - Train loss: 0.83931 - Test loss: 0.87884\n",
      "Epoch 4530 - lr: 0.01000 - Train loss: 0.85544 - Test loss: 0.87338\n",
      "Epoch 4531 - lr: 0.01000 - Train loss: 0.93422 - Test loss: 0.90179\n",
      "Epoch 4532 - lr: 0.01000 - Train loss: 0.93155 - Test loss: 0.89450\n",
      "Epoch 4533 - lr: 0.01000 - Train loss: 0.93442 - Test loss: 0.89257\n",
      "Epoch 4534 - lr: 0.01000 - Train loss: 0.93212 - Test loss: 0.90389\n",
      "Epoch 4535 - lr: 0.01000 - Train loss: 0.98100 - Test loss: 0.87808\n",
      "Epoch 4536 - lr: 0.01000 - Train loss: 1.00162 - Test loss: 0.88538\n",
      "Epoch 4537 - lr: 0.01000 - Train loss: 0.93424 - Test loss: 0.89067\n",
      "Epoch 4538 - lr: 0.01000 - Train loss: 0.93880 - Test loss: 0.90552\n",
      "Epoch 4539 - lr: 0.01000 - Train loss: 0.91548 - Test loss: 0.91346\n",
      "Epoch 4540 - lr: 0.01000 - Train loss: 0.90336 - Test loss: 0.90533\n",
      "Epoch 4541 - lr: 0.01000 - Train loss: 0.91088 - Test loss: 0.89742\n",
      "Epoch 4542 - lr: 0.01000 - Train loss: 0.93724 - Test loss: 0.87408\n",
      "Epoch 4543 - lr: 0.01000 - Train loss: 0.90189 - Test loss: 0.89576\n",
      "Epoch 4544 - lr: 0.01000 - Train loss: 0.90649 - Test loss: 0.89396\n",
      "Epoch 4545 - lr: 0.01000 - Train loss: 0.90813 - Test loss: 0.90912\n",
      "Epoch 4546 - lr: 0.01000 - Train loss: 0.88922 - Test loss: 0.90738\n",
      "Epoch 4547 - lr: 0.01000 - Train loss: 0.85584 - Test loss: 0.87926\n",
      "Epoch 4548 - lr: 0.01000 - Train loss: 0.92673 - Test loss: 0.86988\n",
      "Epoch 4549 - lr: 0.01000 - Train loss: 0.94619 - Test loss: 0.87597\n",
      "Epoch 4550 - lr: 0.01000 - Train loss: 0.84205 - Test loss: 0.87514\n",
      "Epoch 4551 - lr: 0.01000 - Train loss: 0.88617 - Test loss: 0.87793\n",
      "Epoch 4552 - lr: 0.01000 - Train loss: 0.93355 - Test loss: 0.88634\n",
      "Epoch 4553 - lr: 0.01000 - Train loss: 0.98834 - Test loss: 0.90243\n",
      "Epoch 4554 - lr: 0.01000 - Train loss: 0.91823 - Test loss: 0.90486\n",
      "Epoch 4555 - lr: 0.01000 - Train loss: 0.83879 - Test loss: 0.88369\n",
      "Epoch 4556 - lr: 0.01000 - Train loss: 0.83640 - Test loss: 0.88077\n",
      "Epoch 4557 - lr: 0.01000 - Train loss: 0.84467 - Test loss: 0.87861\n",
      "Epoch 4558 - lr: 0.01000 - Train loss: 0.94456 - Test loss: 0.89163\n",
      "Epoch 4559 - lr: 0.01000 - Train loss: 0.86741 - Test loss: 0.88245\n",
      "Epoch 4560 - lr: 0.01000 - Train loss: 0.96101 - Test loss: 0.89363\n",
      "Epoch 4561 - lr: 0.01000 - Train loss: 0.92074 - Test loss: 0.91355\n",
      "Epoch 4562 - lr: 0.01000 - Train loss: 0.91795 - Test loss: 0.91833\n",
      "Epoch 4563 - lr: 0.01000 - Train loss: 0.93977 - Test loss: 0.90087\n",
      "Epoch 4564 - lr: 0.01000 - Train loss: 0.91610 - Test loss: 0.88777\n",
      "Epoch 4565 - lr: 0.01000 - Train loss: 0.99628 - Test loss: 0.90374\n",
      "Epoch 4566 - lr: 0.01000 - Train loss: 0.90615 - Test loss: 0.91447\n",
      "Epoch 4567 - lr: 0.01000 - Train loss: 0.92513 - Test loss: 0.90175\n",
      "Epoch 4568 - lr: 0.01000 - Train loss: 0.89288 - Test loss: 0.90043\n",
      "Epoch 4569 - lr: 0.01000 - Train loss: 0.92579 - Test loss: 0.90791\n",
      "Epoch 4570 - lr: 0.01000 - Train loss: 0.90329 - Test loss: 0.88033\n",
      "Epoch 4571 - lr: 0.01000 - Train loss: 0.92117 - Test loss: 0.90736\n",
      "Epoch 4572 - lr: 0.01000 - Train loss: 0.90445 - Test loss: 0.90421\n",
      "Epoch 4573 - lr: 0.01000 - Train loss: 0.90883 - Test loss: 0.89451\n",
      "Epoch 4574 - lr: 0.01000 - Train loss: 0.89319 - Test loss: 0.87528\n",
      "Epoch 4575 - lr: 0.01000 - Train loss: 0.92610 - Test loss: 0.87907\n",
      "Epoch 4576 - lr: 0.01000 - Train loss: 0.98689 - Test loss: 0.90046\n",
      "Epoch 4577 - lr: 0.01000 - Train loss: 0.92438 - Test loss: 0.88667\n",
      "Epoch 4578 - lr: 0.01000 - Train loss: 0.98117 - Test loss: 0.90322\n",
      "Epoch 4579 - lr: 0.01000 - Train loss: 0.91026 - Test loss: 0.91416\n",
      "Epoch 4580 - lr: 0.01000 - Train loss: 0.88712 - Test loss: 0.90970\n",
      "Epoch 4581 - lr: 0.01000 - Train loss: 0.83894 - Test loss: 0.88128\n",
      "Epoch 4582 - lr: 0.01000 - Train loss: 0.90449 - Test loss: 0.89273\n",
      "Epoch 4583 - lr: 0.01000 - Train loss: 0.90513 - Test loss: 0.90099\n",
      "Epoch 4584 - lr: 0.01000 - Train loss: 0.92632 - Test loss: 0.88546\n",
      "Epoch 4585 - lr: 0.01000 - Train loss: 0.97571 - Test loss: 0.90112\n",
      "Epoch 4586 - lr: 0.01000 - Train loss: 0.93017 - Test loss: 0.89716\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4587 - lr: 0.01000 - Train loss: 0.92015 - Test loss: 0.89739\n",
      "Epoch 4588 - lr: 0.01000 - Train loss: 0.97212 - Test loss: 0.87928\n",
      "Epoch 4589 - lr: 0.01000 - Train loss: 1.00051 - Test loss: 0.90033\n",
      "Epoch 4590 - lr: 0.01000 - Train loss: 0.93317 - Test loss: 0.91176\n",
      "Epoch 4591 - lr: 0.01000 - Train loss: 0.88942 - Test loss: 0.91370\n",
      "Epoch 4592 - lr: 0.01000 - Train loss: 0.90995 - Test loss: 0.91498\n",
      "Epoch 4593 - lr: 0.01000 - Train loss: 0.90780 - Test loss: 0.90610\n",
      "Epoch 4594 - lr: 0.01000 - Train loss: 0.95743 - Test loss: 0.88166\n",
      "Epoch 4595 - lr: 0.01000 - Train loss: 0.93641 - Test loss: 0.90385\n",
      "Epoch 4596 - lr: 0.01000 - Train loss: 0.91576 - Test loss: 0.91145\n",
      "Epoch 4597 - lr: 0.01000 - Train loss: 0.90359 - Test loss: 0.90547\n",
      "Epoch 4598 - lr: 0.01000 - Train loss: 0.84039 - Test loss: 0.87885\n",
      "Epoch 4599 - lr: 0.01000 - Train loss: 0.90724 - Test loss: 0.87142\n",
      "Epoch 4600 - lr: 0.01000 - Train loss: 0.83631 - Test loss: 0.87446\n",
      "Epoch 4601 - lr: 0.01000 - Train loss: 0.83744 - Test loss: 0.87510\n",
      "Epoch 4602 - lr: 0.01000 - Train loss: 0.85445 - Test loss: 0.87350\n",
      "Epoch 4603 - lr: 0.01000 - Train loss: 0.97452 - Test loss: 0.88659\n",
      "Epoch 4604 - lr: 0.01000 - Train loss: 0.86231 - Test loss: 0.87877\n",
      "Epoch 4605 - lr: 0.01000 - Train loss: 0.93639 - Test loss: 0.87593\n",
      "Epoch 4606 - lr: 0.01000 - Train loss: 0.89253 - Test loss: 0.89983\n",
      "Epoch 4607 - lr: 0.01000 - Train loss: 0.90518 - Test loss: 0.91353\n",
      "Epoch 4608 - lr: 0.01000 - Train loss: 0.92750 - Test loss: 0.90098\n",
      "Epoch 4609 - lr: 0.01000 - Train loss: 0.92422 - Test loss: 0.90802\n",
      "Epoch 4610 - lr: 0.01000 - Train loss: 0.96088 - Test loss: 0.88533\n",
      "Epoch 4611 - lr: 0.01000 - Train loss: 0.91963 - Test loss: 0.87628\n",
      "Epoch 4612 - lr: 0.01000 - Train loss: 0.92290 - Test loss: 0.90498\n",
      "Epoch 4613 - lr: 0.01000 - Train loss: 0.92770 - Test loss: 0.91530\n",
      "Epoch 4614 - lr: 0.01000 - Train loss: 0.89328 - Test loss: 0.91329\n",
      "Epoch 4615 - lr: 0.01000 - Train loss: 0.93326 - Test loss: 0.87882\n",
      "Epoch 4616 - lr: 0.01000 - Train loss: 0.89395 - Test loss: 0.89974\n",
      "Epoch 4617 - lr: 0.01000 - Train loss: 0.92460 - Test loss: 0.88735\n",
      "Epoch 4618 - lr: 0.01000 - Train loss: 0.98123 - Test loss: 0.90410\n",
      "Epoch 4619 - lr: 0.01000 - Train loss: 0.90440 - Test loss: 0.91405\n",
      "Epoch 4620 - lr: 0.01000 - Train loss: 0.92794 - Test loss: 0.90051\n",
      "Epoch 4621 - lr: 0.01000 - Train loss: 0.91903 - Test loss: 0.90365\n",
      "Epoch 4622 - lr: 0.01000 - Train loss: 0.89326 - Test loss: 0.87837\n",
      "Epoch 4623 - lr: 0.01000 - Train loss: 0.92440 - Test loss: 0.88115\n",
      "Epoch 4624 - lr: 0.01000 - Train loss: 0.99413 - Test loss: 0.90206\n",
      "Epoch 4625 - lr: 0.01000 - Train loss: 0.93617 - Test loss: 0.89371\n",
      "Epoch 4626 - lr: 0.01000 - Train loss: 0.97270 - Test loss: 0.89372\n",
      "Epoch 4627 - lr: 0.01000 - Train loss: 0.91053 - Test loss: 0.91242\n",
      "Epoch 4628 - lr: 0.01000 - Train loss: 0.90646 - Test loss: 0.90502\n",
      "Epoch 4629 - lr: 0.01000 - Train loss: 0.89608 - Test loss: 0.89569\n",
      "Epoch 4630 - lr: 0.01000 - Train loss: 0.93495 - Test loss: 0.89507\n",
      "Epoch 4631 - lr: 0.01000 - Train loss: 0.92379 - Test loss: 0.89666\n",
      "Epoch 4632 - lr: 0.01000 - Train loss: 0.92214 - Test loss: 0.87965\n",
      "Epoch 4633 - lr: 0.01000 - Train loss: 0.92824 - Test loss: 0.90339\n",
      "Epoch 4634 - lr: 0.01000 - Train loss: 0.90839 - Test loss: 0.89740\n",
      "Epoch 4635 - lr: 0.01000 - Train loss: 0.93587 - Test loss: 0.88725\n",
      "Epoch 4636 - lr: 0.01000 - Train loss: 1.02065 - Test loss: 0.88712\n",
      "Epoch 4637 - lr: 0.01000 - Train loss: 0.98767 - Test loss: 0.88160\n",
      "Epoch 4638 - lr: 0.01000 - Train loss: 0.93485 - Test loss: 0.90457\n",
      "Epoch 4639 - lr: 0.01000 - Train loss: 0.92328 - Test loss: 0.90972\n",
      "Epoch 4640 - lr: 0.01000 - Train loss: 0.90386 - Test loss: 0.90264\n",
      "Epoch 4641 - lr: 0.01000 - Train loss: 0.93419 - Test loss: 0.88494\n",
      "Epoch 4642 - lr: 0.01000 - Train loss: 0.99757 - Test loss: 0.90084\n",
      "Epoch 4643 - lr: 0.01000 - Train loss: 0.95088 - Test loss: 0.89187\n",
      "Epoch 4644 - lr: 0.01000 - Train loss: 0.99647 - Test loss: 0.88365\n",
      "Epoch 4645 - lr: 0.01000 - Train loss: 0.98326 - Test loss: 0.87878\n",
      "Epoch 4646 - lr: 0.01000 - Train loss: 0.96701 - Test loss: 0.89078\n",
      "Epoch 4647 - lr: 0.01000 - Train loss: 0.88778 - Test loss: 0.89980\n",
      "Epoch 4648 - lr: 0.01000 - Train loss: 0.84398 - Test loss: 0.88226\n",
      "Epoch 4649 - lr: 0.01000 - Train loss: 0.92048 - Test loss: 0.89501\n",
      "Epoch 4650 - lr: 0.01000 - Train loss: 0.95505 - Test loss: 0.89549\n",
      "Epoch 4651 - lr: 0.01000 - Train loss: 0.95980 - Test loss: 0.89490\n",
      "Epoch 4652 - lr: 0.01000 - Train loss: 0.92166 - Test loss: 0.89868\n",
      "Epoch 4653 - lr: 0.01000 - Train loss: 0.92534 - Test loss: 0.88618\n",
      "Epoch 4654 - lr: 0.01000 - Train loss: 0.94293 - Test loss: 0.88298\n",
      "Epoch 4655 - lr: 0.01000 - Train loss: 0.87186 - Test loss: 0.87868\n",
      "Epoch 4656 - lr: 0.01000 - Train loss: 0.97863 - Test loss: 0.89165\n",
      "Epoch 4657 - lr: 0.01000 - Train loss: 0.86850 - Test loss: 0.88176\n",
      "Epoch 4658 - lr: 0.01000 - Train loss: 0.97028 - Test loss: 0.89456\n",
      "Epoch 4659 - lr: 0.01000 - Train loss: 0.93141 - Test loss: 0.90251\n",
      "Epoch 4660 - lr: 0.01000 - Train loss: 0.86095 - Test loss: 0.88480\n",
      "Epoch 4661 - lr: 0.01000 - Train loss: 0.97638 - Test loss: 0.89536\n",
      "Epoch 4662 - lr: 0.01000 - Train loss: 0.84772 - Test loss: 0.88534\n",
      "Epoch 4663 - lr: 0.01000 - Train loss: 0.91216 - Test loss: 0.89039\n",
      "Epoch 4664 - lr: 0.01000 - Train loss: 0.98636 - Test loss: 0.88517\n",
      "Epoch 4665 - lr: 0.01000 - Train loss: 0.88442 - Test loss: 0.91032\n",
      "Epoch 4666 - lr: 0.01000 - Train loss: 0.86100 - Test loss: 0.88894\n",
      "Epoch 4667 - lr: 0.01000 - Train loss: 0.97725 - Test loss: 0.89860\n",
      "Epoch 4668 - lr: 0.01000 - Train loss: 0.90735 - Test loss: 0.89883\n",
      "Epoch 4669 - lr: 0.01000 - Train loss: 0.94023 - Test loss: 0.90218\n",
      "Epoch 4670 - lr: 0.01000 - Train loss: 0.92811 - Test loss: 0.89510\n",
      "Epoch 4671 - lr: 0.01000 - Train loss: 0.99747 - Test loss: 0.88760\n",
      "Epoch 4672 - lr: 0.01000 - Train loss: 0.91645 - Test loss: 0.91019\n",
      "Epoch 4673 - lr: 0.01000 - Train loss: 0.92843 - Test loss: 0.92362\n",
      "Epoch 4674 - lr: 0.01000 - Train loss: 0.91583 - Test loss: 0.90243\n",
      "Epoch 4675 - lr: 0.01000 - Train loss: 0.95097 - Test loss: 0.89250\n",
      "Epoch 4676 - lr: 0.01000 - Train loss: 0.92996 - Test loss: 0.91997\n",
      "Epoch 4677 - lr: 0.01000 - Train loss: 0.92788 - Test loss: 0.92483\n",
      "Epoch 4678 - lr: 0.01000 - Train loss: 0.94096 - Test loss: 0.92219\n",
      "Epoch 4679 - lr: 0.01000 - Train loss: 0.90069 - Test loss: 0.92610\n",
      "Epoch 4680 - lr: 0.01000 - Train loss: 0.94130 - Test loss: 0.91013\n",
      "Epoch 4681 - lr: 0.01000 - Train loss: 0.92586 - Test loss: 0.90891\n",
      "Epoch 4682 - lr: 0.01000 - Train loss: 0.93273 - Test loss: 0.91808\n",
      "Epoch 4683 - lr: 0.01000 - Train loss: 0.88599 - Test loss: 0.91399\n",
      "Epoch 4684 - lr: 0.01000 - Train loss: 0.84689 - Test loss: 0.88698\n",
      "Epoch 4685 - lr: 0.01000 - Train loss: 0.89320 - Test loss: 0.89911\n",
      "Epoch 4686 - lr: 0.01000 - Train loss: 0.93270 - Test loss: 0.89542\n",
      "Epoch 4687 - lr: 0.01000 - Train loss: 0.99197 - Test loss: 0.90577\n",
      "Epoch 4688 - lr: 0.01000 - Train loss: 0.91859 - Test loss: 0.90715\n",
      "Epoch 4689 - lr: 0.01000 - Train loss: 0.93420 - Test loss: 0.88466\n",
      "Epoch 4690 - lr: 0.01000 - Train loss: 0.99396 - Test loss: 0.88315\n",
      "Epoch 4691 - lr: 0.01000 - Train loss: 0.94372 - Test loss: 0.91142\n",
      "Epoch 4692 - lr: 0.01000 - Train loss: 0.95265 - Test loss: 0.90617\n",
      "Epoch 4693 - lr: 0.01000 - Train loss: 0.94201 - Test loss: 0.90584\n",
      "Epoch 4694 - lr: 0.01000 - Train loss: 0.92647 - Test loss: 0.90787\n",
      "Epoch 4695 - lr: 0.01000 - Train loss: 0.88878 - Test loss: 0.90654\n",
      "Epoch 4696 - lr: 0.01000 - Train loss: 0.89354 - Test loss: 0.88914\n",
      "Epoch 4697 - lr: 0.01000 - Train loss: 0.97334 - Test loss: 0.89752\n",
      "Epoch 4698 - lr: 0.01000 - Train loss: 0.90050 - Test loss: 0.89814\n",
      "Epoch 4699 - lr: 0.01000 - Train loss: 0.93158 - Test loss: 0.88552\n",
      "Epoch 4700 - lr: 0.01000 - Train loss: 0.90981 - Test loss: 0.90778\n",
      "Epoch 4701 - lr: 0.01000 - Train loss: 0.93129 - Test loss: 0.92167\n",
      "Epoch 4702 - lr: 0.01000 - Train loss: 0.93102 - Test loss: 0.91961\n",
      "Epoch 4703 - lr: 0.01000 - Train loss: 0.85131 - Test loss: 0.89197\n",
      "Epoch 4704 - lr: 0.01000 - Train loss: 0.86503 - Test loss: 0.88414\n",
      "Epoch 4705 - lr: 0.01000 - Train loss: 0.97182 - Test loss: 0.89677\n",
      "Epoch 4706 - lr: 0.01000 - Train loss: 0.93216 - Test loss: 0.88570\n",
      "Epoch 4707 - lr: 0.01000 - Train loss: 0.90784 - Test loss: 0.90730\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4708 - lr: 0.01000 - Train loss: 0.92874 - Test loss: 0.90810\n",
      "Epoch 4709 - lr: 0.01000 - Train loss: 0.91216 - Test loss: 0.89623\n",
      "Epoch 4710 - lr: 0.01000 - Train loss: 0.94431 - Test loss: 0.88931\n",
      "Epoch 4711 - lr: 0.01000 - Train loss: 0.96649 - Test loss: 0.89037\n",
      "Epoch 4712 - lr: 0.01000 - Train loss: 0.99910 - Test loss: 0.88867\n",
      "Epoch 4713 - lr: 0.01000 - Train loss: 0.92266 - Test loss: 0.89095\n",
      "Epoch 4714 - lr: 0.01000 - Train loss: 0.99691 - Test loss: 0.89271\n",
      "Epoch 4715 - lr: 0.01000 - Train loss: 0.99448 - Test loss: 0.90687\n",
      "Epoch 4716 - lr: 0.01000 - Train loss: 0.97515 - Test loss: 0.92455\n",
      "Epoch 4717 - lr: 0.01000 - Train loss: 0.86629 - Test loss: 0.90154\n",
      "Epoch 4718 - lr: 0.01000 - Train loss: 0.94681 - Test loss: 0.91396\n",
      "Epoch 4719 - lr: 0.01000 - Train loss: 0.95229 - Test loss: 0.91559\n",
      "Epoch 4720 - lr: 0.01000 - Train loss: 0.92848 - Test loss: 0.93082\n",
      "Epoch 4721 - lr: 0.01000 - Train loss: 0.92804 - Test loss: 0.93089\n",
      "Epoch 4722 - lr: 0.01000 - Train loss: 0.92244 - Test loss: 0.89941\n",
      "Epoch 4723 - lr: 0.01000 - Train loss: 0.87294 - Test loss: 0.89733\n",
      "Epoch 4724 - lr: 0.01000 - Train loss: 0.97092 - Test loss: 0.90802\n",
      "Epoch 4725 - lr: 0.01000 - Train loss: 0.83992 - Test loss: 0.89887\n",
      "Epoch 4726 - lr: 0.01000 - Train loss: 0.85646 - Test loss: 0.89766\n",
      "Epoch 4727 - lr: 0.01000 - Train loss: 0.92034 - Test loss: 0.89490\n",
      "Epoch 4728 - lr: 0.01000 - Train loss: 0.86227 - Test loss: 0.89582\n",
      "Epoch 4729 - lr: 0.01000 - Train loss: 0.96278 - Test loss: 0.91011\n",
      "Epoch 4730 - lr: 0.01000 - Train loss: 0.93980 - Test loss: 0.92696\n",
      "Epoch 4731 - lr: 0.01000 - Train loss: 0.94287 - Test loss: 0.91825\n",
      "Epoch 4732 - lr: 0.01000 - Train loss: 0.95760 - Test loss: 0.91422\n",
      "Epoch 4733 - lr: 0.01000 - Train loss: 0.92009 - Test loss: 0.90706\n",
      "Epoch 4734 - lr: 0.01000 - Train loss: 0.98811 - Test loss: 0.91979\n",
      "Epoch 4735 - lr: 0.01000 - Train loss: 0.92418 - Test loss: 0.92921\n",
      "Epoch 4736 - lr: 0.01000 - Train loss: 0.92826 - Test loss: 0.92636\n",
      "Epoch 4737 - lr: 0.01000 - Train loss: 0.85990 - Test loss: 0.89750\n",
      "Epoch 4738 - lr: 0.01000 - Train loss: 0.96512 - Test loss: 0.90709\n",
      "Epoch 4739 - lr: 0.01000 - Train loss: 0.87981 - Test loss: 0.89876\n",
      "Epoch 4740 - lr: 0.01000 - Train loss: 0.93113 - Test loss: 0.90152\n",
      "Epoch 4741 - lr: 0.01000 - Train loss: 0.93744 - Test loss: 0.89822\n",
      "Epoch 4742 - lr: 0.01000 - Train loss: 0.92152 - Test loss: 0.92590\n",
      "Epoch 4743 - lr: 0.01000 - Train loss: 0.93468 - Test loss: 0.93104\n",
      "Epoch 4744 - lr: 0.01000 - Train loss: 0.92003 - Test loss: 0.92001\n",
      "Epoch 4745 - lr: 0.01000 - Train loss: 0.88238 - Test loss: 0.89601\n",
      "Epoch 4746 - lr: 0.01000 - Train loss: 0.93008 - Test loss: 0.92039\n",
      "Epoch 4747 - lr: 0.01000 - Train loss: 0.93888 - Test loss: 0.90811\n",
      "Epoch 4748 - lr: 0.01000 - Train loss: 0.91360 - Test loss: 0.91436\n",
      "Epoch 4749 - lr: 0.01000 - Train loss: 0.83887 - Test loss: 0.89463\n",
      "Epoch 4750 - lr: 0.01000 - Train loss: 0.84038 - Test loss: 0.88976\n",
      "Epoch 4751 - lr: 0.01000 - Train loss: 0.93015 - Test loss: 0.88723\n",
      "Epoch 4752 - lr: 0.01000 - Train loss: 0.90772 - Test loss: 0.89050\n",
      "Epoch 4753 - lr: 0.01000 - Train loss: 0.97074 - Test loss: 0.90308\n",
      "Epoch 4754 - lr: 0.01000 - Train loss: 0.94801 - Test loss: 0.89575\n",
      "Epoch 4755 - lr: 0.01000 - Train loss: 0.98153 - Test loss: 0.91760\n",
      "Epoch 4756 - lr: 0.01000 - Train loss: 0.93416 - Test loss: 0.91518\n",
      "Epoch 4757 - lr: 0.01000 - Train loss: 0.92177 - Test loss: 0.92082\n",
      "Epoch 4758 - lr: 0.01000 - Train loss: 0.84794 - Test loss: 0.89578\n",
      "Epoch 4759 - lr: 0.01000 - Train loss: 0.93604 - Test loss: 0.90229\n",
      "Epoch 4760 - lr: 0.01000 - Train loss: 0.95283 - Test loss: 0.89333\n",
      "Epoch 4761 - lr: 0.01000 - Train loss: 0.92862 - Test loss: 0.92230\n",
      "Epoch 4762 - lr: 0.01000 - Train loss: 0.94575 - Test loss: 0.91350\n",
      "Epoch 4763 - lr: 0.01000 - Train loss: 0.95388 - Test loss: 0.91031\n",
      "Epoch 4764 - lr: 0.01000 - Train loss: 0.92476 - Test loss: 0.92074\n",
      "Epoch 4765 - lr: 0.01000 - Train loss: 0.84132 - Test loss: 0.89613\n",
      "Epoch 4766 - lr: 0.01000 - Train loss: 0.90158 - Test loss: 0.90258\n",
      "Epoch 4767 - lr: 0.01000 - Train loss: 0.93072 - Test loss: 0.89107\n",
      "Epoch 4768 - lr: 0.01000 - Train loss: 0.99753 - Test loss: 0.89026\n",
      "Epoch 4769 - lr: 0.01000 - Train loss: 0.91335 - Test loss: 0.91372\n",
      "Epoch 4770 - lr: 0.01000 - Train loss: 0.92854 - Test loss: 0.91514\n",
      "Epoch 4771 - lr: 0.01000 - Train loss: 0.91708 - Test loss: 0.92891\n",
      "Epoch 4772 - lr: 0.01000 - Train loss: 0.92448 - Test loss: 0.93109\n",
      "Epoch 4773 - lr: 0.01000 - Train loss: 0.90862 - Test loss: 0.92867\n",
      "Epoch 4774 - lr: 0.01000 - Train loss: 0.92471 - Test loss: 0.92651\n",
      "Epoch 4775 - lr: 0.01000 - Train loss: 0.93738 - Test loss: 0.91020\n",
      "Epoch 4776 - lr: 0.01000 - Train loss: 0.94042 - Test loss: 0.90631\n",
      "Epoch 4777 - lr: 0.01000 - Train loss: 0.92685 - Test loss: 0.90684\n",
      "Epoch 4778 - lr: 0.01000 - Train loss: 0.94091 - Test loss: 0.89872\n",
      "Epoch 4779 - lr: 0.01000 - Train loss: 0.98940 - Test loss: 0.89199\n",
      "Epoch 4780 - lr: 0.01000 - Train loss: 0.99170 - Test loss: 0.88582\n",
      "Epoch 4781 - lr: 0.01000 - Train loss: 0.92868 - Test loss: 0.91484\n",
      "Epoch 4782 - lr: 0.01000 - Train loss: 0.93125 - Test loss: 0.92460\n",
      "Epoch 4783 - lr: 0.01000 - Train loss: 0.92789 - Test loss: 0.91956\n",
      "Epoch 4784 - lr: 0.01000 - Train loss: 0.85760 - Test loss: 0.88968\n",
      "Epoch 4785 - lr: 0.01000 - Train loss: 0.95884 - Test loss: 0.89991\n",
      "Epoch 4786 - lr: 0.01000 - Train loss: 0.89722 - Test loss: 0.91837\n",
      "Epoch 4787 - lr: 0.01000 - Train loss: 0.92592 - Test loss: 0.91884\n",
      "Epoch 4788 - lr: 0.01000 - Train loss: 0.93013 - Test loss: 0.88672\n",
      "Epoch 4789 - lr: 0.01000 - Train loss: 0.87188 - Test loss: 0.88368\n",
      "Epoch 4790 - lr: 0.01000 - Train loss: 0.97288 - Test loss: 0.89712\n",
      "Epoch 4791 - lr: 0.01000 - Train loss: 0.95135 - Test loss: 0.89187\n",
      "Epoch 4792 - lr: 0.01000 - Train loss: 0.97314 - Test loss: 0.90682\n",
      "Epoch 4793 - lr: 0.01000 - Train loss: 0.92740 - Test loss: 0.88971\n",
      "Epoch 4794 - lr: 0.01000 - Train loss: 0.91262 - Test loss: 0.90856\n",
      "Epoch 4795 - lr: 0.01000 - Train loss: 0.92229 - Test loss: 0.89823\n",
      "Epoch 4796 - lr: 0.01000 - Train loss: 0.95992 - Test loss: 0.89151\n",
      "Epoch 4797 - lr: 0.01000 - Train loss: 0.97207 - Test loss: 0.90336\n",
      "Epoch 4798 - lr: 0.01000 - Train loss: 0.83723 - Test loss: 0.89575\n",
      "Epoch 4799 - lr: 0.01000 - Train loss: 0.84059 - Test loss: 0.89286\n",
      "Epoch 4800 - lr: 0.01000 - Train loss: 0.92919 - Test loss: 0.89140\n",
      "Epoch 4801 - lr: 0.01000 - Train loss: 0.87850 - Test loss: 0.89137\n",
      "Epoch 4802 - lr: 0.01000 - Train loss: 0.93395 - Test loss: 0.90231\n",
      "Epoch 4803 - lr: 0.01000 - Train loss: 0.99770 - Test loss: 0.89769\n",
      "Epoch 4804 - lr: 0.01000 - Train loss: 0.98322 - Test loss: 0.92416\n",
      "Epoch 4805 - lr: 0.01000 - Train loss: 0.92059 - Test loss: 0.92403\n",
      "Epoch 4806 - lr: 0.01000 - Train loss: 0.86140 - Test loss: 0.89953\n",
      "Epoch 4807 - lr: 0.01000 - Train loss: 0.96014 - Test loss: 0.91013\n",
      "Epoch 4808 - lr: 0.01000 - Train loss: 0.92455 - Test loss: 0.90536\n",
      "Epoch 4809 - lr: 0.01000 - Train loss: 0.96396 - Test loss: 0.89815\n",
      "Epoch 4810 - lr: 0.01000 - Train loss: 0.92818 - Test loss: 0.92258\n",
      "Epoch 4811 - lr: 0.01000 - Train loss: 0.85172 - Test loss: 0.90101\n",
      "Epoch 4812 - lr: 0.01000 - Train loss: 0.92208 - Test loss: 0.92787\n",
      "Epoch 4813 - lr: 0.01000 - Train loss: 0.93555 - Test loss: 0.93159\n",
      "Epoch 4814 - lr: 0.01000 - Train loss: 0.92490 - Test loss: 0.90747\n",
      "Epoch 4815 - lr: 0.01000 - Train loss: 0.93016 - Test loss: 0.90002\n",
      "Epoch 4816 - lr: 0.01000 - Train loss: 0.93330 - Test loss: 0.89662\n",
      "Epoch 4817 - lr: 0.01000 - Train loss: 0.99415 - Test loss: 0.89599\n",
      "Epoch 4818 - lr: 0.01000 - Train loss: 0.99507 - Test loss: 0.91046\n",
      "Epoch 4819 - lr: 0.01000 - Train loss: 0.91268 - Test loss: 0.93124\n",
      "Epoch 4820 - lr: 0.01000 - Train loss: 0.92871 - Test loss: 0.91420\n",
      "Epoch 4821 - lr: 0.01000 - Train loss: 0.97872 - Test loss: 0.92647\n",
      "Epoch 4822 - lr: 0.01000 - Train loss: 0.94554 - Test loss: 0.91991\n",
      "Epoch 4823 - lr: 0.01000 - Train loss: 0.94302 - Test loss: 0.91796\n",
      "Epoch 4824 - lr: 0.01000 - Train loss: 0.95019 - Test loss: 0.91602\n",
      "Epoch 4825 - lr: 0.01000 - Train loss: 0.92112 - Test loss: 0.92247\n",
      "Epoch 4826 - lr: 0.01000 - Train loss: 0.85573 - Test loss: 0.89871\n",
      "Epoch 4827 - lr: 0.01000 - Train loss: 0.92201 - Test loss: 0.91490\n",
      "Epoch 4828 - lr: 0.01000 - Train loss: 0.86648 - Test loss: 0.89854\n",
      "Epoch 4829 - lr: 0.01000 - Train loss: 0.96342 - Test loss: 0.90787\n",
      "Epoch 4830 - lr: 0.01000 - Train loss: 0.93579 - Test loss: 0.91858\n",
      "Epoch 4831 - lr: 0.01000 - Train loss: 0.84472 - Test loss: 0.89779\n",
      "Epoch 4832 - lr: 0.01000 - Train loss: 0.94100 - Test loss: 0.91801\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4833 - lr: 0.01000 - Train loss: 0.91015 - Test loss: 0.89775\n",
      "Epoch 4834 - lr: 0.01000 - Train loss: 0.90794 - Test loss: 0.92429\n",
      "Epoch 4835 - lr: 0.01000 - Train loss: 0.93593 - Test loss: 0.91504\n",
      "Epoch 4836 - lr: 0.01000 - Train loss: 0.94094 - Test loss: 0.91177\n",
      "Epoch 4837 - lr: 0.01000 - Train loss: 0.93955 - Test loss: 0.91078\n",
      "Epoch 4838 - lr: 0.01000 - Train loss: 0.94378 - Test loss: 0.90944\n",
      "Epoch 4839 - lr: 0.01000 - Train loss: 0.93741 - Test loss: 0.90925\n",
      "Epoch 4840 - lr: 0.01000 - Train loss: 0.94909 - Test loss: 0.90721\n",
      "Epoch 4841 - lr: 0.01000 - Train loss: 0.95135 - Test loss: 0.90609\n",
      "Epoch 4842 - lr: 0.01000 - Train loss: 0.94812 - Test loss: 0.90573\n",
      "Epoch 4843 - lr: 0.01000 - Train loss: 0.95221 - Test loss: 0.90496\n",
      "Epoch 4844 - lr: 0.01000 - Train loss: 0.94836 - Test loss: 0.90473\n",
      "Epoch 4845 - lr: 0.01000 - Train loss: 0.95288 - Test loss: 0.90399\n",
      "Epoch 4846 - lr: 0.01000 - Train loss: 0.94841 - Test loss: 0.90385\n",
      "Epoch 4847 - lr: 0.01000 - Train loss: 0.95288 - Test loss: 0.90319\n",
      "Epoch 4848 - lr: 0.01000 - Train loss: 0.95270 - Test loss: 0.90261\n",
      "Epoch 4849 - lr: 0.01000 - Train loss: 0.94726 - Test loss: 0.90280\n",
      "Epoch 4850 - lr: 0.01000 - Train loss: 0.94616 - Test loss: 0.90295\n",
      "Epoch 4851 - lr: 0.01000 - Train loss: 0.92451 - Test loss: 0.90593\n",
      "Epoch 4852 - lr: 0.01000 - Train loss: 0.90983 - Test loss: 0.88675\n",
      "Epoch 4853 - lr: 0.01000 - Train loss: 0.97163 - Test loss: 0.89630\n",
      "Epoch 4854 - lr: 0.01000 - Train loss: 0.86792 - Test loss: 0.88606\n",
      "Epoch 4855 - lr: 0.01000 - Train loss: 0.91419 - Test loss: 0.91493\n",
      "Epoch 4856 - lr: 0.01000 - Train loss: 0.93685 - Test loss: 0.89922\n",
      "Epoch 4857 - lr: 0.01000 - Train loss: 0.93802 - Test loss: 0.90798\n",
      "Epoch 4858 - lr: 0.01000 - Train loss: 0.87212 - Test loss: 0.88451\n",
      "Epoch 4859 - lr: 0.01000 - Train loss: 0.97362 - Test loss: 0.89527\n",
      "Epoch 4860 - lr: 0.01000 - Train loss: 0.85881 - Test loss: 0.88624\n",
      "Epoch 4861 - lr: 0.01000 - Train loss: 0.92133 - Test loss: 0.88424\n",
      "Epoch 4862 - lr: 0.01000 - Train loss: 0.93666 - Test loss: 0.89504\n",
      "Epoch 4863 - lr: 0.01000 - Train loss: 0.94237 - Test loss: 0.88664\n",
      "Epoch 4864 - lr: 0.01000 - Train loss: 0.97712 - Test loss: 0.89851\n",
      "Epoch 4865 - lr: 0.01000 - Train loss: 0.92974 - Test loss: 0.88848\n",
      "Epoch 4866 - lr: 0.01000 - Train loss: 1.00245 - Test loss: 0.88768\n",
      "Epoch 4867 - lr: 0.01000 - Train loss: 0.92049 - Test loss: 0.89219\n",
      "Epoch 4868 - lr: 0.01000 - Train loss: 0.92657 - Test loss: 0.90048\n",
      "Epoch 4869 - lr: 0.01000 - Train loss: 0.99828 - Test loss: 0.89456\n",
      "Epoch 4870 - lr: 0.01000 - Train loss: 0.99205 - Test loss: 0.90802\n",
      "Epoch 4871 - lr: 0.01000 - Train loss: 0.98903 - Test loss: 0.91325\n",
      "Epoch 4872 - lr: 0.01000 - Train loss: 0.84858 - Test loss: 0.90354\n",
      "Epoch 4873 - lr: 0.01000 - Train loss: 0.83989 - Test loss: 0.90125\n",
      "Epoch 4874 - lr: 0.01000 - Train loss: 0.83979 - Test loss: 0.90059\n",
      "Epoch 4875 - lr: 0.01000 - Train loss: 0.83949 - Test loss: 0.90063\n",
      "Epoch 4876 - lr: 0.01000 - Train loss: 0.84706 - Test loss: 0.89858\n",
      "Epoch 4877 - lr: 0.01000 - Train loss: 0.92866 - Test loss: 0.90565\n",
      "Epoch 4878 - lr: 0.01000 - Train loss: 0.99407 - Test loss: 0.89855\n",
      "Epoch 4879 - lr: 0.01000 - Train loss: 0.93656 - Test loss: 0.92935\n",
      "Epoch 4880 - lr: 0.01000 - Train loss: 0.92837 - Test loss: 0.92434\n",
      "Epoch 4881 - lr: 0.01000 - Train loss: 0.96083 - Test loss: 0.90627\n",
      "Epoch 4882 - lr: 0.01000 - Train loss: 0.99324 - Test loss: 0.89980\n",
      "Epoch 4883 - lr: 0.01000 - Train loss: 0.99836 - Test loss: 0.91298\n",
      "Epoch 4884 - lr: 0.01000 - Train loss: 1.01335 - Test loss: 0.91756\n",
      "Epoch 4885 - lr: 0.01000 - Train loss: 0.93168 - Test loss: 0.91891\n",
      "Epoch 4886 - lr: 0.01000 - Train loss: 0.93069 - Test loss: 0.93465\n",
      "Epoch 4887 - lr: 0.01000 - Train loss: 0.92709 - Test loss: 0.92788\n",
      "Epoch 4888 - lr: 0.01000 - Train loss: 0.90938 - Test loss: 0.90342\n",
      "Epoch 4889 - lr: 0.01000 - Train loss: 0.85793 - Test loss: 0.90292\n",
      "Epoch 4890 - lr: 0.01000 - Train loss: 0.90786 - Test loss: 0.92996\n",
      "Epoch 4891 - lr: 0.01000 - Train loss: 0.92665 - Test loss: 0.93585\n",
      "Epoch 4892 - lr: 0.01000 - Train loss: 0.94516 - Test loss: 0.92011\n",
      "Epoch 4893 - lr: 0.01000 - Train loss: 0.95478 - Test loss: 0.91520\n",
      "Epoch 4894 - lr: 0.01000 - Train loss: 0.91416 - Test loss: 0.93068\n",
      "Epoch 4895 - lr: 0.01000 - Train loss: 0.93400 - Test loss: 0.93222\n",
      "Epoch 4896 - lr: 0.01000 - Train loss: 0.94514 - Test loss: 0.91529\n",
      "Epoch 4897 - lr: 0.01000 - Train loss: 0.95774 - Test loss: 0.90999\n",
      "Epoch 4898 - lr: 0.01000 - Train loss: 0.90916 - Test loss: 0.92574\n",
      "Epoch 4899 - lr: 0.01000 - Train loss: 0.93155 - Test loss: 0.92782\n",
      "Epoch 4900 - lr: 0.01000 - Train loss: 0.92659 - Test loss: 0.91959\n",
      "Epoch 4901 - lr: 0.01000 - Train loss: 0.87878 - Test loss: 0.88946\n",
      "Epoch 4902 - lr: 0.01000 - Train loss: 0.93353 - Test loss: 0.90330\n",
      "Epoch 4903 - lr: 0.01000 - Train loss: 0.95887 - Test loss: 0.90357\n",
      "Epoch 4904 - lr: 0.01000 - Train loss: 0.88887 - Test loss: 0.91793\n",
      "Epoch 4905 - lr: 0.01000 - Train loss: 0.88979 - Test loss: 0.89174\n",
      "Epoch 4906 - lr: 0.01000 - Train loss: 0.93205 - Test loss: 0.90261\n",
      "Epoch 4907 - lr: 0.01000 - Train loss: 0.95407 - Test loss: 0.90246\n",
      "Epoch 4908 - lr: 0.01000 - Train loss: 0.92265 - Test loss: 0.90575\n",
      "Epoch 4909 - lr: 0.01000 - Train loss: 0.92472 - Test loss: 0.89362\n",
      "Epoch 4910 - lr: 0.01000 - Train loss: 1.00610 - Test loss: 0.88440\n",
      "Epoch 4911 - lr: 0.01000 - Train loss: 0.88955 - Test loss: 0.88497\n",
      "Epoch 4912 - lr: 0.01000 - Train loss: 0.88583 - Test loss: 0.89073\n",
      "Epoch 4913 - lr: 0.01000 - Train loss: 0.91759 - Test loss: 0.89689\n",
      "Epoch 4914 - lr: 0.01000 - Train loss: 1.00338 - Test loss: 0.88983\n",
      "Epoch 4915 - lr: 0.01000 - Train loss: 0.92868 - Test loss: 0.89791\n",
      "Epoch 4916 - lr: 0.01000 - Train loss: 0.99977 - Test loss: 0.89326\n",
      "Epoch 4917 - lr: 0.01000 - Train loss: 0.99894 - Test loss: 0.90700\n",
      "Epoch 4918 - lr: 0.01000 - Train loss: 0.96571 - Test loss: 0.92422\n",
      "Epoch 4919 - lr: 0.01000 - Train loss: 0.88807 - Test loss: 0.89953\n",
      "Epoch 4920 - lr: 0.01000 - Train loss: 0.92580 - Test loss: 0.89847\n",
      "Epoch 4921 - lr: 0.01000 - Train loss: 0.88725 - Test loss: 0.89780\n",
      "Epoch 4922 - lr: 0.01000 - Train loss: 0.85382 - Test loss: 0.90222\n",
      "Epoch 4923 - lr: 0.01000 - Train loss: 0.88363 - Test loss: 0.90159\n",
      "Epoch 4924 - lr: 0.01000 - Train loss: 0.91786 - Test loss: 0.90812\n",
      "Epoch 4925 - lr: 0.01000 - Train loss: 0.99272 - Test loss: 0.90101\n",
      "Epoch 4926 - lr: 0.01000 - Train loss: 1.00145 - Test loss: 0.91415\n",
      "Epoch 4927 - lr: 0.01000 - Train loss: 0.99789 - Test loss: 0.91996\n",
      "Epoch 4928 - lr: 0.01000 - Train loss: 0.88717 - Test loss: 0.90976\n",
      "Epoch 4929 - lr: 0.01000 - Train loss: 0.92798 - Test loss: 0.93520\n",
      "Epoch 4930 - lr: 0.01000 - Train loss: 0.95784 - Test loss: 0.92329\n",
      "Epoch 4931 - lr: 0.01000 - Train loss: 0.91756 - Test loss: 0.91360\n",
      "Epoch 4932 - lr: 0.01000 - Train loss: 0.98213 - Test loss: 0.90318\n",
      "Epoch 4933 - lr: 0.01000 - Train loss: 0.86096 - Test loss: 0.90502\n",
      "Epoch 4934 - lr: 0.01000 - Train loss: 0.93146 - Test loss: 0.92980\n",
      "Epoch 4935 - lr: 0.01000 - Train loss: 0.86234 - Test loss: 0.90725\n",
      "Epoch 4936 - lr: 0.01000 - Train loss: 0.91006 - Test loss: 0.93264\n",
      "Epoch 4937 - lr: 0.01000 - Train loss: 0.92519 - Test loss: 0.92474\n",
      "Epoch 4938 - lr: 0.01000 - Train loss: 0.91624 - Test loss: 0.93210\n",
      "Epoch 4939 - lr: 0.01000 - Train loss: 0.92329 - Test loss: 0.93556\n",
      "Epoch 4940 - lr: 0.01000 - Train loss: 0.92582 - Test loss: 0.92296\n",
      "Epoch 4941 - lr: 0.01000 - Train loss: 0.91589 - Test loss: 0.89723\n",
      "Epoch 4942 - lr: 0.01000 - Train loss: 0.85640 - Test loss: 0.89534\n",
      "Epoch 4943 - lr: 0.01000 - Train loss: 0.92760 - Test loss: 0.91239\n",
      "Epoch 4944 - lr: 0.01000 - Train loss: 0.93571 - Test loss: 0.90841\n",
      "Epoch 4945 - lr: 0.01000 - Train loss: 0.94950 - Test loss: 0.89733\n",
      "Epoch 4946 - lr: 0.01000 - Train loss: 0.93007 - Test loss: 0.92557\n",
      "Epoch 4947 - lr: 0.01000 - Train loss: 0.93338 - Test loss: 0.91777\n",
      "Epoch 4948 - lr: 0.01000 - Train loss: 0.92910 - Test loss: 0.91581\n",
      "Epoch 4949 - lr: 0.01000 - Train loss: 0.92348 - Test loss: 0.92857\n",
      "Epoch 4950 - lr: 0.01000 - Train loss: 0.92596 - Test loss: 0.91766\n",
      "Epoch 4951 - lr: 0.01000 - Train loss: 0.88257 - Test loss: 0.89739\n",
      "Epoch 4952 - lr: 0.01000 - Train loss: 0.92062 - Test loss: 0.89967\n",
      "Epoch 4953 - lr: 0.01000 - Train loss: 0.99395 - Test loss: 0.89155\n",
      "Epoch 4954 - lr: 0.01000 - Train loss: 0.94069 - Test loss: 0.89423\n",
      "Epoch 4955 - lr: 0.01000 - Train loss: 0.98899 - Test loss: 0.89437\n",
      "Epoch 4956 - lr: 0.01000 - Train loss: 0.84222 - Test loss: 0.89891\n",
      "Epoch 4957 - lr: 0.01000 - Train loss: 0.92688 - Test loss: 0.89823\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4958 - lr: 0.01000 - Train loss: 0.94007 - Test loss: 0.91693\n",
      "Epoch 4959 - lr: 0.01000 - Train loss: 0.95713 - Test loss: 0.90593\n",
      "Epoch 4960 - lr: 0.01000 - Train loss: 0.99185 - Test loss: 0.90105\n",
      "Epoch 4961 - lr: 0.01000 - Train loss: 0.99945 - Test loss: 0.91404\n",
      "Epoch 4962 - lr: 0.01000 - Train loss: 0.97980 - Test loss: 0.93569\n",
      "Epoch 4963 - lr: 0.01000 - Train loss: 0.92537 - Test loss: 0.92193\n",
      "Epoch 4964 - lr: 0.01000 - Train loss: 0.88760 - Test loss: 0.90840\n",
      "Epoch 4965 - lr: 0.01000 - Train loss: 0.91161 - Test loss: 0.93346\n",
      "Epoch 4966 - lr: 0.01000 - Train loss: 0.95000 - Test loss: 0.92207\n",
      "Epoch 4967 - lr: 0.01000 - Train loss: 0.92778 - Test loss: 0.93121\n",
      "Epoch 4968 - lr: 0.01000 - Train loss: 0.85836 - Test loss: 0.90396\n",
      "Epoch 4969 - lr: 0.01000 - Train loss: 0.93039 - Test loss: 0.90842\n",
      "Epoch 4970 - lr: 0.01000 - Train loss: 0.92610 - Test loss: 0.90383\n",
      "Epoch 4971 - lr: 0.01000 - Train loss: 0.92018 - Test loss: 0.89931\n",
      "Epoch 4972 - lr: 0.01000 - Train loss: 0.87100 - Test loss: 0.90074\n",
      "Epoch 4973 - lr: 0.01000 - Train loss: 0.93083 - Test loss: 0.92981\n",
      "Epoch 4974 - lr: 0.01000 - Train loss: 0.94029 - Test loss: 0.92154\n",
      "Epoch 4975 - lr: 0.01000 - Train loss: 0.95292 - Test loss: 0.91765\n",
      "Epoch 4976 - lr: 0.01000 - Train loss: 0.92245 - Test loss: 0.93278\n",
      "Epoch 4977 - lr: 0.01000 - Train loss: 0.92887 - Test loss: 0.92814\n",
      "Epoch 4978 - lr: 0.01000 - Train loss: 0.88659 - Test loss: 0.89774\n",
      "Epoch 4979 - lr: 0.01000 - Train loss: 0.93950 - Test loss: 0.92267\n",
      "Epoch 4980 - lr: 0.01000 - Train loss: 0.94575 - Test loss: 0.91596\n",
      "Epoch 4981 - lr: 0.01000 - Train loss: 0.95884 - Test loss: 0.91223\n",
      "Epoch 4982 - lr: 0.01000 - Train loss: 0.92233 - Test loss: 0.90540\n",
      "Epoch 4983 - lr: 0.01000 - Train loss: 0.94163 - Test loss: 0.89733\n",
      "Epoch 4984 - lr: 0.01000 - Train loss: 0.93013 - Test loss: 0.90347\n",
      "Epoch 4985 - lr: 0.01000 - Train loss: 0.98449 - Test loss: 0.89665\n",
      "Epoch 4986 - lr: 0.01000 - Train loss: 0.92309 - Test loss: 0.89724\n",
      "Epoch 4987 - lr: 0.01000 - Train loss: 0.88152 - Test loss: 0.90286\n",
      "Epoch 4988 - lr: 0.01000 - Train loss: 0.91635 - Test loss: 0.90156\n",
      "Epoch 4989 - lr: 0.01000 - Train loss: 0.96861 - Test loss: 0.90588\n",
      "Epoch 4990 - lr: 0.01000 - Train loss: 0.98958 - Test loss: 0.90263\n",
      "Epoch 4991 - lr: 0.01000 - Train loss: 1.00220 - Test loss: 0.91634\n",
      "Epoch 4992 - lr: 0.01000 - Train loss: 0.99366 - Test loss: 0.92187\n",
      "Epoch 4993 - lr: 0.01000 - Train loss: 0.91705 - Test loss: 0.90818\n",
      "Epoch 4994 - lr: 0.01000 - Train loss: 0.92511 - Test loss: 0.90679\n",
      "Epoch 4995 - lr: 0.01000 - Train loss: 0.99237 - Test loss: 0.91951\n",
      "Epoch 4996 - lr: 0.01000 - Train loss: 0.94296 - Test loss: 0.92835\n",
      "Epoch 4997 - lr: 0.01000 - Train loss: 0.90561 - Test loss: 0.93643\n",
      "Epoch 4998 - lr: 0.01000 - Train loss: 0.86289 - Test loss: 0.91025\n",
      "Epoch 4999 - lr: 0.01000 - Train loss: 0.92883 - Test loss: 0.93124\n",
      "Epoch 5000 - lr: 0.01000 - Train loss: 0.88128 - Test loss: 0.90698\n",
      "Epoch 5001 - lr: 0.01000 - Train loss: 0.93720 - Test loss: 0.93218\n",
      "Epoch 5002 - lr: 0.01000 - Train loss: 0.94735 - Test loss: 0.92475\n",
      "Epoch 5003 - lr: 0.01000 - Train loss: 0.92901 - Test loss: 0.92403\n",
      "Epoch 5004 - lr: 0.01000 - Train loss: 0.92591 - Test loss: 0.91658\n",
      "Epoch 5005 - lr: 0.01000 - Train loss: 0.95932 - Test loss: 0.91994\n",
      "Epoch 5006 - lr: 0.01000 - Train loss: 0.92797 - Test loss: 0.93334\n",
      "Epoch 5007 - lr: 0.01000 - Train loss: 0.92133 - Test loss: 0.92393\n",
      "Epoch 5008 - lr: 0.01000 - Train loss: 0.89887 - Test loss: 0.89927\n",
      "Epoch 5009 - lr: 0.01000 - Train loss: 0.93111 - Test loss: 0.90541\n",
      "Epoch 5010 - lr: 0.01000 - Train loss: 0.95374 - Test loss: 0.89957\n",
      "Epoch 5011 - lr: 0.01000 - Train loss: 0.93857 - Test loss: 0.91452\n",
      "Epoch 5012 - lr: 0.01000 - Train loss: 0.94693 - Test loss: 0.91645\n",
      "Epoch 5013 - lr: 0.01000 - Train loss: 0.93192 - Test loss: 0.91772\n",
      "Epoch 5014 - lr: 0.01000 - Train loss: 0.92761 - Test loss: 0.92782\n",
      "Epoch 5015 - lr: 0.01000 - Train loss: 0.90241 - Test loss: 0.89871\n",
      "Epoch 5016 - lr: 0.01000 - Train loss: 0.92222 - Test loss: 0.92358\n",
      "Epoch 5017 - lr: 0.01000 - Train loss: 0.94953 - Test loss: 0.91529\n",
      "Epoch 5018 - lr: 0.01000 - Train loss: 0.92862 - Test loss: 0.91513\n",
      "Epoch 5019 - lr: 0.01000 - Train loss: 0.91701 - Test loss: 0.92866\n",
      "Epoch 5020 - lr: 0.01000 - Train loss: 0.92631 - Test loss: 0.93076\n",
      "Epoch 5021 - lr: 0.01000 - Train loss: 0.91394 - Test loss: 0.92927\n",
      "Epoch 5022 - lr: 0.01000 - Train loss: 0.92505 - Test loss: 0.91447\n",
      "Epoch 5023 - lr: 0.01000 - Train loss: 0.90711 - Test loss: 0.89150\n",
      "Epoch 5024 - lr: 0.01000 - Train loss: 0.96652 - Test loss: 0.90074\n",
      "Epoch 5025 - lr: 0.01000 - Train loss: 0.87468 - Test loss: 0.89408\n",
      "Epoch 5026 - lr: 0.01000 - Train loss: 0.91827 - Test loss: 0.88829\n",
      "Epoch 5027 - lr: 0.01000 - Train loss: 0.98551 - Test loss: 0.91697\n",
      "Epoch 5028 - lr: 0.01000 - Train loss: 0.92635 - Test loss: 0.92463\n",
      "Epoch 5029 - lr: 0.01000 - Train loss: 0.91957 - Test loss: 0.91729\n",
      "Epoch 5030 - lr: 0.01000 - Train loss: 0.87777 - Test loss: 0.88934\n",
      "Epoch 5031 - lr: 0.01000 - Train loss: 0.93019 - Test loss: 0.90372\n",
      "Epoch 5032 - lr: 0.01000 - Train loss: 0.92656 - Test loss: 0.91736\n",
      "Epoch 5033 - lr: 0.01000 - Train loss: 0.92380 - Test loss: 0.88864\n",
      "Epoch 5034 - lr: 0.01000 - Train loss: 0.88810 - Test loss: 0.88544\n",
      "Epoch 5035 - lr: 0.01000 - Train loss: 0.96198 - Test loss: 0.89028\n",
      "Epoch 5036 - lr: 0.01000 - Train loss: 1.00159 - Test loss: 0.88938\n",
      "Epoch 5037 - lr: 0.01000 - Train loss: 0.93987 - Test loss: 0.90103\n",
      "Epoch 5038 - lr: 0.01000 - Train loss: 0.93245 - Test loss: 0.89866\n",
      "Epoch 5039 - lr: 0.01000 - Train loss: 0.89719 - Test loss: 0.89616\n",
      "Epoch 5040 - lr: 0.01000 - Train loss: 0.91729 - Test loss: 0.90386\n",
      "Epoch 5041 - lr: 0.01000 - Train loss: 0.98895 - Test loss: 0.89783\n",
      "Epoch 5042 - lr: 0.01000 - Train loss: 0.89001 - Test loss: 0.89812\n",
      "Epoch 5043 - lr: 0.01000 - Train loss: 0.88561 - Test loss: 0.89942\n",
      "Epoch 5044 - lr: 0.01000 - Train loss: 0.85373 - Test loss: 0.90417\n",
      "Epoch 5045 - lr: 0.01000 - Train loss: 0.89560 - Test loss: 0.90336\n",
      "Epoch 5046 - lr: 0.01000 - Train loss: 0.91686 - Test loss: 0.90230\n",
      "Epoch 5047 - lr: 0.01000 - Train loss: 0.83990 - Test loss: 0.90517\n",
      "Epoch 5048 - lr: 0.01000 - Train loss: 0.84993 - Test loss: 0.90648\n",
      "Epoch 5049 - lr: 0.01000 - Train loss: 0.87333 - Test loss: 0.90457\n",
      "Epoch 5050 - lr: 0.01000 - Train loss: 0.93105 - Test loss: 0.92958\n",
      "Epoch 5051 - lr: 0.01000 - Train loss: 0.85256 - Test loss: 0.90805\n",
      "Epoch 5052 - lr: 0.01000 - Train loss: 0.87375 - Test loss: 0.90853\n",
      "Epoch 5053 - lr: 0.01000 - Train loss: 0.85942 - Test loss: 0.90406\n",
      "Epoch 5054 - lr: 0.01000 - Train loss: 0.89952 - Test loss: 0.91784\n",
      "Epoch 5055 - lr: 0.01000 - Train loss: 0.92676 - Test loss: 0.91262\n",
      "Epoch 5056 - lr: 0.01000 - Train loss: 0.96391 - Test loss: 0.90471\n",
      "Epoch 5057 - lr: 0.01000 - Train loss: 0.93699 - Test loss: 0.91150\n",
      "Epoch 5058 - lr: 0.01000 - Train loss: 0.94877 - Test loss: 0.90654\n",
      "Epoch 5059 - lr: 0.01000 - Train loss: 0.91148 - Test loss: 0.93285\n",
      "Epoch 5060 - lr: 0.01000 - Train loss: 0.93153 - Test loss: 0.92404\n",
      "Epoch 5061 - lr: 0.01000 - Train loss: 0.92527 - Test loss: 0.93513\n",
      "Epoch 5062 - lr: 0.01000 - Train loss: 0.94096 - Test loss: 0.92101\n",
      "Epoch 5063 - lr: 0.01000 - Train loss: 0.94591 - Test loss: 0.91665\n",
      "Epoch 5064 - lr: 0.01000 - Train loss: 0.94220 - Test loss: 0.91540\n",
      "Epoch 5065 - lr: 0.01000 - Train loss: 0.95264 - Test loss: 0.91338\n",
      "Epoch 5066 - lr: 0.01000 - Train loss: 0.92873 - Test loss: 0.92723\n",
      "Epoch 5067 - lr: 0.01000 - Train loss: 0.92841 - Test loss: 0.90883\n",
      "Epoch 5068 - lr: 0.01000 - Train loss: 0.91315 - Test loss: 0.89896\n",
      "Epoch 5069 - lr: 0.01000 - Train loss: 0.89248 - Test loss: 0.89389\n",
      "Epoch 5070 - lr: 0.01000 - Train loss: 0.93114 - Test loss: 0.92225\n",
      "Epoch 5071 - lr: 0.01000 - Train loss: 0.89594 - Test loss: 0.92861\n",
      "Epoch 5072 - lr: 0.01000 - Train loss: 0.93625 - Test loss: 0.91033\n",
      "Epoch 5073 - lr: 0.01000 - Train loss: 0.95360 - Test loss: 0.92613\n",
      "Epoch 5074 - lr: 0.01000 - Train loss: 0.93087 - Test loss: 0.92828\n",
      "Epoch 5075 - lr: 0.01000 - Train loss: 0.92923 - Test loss: 0.92202\n",
      "Epoch 5076 - lr: 0.01000 - Train loss: 0.85951 - Test loss: 0.89125\n",
      "Epoch 5077 - lr: 0.01000 - Train loss: 0.94317 - Test loss: 0.90287\n",
      "Epoch 5078 - lr: 0.01000 - Train loss: 0.95331 - Test loss: 0.90444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5079 - lr: 0.01000 - Train loss: 0.93981 - Test loss: 0.90553\n",
      "Epoch 5080 - lr: 0.01000 - Train loss: 0.94675 - Test loss: 0.90494\n",
      "Epoch 5081 - lr: 0.01000 - Train loss: 0.94900 - Test loss: 0.90414\n",
      "Epoch 5082 - lr: 0.01000 - Train loss: 0.95371 - Test loss: 0.90305\n",
      "Epoch 5083 - lr: 0.01000 - Train loss: 0.94622 - Test loss: 0.90307\n",
      "Epoch 5084 - lr: 0.01000 - Train loss: 0.95280 - Test loss: 0.90235\n",
      "Epoch 5085 - lr: 0.01000 - Train loss: 0.95476 - Test loss: 0.90153\n",
      "Epoch 5086 - lr: 0.01000 - Train loss: 0.92563 - Test loss: 0.90394\n",
      "Epoch 5087 - lr: 0.01000 - Train loss: 0.89456 - Test loss: 0.91695\n",
      "Epoch 5088 - lr: 0.01000 - Train loss: 0.92201 - Test loss: 0.91336\n",
      "Epoch 5089 - lr: 0.01000 - Train loss: 0.88893 - Test loss: 0.88269\n",
      "Epoch 5090 - lr: 0.01000 - Train loss: 0.89990 - Test loss: 0.91081\n",
      "Epoch 5091 - lr: 0.01000 - Train loss: 0.94058 - Test loss: 0.90215\n",
      "Epoch 5092 - lr: 0.01000 - Train loss: 0.92579 - Test loss: 0.90221\n",
      "Epoch 5093 - lr: 0.01000 - Train loss: 0.91826 - Test loss: 0.91210\n",
      "Epoch 5094 - lr: 0.01000 - Train loss: 0.93248 - Test loss: 0.91576\n",
      "Epoch 5095 - lr: 0.01000 - Train loss: 0.93090 - Test loss: 0.90151\n",
      "Epoch 5096 - lr: 0.01000 - Train loss: 0.91860 - Test loss: 0.90078\n",
      "Epoch 5097 - lr: 0.01000 - Train loss: 0.90250 - Test loss: 0.87829\n",
      "Epoch 5098 - lr: 0.01000 - Train loss: 0.96719 - Test loss: 0.88989\n",
      "Epoch 5099 - lr: 0.01000 - Train loss: 0.92762 - Test loss: 0.88599\n",
      "Epoch 5100 - lr: 0.01000 - Train loss: 0.99786 - Test loss: 0.87879\n",
      "Epoch 5101 - lr: 0.01000 - Train loss: 0.91602 - Test loss: 0.88740\n",
      "Epoch 5102 - lr: 0.01000 - Train loss: 1.00173 - Test loss: 0.88190\n",
      "Epoch 5103 - lr: 0.01000 - Train loss: 0.93550 - Test loss: 0.88376\n",
      "Epoch 5104 - lr: 0.01000 - Train loss: 0.95912 - Test loss: 0.89015\n",
      "Epoch 5105 - lr: 0.01000 - Train loss: 1.02082 - Test loss: 0.90107\n",
      "Epoch 5106 - lr: 0.01000 - Train loss: 0.89331 - Test loss: 0.88939\n",
      "Epoch 5107 - lr: 0.01000 - Train loss: 0.92315 - Test loss: 0.89068\n",
      "Epoch 5108 - lr: 0.01000 - Train loss: 0.97784 - Test loss: 0.92125\n",
      "Epoch 5109 - lr: 0.01000 - Train loss: 0.93357 - Test loss: 0.92567\n",
      "Epoch 5110 - lr: 0.01000 - Train loss: 0.92322 - Test loss: 0.89423\n",
      "Epoch 5111 - lr: 0.01000 - Train loss: 0.85365 - Test loss: 0.89184\n",
      "Epoch 5112 - lr: 0.01000 - Train loss: 0.93039 - Test loss: 0.91864\n",
      "Epoch 5113 - lr: 0.01000 - Train loss: 0.92766 - Test loss: 0.89487\n",
      "Epoch 5114 - lr: 0.01000 - Train loss: 0.85953 - Test loss: 0.89261\n",
      "Epoch 5115 - lr: 0.01000 - Train loss: 0.96381 - Test loss: 0.90615\n",
      "Epoch 5116 - lr: 0.01000 - Train loss: 0.97216 - Test loss: 0.90548\n",
      "Epoch 5117 - lr: 0.01000 - Train loss: 0.87548 - Test loss: 0.89626\n",
      "Epoch 5118 - lr: 0.01000 - Train loss: 0.95578 - Test loss: 0.90874\n",
      "Epoch 5119 - lr: 0.01000 - Train loss: 0.90105 - Test loss: 0.92701\n",
      "Epoch 5120 - lr: 0.01000 - Train loss: 0.92293 - Test loss: 0.93061\n",
      "Epoch 5121 - lr: 0.01000 - Train loss: 0.92556 - Test loss: 0.91769\n",
      "Epoch 5122 - lr: 0.01000 - Train loss: 0.89925 - Test loss: 0.89565\n",
      "Epoch 5123 - lr: 0.01000 - Train loss: 0.96860 - Test loss: 0.90414\n",
      "Epoch 5124 - lr: 0.01000 - Train loss: 0.92050 - Test loss: 0.89139\n",
      "Epoch 5125 - lr: 0.01000 - Train loss: 0.89506 - Test loss: 0.88984\n",
      "Epoch 5126 - lr: 0.01000 - Train loss: 0.89557 - Test loss: 0.89085\n",
      "Epoch 5127 - lr: 0.01000 - Train loss: 0.84637 - Test loss: 0.89419\n",
      "Epoch 5128 - lr: 0.01000 - Train loss: 0.93184 - Test loss: 0.90332\n",
      "Epoch 5129 - lr: 0.01000 - Train loss: 0.94489 - Test loss: 0.89877\n",
      "Epoch 5130 - lr: 0.01000 - Train loss: 0.92051 - Test loss: 0.92768\n",
      "Epoch 5131 - lr: 0.01000 - Train loss: 0.93575 - Test loss: 0.93157\n",
      "Epoch 5132 - lr: 0.01000 - Train loss: 0.93986 - Test loss: 0.91444\n",
      "Epoch 5133 - lr: 0.01000 - Train loss: 0.92046 - Test loss: 0.91806\n",
      "Epoch 5134 - lr: 0.01000 - Train loss: 0.85946 - Test loss: 0.89778\n",
      "Epoch 5135 - lr: 0.01000 - Train loss: 0.93314 - Test loss: 0.90291\n",
      "Epoch 5136 - lr: 0.01000 - Train loss: 1.02588 - Test loss: 0.90268\n",
      "Epoch 5137 - lr: 0.01000 - Train loss: 0.85551 - Test loss: 0.89658\n",
      "Epoch 5138 - lr: 0.01000 - Train loss: 0.92376 - Test loss: 0.91805\n",
      "Epoch 5139 - lr: 0.01000 - Train loss: 0.87449 - Test loss: 0.89744\n",
      "Epoch 5140 - lr: 0.01000 - Train loss: 0.92701 - Test loss: 0.92542\n",
      "Epoch 5141 - lr: 0.01000 - Train loss: 0.94311 - Test loss: 0.91635\n",
      "Epoch 5142 - lr: 0.01000 - Train loss: 0.95113 - Test loss: 0.91277\n",
      "Epoch 5143 - lr: 0.01000 - Train loss: 0.92025 - Test loss: 0.91896\n",
      "Epoch 5144 - lr: 0.01000 - Train loss: 0.88254 - Test loss: 0.89401\n",
      "Epoch 5145 - lr: 0.01000 - Train loss: 0.92869 - Test loss: 0.90147\n",
      "Epoch 5146 - lr: 0.01000 - Train loss: 1.00224 - Test loss: 0.89589\n",
      "Epoch 5147 - lr: 0.01000 - Train loss: 0.95304 - Test loss: 0.89593\n",
      "Epoch 5148 - lr: 0.01000 - Train loss: 0.93644 - Test loss: 0.91236\n",
      "Epoch 5149 - lr: 0.01000 - Train loss: 0.92505 - Test loss: 0.91726\n",
      "Epoch 5150 - lr: 0.01000 - Train loss: 0.92537 - Test loss: 0.90615\n",
      "Epoch 5151 - lr: 0.01000 - Train loss: 0.94916 - Test loss: 0.89858\n",
      "Epoch 5152 - lr: 0.01000 - Train loss: 0.93073 - Test loss: 0.92414\n",
      "Epoch 5153 - lr: 0.01000 - Train loss: 0.92464 - Test loss: 0.89993\n",
      "Epoch 5154 - lr: 0.01000 - Train loss: 0.87695 - Test loss: 0.89654\n",
      "Epoch 5155 - lr: 0.01000 - Train loss: 0.92942 - Test loss: 0.90619\n",
      "Epoch 5156 - lr: 0.01000 - Train loss: 0.94002 - Test loss: 0.90176\n",
      "Epoch 5157 - lr: 0.01000 - Train loss: 0.92486 - Test loss: 0.90949\n",
      "Epoch 5158 - lr: 0.01000 - Train loss: 0.98154 - Test loss: 0.90362\n",
      "Epoch 5159 - lr: 0.01000 - Train loss: 0.98976 - Test loss: 0.90269\n",
      "Epoch 5160 - lr: 0.01000 - Train loss: 0.92222 - Test loss: 0.90452\n",
      "Epoch 5161 - lr: 0.01000 - Train loss: 0.85059 - Test loss: 0.90772\n",
      "Epoch 5162 - lr: 0.01000 - Train loss: 0.88332 - Test loss: 0.90590\n",
      "Epoch 5163 - lr: 0.01000 - Train loss: 0.93241 - Test loss: 0.93423\n",
      "Epoch 5164 - lr: 0.01000 - Train loss: 0.92913 - Test loss: 0.94039\n",
      "Epoch 5165 - lr: 0.01000 - Train loss: 0.92453 - Test loss: 0.94101\n",
      "Epoch 5166 - lr: 0.01000 - Train loss: 0.93452 - Test loss: 0.92512\n",
      "Epoch 5167 - lr: 0.01000 - Train loss: 0.92756 - Test loss: 0.93443\n",
      "Epoch 5168 - lr: 0.01000 - Train loss: 0.92722 - Test loss: 0.93234\n",
      "Epoch 5169 - lr: 0.01000 - Train loss: 0.90767 - Test loss: 0.90007\n",
      "Epoch 5170 - lr: 0.01000 - Train loss: 0.92628 - Test loss: 0.89702\n",
      "Epoch 5171 - lr: 0.01000 - Train loss: 0.83758 - Test loss: 0.89967\n",
      "Epoch 5172 - lr: 0.01000 - Train loss: 0.84502 - Test loss: 0.89969\n",
      "Epoch 5173 - lr: 0.01000 - Train loss: 0.89889 - Test loss: 0.91350\n",
      "Epoch 5174 - lr: 0.01000 - Train loss: 0.92386 - Test loss: 0.91152\n",
      "Epoch 5175 - lr: 0.01000 - Train loss: 0.87429 - Test loss: 0.90350\n",
      "Epoch 5176 - lr: 0.01000 - Train loss: 0.83969 - Test loss: 0.89892\n",
      "Epoch 5177 - lr: 0.01000 - Train loss: 0.85468 - Test loss: 0.89749\n",
      "Epoch 5178 - lr: 0.01000 - Train loss: 0.93064 - Test loss: 0.90376\n",
      "Epoch 5179 - lr: 0.01000 - Train loss: 0.92713 - Test loss: 0.90060\n",
      "Epoch 5180 - lr: 0.01000 - Train loss: 0.91008 - Test loss: 0.89657\n",
      "Epoch 5181 - lr: 0.01000 - Train loss: 0.87329 - Test loss: 0.89810\n",
      "Epoch 5182 - lr: 0.01000 - Train loss: 0.92945 - Test loss: 0.92364\n",
      "Epoch 5183 - lr: 0.01000 - Train loss: 0.84501 - Test loss: 0.90383\n",
      "Epoch 5184 - lr: 0.01000 - Train loss: 0.84166 - Test loss: 0.89993\n",
      "Epoch 5185 - lr: 0.01000 - Train loss: 0.92673 - Test loss: 0.89672\n",
      "Epoch 5186 - lr: 0.01000 - Train loss: 0.91983 - Test loss: 0.89678\n",
      "Epoch 5187 - lr: 0.01000 - Train loss: 0.84132 - Test loss: 0.89972\n",
      "Epoch 5188 - lr: 0.01000 - Train loss: 0.89947 - Test loss: 0.90163\n",
      "Epoch 5189 - lr: 0.01000 - Train loss: 0.92445 - Test loss: 0.92262\n",
      "Epoch 5190 - lr: 0.01000 - Train loss: 0.86508 - Test loss: 0.90218\n",
      "Epoch 5191 - lr: 0.01000 - Train loss: 0.92288 - Test loss: 0.92088\n",
      "Epoch 5192 - lr: 0.01000 - Train loss: 0.87686 - Test loss: 0.90047\n",
      "Epoch 5193 - lr: 0.01000 - Train loss: 0.91793 - Test loss: 0.90626\n",
      "Epoch 5194 - lr: 0.01000 - Train loss: 0.97087 - Test loss: 0.90042\n",
      "Epoch 5195 - lr: 0.01000 - Train loss: 0.92318 - Test loss: 0.90739\n",
      "Epoch 5196 - lr: 0.01000 - Train loss: 0.97989 - Test loss: 0.90246\n",
      "Epoch 5197 - lr: 0.01000 - Train loss: 0.87885 - Test loss: 0.90456\n",
      "Epoch 5198 - lr: 0.01000 - Train loss: 0.92385 - Test loss: 0.91193\n",
      "Epoch 5199 - lr: 0.01000 - Train loss: 0.96714 - Test loss: 0.90618\n",
      "Epoch 5200 - lr: 0.01000 - Train loss: 0.94112 - Test loss: 0.93257\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5201 - lr: 0.01000 - Train loss: 0.92874 - Test loss: 0.93866\n",
      "Epoch 5202 - lr: 0.01000 - Train loss: 0.88417 - Test loss: 0.90924\n",
      "Epoch 5203 - lr: 0.01000 - Train loss: 0.92577 - Test loss: 0.93502\n",
      "Epoch 5204 - lr: 0.01000 - Train loss: 0.92844 - Test loss: 0.92693\n",
      "Epoch 5205 - lr: 0.01000 - Train loss: 0.92909 - Test loss: 0.91364\n",
      "Epoch 5206 - lr: 0.01000 - Train loss: 0.97337 - Test loss: 0.92456\n",
      "Epoch 5207 - lr: 0.01000 - Train loss: 0.86210 - Test loss: 0.90585\n",
      "Epoch 5208 - lr: 0.01000 - Train loss: 0.91829 - Test loss: 0.90959\n",
      "Epoch 5209 - lr: 0.01000 - Train loss: 0.92312 - Test loss: 0.90636\n",
      "Epoch 5210 - lr: 0.01000 - Train loss: 0.85990 - Test loss: 0.90403\n",
      "Epoch 5211 - lr: 0.01000 - Train loss: 0.91010 - Test loss: 0.92730\n",
      "Epoch 5212 - lr: 0.01000 - Train loss: 0.91296 - Test loss: 0.90394\n",
      "Epoch 5213 - lr: 0.01000 - Train loss: 0.84731 - Test loss: 0.90228\n",
      "Epoch 5214 - lr: 0.01000 - Train loss: 0.91970 - Test loss: 0.90927\n",
      "Epoch 5215 - lr: 0.01000 - Train loss: 0.94504 - Test loss: 0.90581\n",
      "Epoch 5216 - lr: 0.01000 - Train loss: 0.93071 - Test loss: 0.93006\n",
      "Epoch 5217 - lr: 0.01000 - Train loss: 0.91616 - Test loss: 0.90505\n",
      "Epoch 5218 - lr: 0.01000 - Train loss: 0.84901 - Test loss: 0.90295\n",
      "Epoch 5219 - lr: 0.01000 - Train loss: 0.91061 - Test loss: 0.93126\n",
      "Epoch 5220 - lr: 0.01000 - Train loss: 0.92818 - Test loss: 0.93666\n",
      "Epoch 5221 - lr: 0.01000 - Train loss: 0.91990 - Test loss: 0.91450\n",
      "Epoch 5222 - lr: 0.01000 - Train loss: 0.96189 - Test loss: 0.90037\n",
      "Epoch 5223 - lr: 0.01000 - Train loss: 0.92956 - Test loss: 0.90026\n",
      "Epoch 5224 - lr: 0.01000 - Train loss: 0.97952 - Test loss: 0.90147\n",
      "Epoch 5225 - lr: 0.01000 - Train loss: 0.84742 - Test loss: 0.90431\n",
      "Epoch 5226 - lr: 0.01000 - Train loss: 0.92473 - Test loss: 0.91318\n",
      "Epoch 5227 - lr: 0.01000 - Train loss: 0.97042 - Test loss: 0.90807\n",
      "Epoch 5228 - lr: 0.01000 - Train loss: 0.96326 - Test loss: 0.92168\n",
      "Epoch 5229 - lr: 0.01000 - Train loss: 0.92732 - Test loss: 0.93449\n",
      "Epoch 5230 - lr: 0.01000 - Train loss: 0.84459 - Test loss: 0.90961\n",
      "Epoch 5231 - lr: 0.01000 - Train loss: 0.87907 - Test loss: 0.90770\n",
      "Epoch 5232 - lr: 0.01000 - Train loss: 0.90938 - Test loss: 0.91827\n",
      "Epoch 5233 - lr: 0.01000 - Train loss: 0.92274 - Test loss: 0.92785\n",
      "Epoch 5234 - lr: 0.01000 - Train loss: 0.84585 - Test loss: 0.90507\n",
      "Epoch 5235 - lr: 0.01000 - Train loss: 0.92974 - Test loss: 0.90843\n",
      "Epoch 5236 - lr: 0.01000 - Train loss: 0.97711 - Test loss: 0.92543\n",
      "Epoch 5237 - lr: 0.01000 - Train loss: 0.90050 - Test loss: 0.93505\n",
      "Epoch 5238 - lr: 0.01000 - Train loss: 0.92676 - Test loss: 0.91333\n",
      "Epoch 5239 - lr: 0.01000 - Train loss: 0.97542 - Test loss: 0.92546\n",
      "Epoch 5240 - lr: 0.01000 - Train loss: 0.92696 - Test loss: 0.93393\n",
      "Epoch 5241 - lr: 0.01000 - Train loss: 0.90289 - Test loss: 0.93483\n",
      "Epoch 5242 - lr: 0.01000 - Train loss: 0.92687 - Test loss: 0.93250\n",
      "Epoch 5243 - lr: 0.01000 - Train loss: 0.92585 - Test loss: 0.90923\n",
      "Epoch 5244 - lr: 0.01000 - Train loss: 0.96707 - Test loss: 0.89452\n",
      "Epoch 5245 - lr: 0.01000 - Train loss: 0.91028 - Test loss: 0.91569\n",
      "Epoch 5246 - lr: 0.01000 - Train loss: 0.91906 - Test loss: 0.92988\n",
      "Epoch 5247 - lr: 0.01000 - Train loss: 0.93135 - Test loss: 0.92967\n",
      "Epoch 5248 - lr: 0.01000 - Train loss: 0.92071 - Test loss: 0.92765\n",
      "Epoch 5249 - lr: 0.01000 - Train loss: 0.92982 - Test loss: 0.92656\n",
      "Epoch 5250 - lr: 0.01000 - Train loss: 0.87034 - Test loss: 0.89978\n",
      "Epoch 5251 - lr: 0.01000 - Train loss: 0.90682 - Test loss: 0.91260\n",
      "Epoch 5252 - lr: 0.01000 - Train loss: 0.92327 - Test loss: 0.92380\n",
      "Epoch 5253 - lr: 0.01000 - Train loss: 0.92585 - Test loss: 0.92328\n",
      "Epoch 5254 - lr: 0.01000 - Train loss: 0.94008 - Test loss: 0.89398\n",
      "Epoch 5255 - lr: 0.01000 - Train loss: 0.92753 - Test loss: 0.91826\n",
      "Epoch 5256 - lr: 0.01000 - Train loss: 0.92815 - Test loss: 0.92328\n",
      "Epoch 5257 - lr: 0.01000 - Train loss: 0.92548 - Test loss: 0.92014\n",
      "Epoch 5258 - lr: 0.01000 - Train loss: 0.90579 - Test loss: 0.88785\n",
      "Epoch 5259 - lr: 0.01000 - Train loss: 0.87816 - Test loss: 0.90786\n",
      "Epoch 5260 - lr: 0.01000 - Train loss: 0.89354 - Test loss: 0.90148\n",
      "Epoch 5261 - lr: 0.01000 - Train loss: 0.89855 - Test loss: 0.90364\n",
      "Epoch 5262 - lr: 0.01000 - Train loss: 0.90407 - Test loss: 0.90721\n",
      "Epoch 5263 - lr: 0.01000 - Train loss: 0.93191 - Test loss: 0.89539\n",
      "Epoch 5264 - lr: 0.01000 - Train loss: 0.98544 - Test loss: 0.90805\n",
      "Epoch 5265 - lr: 0.01000 - Train loss: 0.92844 - Test loss: 0.91823\n",
      "Epoch 5266 - lr: 0.01000 - Train loss: 0.90055 - Test loss: 0.92053\n",
      "Epoch 5267 - lr: 0.01000 - Train loss: 0.92710 - Test loss: 0.91923\n",
      "Epoch 5268 - lr: 0.01000 - Train loss: 0.92466 - Test loss: 0.91689\n",
      "Epoch 5269 - lr: 0.01000 - Train loss: 0.93162 - Test loss: 0.89626\n",
      "Epoch 5270 - lr: 0.01000 - Train loss: 1.06338 - Test loss: 0.88939\n",
      "Epoch 5271 - lr: 0.01000 - Train loss: 1.00094 - Test loss: 0.89421\n",
      "Epoch 5272 - lr: 0.01000 - Train loss: 0.96756 - Test loss: 0.91146\n",
      "Epoch 5273 - lr: 0.01000 - Train loss: 0.92574 - Test loss: 0.90043\n",
      "Epoch 5274 - lr: 0.01000 - Train loss: 0.88524 - Test loss: 0.90229\n",
      "Epoch 5275 - lr: 0.01000 - Train loss: 0.94936 - Test loss: 0.88476\n",
      "Epoch 5276 - lr: 0.01000 - Train loss: 0.98363 - Test loss: 0.88758\n",
      "Epoch 5277 - lr: 0.01000 - Train loss: 0.97971 - Test loss: 0.90650\n",
      "Epoch 5278 - lr: 0.01000 - Train loss: 0.92994 - Test loss: 0.91569\n",
      "Epoch 5279 - lr: 0.01000 - Train loss: 0.92181 - Test loss: 0.91636\n",
      "Epoch 5280 - lr: 0.01000 - Train loss: 0.91089 - Test loss: 0.91934\n",
      "Epoch 5281 - lr: 0.01000 - Train loss: 0.91375 - Test loss: 0.91895\n",
      "Epoch 5282 - lr: 0.01000 - Train loss: 0.92832 - Test loss: 0.89186\n",
      "Epoch 5283 - lr: 0.01000 - Train loss: 0.95292 - Test loss: 0.87848\n",
      "Epoch 5284 - lr: 0.01000 - Train loss: 0.93299 - Test loss: 0.90561\n",
      "Epoch 5285 - lr: 0.01000 - Train loss: 0.87071 - Test loss: 0.88792\n",
      "Epoch 5286 - lr: 0.01000 - Train loss: 0.94702 - Test loss: 0.88403\n",
      "Epoch 5287 - lr: 0.01000 - Train loss: 0.94399 - Test loss: 0.89164\n",
      "Epoch 5288 - lr: 0.01000 - Train loss: 0.96778 - Test loss: 0.91002\n",
      "Epoch 5289 - lr: 0.01000 - Train loss: 0.93742 - Test loss: 0.91248\n",
      "Epoch 5290 - lr: 0.01000 - Train loss: 0.92114 - Test loss: 0.90996\n",
      "Epoch 5291 - lr: 0.01000 - Train loss: 0.91816 - Test loss: 0.88166\n",
      "Epoch 5292 - lr: 0.01000 - Train loss: 0.97403 - Test loss: 0.88576\n",
      "Epoch 5293 - lr: 0.01000 - Train loss: 0.96421 - Test loss: 0.90641\n",
      "Epoch 5294 - lr: 0.01000 - Train loss: 0.92510 - Test loss: 0.91216\n",
      "Epoch 5295 - lr: 0.01000 - Train loss: 0.87458 - Test loss: 0.88538\n",
      "Epoch 5296 - lr: 0.01000 - Train loss: 0.91988 - Test loss: 0.90192\n",
      "Epoch 5297 - lr: 0.01000 - Train loss: 0.91749 - Test loss: 0.88230\n",
      "Epoch 5298 - lr: 0.01000 - Train loss: 0.98706 - Test loss: 0.88247\n",
      "Epoch 5299 - lr: 0.01000 - Train loss: 0.92287 - Test loss: 0.91284\n",
      "Epoch 5300 - lr: 0.01000 - Train loss: 0.93078 - Test loss: 0.91864\n",
      "Epoch 5301 - lr: 0.01000 - Train loss: 0.93326 - Test loss: 0.90213\n",
      "Epoch 5302 - lr: 0.01000 - Train loss: 0.92919 - Test loss: 0.88356\n",
      "Epoch 5303 - lr: 0.01000 - Train loss: 0.97088 - Test loss: 0.90764\n",
      "Epoch 5304 - lr: 0.01000 - Train loss: 0.92842 - Test loss: 0.91970\n",
      "Epoch 5305 - lr: 0.01000 - Train loss: 0.92668 - Test loss: 0.91711\n",
      "Epoch 5306 - lr: 0.01000 - Train loss: 0.94870 - Test loss: 0.88785\n",
      "Epoch 5307 - lr: 0.01000 - Train loss: 0.97152 - Test loss: 0.90882\n",
      "Epoch 5308 - lr: 0.01000 - Train loss: 0.92704 - Test loss: 0.92013\n",
      "Epoch 5309 - lr: 0.01000 - Train loss: 0.93018 - Test loss: 0.92018\n",
      "Epoch 5310 - lr: 0.01000 - Train loss: 0.89114 - Test loss: 0.91916\n",
      "Epoch 5311 - lr: 0.01000 - Train loss: 0.89294 - Test loss: 0.91817\n",
      "Epoch 5312 - lr: 0.01000 - Train loss: 0.92505 - Test loss: 0.91484\n",
      "Epoch 5313 - lr: 0.01000 - Train loss: 0.96612 - Test loss: 0.88454\n",
      "Epoch 5314 - lr: 0.01000 - Train loss: 0.97716 - Test loss: 0.90222\n",
      "Epoch 5315 - lr: 0.01000 - Train loss: 0.92030 - Test loss: 0.91598\n",
      "Epoch 5316 - lr: 0.01000 - Train loss: 0.93346 - Test loss: 0.89513\n",
      "Epoch 5317 - lr: 0.01000 - Train loss: 0.95626 - Test loss: 0.90048\n",
      "Epoch 5318 - lr: 0.01000 - Train loss: 0.93934 - Test loss: 0.87950\n",
      "Epoch 5319 - lr: 0.01000 - Train loss: 0.94829 - Test loss: 0.90004\n",
      "Epoch 5320 - lr: 0.01000 - Train loss: 0.91098 - Test loss: 0.90415\n",
      "Epoch 5321 - lr: 0.01000 - Train loss: 0.93740 - Test loss: 0.89668\n",
      "Epoch 5322 - lr: 0.01000 - Train loss: 0.85217 - Test loss: 0.88464\n",
      "Epoch 5323 - lr: 0.01000 - Train loss: 0.94385 - Test loss: 0.88495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5324 - lr: 0.01000 - Train loss: 0.89907 - Test loss: 0.88065\n",
      "Epoch 5325 - lr: 0.01000 - Train loss: 0.83332 - Test loss: 0.88312\n",
      "Epoch 5326 - lr: 0.01000 - Train loss: 0.89195 - Test loss: 0.90441\n",
      "Epoch 5327 - lr: 0.01000 - Train loss: 0.93001 - Test loss: 0.91481\n",
      "Epoch 5328 - lr: 0.01000 - Train loss: 0.87297 - Test loss: 0.89070\n",
      "Epoch 5329 - lr: 0.01000 - Train loss: 0.87945 - Test loss: 0.88520\n",
      "Epoch 5330 - lr: 0.01000 - Train loss: 0.92726 - Test loss: 0.88785\n",
      "Epoch 5331 - lr: 0.01000 - Train loss: 1.02485 - Test loss: 0.88881\n",
      "Epoch 5332 - lr: 0.01000 - Train loss: 0.98070 - Test loss: 0.89629\n",
      "Epoch 5333 - lr: 0.01000 - Train loss: 1.00725 - Test loss: 0.91142\n",
      "Epoch 5334 - lr: 0.01000 - Train loss: 0.86978 - Test loss: 0.89512\n",
      "Epoch 5335 - lr: 0.01000 - Train loss: 0.90473 - Test loss: 0.90983\n",
      "Epoch 5336 - lr: 0.01000 - Train loss: 0.92737 - Test loss: 0.91755\n",
      "Epoch 5337 - lr: 0.01000 - Train loss: 0.89160 - Test loss: 0.88978\n",
      "Epoch 5338 - lr: 0.01000 - Train loss: 0.88922 - Test loss: 0.90136\n",
      "Epoch 5339 - lr: 0.01000 - Train loss: 0.93079 - Test loss: 0.91614\n",
      "Epoch 5340 - lr: 0.01000 - Train loss: 0.92041 - Test loss: 0.89355\n",
      "Epoch 5341 - lr: 0.01000 - Train loss: 0.91929 - Test loss: 0.88731\n",
      "Epoch 5342 - lr: 0.01000 - Train loss: 0.90503 - Test loss: 0.89659\n",
      "Epoch 5343 - lr: 0.01000 - Train loss: 0.91207 - Test loss: 0.90780\n",
      "Epoch 5344 - lr: 0.01000 - Train loss: 0.91586 - Test loss: 0.89608\n",
      "Epoch 5345 - lr: 0.01000 - Train loss: 0.91791 - Test loss: 0.88736\n",
      "Epoch 5346 - lr: 0.01000 - Train loss: 0.95763 - Test loss: 0.88879\n",
      "Epoch 5347 - lr: 0.01000 - Train loss: 0.98544 - Test loss: 0.89231\n",
      "Epoch 5348 - lr: 0.01000 - Train loss: 0.97862 - Test loss: 0.91047\n",
      "Epoch 5349 - lr: 0.01000 - Train loss: 0.87311 - Test loss: 0.89699\n",
      "Epoch 5350 - lr: 0.01000 - Train loss: 0.93748 - Test loss: 0.91520\n",
      "Epoch 5351 - lr: 0.01000 - Train loss: 0.90198 - Test loss: 0.91976\n",
      "Epoch 5352 - lr: 0.01000 - Train loss: 0.91157 - Test loss: 0.89147\n",
      "Epoch 5353 - lr: 0.01000 - Train loss: 0.93445 - Test loss: 0.91126\n",
      "Epoch 5354 - lr: 0.01000 - Train loss: 0.90376 - Test loss: 0.91565\n",
      "Epoch 5355 - lr: 0.01000 - Train loss: 0.92621 - Test loss: 0.92267\n",
      "Epoch 5356 - lr: 0.01000 - Train loss: 0.92919 - Test loss: 0.90015\n",
      "Epoch 5357 - lr: 0.01000 - Train loss: 1.00357 - Test loss: 0.91475\n",
      "Epoch 5358 - lr: 0.01000 - Train loss: 0.92872 - Test loss: 0.90118\n",
      "Epoch 5359 - lr: 0.01000 - Train loss: 0.95149 - Test loss: 0.89372\n",
      "Epoch 5360 - lr: 0.01000 - Train loss: 0.92576 - Test loss: 0.91810\n",
      "Epoch 5361 - lr: 0.01000 - Train loss: 0.91929 - Test loss: 0.90360\n",
      "Epoch 5362 - lr: 0.01000 - Train loss: 0.92094 - Test loss: 0.89445\n",
      "Epoch 5363 - lr: 0.01000 - Train loss: 0.97735 - Test loss: 0.89262\n",
      "Epoch 5364 - lr: 0.01000 - Train loss: 1.01500 - Test loss: 0.88992\n",
      "Epoch 5365 - lr: 0.01000 - Train loss: 0.97132 - Test loss: 0.89155\n",
      "Epoch 5366 - lr: 0.01000 - Train loss: 0.95000 - Test loss: 0.91037\n",
      "Epoch 5367 - lr: 0.01000 - Train loss: 0.86872 - Test loss: 0.89499\n",
      "Epoch 5368 - lr: 0.01000 - Train loss: 0.91577 - Test loss: 0.92173\n",
      "Epoch 5369 - lr: 0.01000 - Train loss: 0.92753 - Test loss: 0.92623\n",
      "Epoch 5370 - lr: 0.01000 - Train loss: 0.90967 - Test loss: 0.90132\n",
      "Epoch 5371 - lr: 0.01000 - Train loss: 1.00395 - Test loss: 0.89895\n",
      "Epoch 5372 - lr: 0.01000 - Train loss: 0.92774 - Test loss: 0.91917\n",
      "Epoch 5373 - lr: 0.01000 - Train loss: 0.96453 - Test loss: 0.89612\n",
      "Epoch 5374 - lr: 0.01000 - Train loss: 0.93976 - Test loss: 0.90273\n",
      "Epoch 5375 - lr: 0.01000 - Train loss: 0.86934 - Test loss: 0.89620\n",
      "Epoch 5376 - lr: 0.01000 - Train loss: 0.90553 - Test loss: 0.90609\n",
      "Epoch 5377 - lr: 0.01000 - Train loss: 0.89729 - Test loss: 0.91447\n",
      "Epoch 5378 - lr: 0.01000 - Train loss: 0.92595 - Test loss: 0.92302\n",
      "Epoch 5379 - lr: 0.01000 - Train loss: 0.92968 - Test loss: 0.90237\n",
      "Epoch 5380 - lr: 0.01000 - Train loss: 1.00690 - Test loss: 0.89180\n",
      "Epoch 5381 - lr: 0.01000 - Train loss: 0.94468 - Test loss: 0.91806\n",
      "Epoch 5382 - lr: 0.01000 - Train loss: 0.88571 - Test loss: 0.92130\n",
      "Epoch 5383 - lr: 0.01000 - Train loss: 0.88763 - Test loss: 0.89222\n",
      "Epoch 5384 - lr: 0.01000 - Train loss: 0.91841 - Test loss: 0.91744\n",
      "Epoch 5385 - lr: 0.01000 - Train loss: 0.92586 - Test loss: 0.92185\n",
      "Epoch 5386 - lr: 0.01000 - Train loss: 0.89192 - Test loss: 0.92121\n",
      "Epoch 5387 - lr: 0.01000 - Train loss: 0.92197 - Test loss: 0.89713\n",
      "Epoch 5388 - lr: 0.01000 - Train loss: 0.94473 - Test loss: 0.88886\n",
      "Epoch 5389 - lr: 0.01000 - Train loss: 0.93360 - Test loss: 0.90648\n",
      "Epoch 5390 - lr: 0.01000 - Train loss: 0.89283 - Test loss: 0.91081\n",
      "Epoch 5391 - lr: 0.01000 - Train loss: 0.92260 - Test loss: 0.91834\n",
      "Epoch 5392 - lr: 0.01000 - Train loss: 0.90695 - Test loss: 0.91988\n",
      "Epoch 5393 - lr: 0.01000 - Train loss: 0.92114 - Test loss: 0.91902\n",
      "Epoch 5394 - lr: 0.01000 - Train loss: 0.92274 - Test loss: 0.91737\n",
      "Epoch 5395 - lr: 0.01000 - Train loss: 0.92089 - Test loss: 0.91594\n",
      "Epoch 5396 - lr: 0.01000 - Train loss: 0.91949 - Test loss: 0.91499\n",
      "Epoch 5397 - lr: 0.01000 - Train loss: 0.92192 - Test loss: 0.91368\n",
      "Epoch 5398 - lr: 0.01000 - Train loss: 0.91713 - Test loss: 0.91315\n",
      "Epoch 5399 - lr: 0.01000 - Train loss: 0.92136 - Test loss: 0.91225\n",
      "Epoch 5400 - lr: 0.01000 - Train loss: 0.92207 - Test loss: 0.91102\n",
      "Epoch 5401 - lr: 0.01000 - Train loss: 0.92179 - Test loss: 0.90974\n",
      "Epoch 5402 - lr: 0.01000 - Train loss: 0.91903 - Test loss: 0.90912\n",
      "Epoch 5403 - lr: 0.01000 - Train loss: 0.92190 - Test loss: 0.90824\n",
      "Epoch 5404 - lr: 0.01000 - Train loss: 0.92159 - Test loss: 0.90722\n",
      "Epoch 5405 - lr: 0.01000 - Train loss: 0.92030 - Test loss: 0.90651\n",
      "Epoch 5406 - lr: 0.01000 - Train loss: 0.92150 - Test loss: 0.90572\n",
      "Epoch 5407 - lr: 0.01000 - Train loss: 0.92093 - Test loss: 0.90499\n",
      "Epoch 5408 - lr: 0.01000 - Train loss: 0.92116 - Test loss: 0.90430\n",
      "Epoch 5409 - lr: 0.01000 - Train loss: 0.92117 - Test loss: 0.90364\n",
      "Epoch 5410 - lr: 0.01000 - Train loss: 0.92124 - Test loss: 0.90301\n",
      "Epoch 5411 - lr: 0.01000 - Train loss: 0.92131 - Test loss: 0.90241\n",
      "Epoch 5412 - lr: 0.01000 - Train loss: 0.92133 - Test loss: 0.90186\n",
      "Epoch 5413 - lr: 0.01000 - Train loss: 0.92114 - Test loss: 0.90140\n",
      "Epoch 5414 - lr: 0.01000 - Train loss: 0.91977 - Test loss: 0.90110\n",
      "Epoch 5415 - lr: 0.01000 - Train loss: 0.91249 - Test loss: 0.90024\n",
      "Epoch 5416 - lr: 0.01000 - Train loss: 0.92316 - Test loss: 0.89894\n",
      "Epoch 5417 - lr: 0.01000 - Train loss: 0.92292 - Test loss: 0.89816\n",
      "Epoch 5418 - lr: 0.01000 - Train loss: 0.92221 - Test loss: 0.89770\n",
      "Epoch 5419 - lr: 0.01000 - Train loss: 0.92261 - Test loss: 0.89728\n",
      "Epoch 5420 - lr: 0.01000 - Train loss: 0.92255 - Test loss: 0.89689\n",
      "Epoch 5421 - lr: 0.01000 - Train loss: 0.92258 - Test loss: 0.89656\n",
      "Epoch 5422 - lr: 0.01000 - Train loss: 0.92246 - Test loss: 0.89631\n",
      "Epoch 5423 - lr: 0.01000 - Train loss: 0.92121 - Test loss: 0.89632\n",
      "Epoch 5424 - lr: 0.01000 - Train loss: 0.90766 - Test loss: 0.89544\n",
      "Epoch 5425 - lr: 0.01000 - Train loss: 0.92209 - Test loss: 0.89445\n",
      "Epoch 5426 - lr: 0.01000 - Train loss: 0.92384 - Test loss: 0.89419\n",
      "Epoch 5427 - lr: 0.01000 - Train loss: 0.92306 - Test loss: 0.89420\n",
      "Epoch 5428 - lr: 0.01000 - Train loss: 0.91193 - Test loss: 0.89323\n",
      "Epoch 5429 - lr: 0.01000 - Train loss: 0.91646 - Test loss: 0.89326\n",
      "Epoch 5430 - lr: 0.01000 - Train loss: 0.86646 - Test loss: 0.88296\n",
      "Epoch 5431 - lr: 0.01000 - Train loss: 0.85202 - Test loss: 0.87682\n",
      "Epoch 5432 - lr: 0.01000 - Train loss: 0.85151 - Test loss: 0.87541\n",
      "Epoch 5433 - lr: 0.01000 - Train loss: 0.85183 - Test loss: 0.87496\n",
      "Epoch 5434 - lr: 0.01000 - Train loss: 0.85143 - Test loss: 0.87380\n",
      "Epoch 5435 - lr: 0.01000 - Train loss: 0.89439 - Test loss: 0.86538\n",
      "Epoch 5436 - lr: 0.01000 - Train loss: 0.91393 - Test loss: 0.88514\n",
      "Epoch 5437 - lr: 0.01000 - Train loss: 0.92973 - Test loss: 0.88739\n",
      "Epoch 5438 - lr: 0.01000 - Train loss: 0.89287 - Test loss: 0.89037\n",
      "Epoch 5439 - lr: 0.01000 - Train loss: 0.88146 - Test loss: 0.86192\n",
      "Epoch 5440 - lr: 0.01000 - Train loss: 0.85346 - Test loss: 0.86749\n",
      "Epoch 5441 - lr: 0.01000 - Train loss: 0.92272 - Test loss: 0.88641\n",
      "Epoch 5442 - lr: 0.01000 - Train loss: 0.91106 - Test loss: 0.89171\n",
      "Epoch 5443 - lr: 0.01000 - Train loss: 0.93667 - Test loss: 0.88570\n",
      "Epoch 5444 - lr: 0.01000 - Train loss: 0.92296 - Test loss: 0.89026\n",
      "Epoch 5445 - lr: 0.01000 - Train loss: 0.91957 - Test loss: 0.89204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5446 - lr: 0.01000 - Train loss: 0.92874 - Test loss: 0.85965\n",
      "Epoch 5447 - lr: 0.01000 - Train loss: 1.06119 - Test loss: 0.85165\n",
      "Epoch 5448 - lr: 0.01000 - Train loss: 0.85841 - Test loss: 0.87009\n",
      "Epoch 5449 - lr: 0.01000 - Train loss: 0.96398 - Test loss: 0.85487\n",
      "Epoch 5450 - lr: 0.01000 - Train loss: 1.04124 - Test loss: 0.85384\n",
      "Epoch 5451 - lr: 0.01000 - Train loss: 0.97051 - Test loss: 0.87773\n",
      "Epoch 5452 - lr: 0.01000 - Train loss: 0.92036 - Test loss: 0.88593\n",
      "Epoch 5453 - lr: 0.01000 - Train loss: 0.92283 - Test loss: 0.87167\n",
      "Epoch 5454 - lr: 0.01000 - Train loss: 0.91061 - Test loss: 0.88709\n",
      "Epoch 5455 - lr: 0.01000 - Train loss: 0.84151 - Test loss: 0.86543\n",
      "Epoch 5456 - lr: 0.01000 - Train loss: 0.91478 - Test loss: 0.86517\n",
      "Epoch 5457 - lr: 0.01000 - Train loss: 1.01515 - Test loss: 0.89196\n",
      "Epoch 5458 - lr: 0.01000 - Train loss: 0.95803 - Test loss: 0.87288\n",
      "Epoch 5459 - lr: 0.01000 - Train loss: 1.00126 - Test loss: 0.87055\n",
      "Epoch 5460 - lr: 0.01000 - Train loss: 1.01994 - Test loss: 0.86558\n",
      "Epoch 5461 - lr: 0.01000 - Train loss: 0.92700 - Test loss: 0.86901\n",
      "Epoch 5462 - lr: 0.01000 - Train loss: 0.89523 - Test loss: 0.89179\n",
      "Epoch 5463 - lr: 0.01000 - Train loss: 0.90394 - Test loss: 0.89405\n",
      "Epoch 5464 - lr: 0.01000 - Train loss: 0.92519 - Test loss: 0.90335\n",
      "Epoch 5465 - lr: 0.01000 - Train loss: 0.91990 - Test loss: 0.90632\n",
      "Epoch 5466 - lr: 0.01000 - Train loss: 0.92155 - Test loss: 0.90787\n",
      "Epoch 5467 - lr: 0.01000 - Train loss: 0.96124 - Test loss: 0.88075\n",
      "Epoch 5468 - lr: 0.01000 - Train loss: 0.97788 - Test loss: 0.87196\n",
      "Epoch 5469 - lr: 0.01000 - Train loss: 1.03472 - Test loss: 0.87803\n",
      "Epoch 5470 - lr: 0.01000 - Train loss: 0.94164 - Test loss: 0.88860\n",
      "Epoch 5471 - lr: 0.01000 - Train loss: 0.93313 - Test loss: 0.90374\n",
      "Epoch 5472 - lr: 0.01000 - Train loss: 0.93294 - Test loss: 0.88288\n",
      "Epoch 5473 - lr: 0.01000 - Train loss: 0.94242 - Test loss: 0.89376\n",
      "Epoch 5474 - lr: 0.01000 - Train loss: 0.85812 - Test loss: 0.87452\n",
      "Epoch 5475 - lr: 0.01000 - Train loss: 0.93683 - Test loss: 0.88888\n",
      "Epoch 5476 - lr: 0.01000 - Train loss: 0.92239 - Test loss: 0.89666\n",
      "Epoch 5477 - lr: 0.01000 - Train loss: 0.84153 - Test loss: 0.87812\n",
      "Epoch 5478 - lr: 0.01000 - Train loss: 0.96439 - Test loss: 0.87636\n",
      "Epoch 5479 - lr: 0.01000 - Train loss: 0.95355 - Test loss: 0.87927\n",
      "Epoch 5480 - lr: 0.01000 - Train loss: 0.84211 - Test loss: 0.87790\n",
      "Epoch 5481 - lr: 0.01000 - Train loss: 0.92741 - Test loss: 0.90462\n",
      "Epoch 5482 - lr: 0.01000 - Train loss: 0.93621 - Test loss: 0.89034\n",
      "Epoch 5483 - lr: 0.01000 - Train loss: 0.93919 - Test loss: 0.89836\n",
      "Epoch 5484 - lr: 0.01000 - Train loss: 0.84426 - Test loss: 0.88081\n",
      "Epoch 5485 - lr: 0.01000 - Train loss: 0.93613 - Test loss: 0.87670\n",
      "Epoch 5486 - lr: 0.01000 - Train loss: 0.89378 - Test loss: 0.87842\n",
      "Epoch 5487 - lr: 0.01000 - Train loss: 0.92365 - Test loss: 0.88031\n",
      "Epoch 5488 - lr: 0.01000 - Train loss: 0.93569 - Test loss: 0.89002\n",
      "Epoch 5489 - lr: 0.01000 - Train loss: 0.96494 - Test loss: 0.90930\n",
      "Epoch 5490 - lr: 0.01000 - Train loss: 0.92833 - Test loss: 0.90610\n",
      "Epoch 5491 - lr: 0.01000 - Train loss: 0.91454 - Test loss: 0.91923\n",
      "Epoch 5492 - lr: 0.01000 - Train loss: 0.92659 - Test loss: 0.90169\n",
      "Epoch 5493 - lr: 0.01000 - Train loss: 0.92188 - Test loss: 0.88373\n",
      "Epoch 5494 - lr: 0.01000 - Train loss: 0.92343 - Test loss: 0.88807\n",
      "Epoch 5495 - lr: 0.01000 - Train loss: 1.01387 - Test loss: 0.88490\n",
      "Epoch 5496 - lr: 0.01000 - Train loss: 0.97631 - Test loss: 0.88429\n",
      "Epoch 5497 - lr: 0.01000 - Train loss: 0.87984 - Test loss: 0.88640\n",
      "Epoch 5498 - lr: 0.01000 - Train loss: 0.93502 - Test loss: 0.89848\n",
      "Epoch 5499 - lr: 0.01000 - Train loss: 0.93768 - Test loss: 0.89277\n",
      "Epoch 5500 - lr: 0.01000 - Train loss: 0.93768 - Test loss: 0.90774\n",
      "Epoch 5501 - lr: 0.01000 - Train loss: 0.93439 - Test loss: 0.91115\n",
      "Epoch 5502 - lr: 0.01000 - Train loss: 0.92368 - Test loss: 0.91270\n",
      "Epoch 5503 - lr: 0.01000 - Train loss: 0.93017 - Test loss: 0.90147\n",
      "Epoch 5504 - lr: 0.01000 - Train loss: 0.92591 - Test loss: 0.89335\n",
      "Epoch 5505 - lr: 0.01000 - Train loss: 0.91746 - Test loss: 0.89900\n",
      "Epoch 5506 - lr: 0.01000 - Train loss: 0.98723 - Test loss: 0.91389\n",
      "Epoch 5507 - lr: 0.01000 - Train loss: 0.90409 - Test loss: 0.89496\n",
      "Epoch 5508 - lr: 0.01000 - Train loss: 0.93685 - Test loss: 0.92042\n",
      "Epoch 5509 - lr: 0.01000 - Train loss: 0.92310 - Test loss: 0.91829\n",
      "Epoch 5510 - lr: 0.01000 - Train loss: 0.86165 - Test loss: 0.89661\n",
      "Epoch 5511 - lr: 0.01000 - Train loss: 0.92773 - Test loss: 0.90149\n",
      "Epoch 5512 - lr: 0.01000 - Train loss: 0.97589 - Test loss: 0.91845\n",
      "Epoch 5513 - lr: 0.01000 - Train loss: 0.92429 - Test loss: 0.91629\n",
      "Epoch 5514 - lr: 0.01000 - Train loss: 0.93569 - Test loss: 0.89824\n",
      "Epoch 5515 - lr: 0.01000 - Train loss: 0.95128 - Test loss: 0.89680\n",
      "Epoch 5516 - lr: 0.01000 - Train loss: 0.97216 - Test loss: 0.91191\n",
      "Epoch 5517 - lr: 0.01000 - Train loss: 0.87943 - Test loss: 0.90056\n",
      "Epoch 5518 - lr: 0.01000 - Train loss: 0.91955 - Test loss: 0.90256\n",
      "Epoch 5519 - lr: 0.01000 - Train loss: 0.91920 - Test loss: 0.89738\n",
      "Epoch 5520 - lr: 0.01000 - Train loss: 0.91383 - Test loss: 0.90099\n",
      "Epoch 5521 - lr: 0.01000 - Train loss: 1.02092 - Test loss: 0.90266\n",
      "Epoch 5522 - lr: 0.01000 - Train loss: 0.84419 - Test loss: 0.89821\n",
      "Epoch 5523 - lr: 0.01000 - Train loss: 0.92727 - Test loss: 0.90370\n",
      "Epoch 5524 - lr: 0.01000 - Train loss: 0.98379 - Test loss: 0.91957\n",
      "Epoch 5525 - lr: 0.01000 - Train loss: 0.95036 - Test loss: 0.91246\n",
      "Epoch 5526 - lr: 0.01000 - Train loss: 0.92679 - Test loss: 0.92704\n",
      "Epoch 5527 - lr: 0.01000 - Train loss: 0.89497 - Test loss: 0.89954\n",
      "Epoch 5528 - lr: 0.01000 - Train loss: 0.92504 - Test loss: 0.90736\n",
      "Epoch 5529 - lr: 0.01000 - Train loss: 0.94710 - Test loss: 0.91474\n",
      "Epoch 5530 - lr: 0.01000 - Train loss: 0.92272 - Test loss: 0.90883\n",
      "Epoch 5531 - lr: 0.01000 - Train loss: 0.95890 - Test loss: 0.92808\n",
      "Epoch 5532 - lr: 0.01000 - Train loss: 0.93318 - Test loss: 0.92997\n",
      "Epoch 5533 - lr: 0.01000 - Train loss: 0.91919 - Test loss: 0.93002\n",
      "Epoch 5534 - lr: 0.01000 - Train loss: 0.93275 - Test loss: 0.92704\n",
      "Epoch 5535 - lr: 0.01000 - Train loss: 0.93022 - Test loss: 0.90310\n",
      "Epoch 5536 - lr: 0.01000 - Train loss: 0.93905 - Test loss: 0.89122\n",
      "Epoch 5537 - lr: 0.01000 - Train loss: 0.93015 - Test loss: 0.90623\n",
      "Epoch 5538 - lr: 0.01000 - Train loss: 0.91914 - Test loss: 0.92329\n",
      "Epoch 5539 - lr: 0.01000 - Train loss: 0.93238 - Test loss: 0.92502\n",
      "Epoch 5540 - lr: 0.01000 - Train loss: 0.91025 - Test loss: 0.92577\n",
      "Epoch 5541 - lr: 0.01000 - Train loss: 0.89343 - Test loss: 0.92015\n",
      "Epoch 5542 - lr: 0.01000 - Train loss: 0.86017 - Test loss: 0.88936\n",
      "Epoch 5543 - lr: 0.01000 - Train loss: 0.92518 - Test loss: 0.90122\n",
      "Epoch 5544 - lr: 0.01000 - Train loss: 0.92780 - Test loss: 0.89531\n",
      "Epoch 5545 - lr: 0.01000 - Train loss: 0.99106 - Test loss: 0.90395\n",
      "Epoch 5546 - lr: 0.01000 - Train loss: 0.92623 - Test loss: 0.88501\n",
      "Epoch 5547 - lr: 0.01000 - Train loss: 0.83565 - Test loss: 0.88657\n",
      "Epoch 5548 - lr: 0.01000 - Train loss: 0.83528 - Test loss: 0.88728\n",
      "Epoch 5549 - lr: 0.01000 - Train loss: 0.83549 - Test loss: 0.88757\n",
      "Epoch 5550 - lr: 0.01000 - Train loss: 0.83592 - Test loss: 0.88768\n",
      "Epoch 5551 - lr: 0.01000 - Train loss: 0.83576 - Test loss: 0.88790\n",
      "Epoch 5552 - lr: 0.01000 - Train loss: 0.90346 - Test loss: 0.90069\n",
      "Epoch 5553 - lr: 0.01000 - Train loss: 0.83619 - Test loss: 0.89050\n",
      "Epoch 5554 - lr: 0.01000 - Train loss: 0.83653 - Test loss: 0.88850\n",
      "Epoch 5555 - lr: 0.01000 - Train loss: 0.90136 - Test loss: 0.88630\n",
      "Epoch 5556 - lr: 0.01000 - Train loss: 0.83583 - Test loss: 0.88860\n",
      "Epoch 5557 - lr: 0.01000 - Train loss: 0.86835 - Test loss: 0.88817\n",
      "Epoch 5558 - lr: 0.01000 - Train loss: 0.92423 - Test loss: 0.90525\n",
      "Epoch 5559 - lr: 0.01000 - Train loss: 0.91945 - Test loss: 0.91923\n",
      "Epoch 5560 - lr: 0.01000 - Train loss: 0.92774 - Test loss: 0.91947\n",
      "Epoch 5561 - lr: 0.01000 - Train loss: 0.84028 - Test loss: 0.89304\n",
      "Epoch 5562 - lr: 0.01000 - Train loss: 0.84063 - Test loss: 0.88719\n",
      "Epoch 5563 - lr: 0.01000 - Train loss: 0.92320 - Test loss: 0.88573\n",
      "Epoch 5564 - lr: 0.01000 - Train loss: 0.91317 - Test loss: 0.88757\n",
      "Epoch 5565 - lr: 0.01000 - Train loss: 0.89626 - Test loss: 0.91586\n",
      "Epoch 5566 - lr: 0.01000 - Train loss: 0.92904 - Test loss: 0.89965\n",
      "Epoch 5567 - lr: 0.01000 - Train loss: 0.97766 - Test loss: 0.88779\n",
      "Epoch 5568 - lr: 0.01000 - Train loss: 0.90833 - Test loss: 0.90820\n",
      "Epoch 5569 - lr: 0.01000 - Train loss: 0.93640 - Test loss: 0.91775\n",
      "Epoch 5570 - lr: 0.01000 - Train loss: 0.91507 - Test loss: 0.89389\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5571 - lr: 0.01000 - Train loss: 0.93447 - Test loss: 0.91816\n",
      "Epoch 5572 - lr: 0.01000 - Train loss: 0.92685 - Test loss: 0.92248\n",
      "Epoch 5573 - lr: 0.01000 - Train loss: 0.91267 - Test loss: 0.89360\n",
      "Epoch 5574 - lr: 0.01000 - Train loss: 0.93716 - Test loss: 0.90662\n",
      "Epoch 5575 - lr: 0.01000 - Train loss: 0.92055 - Test loss: 0.91312\n",
      "Epoch 5576 - lr: 0.01000 - Train loss: 0.84079 - Test loss: 0.89331\n",
      "Epoch 5577 - lr: 0.01000 - Train loss: 0.88227 - Test loss: 0.89260\n",
      "Epoch 5578 - lr: 0.01000 - Train loss: 0.91828 - Test loss: 0.91890\n",
      "Epoch 5579 - lr: 0.01000 - Train loss: 0.92797 - Test loss: 0.92447\n",
      "Epoch 5580 - lr: 0.01000 - Train loss: 0.92945 - Test loss: 0.92122\n",
      "Epoch 5581 - lr: 0.01000 - Train loss: 0.92180 - Test loss: 0.88844\n",
      "Epoch 5582 - lr: 0.01000 - Train loss: 0.92486 - Test loss: 0.88476\n",
      "Epoch 5583 - lr: 0.01000 - Train loss: 0.83404 - Test loss: 0.88814\n",
      "Epoch 5584 - lr: 0.01000 - Train loss: 0.91350 - Test loss: 0.90718\n",
      "Epoch 5585 - lr: 0.01000 - Train loss: 0.90098 - Test loss: 0.91671\n",
      "Epoch 5586 - lr: 0.01000 - Train loss: 0.90149 - Test loss: 0.88992\n",
      "Epoch 5587 - lr: 0.01000 - Train loss: 0.88120 - Test loss: 0.91129\n",
      "Epoch 5588 - lr: 0.01000 - Train loss: 0.85141 - Test loss: 0.89045\n",
      "Epoch 5589 - lr: 0.01000 - Train loss: 0.92236 - Test loss: 0.91014\n",
      "Epoch 5590 - lr: 0.01000 - Train loss: 0.85820 - Test loss: 0.88944\n",
      "Epoch 5591 - lr: 0.01000 - Train loss: 0.92242 - Test loss: 0.90781\n",
      "Epoch 5592 - lr: 0.01000 - Train loss: 0.87827 - Test loss: 0.88771\n",
      "Epoch 5593 - lr: 0.01000 - Train loss: 0.89818 - Test loss: 0.91411\n",
      "Epoch 5594 - lr: 0.01000 - Train loss: 0.92217 - Test loss: 0.91999\n",
      "Epoch 5595 - lr: 0.01000 - Train loss: 0.92348 - Test loss: 0.91643\n",
      "Epoch 5596 - lr: 0.01000 - Train loss: 0.95002 - Test loss: 0.88833\n",
      "Epoch 5597 - lr: 0.01000 - Train loss: 0.98138 - Test loss: 0.90556\n",
      "Epoch 5598 - lr: 0.01000 - Train loss: 0.92897 - Test loss: 0.91597\n",
      "Epoch 5599 - lr: 0.01000 - Train loss: 0.89072 - Test loss: 0.88898\n",
      "Epoch 5600 - lr: 0.01000 - Train loss: 0.92223 - Test loss: 0.90708\n",
      "Epoch 5601 - lr: 0.01000 - Train loss: 0.92207 - Test loss: 0.88431\n",
      "Epoch 5602 - lr: 0.01000 - Train loss: 0.96682 - Test loss: 0.90817\n",
      "Epoch 5603 - lr: 0.01000 - Train loss: 0.92931 - Test loss: 0.91795\n",
      "Epoch 5604 - lr: 0.01000 - Train loss: 0.93556 - Test loss: 0.91446\n",
      "Epoch 5605 - lr: 0.01000 - Train loss: 0.87080 - Test loss: 0.89235\n",
      "Epoch 5606 - lr: 0.01000 - Train loss: 0.89881 - Test loss: 0.90788\n",
      "Epoch 5607 - lr: 0.01000 - Train loss: 0.92635 - Test loss: 0.91889\n",
      "Epoch 5608 - lr: 0.01000 - Train loss: 0.92717 - Test loss: 0.91777\n",
      "Epoch 5609 - lr: 0.01000 - Train loss: 0.90904 - Test loss: 0.88925\n",
      "Epoch 5610 - lr: 0.01000 - Train loss: 0.92589 - Test loss: 0.91080\n",
      "Epoch 5611 - lr: 0.01000 - Train loss: 0.91664 - Test loss: 0.91470\n",
      "Epoch 5612 - lr: 0.01000 - Train loss: 0.92641 - Test loss: 0.91734\n",
      "Epoch 5613 - lr: 0.01000 - Train loss: 0.92373 - Test loss: 0.91606\n",
      "Epoch 5614 - lr: 0.01000 - Train loss: 0.90708 - Test loss: 0.88988\n",
      "Epoch 5615 - lr: 0.01000 - Train loss: 0.95795 - Test loss: 0.88171\n",
      "Epoch 5616 - lr: 0.01000 - Train loss: 0.87534 - Test loss: 0.89620\n",
      "Epoch 5617 - lr: 0.01000 - Train loss: 0.87537 - Test loss: 0.89818\n",
      "Epoch 5618 - lr: 0.01000 - Train loss: 0.92460 - Test loss: 0.90974\n",
      "Epoch 5619 - lr: 0.01000 - Train loss: 0.92458 - Test loss: 0.88678\n",
      "Epoch 5620 - lr: 0.01000 - Train loss: 0.99632 - Test loss: 0.87699\n",
      "Epoch 5621 - lr: 0.01000 - Train loss: 0.90952 - Test loss: 0.90369\n",
      "Epoch 5622 - lr: 0.01000 - Train loss: 0.93265 - Test loss: 0.89526\n",
      "Epoch 5623 - lr: 0.01000 - Train loss: 0.87565 - Test loss: 0.88368\n",
      "Epoch 5624 - lr: 0.01000 - Train loss: 0.91259 - Test loss: 0.87876\n",
      "Epoch 5625 - lr: 0.01000 - Train loss: 0.90867 - Test loss: 0.88510\n",
      "Epoch 5626 - lr: 0.01000 - Train loss: 0.96622 - Test loss: 0.91111\n",
      "Epoch 5627 - lr: 0.01000 - Train loss: 0.92533 - Test loss: 0.91654\n",
      "Epoch 5628 - lr: 0.01000 - Train loss: 0.91491 - Test loss: 0.91769\n",
      "Epoch 5629 - lr: 0.01000 - Train loss: 0.91891 - Test loss: 0.91742\n",
      "Epoch 5630 - lr: 0.01000 - Train loss: 0.90689 - Test loss: 0.91628\n",
      "Epoch 5631 - lr: 0.01000 - Train loss: 0.91692 - Test loss: 0.91511\n",
      "Epoch 5632 - lr: 0.01000 - Train loss: 0.88136 - Test loss: 0.89900\n",
      "Epoch 5633 - lr: 0.01000 - Train loss: 0.91028 - Test loss: 0.90904\n",
      "Epoch 5634 - lr: 0.01000 - Train loss: 0.92381 - Test loss: 0.88418\n",
      "Epoch 5635 - lr: 0.01000 - Train loss: 0.97164 - Test loss: 0.87464\n",
      "Epoch 5636 - lr: 0.01000 - Train loss: 0.92760 - Test loss: 0.89930\n",
      "Epoch 5637 - lr: 0.01000 - Train loss: 0.87242 - Test loss: 0.87794\n",
      "Epoch 5638 - lr: 0.01000 - Train loss: 0.92205 - Test loss: 0.90467\n",
      "Epoch 5639 - lr: 0.01000 - Train loss: 0.92513 - Test loss: 0.88471\n",
      "Epoch 5640 - lr: 0.01000 - Train loss: 1.01900 - Test loss: 0.88230\n",
      "Epoch 5641 - lr: 0.01000 - Train loss: 0.92927 - Test loss: 0.90526\n",
      "Epoch 5642 - lr: 0.01000 - Train loss: 0.92367 - Test loss: 0.90658\n",
      "Epoch 5643 - lr: 0.01000 - Train loss: 0.94128 - Test loss: 0.88468\n",
      "Epoch 5644 - lr: 0.01000 - Train loss: 0.92409 - Test loss: 0.90482\n",
      "Epoch 5645 - lr: 0.01000 - Train loss: 0.91132 - Test loss: 0.91091\n",
      "Epoch 5646 - lr: 0.01000 - Train loss: 0.93480 - Test loss: 0.88658\n",
      "Epoch 5647 - lr: 0.01000 - Train loss: 1.00343 - Test loss: 0.87373\n",
      "Epoch 5648 - lr: 0.01000 - Train loss: 0.89720 - Test loss: 0.88241\n",
      "Epoch 5649 - lr: 0.01000 - Train loss: 0.91610 - Test loss: 0.88291\n",
      "Epoch 5650 - lr: 0.01000 - Train loss: 0.94494 - Test loss: 0.88676\n",
      "Epoch 5651 - lr: 0.01000 - Train loss: 0.89784 - Test loss: 0.90788\n",
      "Epoch 5652 - lr: 0.01000 - Train loss: 0.92312 - Test loss: 0.91256\n",
      "Epoch 5653 - lr: 0.01000 - Train loss: 0.93274 - Test loss: 0.88974\n",
      "Epoch 5654 - lr: 0.01000 - Train loss: 1.04640 - Test loss: 0.88700\n",
      "Epoch 5655 - lr: 0.01000 - Train loss: 0.91858 - Test loss: 0.90708\n",
      "Epoch 5656 - lr: 0.01000 - Train loss: 0.91059 - Test loss: 0.91217\n",
      "Epoch 5657 - lr: 0.01000 - Train loss: 0.90212 - Test loss: 0.90029\n",
      "Epoch 5658 - lr: 0.01000 - Train loss: 0.88276 - Test loss: 0.89746\n",
      "Epoch 5659 - lr: 0.01000 - Train loss: 0.88737 - Test loss: 0.87522\n",
      "Epoch 5660 - lr: 0.01000 - Train loss: 0.89688 - Test loss: 0.90150\n",
      "Epoch 5661 - lr: 0.01000 - Train loss: 0.90824 - Test loss: 0.90831\n",
      "Epoch 5662 - lr: 0.01000 - Train loss: 0.90683 - Test loss: 0.90353\n",
      "Epoch 5663 - lr: 0.01000 - Train loss: 0.84074 - Test loss: 0.87462\n",
      "Epoch 5664 - lr: 0.01000 - Train loss: 0.83872 - Test loss: 0.87082\n",
      "Epoch 5665 - lr: 0.01000 - Train loss: 0.83708 - Test loss: 0.86977\n",
      "Epoch 5666 - lr: 0.01000 - Train loss: 0.91512 - Test loss: 0.89067\n",
      "Epoch 5667 - lr: 0.01000 - Train loss: 0.92768 - Test loss: 0.90478\n",
      "Epoch 5668 - lr: 0.01000 - Train loss: 0.92162 - Test loss: 0.90825\n",
      "Epoch 5669 - lr: 0.01000 - Train loss: 0.93478 - Test loss: 0.89258\n",
      "Epoch 5670 - lr: 0.01000 - Train loss: 0.90250 - Test loss: 0.90383\n",
      "Epoch 5671 - lr: 0.01000 - Train loss: 0.91382 - Test loss: 0.90636\n",
      "Epoch 5672 - lr: 0.01000 - Train loss: 0.94030 - Test loss: 0.90253\n",
      "Epoch 5673 - lr: 0.01000 - Train loss: 0.92441 - Test loss: 0.89532\n",
      "Epoch 5674 - lr: 0.01000 - Train loss: 0.84554 - Test loss: 0.87057\n",
      "Epoch 5675 - lr: 0.01000 - Train loss: 0.84236 - Test loss: 0.86695\n",
      "Epoch 5676 - lr: 0.01000 - Train loss: 0.86478 - Test loss: 0.86420\n",
      "Epoch 5677 - lr: 0.01000 - Train loss: 0.94157 - Test loss: 0.88146\n",
      "Epoch 5678 - lr: 0.01000 - Train loss: 0.92886 - Test loss: 0.89568\n",
      "Epoch 5679 - lr: 0.01000 - Train loss: 0.84363 - Test loss: 0.87392\n",
      "Epoch 5680 - lr: 0.01000 - Train loss: 0.84001 - Test loss: 0.87059\n",
      "Epoch 5681 - lr: 0.01000 - Train loss: 0.84063 - Test loss: 0.87008\n",
      "Epoch 5682 - lr: 0.01000 - Train loss: 0.86428 - Test loss: 0.86758\n",
      "Epoch 5683 - lr: 0.01000 - Train loss: 0.95290 - Test loss: 0.88384\n",
      "Epoch 5684 - lr: 0.01000 - Train loss: 0.92970 - Test loss: 0.89923\n",
      "Epoch 5685 - lr: 0.01000 - Train loss: 0.84484 - Test loss: 0.87611\n",
      "Epoch 5686 - lr: 0.01000 - Train loss: 0.86552 - Test loss: 0.87165\n",
      "Epoch 5687 - lr: 0.01000 - Train loss: 0.94691 - Test loss: 0.88301\n",
      "Epoch 5688 - lr: 0.01000 - Train loss: 0.93526 - Test loss: 0.89071\n",
      "Epoch 5689 - lr: 0.01000 - Train loss: 0.92721 - Test loss: 0.89323\n",
      "Epoch 5690 - lr: 0.01000 - Train loss: 0.91333 - Test loss: 0.88221\n",
      "Epoch 5691 - lr: 0.01000 - Train loss: 0.97273 - Test loss: 0.89830\n",
      "Epoch 5692 - lr: 0.01000 - Train loss: 0.92105 - Test loss: 0.89656\n",
      "Epoch 5693 - lr: 0.01000 - Train loss: 0.87546 - Test loss: 0.88134\n",
      "Epoch 5694 - lr: 0.01000 - Train loss: 0.87946 - Test loss: 0.87235\n",
      "Epoch 5695 - lr: 0.01000 - Train loss: 0.95968 - Test loss: 0.88732\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5696 - lr: 0.01000 - Train loss: 0.92543 - Test loss: 0.89422\n",
      "Epoch 5697 - lr: 0.01000 - Train loss: 0.88683 - Test loss: 0.90781\n",
      "Epoch 5698 - lr: 0.01000 - Train loss: 0.89338 - Test loss: 0.89406\n",
      "Epoch 5699 - lr: 0.01000 - Train loss: 0.92506 - Test loss: 0.89489\n",
      "Epoch 5700 - lr: 0.01000 - Train loss: 0.93303 - Test loss: 0.90460\n",
      "Epoch 5701 - lr: 0.01000 - Train loss: 0.90494 - Test loss: 0.90382\n",
      "Epoch 5702 - lr: 0.01000 - Train loss: 0.87193 - Test loss: 0.87360\n",
      "Epoch 5703 - lr: 0.01000 - Train loss: 0.93317 - Test loss: 0.88802\n",
      "Epoch 5704 - lr: 0.01000 - Train loss: 0.92035 - Test loss: 0.89865\n",
      "Epoch 5705 - lr: 0.01000 - Train loss: 0.87550 - Test loss: 0.87318\n",
      "Epoch 5706 - lr: 0.01000 - Train loss: 0.93678 - Test loss: 0.88795\n",
      "Epoch 5707 - lr: 0.01000 - Train loss: 0.92959 - Test loss: 0.89223\n",
      "Epoch 5708 - lr: 0.01000 - Train loss: 0.92117 - Test loss: 0.90631\n",
      "Epoch 5709 - lr: 0.01000 - Train loss: 0.92864 - Test loss: 0.90431\n",
      "Epoch 5710 - lr: 0.01000 - Train loss: 0.84752 - Test loss: 0.87672\n",
      "Epoch 5711 - lr: 0.01000 - Train loss: 0.85870 - Test loss: 0.87069\n",
      "Epoch 5712 - lr: 0.01000 - Train loss: 0.94213 - Test loss: 0.88643\n",
      "Epoch 5713 - lr: 0.01000 - Train loss: 0.92473 - Test loss: 0.89768\n",
      "Epoch 5714 - lr: 0.01000 - Train loss: 0.87645 - Test loss: 0.87368\n",
      "Epoch 5715 - lr: 0.01000 - Train loss: 0.95186 - Test loss: 0.88747\n",
      "Epoch 5716 - lr: 0.01000 - Train loss: 0.92738 - Test loss: 0.89347\n",
      "Epoch 5717 - lr: 0.01000 - Train loss: 0.89760 - Test loss: 0.89475\n",
      "Epoch 5718 - lr: 0.01000 - Train loss: 0.92493 - Test loss: 0.89469\n",
      "Epoch 5719 - lr: 0.01000 - Train loss: 0.91452 - Test loss: 0.90428\n",
      "Epoch 5720 - lr: 0.01000 - Train loss: 0.93318 - Test loss: 0.90664\n",
      "Epoch 5721 - lr: 0.01000 - Train loss: 0.93931 - Test loss: 0.88490\n",
      "Epoch 5722 - lr: 0.01000 - Train loss: 0.96918 - Test loss: 0.90582\n",
      "Epoch 5723 - lr: 0.01000 - Train loss: 0.96825 - Test loss: 0.88729\n",
      "Epoch 5724 - lr: 0.01000 - Train loss: 1.00309 - Test loss: 0.88990\n",
      "Epoch 5725 - lr: 0.01000 - Train loss: 0.92572 - Test loss: 0.89326\n",
      "Epoch 5726 - lr: 0.01000 - Train loss: 0.96934 - Test loss: 0.87586\n",
      "Epoch 5727 - lr: 0.01000 - Train loss: 0.98748 - Test loss: 0.89812\n",
      "Epoch 5728 - lr: 0.01000 - Train loss: 0.94161 - Test loss: 0.89325\n",
      "Epoch 5729 - lr: 0.01000 - Train loss: 0.96274 - Test loss: 0.87847\n",
      "Epoch 5730 - lr: 0.01000 - Train loss: 0.97396 - Test loss: 0.87513\n",
      "Epoch 5731 - lr: 0.01000 - Train loss: 0.96826 - Test loss: 0.88984\n",
      "Epoch 5732 - lr: 0.01000 - Train loss: 0.93220 - Test loss: 0.89099\n",
      "Epoch 5733 - lr: 0.01000 - Train loss: 0.98694 - Test loss: 0.90758\n",
      "Epoch 5734 - lr: 0.01000 - Train loss: 0.92945 - Test loss: 0.88034\n",
      "Epoch 5735 - lr: 0.01000 - Train loss: 0.90479 - Test loss: 0.87729\n",
      "Epoch 5736 - lr: 0.01000 - Train loss: 0.88076 - Test loss: 0.87907\n",
      "Epoch 5737 - lr: 0.01000 - Train loss: 0.92893 - Test loss: 0.91035\n",
      "Epoch 5738 - lr: 0.01000 - Train loss: 0.92811 - Test loss: 0.90510\n",
      "Epoch 5739 - lr: 0.01000 - Train loss: 0.92132 - Test loss: 0.91737\n",
      "Epoch 5740 - lr: 0.01000 - Train loss: 0.92360 - Test loss: 0.90893\n",
      "Epoch 5741 - lr: 0.01000 - Train loss: 0.87244 - Test loss: 0.88296\n",
      "Epoch 5742 - lr: 0.01000 - Train loss: 0.96360 - Test loss: 0.89472\n",
      "Epoch 5743 - lr: 0.01000 - Train loss: 0.89265 - Test loss: 0.91335\n",
      "Epoch 5744 - lr: 0.01000 - Train loss: 0.92391 - Test loss: 0.91683\n",
      "Epoch 5745 - lr: 0.01000 - Train loss: 0.92380 - Test loss: 0.90518\n",
      "Epoch 5746 - lr: 0.01000 - Train loss: 0.87778 - Test loss: 0.88579\n",
      "Epoch 5747 - lr: 0.01000 - Train loss: 0.89702 - Test loss: 0.87865\n",
      "Epoch 5748 - lr: 0.01000 - Train loss: 0.96291 - Test loss: 0.89243\n",
      "Epoch 5749 - lr: 0.01000 - Train loss: 0.91746 - Test loss: 0.89008\n",
      "Epoch 5750 - lr: 0.01000 - Train loss: 1.04435 - Test loss: 0.88628\n",
      "Epoch 5751 - lr: 0.01000 - Train loss: 0.98630 - Test loss: 0.88241\n",
      "Epoch 5752 - lr: 0.01000 - Train loss: 0.98108 - Test loss: 0.88967\n",
      "Epoch 5753 - lr: 0.01000 - Train loss: 0.93980 - Test loss: 0.89119\n",
      "Epoch 5754 - lr: 0.01000 - Train loss: 0.88560 - Test loss: 0.89329\n",
      "Epoch 5755 - lr: 0.01000 - Train loss: 0.90116 - Test loss: 0.90600\n",
      "Epoch 5756 - lr: 0.01000 - Train loss: 0.92988 - Test loss: 0.90996\n",
      "Epoch 5757 - lr: 0.01000 - Train loss: 0.92747 - Test loss: 0.92265\n",
      "Epoch 5758 - lr: 0.01000 - Train loss: 0.93288 - Test loss: 0.90806\n",
      "Epoch 5759 - lr: 0.01000 - Train loss: 0.89192 - Test loss: 0.88869\n",
      "Epoch 5760 - lr: 0.01000 - Train loss: 0.92360 - Test loss: 0.88892\n",
      "Epoch 5761 - lr: 0.01000 - Train loss: 0.92058 - Test loss: 0.89049\n",
      "Epoch 5762 - lr: 0.01000 - Train loss: 0.97753 - Test loss: 0.89719\n",
      "Epoch 5763 - lr: 0.01000 - Train loss: 0.99199 - Test loss: 0.89591\n",
      "Epoch 5764 - lr: 0.01000 - Train loss: 0.92490 - Test loss: 0.90030\n",
      "Epoch 5765 - lr: 0.01000 - Train loss: 0.84529 - Test loss: 0.90016\n",
      "Epoch 5766 - lr: 0.01000 - Train loss: 0.92455 - Test loss: 0.89799\n",
      "Epoch 5767 - lr: 0.01000 - Train loss: 0.88864 - Test loss: 0.89725\n",
      "Epoch 5768 - lr: 0.01000 - Train loss: 0.84217 - Test loss: 0.90081\n",
      "Epoch 5769 - lr: 0.01000 - Train loss: 0.92276 - Test loss: 0.89972\n",
      "Epoch 5770 - lr: 0.01000 - Train loss: 0.92498 - Test loss: 0.92703\n",
      "Epoch 5771 - lr: 0.01000 - Train loss: 0.88795 - Test loss: 0.92254\n",
      "Epoch 5772 - lr: 0.01000 - Train loss: 0.92634 - Test loss: 0.93375\n",
      "Epoch 5773 - lr: 0.01000 - Train loss: 0.92107 - Test loss: 0.92763\n",
      "Epoch 5774 - lr: 0.01000 - Train loss: 0.87142 - Test loss: 0.89935\n",
      "Epoch 5775 - lr: 0.01000 - Train loss: 0.92970 - Test loss: 0.92412\n",
      "Epoch 5776 - lr: 0.01000 - Train loss: 0.95024 - Test loss: 0.90247\n",
      "Epoch 5777 - lr: 0.01000 - Train loss: 0.92885 - Test loss: 0.89744\n",
      "Epoch 5778 - lr: 0.01000 - Train loss: 0.93021 - Test loss: 0.92570\n",
      "Epoch 5779 - lr: 0.01000 - Train loss: 0.91049 - Test loss: 0.93347\n",
      "Epoch 5780 - lr: 0.01000 - Train loss: 0.93319 - Test loss: 0.93055\n",
      "Epoch 5781 - lr: 0.01000 - Train loss: 0.91974 - Test loss: 0.89813\n",
      "Epoch 5782 - lr: 0.01000 - Train loss: 0.90340 - Test loss: 0.89403\n",
      "Epoch 5783 - lr: 0.01000 - Train loss: 0.94033 - Test loss: 0.92054\n",
      "Epoch 5784 - lr: 0.01000 - Train loss: 0.92153 - Test loss: 0.92412\n",
      "Epoch 5785 - lr: 0.01000 - Train loss: 0.85606 - Test loss: 0.89778\n",
      "Epoch 5786 - lr: 0.01000 - Train loss: 0.92893 - Test loss: 0.92224\n",
      "Epoch 5787 - lr: 0.01000 - Train loss: 0.93340 - Test loss: 0.92642\n",
      "Epoch 5788 - lr: 0.01000 - Train loss: 0.92361 - Test loss: 0.92160\n",
      "Epoch 5789 - lr: 0.01000 - Train loss: 0.84779 - Test loss: 0.89537\n",
      "Epoch 5790 - lr: 0.01000 - Train loss: 0.93113 - Test loss: 0.90005\n",
      "Epoch 5791 - lr: 0.01000 - Train loss: 0.98708 - Test loss: 0.89380\n",
      "Epoch 5792 - lr: 0.01000 - Train loss: 0.93741 - Test loss: 0.89849\n",
      "Epoch 5793 - lr: 0.01000 - Train loss: 0.88653 - Test loss: 0.89563\n",
      "Epoch 5794 - lr: 0.01000 - Train loss: 0.91501 - Test loss: 0.90149\n",
      "Epoch 5795 - lr: 0.01000 - Train loss: 0.98408 - Test loss: 0.91769\n",
      "Epoch 5796 - lr: 0.01000 - Train loss: 0.88903 - Test loss: 0.90119\n",
      "Epoch 5797 - lr: 0.01000 - Train loss: 0.92568 - Test loss: 0.92214\n",
      "Epoch 5798 - lr: 0.01000 - Train loss: 0.83924 - Test loss: 0.90120\n",
      "Epoch 5799 - lr: 0.01000 - Train loss: 0.92773 - Test loss: 0.89524\n",
      "Epoch 5800 - lr: 0.01000 - Train loss: 0.92515 - Test loss: 0.89940\n",
      "Epoch 5801 - lr: 0.01000 - Train loss: 0.89755 - Test loss: 0.89735\n",
      "Epoch 5802 - lr: 0.01000 - Train loss: 0.92404 - Test loss: 0.90506\n",
      "Epoch 5803 - lr: 0.01000 - Train loss: 0.98001 - Test loss: 0.92194\n",
      "Epoch 5804 - lr: 0.01000 - Train loss: 0.89574 - Test loss: 0.90700\n",
      "Epoch 5805 - lr: 0.01000 - Train loss: 0.92751 - Test loss: 0.92735\n",
      "Epoch 5806 - lr: 0.01000 - Train loss: 0.87467 - Test loss: 0.90265\n",
      "Epoch 5807 - lr: 0.01000 - Train loss: 0.92726 - Test loss: 0.92491\n",
      "Epoch 5808 - lr: 0.01000 - Train loss: 0.84708 - Test loss: 0.90352\n",
      "Epoch 5809 - lr: 0.01000 - Train loss: 0.91070 - Test loss: 0.89679\n",
      "Epoch 5810 - lr: 0.01000 - Train loss: 0.90787 - Test loss: 0.89776\n",
      "Epoch 5811 - lr: 0.01000 - Train loss: 0.83509 - Test loss: 0.89963\n",
      "Epoch 5812 - lr: 0.01000 - Train loss: 0.93472 - Test loss: 0.89863\n",
      "Epoch 5813 - lr: 0.01000 - Train loss: 0.99597 - Test loss: 0.90082\n",
      "Epoch 5814 - lr: 0.01000 - Train loss: 0.99779 - Test loss: 0.90394\n",
      "Epoch 5815 - lr: 0.01000 - Train loss: 0.92996 - Test loss: 0.90814\n",
      "Epoch 5816 - lr: 0.01000 - Train loss: 0.90980 - Test loss: 0.93511\n",
      "Epoch 5817 - lr: 0.01000 - Train loss: 0.88355 - Test loss: 0.93320\n",
      "Epoch 5818 - lr: 0.01000 - Train loss: 0.83921 - Test loss: 0.90858\n",
      "Epoch 5819 - lr: 0.01000 - Train loss: 0.85473 - Test loss: 0.90443\n",
      "Epoch 5820 - lr: 0.01000 - Train loss: 0.91583 - Test loss: 0.90200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5821 - lr: 0.01000 - Train loss: 0.91534 - Test loss: 0.90255\n",
      "Epoch 5822 - lr: 0.01000 - Train loss: 0.95915 - Test loss: 0.92481\n",
      "Epoch 5823 - lr: 0.01000 - Train loss: 0.83803 - Test loss: 0.90766\n",
      "Epoch 5824 - lr: 0.01000 - Train loss: 0.85039 - Test loss: 0.90511\n",
      "Epoch 5825 - lr: 0.01000 - Train loss: 0.85150 - Test loss: 0.90430\n",
      "Epoch 5826 - lr: 0.01000 - Train loss: 0.85197 - Test loss: 0.90374\n",
      "Epoch 5827 - lr: 0.01000 - Train loss: 0.88781 - Test loss: 0.90364\n",
      "Epoch 5828 - lr: 0.01000 - Train loss: 0.92747 - Test loss: 0.93044\n",
      "Epoch 5829 - lr: 0.01000 - Train loss: 0.92512 - Test loss: 0.93606\n",
      "Epoch 5830 - lr: 0.01000 - Train loss: 0.91976 - Test loss: 0.93662\n",
      "Epoch 5831 - lr: 0.01000 - Train loss: 0.92688 - Test loss: 0.93069\n",
      "Epoch 5832 - lr: 0.01000 - Train loss: 0.86454 - Test loss: 0.90097\n",
      "Epoch 5833 - lr: 0.01000 - Train loss: 0.92355 - Test loss: 0.91968\n",
      "Epoch 5834 - lr: 0.01000 - Train loss: 0.83625 - Test loss: 0.89929\n",
      "Epoch 5835 - lr: 0.01000 - Train loss: 0.89977 - Test loss: 0.90506\n",
      "Epoch 5836 - lr: 0.01000 - Train loss: 0.91862 - Test loss: 0.89393\n",
      "Epoch 5837 - lr: 0.01000 - Train loss: 0.89885 - Test loss: 0.90939\n",
      "Epoch 5838 - lr: 0.01000 - Train loss: 0.83873 - Test loss: 0.89754\n",
      "Epoch 5839 - lr: 0.01000 - Train loss: 0.93676 - Test loss: 0.91987\n",
      "Epoch 5840 - lr: 0.01000 - Train loss: 0.91082 - Test loss: 0.92950\n",
      "Epoch 5841 - lr: 0.01000 - Train loss: 0.92961 - Test loss: 0.92918\n",
      "Epoch 5842 - lr: 0.01000 - Train loss: 0.90222 - Test loss: 0.92858\n",
      "Epoch 5843 - lr: 0.01000 - Train loss: 0.92688 - Test loss: 0.92409\n",
      "Epoch 5844 - lr: 0.01000 - Train loss: 0.91321 - Test loss: 0.89118\n",
      "Epoch 5845 - lr: 0.01000 - Train loss: 0.97643 - Test loss: 0.89441\n",
      "Epoch 5846 - lr: 0.01000 - Train loss: 0.97517 - Test loss: 0.92007\n",
      "Epoch 5847 - lr: 0.01000 - Train loss: 0.92694 - Test loss: 0.92345\n",
      "Epoch 5848 - lr: 0.01000 - Train loss: 0.87910 - Test loss: 0.89532\n",
      "Epoch 5849 - lr: 0.01000 - Train loss: 0.91720 - Test loss: 0.89696\n",
      "Epoch 5850 - lr: 0.01000 - Train loss: 0.93831 - Test loss: 0.89045\n",
      "Epoch 5851 - lr: 0.01000 - Train loss: 0.91856 - Test loss: 0.89596\n",
      "Epoch 5852 - lr: 0.01000 - Train loss: 0.97755 - Test loss: 0.91195\n",
      "Epoch 5853 - lr: 0.01000 - Train loss: 0.92533 - Test loss: 0.92280\n",
      "Epoch 5854 - lr: 0.01000 - Train loss: 0.92575 - Test loss: 0.92321\n",
      "Epoch 5855 - lr: 0.01000 - Train loss: 0.92677 - Test loss: 0.92521\n",
      "Epoch 5856 - lr: 0.01000 - Train loss: 0.91986 - Test loss: 0.92468\n",
      "Epoch 5857 - lr: 0.01000 - Train loss: 0.92526 - Test loss: 0.92192\n",
      "Epoch 5858 - lr: 0.01000 - Train loss: 0.93025 - Test loss: 0.89989\n",
      "Epoch 5859 - lr: 0.01000 - Train loss: 0.90736 - Test loss: 0.88498\n",
      "Epoch 5860 - lr: 0.01000 - Train loss: 0.90709 - Test loss: 0.88989\n",
      "Epoch 5861 - lr: 0.01000 - Train loss: 1.03227 - Test loss: 0.88811\n",
      "Epoch 5862 - lr: 0.01000 - Train loss: 0.98622 - Test loss: 0.88717\n",
      "Epoch 5863 - lr: 0.01000 - Train loss: 0.87120 - Test loss: 0.89287\n",
      "Epoch 5864 - lr: 0.01000 - Train loss: 0.90312 - Test loss: 0.90234\n",
      "Epoch 5865 - lr: 0.01000 - Train loss: 0.89313 - Test loss: 0.91456\n",
      "Epoch 5866 - lr: 0.01000 - Train loss: 0.92420 - Test loss: 0.92347\n",
      "Epoch 5867 - lr: 0.01000 - Train loss: 0.90919 - Test loss: 0.89974\n",
      "Epoch 5868 - lr: 0.01000 - Train loss: 0.97403 - Test loss: 0.91572\n",
      "Epoch 5869 - lr: 0.01000 - Train loss: 0.90772 - Test loss: 0.90553\n",
      "Epoch 5870 - lr: 0.01000 - Train loss: 0.92490 - Test loss: 0.91968\n",
      "Epoch 5871 - lr: 0.01000 - Train loss: 0.87833 - Test loss: 0.91719\n",
      "Epoch 5872 - lr: 0.01000 - Train loss: 0.89615 - Test loss: 0.91210\n",
      "Epoch 5873 - lr: 0.01000 - Train loss: 0.92306 - Test loss: 0.91854\n",
      "Epoch 5874 - lr: 0.01000 - Train loss: 0.90170 - Test loss: 0.91978\n",
      "Epoch 5875 - lr: 0.01000 - Train loss: 0.92153 - Test loss: 0.91875\n",
      "Epoch 5876 - lr: 0.01000 - Train loss: 0.92229 - Test loss: 0.91698\n",
      "Epoch 5877 - lr: 0.01000 - Train loss: 0.91263 - Test loss: 0.91658\n",
      "Epoch 5878 - lr: 0.01000 - Train loss: 0.91682 - Test loss: 0.91589\n",
      "Epoch 5879 - lr: 0.01000 - Train loss: 0.91151 - Test loss: 0.91490\n",
      "Epoch 5880 - lr: 0.01000 - Train loss: 0.90351 - Test loss: 0.91338\n",
      "Epoch 5881 - lr: 0.01000 - Train loss: 0.92034 - Test loss: 0.91197\n",
      "Epoch 5882 - lr: 0.01000 - Train loss: 0.91045 - Test loss: 0.91130\n",
      "Epoch 5883 - lr: 0.01000 - Train loss: 0.91260 - Test loss: 0.89089\n",
      "Epoch 5884 - lr: 0.01000 - Train loss: 0.91134 - Test loss: 0.90481\n",
      "Epoch 5885 - lr: 0.01000 - Train loss: 0.92455 - Test loss: 0.88135\n",
      "Epoch 5886 - lr: 0.01000 - Train loss: 1.00971 - Test loss: 0.88371\n",
      "Epoch 5887 - lr: 0.01000 - Train loss: 0.85675 - Test loss: 0.87322\n",
      "Epoch 5888 - lr: 0.01000 - Train loss: 0.87848 - Test loss: 0.87257\n",
      "Epoch 5889 - lr: 0.01000 - Train loss: 0.88770 - Test loss: 0.87230\n",
      "Epoch 5890 - lr: 0.01000 - Train loss: 0.92533 - Test loss: 0.89996\n",
      "Epoch 5891 - lr: 0.01000 - Train loss: 0.89805 - Test loss: 0.89039\n",
      "Epoch 5892 - lr: 0.01000 - Train loss: 0.92440 - Test loss: 0.90364\n",
      "Epoch 5893 - lr: 0.01000 - Train loss: 0.91578 - Test loss: 0.89205\n",
      "Epoch 5894 - lr: 0.01000 - Train loss: 0.92547 - Test loss: 0.90423\n",
      "Epoch 5895 - lr: 0.01000 - Train loss: 0.91134 - Test loss: 0.90774\n",
      "Epoch 5896 - lr: 0.01000 - Train loss: 0.92777 - Test loss: 0.88619\n",
      "Epoch 5897 - lr: 0.01000 - Train loss: 0.88545 - Test loss: 0.87041\n",
      "Epoch 5898 - lr: 0.01000 - Train loss: 0.99073 - Test loss: 0.89605\n",
      "Epoch 5899 - lr: 0.01000 - Train loss: 0.96879 - Test loss: 0.87367\n",
      "Epoch 5900 - lr: 0.01000 - Train loss: 0.98777 - Test loss: 0.90036\n",
      "Epoch 5901 - lr: 0.01000 - Train loss: 0.92603 - Test loss: 0.90533\n",
      "Epoch 5902 - lr: 0.01000 - Train loss: 0.92924 - Test loss: 0.90483\n",
      "Epoch 5903 - lr: 0.01000 - Train loss: 0.91116 - Test loss: 0.89005\n",
      "Epoch 5904 - lr: 0.01000 - Train loss: 0.92362 - Test loss: 0.90257\n",
      "Epoch 5905 - lr: 0.01000 - Train loss: 0.90687 - Test loss: 0.87978\n",
      "Epoch 5906 - lr: 0.01000 - Train loss: 0.99342 - Test loss: 0.87785\n",
      "Epoch 5907 - lr: 0.01000 - Train loss: 0.96261 - Test loss: 0.87307\n",
      "Epoch 5908 - lr: 0.01000 - Train loss: 0.92517 - Test loss: 0.88585\n",
      "Epoch 5909 - lr: 0.01000 - Train loss: 0.92912 - Test loss: 0.88724\n",
      "Epoch 5910 - lr: 0.01000 - Train loss: 0.86496 - Test loss: 0.89405\n",
      "Epoch 5911 - lr: 0.01000 - Train loss: 0.91234 - Test loss: 0.88462\n",
      "Epoch 5912 - lr: 0.01000 - Train loss: 0.91567 - Test loss: 0.90475\n",
      "Epoch 5913 - lr: 0.01000 - Train loss: 0.89899 - Test loss: 0.90851\n",
      "Epoch 5914 - lr: 0.01000 - Train loss: 0.92532 - Test loss: 0.90597\n",
      "Epoch 5915 - lr: 0.01000 - Train loss: 0.91582 - Test loss: 0.90432\n",
      "Epoch 5916 - lr: 0.01000 - Train loss: 0.89333 - Test loss: 0.90569\n",
      "Epoch 5917 - lr: 0.01000 - Train loss: 0.91614 - Test loss: 0.88134\n",
      "Epoch 5918 - lr: 0.01000 - Train loss: 0.99109 - Test loss: 0.89734\n",
      "Epoch 5919 - lr: 0.01000 - Train loss: 0.90532 - Test loss: 0.87129\n",
      "Epoch 5920 - lr: 0.01000 - Train loss: 0.90245 - Test loss: 0.89887\n",
      "Epoch 5921 - lr: 0.01000 - Train loss: 0.92586 - Test loss: 0.90288\n",
      "Epoch 5922 - lr: 0.01000 - Train loss: 0.91918 - Test loss: 0.90269\n",
      "Epoch 5923 - lr: 0.01000 - Train loss: 0.92369 - Test loss: 0.87974\n",
      "Epoch 5924 - lr: 0.01000 - Train loss: 0.99431 - Test loss: 0.88501\n",
      "Epoch 5925 - lr: 0.01000 - Train loss: 0.92762 - Test loss: 0.90001\n",
      "Epoch 5926 - lr: 0.01000 - Train loss: 0.92752 - Test loss: 0.88702\n",
      "Epoch 5927 - lr: 0.01000 - Train loss: 0.88874 - Test loss: 0.89000\n",
      "Epoch 5928 - lr: 0.01000 - Train loss: 0.92461 - Test loss: 0.89990\n",
      "Epoch 5929 - lr: 0.01000 - Train loss: 0.93423 - Test loss: 0.87809\n",
      "Epoch 5930 - lr: 0.01000 - Train loss: 0.99153 - Test loss: 0.89638\n",
      "Epoch 5931 - lr: 0.01000 - Train loss: 0.90935 - Test loss: 0.87118\n",
      "Epoch 5932 - lr: 0.01000 - Train loss: 0.90035 - Test loss: 0.89827\n",
      "Epoch 5933 - lr: 0.01000 - Train loss: 0.92295 - Test loss: 0.90349\n",
      "Epoch 5934 - lr: 0.01000 - Train loss: 0.89088 - Test loss: 0.90552\n",
      "Epoch 5935 - lr: 0.01000 - Train loss: 0.91691 - Test loss: 0.90604\n",
      "Epoch 5936 - lr: 0.01000 - Train loss: 0.93928 - Test loss: 0.88643\n",
      "Epoch 5937 - lr: 0.01000 - Train loss: 0.90678 - Test loss: 0.86835\n",
      "Epoch 5938 - lr: 0.01000 - Train loss: 0.95001 - Test loss: 0.87726\n",
      "Epoch 5939 - lr: 0.01000 - Train loss: 1.00752 - Test loss: 0.87104\n",
      "Epoch 5940 - lr: 0.01000 - Train loss: 0.92464 - Test loss: 0.89935\n",
      "Epoch 5941 - lr: 0.01000 - Train loss: 0.89812 - Test loss: 0.89569\n",
      "Epoch 5942 - lr: 0.01000 - Train loss: 0.87542 - Test loss: 0.87599\n",
      "Epoch 5943 - lr: 0.01000 - Train loss: 0.91894 - Test loss: 0.86673\n",
      "Epoch 5944 - lr: 0.01000 - Train loss: 1.01374 - Test loss: 0.89634\n",
      "Epoch 5945 - lr: 0.01000 - Train loss: 0.89452 - Test loss: 0.90479\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5946 - lr: 0.01000 - Train loss: 0.89917 - Test loss: 0.89258\n",
      "Epoch 5947 - lr: 0.01000 - Train loss: 0.90282 - Test loss: 0.90425\n",
      "Epoch 5948 - lr: 0.01000 - Train loss: 0.91067 - Test loss: 0.90679\n",
      "Epoch 5949 - lr: 0.01000 - Train loss: 0.95446 - Test loss: 0.88482\n",
      "Epoch 5950 - lr: 0.01000 - Train loss: 0.91379 - Test loss: 0.87508\n",
      "Epoch 5951 - lr: 0.01000 - Train loss: 0.96690 - Test loss: 0.89220\n",
      "Epoch 5952 - lr: 0.01000 - Train loss: 0.92498 - Test loss: 0.89612\n",
      "Epoch 5953 - lr: 0.01000 - Train loss: 0.84417 - Test loss: 0.87224\n",
      "Epoch 5954 - lr: 0.01000 - Train loss: 0.85025 - Test loss: 0.86689\n",
      "Epoch 5955 - lr: 0.01000 - Train loss: 0.88796 - Test loss: 0.88776\n",
      "Epoch 5956 - lr: 0.01000 - Train loss: 0.92033 - Test loss: 0.87001\n",
      "Epoch 5957 - lr: 0.01000 - Train loss: 0.93061 - Test loss: 0.88542\n",
      "Epoch 5958 - lr: 0.01000 - Train loss: 0.92462 - Test loss: 0.89959\n",
      "Epoch 5959 - lr: 0.01000 - Train loss: 0.93392 - Test loss: 0.87102\n",
      "Epoch 5960 - lr: 0.01000 - Train loss: 0.93741 - Test loss: 0.86888\n",
      "Epoch 5961 - lr: 0.01000 - Train loss: 0.89828 - Test loss: 0.89371\n",
      "Epoch 5962 - lr: 0.01000 - Train loss: 0.92853 - Test loss: 0.88346\n",
      "Epoch 5963 - lr: 0.01000 - Train loss: 0.98796 - Test loss: 0.89478\n",
      "Epoch 5964 - lr: 0.01000 - Train loss: 0.92058 - Test loss: 0.87658\n",
      "Epoch 5965 - lr: 0.01000 - Train loss: 0.93891 - Test loss: 0.88191\n",
      "Epoch 5966 - lr: 0.01000 - Train loss: 0.91928 - Test loss: 0.89636\n",
      "Epoch 5967 - lr: 0.01000 - Train loss: 0.85297 - Test loss: 0.88159\n",
      "Epoch 5968 - lr: 0.01000 - Train loss: 0.84379 - Test loss: 0.87703\n",
      "Epoch 5969 - lr: 0.01000 - Train loss: 0.97466 - Test loss: 0.88563\n",
      "Epoch 5970 - lr: 0.01000 - Train loss: 0.95571 - Test loss: 0.88120\n",
      "Epoch 5971 - lr: 0.01000 - Train loss: 1.00395 - Test loss: 0.87910\n",
      "Epoch 5972 - lr: 0.01000 - Train loss: 0.84523 - Test loss: 0.88176\n",
      "Epoch 5973 - lr: 0.01000 - Train loss: 0.93212 - Test loss: 0.89235\n",
      "Epoch 5974 - lr: 0.01000 - Train loss: 0.93924 - Test loss: 0.88639\n",
      "Epoch 5975 - lr: 0.01000 - Train loss: 0.92231 - Test loss: 0.89129\n",
      "Epoch 5976 - lr: 0.01000 - Train loss: 0.96772 - Test loss: 0.91188\n",
      "Epoch 5977 - lr: 0.01000 - Train loss: 0.93085 - Test loss: 0.92013\n",
      "Epoch 5978 - lr: 0.01000 - Train loss: 0.86987 - Test loss: 0.89605\n",
      "Epoch 5979 - lr: 0.01000 - Train loss: 0.89259 - Test loss: 0.88686\n",
      "Epoch 5980 - lr: 0.01000 - Train loss: 0.85249 - Test loss: 0.88872\n",
      "Epoch 5981 - lr: 0.01000 - Train loss: 0.93531 - Test loss: 0.88717\n",
      "Epoch 5982 - lr: 0.01000 - Train loss: 0.97972 - Test loss: 0.91191\n",
      "Epoch 5983 - lr: 0.01000 - Train loss: 0.92508 - Test loss: 0.91878\n",
      "Epoch 5984 - lr: 0.01000 - Train loss: 0.83782 - Test loss: 0.89460\n",
      "Epoch 5985 - lr: 0.01000 - Train loss: 0.90926 - Test loss: 0.89767\n",
      "Epoch 5986 - lr: 0.01000 - Train loss: 0.91954 - Test loss: 0.91129\n",
      "Epoch 5987 - lr: 0.01000 - Train loss: 0.83715 - Test loss: 0.89264\n",
      "Epoch 5988 - lr: 0.01000 - Train loss: 0.89718 - Test loss: 0.89878\n",
      "Epoch 5989 - lr: 0.01000 - Train loss: 0.91908 - Test loss: 0.88861\n",
      "Epoch 5990 - lr: 0.01000 - Train loss: 0.91662 - Test loss: 0.88857\n",
      "Epoch 5991 - lr: 0.01000 - Train loss: 0.93553 - Test loss: 0.91562\n",
      "Epoch 5992 - lr: 0.01000 - Train loss: 0.92332 - Test loss: 0.91988\n",
      "Epoch 5993 - lr: 0.01000 - Train loss: 0.83844 - Test loss: 0.89489\n",
      "Epoch 5994 - lr: 0.01000 - Train loss: 0.91279 - Test loss: 0.88948\n",
      "Epoch 5995 - lr: 0.01000 - Train loss: 0.92214 - Test loss: 0.89558\n",
      "Epoch 5996 - lr: 0.01000 - Train loss: 0.95322 - Test loss: 0.89060\n",
      "Epoch 5997 - lr: 0.01000 - Train loss: 0.91240 - Test loss: 0.89763\n",
      "Epoch 5998 - lr: 0.01000 - Train loss: 0.93281 - Test loss: 0.89383\n",
      "Epoch 5999 - lr: 0.01000 - Train loss: 0.92696 - Test loss: 0.91909\n",
      "Epoch 6000 - lr: 0.01000 - Train loss: 0.88217 - Test loss: 0.89737\n",
      "Epoch 6001 - lr: 0.01000 - Train loss: 0.90577 - Test loss: 0.92296\n",
      "Epoch 6002 - lr: 0.01000 - Train loss: 0.92390 - Test loss: 0.92381\n",
      "Epoch 6003 - lr: 0.01000 - Train loss: 0.92136 - Test loss: 0.89363\n",
      "Epoch 6004 - lr: 0.01000 - Train loss: 0.91294 - Test loss: 0.89422\n",
      "Epoch 6005 - lr: 0.01000 - Train loss: 0.85769 - Test loss: 0.89401\n",
      "Epoch 6006 - lr: 0.01000 - Train loss: 0.89385 - Test loss: 0.89432\n",
      "Epoch 6007 - lr: 0.01000 - Train loss: 0.92600 - Test loss: 0.91834\n",
      "Epoch 6008 - lr: 0.01000 - Train loss: 0.93304 - Test loss: 0.89666\n",
      "Epoch 6009 - lr: 0.01000 - Train loss: 0.91387 - Test loss: 0.89638\n",
      "Epoch 6010 - lr: 0.01000 - Train loss: 0.88584 - Test loss: 0.89707\n",
      "Epoch 6011 - lr: 0.01000 - Train loss: 0.92495 - Test loss: 0.92433\n",
      "Epoch 6012 - lr: 0.01000 - Train loss: 0.92364 - Test loss: 0.93003\n",
      "Epoch 6013 - lr: 0.01000 - Train loss: 0.90252 - Test loss: 0.93063\n",
      "Epoch 6014 - lr: 0.01000 - Train loss: 0.92561 - Test loss: 0.92701\n",
      "Epoch 6015 - lr: 0.01000 - Train loss: 0.86949 - Test loss: 0.89911\n",
      "Epoch 6016 - lr: 0.01000 - Train loss: 0.91291 - Test loss: 0.89073\n",
      "Epoch 6017 - lr: 0.01000 - Train loss: 0.89797 - Test loss: 0.88955\n",
      "Epoch 6018 - lr: 0.01000 - Train loss: 0.90537 - Test loss: 0.91913\n",
      "Epoch 6019 - lr: 0.01000 - Train loss: 0.92207 - Test loss: 0.92580\n",
      "Epoch 6020 - lr: 0.01000 - Train loss: 0.91441 - Test loss: 0.92701\n",
      "Epoch 6021 - lr: 0.01000 - Train loss: 0.92564 - Test loss: 0.92554\n",
      "Epoch 6022 - lr: 0.01000 - Train loss: 0.92297 - Test loss: 0.92332\n",
      "Epoch 6023 - lr: 0.01000 - Train loss: 0.86954 - Test loss: 0.91084\n",
      "Epoch 6024 - lr: 0.01000 - Train loss: 0.86284 - Test loss: 0.88768\n",
      "Epoch 6025 - lr: 0.01000 - Train loss: 0.92693 - Test loss: 0.88955\n",
      "Epoch 6026 - lr: 0.01000 - Train loss: 0.94784 - Test loss: 0.88706\n",
      "Epoch 6027 - lr: 0.01000 - Train loss: 0.93168 - Test loss: 0.89453\n",
      "Epoch 6028 - lr: 0.01000 - Train loss: 0.89208 - Test loss: 0.90874\n",
      "Epoch 6029 - lr: 0.01000 - Train loss: 0.92274 - Test loss: 0.91872\n",
      "Epoch 6030 - lr: 0.01000 - Train loss: 0.86977 - Test loss: 0.90822\n",
      "Epoch 6031 - lr: 0.01000 - Train loss: 0.86771 - Test loss: 0.88985\n",
      "Epoch 6032 - lr: 0.01000 - Train loss: 0.86497 - Test loss: 0.89989\n",
      "Epoch 6033 - lr: 0.01000 - Train loss: 0.87740 - Test loss: 0.90610\n",
      "Epoch 6034 - lr: 0.01000 - Train loss: 0.89822 - Test loss: 0.91576\n",
      "Epoch 6035 - lr: 0.01000 - Train loss: 0.92287 - Test loss: 0.91626\n",
      "Epoch 6036 - lr: 0.01000 - Train loss: 0.91370 - Test loss: 0.91629\n",
      "Epoch 6037 - lr: 0.01000 - Train loss: 0.91729 - Test loss: 0.91578\n",
      "Epoch 6038 - lr: 0.01000 - Train loss: 0.90811 - Test loss: 0.91481\n",
      "Epoch 6039 - lr: 0.01000 - Train loss: 0.90433 - Test loss: 0.91337\n",
      "Epoch 6040 - lr: 0.01000 - Train loss: 0.91558 - Test loss: 0.91228\n",
      "Epoch 6041 - lr: 0.01000 - Train loss: 0.92975 - Test loss: 0.88796\n",
      "Epoch 6042 - lr: 0.01000 - Train loss: 0.94582 - Test loss: 0.89189\n",
      "Epoch 6043 - lr: 0.01000 - Train loss: 0.93007 - Test loss: 0.88232\n",
      "Epoch 6044 - lr: 0.01000 - Train loss: 0.96661 - Test loss: 0.90321\n",
      "Epoch 6045 - lr: 0.01000 - Train loss: 0.90003 - Test loss: 0.90780\n",
      "Epoch 6046 - lr: 0.01000 - Train loss: 0.92747 - Test loss: 0.90489\n",
      "Epoch 6047 - lr: 0.01000 - Train loss: 0.87796 - Test loss: 0.87715\n",
      "Epoch 6048 - lr: 0.01000 - Train loss: 0.89762 - Test loss: 0.88797\n",
      "Epoch 6049 - lr: 0.01000 - Train loss: 0.88819 - Test loss: 0.90007\n",
      "Epoch 6050 - lr: 0.01000 - Train loss: 0.91975 - Test loss: 0.87031\n",
      "Epoch 6051 - lr: 0.01000 - Train loss: 0.92531 - Test loss: 0.89483\n",
      "Epoch 6052 - lr: 0.01000 - Train loss: 0.86682 - Test loss: 0.87278\n",
      "Epoch 6053 - lr: 0.01000 - Train loss: 0.93784 - Test loss: 0.89126\n",
      "Epoch 6054 - lr: 0.01000 - Train loss: 0.90514 - Test loss: 0.89701\n",
      "Epoch 6055 - lr: 0.01000 - Train loss: 0.92502 - Test loss: 0.88635\n",
      "Epoch 6056 - lr: 0.01000 - Train loss: 0.90543 - Test loss: 0.89556\n",
      "Epoch 6057 - lr: 0.01000 - Train loss: 0.92313 - Test loss: 0.87991\n",
      "Epoch 6058 - lr: 0.01000 - Train loss: 0.97114 - Test loss: 0.87217\n",
      "Epoch 6059 - lr: 0.01000 - Train loss: 0.92448 - Test loss: 0.89640\n",
      "Epoch 6060 - lr: 0.01000 - Train loss: 0.90248 - Test loss: 0.89879\n",
      "Epoch 6061 - lr: 0.01000 - Train loss: 0.93142 - Test loss: 0.88490\n",
      "Epoch 6062 - lr: 0.01000 - Train loss: 0.99742 - Test loss: 0.87892\n",
      "Epoch 6063 - lr: 0.01000 - Train loss: 0.92353 - Test loss: 0.87430\n",
      "Epoch 6064 - lr: 0.01000 - Train loss: 0.90013 - Test loss: 0.89554\n",
      "Epoch 6065 - lr: 0.01000 - Train loss: 0.92825 - Test loss: 0.88381\n",
      "Epoch 6066 - lr: 0.01000 - Train loss: 0.99133 - Test loss: 0.87204\n",
      "Epoch 6067 - lr: 0.01000 - Train loss: 0.89815 - Test loss: 0.89626\n",
      "Epoch 6068 - lr: 0.01000 - Train loss: 0.88521 - Test loss: 0.89688\n",
      "Epoch 6069 - lr: 0.01000 - Train loss: 0.91954 - Test loss: 0.90945\n",
      "Epoch 6070 - lr: 0.01000 - Train loss: 0.92980 - Test loss: 0.90931\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6071 - lr: 0.01000 - Train loss: 0.97172 - Test loss: 0.87988\n",
      "Epoch 6072 - lr: 0.01000 - Train loss: 0.94255 - Test loss: 0.87743\n",
      "Epoch 6073 - lr: 0.01000 - Train loss: 0.92214 - Test loss: 0.89551\n",
      "Epoch 6074 - lr: 0.01000 - Train loss: 0.92767 - Test loss: 0.88631\n",
      "Epoch 6075 - lr: 0.01000 - Train loss: 0.92058 - Test loss: 0.89800\n",
      "Epoch 6076 - lr: 0.01000 - Train loss: 0.95339 - Test loss: 0.88056\n",
      "Epoch 6077 - lr: 0.01000 - Train loss: 0.96930 - Test loss: 0.87630\n",
      "Epoch 6078 - lr: 0.01000 - Train loss: 0.91567 - Test loss: 0.88523\n",
      "Epoch 6079 - lr: 0.01000 - Train loss: 0.97812 - Test loss: 0.90568\n",
      "Epoch 6080 - lr: 0.01000 - Train loss: 0.93038 - Test loss: 0.91644\n",
      "Epoch 6081 - lr: 0.01000 - Train loss: 0.92456 - Test loss: 0.91651\n",
      "Epoch 6082 - lr: 0.01000 - Train loss: 0.92592 - Test loss: 0.89160\n",
      "Epoch 6083 - lr: 0.01000 - Train loss: 0.97434 - Test loss: 0.87968\n",
      "Epoch 6084 - lr: 0.01000 - Train loss: 0.87226 - Test loss: 0.88622\n",
      "Epoch 6085 - lr: 0.01000 - Train loss: 0.90708 - Test loss: 0.90309\n",
      "Epoch 6086 - lr: 0.01000 - Train loss: 0.93478 - Test loss: 0.89787\n",
      "Epoch 6087 - lr: 0.01000 - Train loss: 0.94978 - Test loss: 0.91609\n",
      "Epoch 6088 - lr: 0.01000 - Train loss: 0.92159 - Test loss: 0.91279\n",
      "Epoch 6089 - lr: 0.01000 - Train loss: 0.83842 - Test loss: 0.88718\n",
      "Epoch 6090 - lr: 0.01000 - Train loss: 0.86998 - Test loss: 0.88205\n",
      "Epoch 6091 - lr: 0.01000 - Train loss: 0.91203 - Test loss: 0.91142\n",
      "Epoch 6092 - lr: 0.01000 - Train loss: 0.89164 - Test loss: 0.91409\n",
      "Epoch 6093 - lr: 0.01000 - Train loss: 0.85956 - Test loss: 0.88521\n",
      "Epoch 6094 - lr: 0.01000 - Train loss: 0.92575 - Test loss: 0.90860\n",
      "Epoch 6095 - lr: 0.01000 - Train loss: 0.92762 - Test loss: 0.89387\n",
      "Epoch 6096 - lr: 0.01000 - Train loss: 0.98570 - Test loss: 0.91022\n",
      "Epoch 6097 - lr: 0.01000 - Train loss: 0.93640 - Test loss: 0.89437\n",
      "Epoch 6098 - lr: 0.01000 - Train loss: 0.98750 - Test loss: 0.89176\n",
      "Epoch 6099 - lr: 0.01000 - Train loss: 0.94723 - Test loss: 0.88560\n",
      "Epoch 6100 - lr: 0.01000 - Train loss: 0.90316 - Test loss: 0.88181\n",
      "Epoch 6101 - lr: 0.01000 - Train loss: 0.92494 - Test loss: 0.89854\n",
      "Epoch 6102 - lr: 0.01000 - Train loss: 0.90164 - Test loss: 0.91030\n",
      "Epoch 6103 - lr: 0.01000 - Train loss: 0.87573 - Test loss: 0.88884\n",
      "Epoch 6104 - lr: 0.01000 - Train loss: 0.92789 - Test loss: 0.88860\n",
      "Epoch 6105 - lr: 0.01000 - Train loss: 0.96966 - Test loss: 0.88085\n",
      "Epoch 6106 - lr: 0.01000 - Train loss: 0.92970 - Test loss: 0.88864\n",
      "Epoch 6107 - lr: 0.01000 - Train loss: 0.97767 - Test loss: 0.88276\n",
      "Epoch 6108 - lr: 0.01000 - Train loss: 0.83393 - Test loss: 0.88698\n",
      "Epoch 6109 - lr: 0.01000 - Train loss: 0.90692 - Test loss: 0.90642\n",
      "Epoch 6110 - lr: 0.01000 - Train loss: 0.93198 - Test loss: 0.90053\n",
      "Epoch 6111 - lr: 0.01000 - Train loss: 0.97744 - Test loss: 0.91972\n",
      "Epoch 6112 - lr: 0.01000 - Train loss: 0.92389 - Test loss: 0.91852\n",
      "Epoch 6113 - lr: 0.01000 - Train loss: 0.84327 - Test loss: 0.89123\n",
      "Epoch 6114 - lr: 0.01000 - Train loss: 0.92321 - Test loss: 0.89231\n",
      "Epoch 6115 - lr: 0.01000 - Train loss: 0.95669 - Test loss: 0.88603\n",
      "Epoch 6116 - lr: 0.01000 - Train loss: 0.90045 - Test loss: 0.91532\n",
      "Epoch 6117 - lr: 0.01000 - Train loss: 0.91915 - Test loss: 0.92280\n",
      "Epoch 6118 - lr: 0.01000 - Train loss: 0.92674 - Test loss: 0.92003\n",
      "Epoch 6119 - lr: 0.01000 - Train loss: 0.87414 - Test loss: 0.88919\n",
      "Epoch 6120 - lr: 0.01000 - Train loss: 0.92428 - Test loss: 0.91077\n",
      "Epoch 6121 - lr: 0.01000 - Train loss: 0.94613 - Test loss: 0.89271\n",
      "Epoch 6122 - lr: 0.01000 - Train loss: 0.93863 - Test loss: 0.89040\n",
      "Epoch 6123 - lr: 0.01000 - Train loss: 0.89592 - Test loss: 0.88551\n",
      "Epoch 6124 - lr: 0.01000 - Train loss: 0.86264 - Test loss: 0.88626\n",
      "Epoch 6125 - lr: 0.01000 - Train loss: 0.92334 - Test loss: 0.90981\n",
      "Epoch 6126 - lr: 0.01000 - Train loss: 0.85707 - Test loss: 0.89031\n",
      "Epoch 6127 - lr: 0.01000 - Train loss: 0.92155 - Test loss: 0.90917\n",
      "Epoch 6128 - lr: 0.01000 - Train loss: 0.89728 - Test loss: 0.88830\n",
      "Epoch 6129 - lr: 0.01000 - Train loss: 0.91630 - Test loss: 0.88496\n",
      "Epoch 6130 - lr: 0.01000 - Train loss: 0.99306 - Test loss: 0.88762\n",
      "Epoch 6131 - lr: 0.01000 - Train loss: 0.97723 - Test loss: 0.91215\n",
      "Epoch 6132 - lr: 0.01000 - Train loss: 0.87435 - Test loss: 0.91622\n",
      "Epoch 6133 - lr: 0.01000 - Train loss: 0.89147 - Test loss: 0.89318\n",
      "Epoch 6134 - lr: 0.01000 - Train loss: 0.93464 - Test loss: 0.88991\n",
      "Epoch 6135 - lr: 0.01000 - Train loss: 0.92466 - Test loss: 0.89365\n",
      "Epoch 6136 - lr: 0.01000 - Train loss: 0.92708 - Test loss: 0.92087\n",
      "Epoch 6137 - lr: 0.01000 - Train loss: 0.91182 - Test loss: 0.90372\n",
      "Epoch 6138 - lr: 0.01000 - Train loss: 0.91531 - Test loss: 0.89439\n",
      "Epoch 6139 - lr: 0.01000 - Train loss: 0.90213 - Test loss: 0.89352\n",
      "Epoch 6140 - lr: 0.01000 - Train loss: 0.92392 - Test loss: 0.92200\n",
      "Epoch 6141 - lr: 0.01000 - Train loss: 0.92505 - Test loss: 0.92590\n",
      "Epoch 6142 - lr: 0.01000 - Train loss: 0.91077 - Test loss: 0.89510\n",
      "Epoch 6143 - lr: 0.01000 - Train loss: 0.87356 - Test loss: 0.89640\n",
      "Epoch 6144 - lr: 0.01000 - Train loss: 0.93449 - Test loss: 0.91834\n",
      "Epoch 6145 - lr: 0.01000 - Train loss: 0.90957 - Test loss: 0.92774\n",
      "Epoch 6146 - lr: 0.01000 - Train loss: 0.92708 - Test loss: 0.92745\n",
      "Epoch 6147 - lr: 0.01000 - Train loss: 0.88999 - Test loss: 0.92494\n",
      "Epoch 6148 - lr: 0.01000 - Train loss: 0.91736 - Test loss: 0.92205\n",
      "Epoch 6149 - lr: 0.01000 - Train loss: 0.92534 - Test loss: 0.92267\n",
      "Epoch 6150 - lr: 0.01000 - Train loss: 0.91079 - Test loss: 0.89755\n",
      "Epoch 6151 - lr: 0.01000 - Train loss: 1.00948 - Test loss: 0.89870\n",
      "Epoch 6152 - lr: 0.01000 - Train loss: 0.92645 - Test loss: 0.91706\n",
      "Epoch 6153 - lr: 0.01000 - Train loss: 0.88152 - Test loss: 0.90629\n",
      "Epoch 6154 - lr: 0.01000 - Train loss: 0.92542 - Test loss: 0.91718\n",
      "Epoch 6155 - lr: 0.01000 - Train loss: 0.90706 - Test loss: 0.91977\n",
      "Epoch 6156 - lr: 0.01000 - Train loss: 0.91274 - Test loss: 0.91958\n",
      "Epoch 6157 - lr: 0.01000 - Train loss: 0.90650 - Test loss: 0.91807\n",
      "Epoch 6158 - lr: 0.01000 - Train loss: 0.92316 - Test loss: 0.91621\n",
      "Epoch 6159 - lr: 0.01000 - Train loss: 0.92532 - Test loss: 0.91414\n",
      "Epoch 6160 - lr: 0.01000 - Train loss: 0.91809 - Test loss: 0.91342\n",
      "Epoch 6161 - lr: 0.01000 - Train loss: 0.92532 - Test loss: 0.91177\n",
      "Epoch 6162 - lr: 0.01000 - Train loss: 0.90286 - Test loss: 0.91224\n",
      "Epoch 6163 - lr: 0.01000 - Train loss: 0.92953 - Test loss: 0.88760\n",
      "Epoch 6164 - lr: 0.01000 - Train loss: 1.00059 - Test loss: 0.88029\n",
      "Epoch 6165 - lr: 0.01000 - Train loss: 0.94182 - Test loss: 0.87544\n",
      "Epoch 6166 - lr: 0.01000 - Train loss: 0.89559 - Test loss: 0.89661\n",
      "Epoch 6167 - lr: 0.01000 - Train loss: 0.91418 - Test loss: 0.91033\n",
      "Epoch 6168 - lr: 0.01000 - Train loss: 0.94563 - Test loss: 0.89176\n",
      "Epoch 6169 - lr: 0.01000 - Train loss: 0.90527 - Test loss: 0.90181\n",
      "Epoch 6170 - lr: 0.01000 - Train loss: 0.92258 - Test loss: 0.90618\n",
      "Epoch 6171 - lr: 0.01000 - Train loss: 0.93763 - Test loss: 0.88636\n",
      "Epoch 6172 - lr: 0.01000 - Train loss: 1.01507 - Test loss: 0.89763\n",
      "Epoch 6173 - lr: 0.01000 - Train loss: 0.91293 - Test loss: 0.87592\n",
      "Epoch 6174 - lr: 0.01000 - Train loss: 0.92463 - Test loss: 0.89989\n",
      "Epoch 6175 - lr: 0.01000 - Train loss: 0.91127 - Test loss: 0.89989\n",
      "Epoch 6176 - lr: 0.01000 - Train loss: 0.87340 - Test loss: 0.90471\n",
      "Epoch 6177 - lr: 0.01000 - Train loss: 0.90572 - Test loss: 0.89927\n",
      "Epoch 6178 - lr: 0.01000 - Train loss: 0.90686 - Test loss: 0.88279\n",
      "Epoch 6179 - lr: 0.01000 - Train loss: 0.99585 - Test loss: 0.90215\n",
      "Epoch 6180 - lr: 0.01000 - Train loss: 0.90436 - Test loss: 0.91046\n",
      "Epoch 6181 - lr: 0.01000 - Train loss: 0.91962 - Test loss: 0.91154\n",
      "Epoch 6182 - lr: 0.01000 - Train loss: 0.92690 - Test loss: 0.88444\n",
      "Epoch 6183 - lr: 0.01000 - Train loss: 0.95927 - Test loss: 0.87532\n",
      "Epoch 6184 - lr: 0.01000 - Train loss: 0.93819 - Test loss: 0.89535\n",
      "Epoch 6185 - lr: 0.01000 - Train loss: 0.90964 - Test loss: 0.87523\n",
      "Epoch 6186 - lr: 0.01000 - Train loss: 0.92577 - Test loss: 0.90210\n",
      "Epoch 6187 - lr: 0.01000 - Train loss: 0.92635 - Test loss: 0.89278\n",
      "Epoch 6188 - lr: 0.01000 - Train loss: 0.95351 - Test loss: 0.87729\n",
      "Epoch 6189 - lr: 0.01000 - Train loss: 0.98712 - Test loss: 0.88024\n",
      "Epoch 6190 - lr: 0.01000 - Train loss: 0.97713 - Test loss: 0.88308\n",
      "Epoch 6191 - lr: 0.01000 - Train loss: 1.00718 - Test loss: 0.90452\n",
      "Epoch 6192 - lr: 0.01000 - Train loss: 0.92427 - Test loss: 0.91252\n",
      "Epoch 6193 - lr: 0.01000 - Train loss: 0.88719 - Test loss: 0.89927\n",
      "Epoch 6194 - lr: 0.01000 - Train loss: 0.90218 - Test loss: 0.91237\n",
      "Epoch 6195 - lr: 0.01000 - Train loss: 0.90488 - Test loss: 0.91438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6196 - lr: 0.01000 - Train loss: 0.92199 - Test loss: 0.91359\n",
      "Epoch 6197 - lr: 0.01000 - Train loss: 0.91657 - Test loss: 0.91316\n",
      "Epoch 6198 - lr: 0.01000 - Train loss: 0.92470 - Test loss: 0.88527\n",
      "Epoch 6199 - lr: 0.01000 - Train loss: 0.97725 - Test loss: 0.90011\n",
      "Epoch 6200 - lr: 0.01000 - Train loss: 0.92872 - Test loss: 0.90533\n",
      "Epoch 6201 - lr: 0.01000 - Train loss: 0.87866 - Test loss: 0.90248\n",
      "Epoch 6202 - lr: 0.01000 - Train loss: 0.90001 - Test loss: 0.89894\n",
      "Epoch 6203 - lr: 0.01000 - Train loss: 0.91258 - Test loss: 0.90810\n",
      "Epoch 6204 - lr: 0.01000 - Train loss: 0.93865 - Test loss: 0.89054\n",
      "Epoch 6205 - lr: 0.01000 - Train loss: 0.92029 - Test loss: 0.87414\n",
      "Epoch 6206 - lr: 0.01000 - Train loss: 0.91289 - Test loss: 0.90097\n",
      "Epoch 6207 - lr: 0.01000 - Train loss: 0.93494 - Test loss: 0.88276\n",
      "Epoch 6208 - lr: 0.01000 - Train loss: 1.00240 - Test loss: 0.89965\n",
      "Epoch 6209 - lr: 0.01000 - Train loss: 0.92835 - Test loss: 0.88197\n",
      "Epoch 6210 - lr: 0.01000 - Train loss: 0.97462 - Test loss: 0.87308\n",
      "Epoch 6211 - lr: 0.01000 - Train loss: 0.92552 - Test loss: 0.89642\n",
      "Epoch 6212 - lr: 0.01000 - Train loss: 0.90379 - Test loss: 0.88440\n",
      "Epoch 6213 - lr: 0.01000 - Train loss: 0.92303 - Test loss: 0.90019\n",
      "Epoch 6214 - lr: 0.01000 - Train loss: 0.90750 - Test loss: 0.89822\n",
      "Epoch 6215 - lr: 0.01000 - Train loss: 0.92051 - Test loss: 0.90727\n",
      "Epoch 6216 - lr: 0.01000 - Train loss: 0.92481 - Test loss: 0.90606\n",
      "Epoch 6217 - lr: 0.01000 - Train loss: 0.91347 - Test loss: 0.87440\n",
      "Epoch 6218 - lr: 0.01000 - Train loss: 0.92502 - Test loss: 0.89808\n",
      "Epoch 6219 - lr: 0.01000 - Train loss: 0.95294 - Test loss: 0.87503\n",
      "Epoch 6220 - lr: 0.01000 - Train loss: 0.99970 - Test loss: 0.87273\n",
      "Epoch 6221 - lr: 0.01000 - Train loss: 0.93664 - Test loss: 0.88564\n",
      "Epoch 6222 - lr: 0.01000 - Train loss: 0.92666 - Test loss: 0.90851\n",
      "Epoch 6223 - lr: 0.01000 - Train loss: 0.92728 - Test loss: 0.91189\n",
      "Epoch 6224 - lr: 0.01000 - Train loss: 0.91612 - Test loss: 0.91285\n",
      "Epoch 6225 - lr: 0.01000 - Train loss: 0.91497 - Test loss: 0.91330\n",
      "Epoch 6226 - lr: 0.01000 - Train loss: 0.93468 - Test loss: 0.89466\n",
      "Epoch 6227 - lr: 0.01000 - Train loss: 0.90885 - Test loss: 0.88113\n",
      "Epoch 6228 - lr: 0.01000 - Train loss: 0.98476 - Test loss: 0.87225\n",
      "Epoch 6229 - lr: 0.01000 - Train loss: 0.92226 - Test loss: 0.89494\n",
      "Epoch 6230 - lr: 0.01000 - Train loss: 0.92083 - Test loss: 0.88396\n",
      "Epoch 6231 - lr: 0.01000 - Train loss: 0.92809 - Test loss: 0.90293\n",
      "Epoch 6232 - lr: 0.01000 - Train loss: 0.89095 - Test loss: 0.87731\n",
      "Epoch 6233 - lr: 0.01000 - Train loss: 0.93129 - Test loss: 0.90283\n",
      "Epoch 6234 - lr: 0.01000 - Train loss: 0.92314 - Test loss: 0.88400\n",
      "Epoch 6235 - lr: 0.01000 - Train loss: 0.98277 - Test loss: 0.89936\n",
      "Epoch 6236 - lr: 0.01000 - Train loss: 0.89863 - Test loss: 0.91015\n",
      "Epoch 6237 - lr: 0.01000 - Train loss: 0.87863 - Test loss: 0.90903\n",
      "Epoch 6238 - lr: 0.01000 - Train loss: 0.91515 - Test loss: 0.87698\n",
      "Epoch 6239 - lr: 0.01000 - Train loss: 0.92578 - Test loss: 0.90211\n",
      "Epoch 6240 - lr: 0.01000 - Train loss: 0.93205 - Test loss: 0.88546\n",
      "Epoch 6241 - lr: 0.01000 - Train loss: 0.96629 - Test loss: 0.90700\n",
      "Epoch 6242 - lr: 0.01000 - Train loss: 0.90759 - Test loss: 0.91235\n",
      "Epoch 6243 - lr: 0.01000 - Train loss: 0.90657 - Test loss: 0.89425\n",
      "Epoch 6244 - lr: 0.01000 - Train loss: 0.92405 - Test loss: 0.90162\n",
      "Epoch 6245 - lr: 0.01000 - Train loss: 0.93926 - Test loss: 0.87384\n",
      "Epoch 6246 - lr: 0.01000 - Train loss: 0.98982 - Test loss: 0.89332\n",
      "Epoch 6247 - lr: 0.01000 - Train loss: 0.92153 - Test loss: 0.90213\n",
      "Epoch 6248 - lr: 0.01000 - Train loss: 0.91119 - Test loss: 0.87601\n",
      "Epoch 6249 - lr: 0.01000 - Train loss: 0.88267 - Test loss: 0.87647\n",
      "Epoch 6250 - lr: 0.01000 - Train loss: 0.93431 - Test loss: 0.90186\n",
      "Epoch 6251 - lr: 0.01000 - Train loss: 0.91756 - Test loss: 0.91270\n",
      "Epoch 6252 - lr: 0.01000 - Train loss: 0.94539 - Test loss: 0.89255\n",
      "Epoch 6253 - lr: 0.01000 - Train loss: 0.95342 - Test loss: 0.89513\n",
      "Epoch 6254 - lr: 0.01000 - Train loss: 0.92914 - Test loss: 0.90549\n",
      "Epoch 6255 - lr: 0.01000 - Train loss: 0.92587 - Test loss: 0.87584\n",
      "Epoch 6256 - lr: 0.01000 - Train loss: 0.94135 - Test loss: 0.89887\n",
      "Epoch 6257 - lr: 0.01000 - Train loss: 0.91180 - Test loss: 0.88551\n",
      "Epoch 6258 - lr: 0.01000 - Train loss: 1.00719 - Test loss: 0.87514\n",
      "Epoch 6259 - lr: 0.01000 - Train loss: 0.96857 - Test loss: 0.87619\n",
      "Epoch 6260 - lr: 0.01000 - Train loss: 0.91157 - Test loss: 0.88487\n",
      "Epoch 6261 - lr: 0.01000 - Train loss: 0.94574 - Test loss: 0.88154\n",
      "Epoch 6262 - lr: 0.01000 - Train loss: 0.92207 - Test loss: 0.90056\n",
      "Epoch 6263 - lr: 0.01000 - Train loss: 0.87856 - Test loss: 0.88478\n",
      "Epoch 6264 - lr: 0.01000 - Train loss: 0.92039 - Test loss: 0.90236\n",
      "Epoch 6265 - lr: 0.01000 - Train loss: 0.94156 - Test loss: 0.88541\n",
      "Epoch 6266 - lr: 0.01000 - Train loss: 0.97745 - Test loss: 0.90789\n",
      "Epoch 6267 - lr: 0.01000 - Train loss: 0.92454 - Test loss: 0.91462\n",
      "Epoch 6268 - lr: 0.01000 - Train loss: 0.83711 - Test loss: 0.89034\n",
      "Epoch 6269 - lr: 0.01000 - Train loss: 0.90322 - Test loss: 0.90202\n",
      "Epoch 6270 - lr: 0.01000 - Train loss: 0.92704 - Test loss: 0.88520\n",
      "Epoch 6271 - lr: 0.01000 - Train loss: 0.97990 - Test loss: 0.90653\n",
      "Epoch 6272 - lr: 0.01000 - Train loss: 0.93508 - Test loss: 0.91637\n",
      "Epoch 6273 - lr: 0.01000 - Train loss: 0.91274 - Test loss: 0.92330\n",
      "Epoch 6274 - lr: 0.01000 - Train loss: 0.91979 - Test loss: 0.92410\n",
      "Epoch 6275 - lr: 0.01000 - Train loss: 0.90662 - Test loss: 0.92228\n",
      "Epoch 6276 - lr: 0.01000 - Train loss: 0.92416 - Test loss: 0.91615\n",
      "Epoch 6277 - lr: 0.01000 - Train loss: 0.93908 - Test loss: 0.88467\n",
      "Epoch 6278 - lr: 0.01000 - Train loss: 0.93951 - Test loss: 0.88174\n",
      "Epoch 6279 - lr: 0.01000 - Train loss: 0.92159 - Test loss: 0.90419\n",
      "Epoch 6280 - lr: 0.01000 - Train loss: 0.90434 - Test loss: 0.88524\n",
      "Epoch 6281 - lr: 0.01000 - Train loss: 0.91493 - Test loss: 0.88380\n",
      "Epoch 6282 - lr: 0.01000 - Train loss: 0.91550 - Test loss: 0.89154\n",
      "Epoch 6283 - lr: 0.01000 - Train loss: 0.92114 - Test loss: 0.88701\n",
      "Epoch 6284 - lr: 0.01000 - Train loss: 0.92804 - Test loss: 0.91350\n",
      "Epoch 6285 - lr: 0.01000 - Train loss: 0.92856 - Test loss: 0.92228\n",
      "Epoch 6286 - lr: 0.01000 - Train loss: 0.92422 - Test loss: 0.92172\n",
      "Epoch 6287 - lr: 0.01000 - Train loss: 0.91960 - Test loss: 0.89598\n",
      "Epoch 6288 - lr: 0.01000 - Train loss: 0.98069 - Test loss: 0.90877\n",
      "Epoch 6289 - lr: 0.01000 - Train loss: 0.92804 - Test loss: 0.91775\n",
      "Epoch 6290 - lr: 0.01000 - Train loss: 0.87080 - Test loss: 0.89273\n",
      "Epoch 6291 - lr: 0.01000 - Train loss: 0.94447 - Test loss: 0.88549\n",
      "Epoch 6292 - lr: 0.01000 - Train loss: 0.94831 - Test loss: 0.88462\n",
      "Epoch 6293 - lr: 0.01000 - Train loss: 0.92761 - Test loss: 0.91398\n",
      "Epoch 6294 - lr: 0.01000 - Train loss: 0.92594 - Test loss: 0.89931\n",
      "Epoch 6295 - lr: 0.01000 - Train loss: 0.93075 - Test loss: 0.89069\n",
      "Epoch 6296 - lr: 0.01000 - Train loss: 0.91361 - Test loss: 0.88740\n",
      "Epoch 6297 - lr: 0.01000 - Train loss: 0.91783 - Test loss: 0.88832\n",
      "Epoch 6298 - lr: 0.01000 - Train loss: 0.86308 - Test loss: 0.89134\n",
      "Epoch 6299 - lr: 0.01000 - Train loss: 0.93178 - Test loss: 0.89905\n",
      "Epoch 6300 - lr: 0.01000 - Train loss: 0.97167 - Test loss: 0.92303\n",
      "Epoch 6301 - lr: 0.01000 - Train loss: 0.92670 - Test loss: 0.92582\n",
      "Epoch 6302 - lr: 0.01000 - Train loss: 0.86805 - Test loss: 0.89972\n",
      "Epoch 6303 - lr: 0.01000 - Train loss: 0.90266 - Test loss: 0.90727\n",
      "Epoch 6304 - lr: 0.01000 - Train loss: 0.89125 - Test loss: 0.90659\n",
      "Epoch 6305 - lr: 0.01000 - Train loss: 0.89869 - Test loss: 0.91061\n",
      "Epoch 6306 - lr: 0.01000 - Train loss: 0.91334 - Test loss: 0.89165\n",
      "Epoch 6307 - lr: 0.01000 - Train loss: 0.93765 - Test loss: 0.91471\n",
      "Epoch 6308 - lr: 0.01000 - Train loss: 0.90772 - Test loss: 0.92498\n",
      "Epoch 6309 - lr: 0.01000 - Train loss: 0.92471 - Test loss: 0.92520\n",
      "Epoch 6310 - lr: 0.01000 - Train loss: 0.90593 - Test loss: 0.92494\n",
      "Epoch 6311 - lr: 0.01000 - Train loss: 0.92271 - Test loss: 0.92343\n",
      "Epoch 6312 - lr: 0.01000 - Train loss: 0.92371 - Test loss: 0.92090\n",
      "Epoch 6313 - lr: 0.01000 - Train loss: 0.91119 - Test loss: 0.89535\n",
      "Epoch 6314 - lr: 0.01000 - Train loss: 0.99355 - Test loss: 0.89268\n",
      "Epoch 6315 - lr: 0.01000 - Train loss: 0.98310 - Test loss: 0.88980\n",
      "Epoch 6316 - lr: 0.01000 - Train loss: 0.98419 - Test loss: 0.89047\n",
      "Epoch 6317 - lr: 0.01000 - Train loss: 1.00256 - Test loss: 0.89819\n",
      "Epoch 6318 - lr: 0.01000 - Train loss: 0.91991 - Test loss: 0.91781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6319 - lr: 0.01000 - Train loss: 0.92286 - Test loss: 0.92070\n",
      "Epoch 6320 - lr: 0.01000 - Train loss: 0.90992 - Test loss: 0.89608\n",
      "Epoch 6321 - lr: 0.01000 - Train loss: 0.95346 - Test loss: 0.90192\n",
      "Epoch 6322 - lr: 0.01000 - Train loss: 0.88746 - Test loss: 0.88702\n",
      "Epoch 6323 - lr: 0.01000 - Train loss: 0.92681 - Test loss: 0.91215\n",
      "Epoch 6324 - lr: 0.01000 - Train loss: 0.92928 - Test loss: 0.89734\n",
      "Epoch 6325 - lr: 0.01000 - Train loss: 0.96116 - Test loss: 0.91643\n",
      "Epoch 6326 - lr: 0.01000 - Train loss: 0.92494 - Test loss: 0.91864\n",
      "Epoch 6327 - lr: 0.01000 - Train loss: 0.89844 - Test loss: 0.91900\n",
      "Epoch 6328 - lr: 0.01000 - Train loss: 0.92391 - Test loss: 0.91752\n",
      "Epoch 6329 - lr: 0.01000 - Train loss: 0.92118 - Test loss: 0.91589\n",
      "Epoch 6330 - lr: 0.01000 - Train loss: 0.91099 - Test loss: 0.91597\n",
      "Epoch 6331 - lr: 0.01000 - Train loss: 0.90484 - Test loss: 0.91514\n",
      "Epoch 6332 - lr: 0.01000 - Train loss: 0.92069 - Test loss: 0.91382\n",
      "Epoch 6333 - lr: 0.01000 - Train loss: 0.91298 - Test loss: 0.91321\n",
      "Epoch 6334 - lr: 0.01000 - Train loss: 0.92245 - Test loss: 0.88511\n",
      "Epoch 6335 - lr: 0.01000 - Train loss: 1.00954 - Test loss: 0.88576\n",
      "Epoch 6336 - lr: 0.01000 - Train loss: 0.87736 - Test loss: 0.87713\n",
      "Epoch 6337 - lr: 0.01000 - Train loss: 0.90383 - Test loss: 0.87328\n",
      "Epoch 6338 - lr: 0.01000 - Train loss: 0.92759 - Test loss: 0.90068\n",
      "Epoch 6339 - lr: 0.01000 - Train loss: 0.90336 - Test loss: 0.87849\n",
      "Epoch 6340 - lr: 0.01000 - Train loss: 0.92817 - Test loss: 0.90488\n",
      "Epoch 6341 - lr: 0.01000 - Train loss: 0.90272 - Test loss: 0.91249\n",
      "Epoch 6342 - lr: 0.01000 - Train loss: 0.87911 - Test loss: 0.90532\n",
      "Epoch 6343 - lr: 0.01000 - Train loss: 0.90166 - Test loss: 0.90060\n",
      "Epoch 6344 - lr: 0.01000 - Train loss: 0.92578 - Test loss: 0.90756\n",
      "Epoch 6345 - lr: 0.01000 - Train loss: 0.90893 - Test loss: 0.88367\n",
      "Epoch 6346 - lr: 0.01000 - Train loss: 1.00369 - Test loss: 0.88486\n",
      "Epoch 6347 - lr: 0.01000 - Train loss: 0.94371 - Test loss: 0.88082\n",
      "Epoch 6348 - lr: 0.01000 - Train loss: 0.96800 - Test loss: 0.87548\n",
      "Epoch 6349 - lr: 0.01000 - Train loss: 0.92485 - Test loss: 0.90012\n",
      "Epoch 6350 - lr: 0.01000 - Train loss: 0.90842 - Test loss: 0.89736\n",
      "Epoch 6351 - lr: 0.01000 - Train loss: 0.90946 - Test loss: 0.90049\n",
      "Epoch 6352 - lr: 0.01000 - Train loss: 0.92291 - Test loss: 0.90898\n",
      "Epoch 6353 - lr: 0.01000 - Train loss: 0.90724 - Test loss: 0.88534\n",
      "Epoch 6354 - lr: 0.01000 - Train loss: 0.98126 - Test loss: 0.90246\n",
      "Epoch 6355 - lr: 0.01000 - Train loss: 0.87346 - Test loss: 0.90539\n",
      "Epoch 6356 - lr: 0.01000 - Train loss: 0.90105 - Test loss: 0.90130\n",
      "Epoch 6357 - lr: 0.01000 - Train loss: 0.92292 - Test loss: 0.90931\n",
      "Epoch 6358 - lr: 0.01000 - Train loss: 0.92361 - Test loss: 0.91012\n",
      "Epoch 6359 - lr: 0.01000 - Train loss: 0.92567 - Test loss: 0.90858\n",
      "Epoch 6360 - lr: 0.01000 - Train loss: 0.90264 - Test loss: 0.90933\n",
      "Epoch 6361 - lr: 0.01000 - Train loss: 0.93659 - Test loss: 0.88429\n",
      "Epoch 6362 - lr: 0.01000 - Train loss: 0.96151 - Test loss: 0.90209\n",
      "Epoch 6363 - lr: 0.01000 - Train loss: 0.92606 - Test loss: 0.90406\n",
      "Epoch 6364 - lr: 0.01000 - Train loss: 0.91236 - Test loss: 0.90316\n",
      "Epoch 6365 - lr: 0.01000 - Train loss: 0.91465 - Test loss: 0.90691\n",
      "Epoch 6366 - lr: 0.01000 - Train loss: 0.90325 - Test loss: 0.89379\n",
      "Epoch 6367 - lr: 0.01000 - Train loss: 0.92515 - Test loss: 0.89914\n",
      "Epoch 6368 - lr: 0.01000 - Train loss: 0.90100 - Test loss: 0.87043\n",
      "Epoch 6369 - lr: 0.01000 - Train loss: 0.88264 - Test loss: 0.89169\n",
      "Epoch 6370 - lr: 0.01000 - Train loss: 0.90582 - Test loss: 0.89023\n",
      "Epoch 6371 - lr: 0.01000 - Train loss: 0.88828 - Test loss: 0.88932\n",
      "Epoch 6372 - lr: 0.01000 - Train loss: 0.92131 - Test loss: 0.90007\n",
      "Epoch 6373 - lr: 0.01000 - Train loss: 0.92460 - Test loss: 0.90071\n",
      "Epoch 6374 - lr: 0.01000 - Train loss: 0.92632 - Test loss: 0.87531\n",
      "Epoch 6375 - lr: 0.01000 - Train loss: 0.99095 - Test loss: 0.89675\n",
      "Epoch 6376 - lr: 0.01000 - Train loss: 0.92605 - Test loss: 0.90036\n",
      "Epoch 6377 - lr: 0.01000 - Train loss: 0.90781 - Test loss: 0.88391\n",
      "Epoch 6378 - lr: 0.01000 - Train loss: 0.92558 - Test loss: 0.87477\n",
      "Epoch 6379 - lr: 0.01000 - Train loss: 0.97434 - Test loss: 0.88453\n",
      "Epoch 6380 - lr: 0.01000 - Train loss: 0.90296 - Test loss: 0.90066\n",
      "Epoch 6381 - lr: 0.01000 - Train loss: 0.92465 - Test loss: 0.87665\n",
      "Epoch 6382 - lr: 0.01000 - Train loss: 1.05263 - Test loss: 0.86672\n",
      "Epoch 6383 - lr: 0.01000 - Train loss: 0.97771 - Test loss: 0.86671\n",
      "Epoch 6384 - lr: 0.01000 - Train loss: 0.92990 - Test loss: 0.89766\n",
      "Epoch 6385 - lr: 0.01000 - Train loss: 0.92835 - Test loss: 0.90355\n",
      "Epoch 6386 - lr: 0.01000 - Train loss: 0.91597 - Test loss: 0.90381\n",
      "Epoch 6387 - lr: 0.01000 - Train loss: 0.92554 - Test loss: 0.88103\n",
      "Epoch 6388 - lr: 0.01000 - Train loss: 1.00835 - Test loss: 0.86908\n",
      "Epoch 6389 - lr: 0.01000 - Train loss: 0.93440 - Test loss: 0.88161\n",
      "Epoch 6390 - lr: 0.01000 - Train loss: 0.92974 - Test loss: 0.90039\n",
      "Epoch 6391 - lr: 0.01000 - Train loss: 0.93542 - Test loss: 0.88444\n",
      "Epoch 6392 - lr: 0.01000 - Train loss: 0.96184 - Test loss: 0.87529\n",
      "Epoch 6393 - lr: 0.01000 - Train loss: 0.93119 - Test loss: 0.88626\n",
      "Epoch 6394 - lr: 0.01000 - Train loss: 0.92066 - Test loss: 0.87384\n",
      "Epoch 6395 - lr: 0.01000 - Train loss: 0.93473 - Test loss: 0.88572\n",
      "Epoch 6396 - lr: 0.01000 - Train loss: 0.91666 - Test loss: 0.90378\n",
      "Epoch 6397 - lr: 0.01000 - Train loss: 0.92483 - Test loss: 0.90794\n",
      "Epoch 6398 - lr: 0.01000 - Train loss: 0.89433 - Test loss: 0.87818\n",
      "Epoch 6399 - lr: 0.01000 - Train loss: 0.93005 - Test loss: 0.90387\n",
      "Epoch 6400 - lr: 0.01000 - Train loss: 0.90991 - Test loss: 0.88576\n",
      "Epoch 6401 - lr: 0.01000 - Train loss: 0.99086 - Test loss: 0.87614\n",
      "Epoch 6402 - lr: 0.01000 - Train loss: 0.92560 - Test loss: 0.90429\n",
      "Epoch 6403 - lr: 0.01000 - Train loss: 0.90800 - Test loss: 0.88703\n",
      "Epoch 6404 - lr: 0.01000 - Train loss: 0.99000 - Test loss: 0.87786\n",
      "Epoch 6405 - lr: 0.01000 - Train loss: 0.91511 - Test loss: 0.90778\n",
      "Epoch 6406 - lr: 0.01000 - Train loss: 0.90735 - Test loss: 0.91519\n",
      "Epoch 6407 - lr: 0.01000 - Train loss: 0.92273 - Test loss: 0.91559\n",
      "Epoch 6408 - lr: 0.01000 - Train loss: 0.91364 - Test loss: 0.91536\n",
      "Epoch 6409 - lr: 0.01000 - Train loss: 0.94404 - Test loss: 0.89199\n",
      "Epoch 6410 - lr: 0.01000 - Train loss: 0.95963 - Test loss: 0.90263\n",
      "Epoch 6411 - lr: 0.01000 - Train loss: 0.93270 - Test loss: 0.87548\n",
      "Epoch 6412 - lr: 0.01000 - Train loss: 0.94848 - Test loss: 0.87666\n",
      "Epoch 6413 - lr: 0.01000 - Train loss: 0.92203 - Test loss: 0.89655\n",
      "Epoch 6414 - lr: 0.01000 - Train loss: 0.83745 - Test loss: 0.88079\n",
      "Epoch 6415 - lr: 0.01000 - Train loss: 0.85160 - Test loss: 0.87922\n",
      "Epoch 6416 - lr: 0.01000 - Train loss: 0.90867 - Test loss: 0.89819\n",
      "Epoch 6417 - lr: 0.01000 - Train loss: 0.92064 - Test loss: 0.90468\n",
      "Epoch 6418 - lr: 0.01000 - Train loss: 0.84623 - Test loss: 0.88116\n",
      "Epoch 6419 - lr: 0.01000 - Train loss: 0.91584 - Test loss: 0.88438\n",
      "Epoch 6420 - lr: 0.01000 - Train loss: 0.98804 - Test loss: 0.90008\n",
      "Epoch 6421 - lr: 0.01000 - Train loss: 0.93001 - Test loss: 0.91077\n",
      "Epoch 6422 - lr: 0.01000 - Train loss: 0.92314 - Test loss: 0.91193\n",
      "Epoch 6423 - lr: 0.01000 - Train loss: 0.92769 - Test loss: 0.88977\n",
      "Epoch 6424 - lr: 0.01000 - Train loss: 0.98428 - Test loss: 0.89448\n",
      "Epoch 6425 - lr: 0.01000 - Train loss: 0.83606 - Test loss: 0.88189\n",
      "Epoch 6426 - lr: 0.01000 - Train loss: 0.83484 - Test loss: 0.87991\n",
      "Epoch 6427 - lr: 0.01000 - Train loss: 0.92199 - Test loss: 0.87655\n",
      "Epoch 6428 - lr: 0.01000 - Train loss: 0.91777 - Test loss: 0.88228\n",
      "Epoch 6429 - lr: 0.01000 - Train loss: 0.86820 - Test loss: 0.88141\n",
      "Epoch 6430 - lr: 0.01000 - Train loss: 0.91994 - Test loss: 0.90200\n",
      "Epoch 6431 - lr: 0.01000 - Train loss: 0.83790 - Test loss: 0.88602\n",
      "Epoch 6432 - lr: 0.01000 - Train loss: 0.94638 - Test loss: 0.88631\n",
      "Epoch 6433 - lr: 0.01000 - Train loss: 0.89469 - Test loss: 0.88454\n",
      "Epoch 6434 - lr: 0.01000 - Train loss: 0.91996 - Test loss: 0.90438\n",
      "Epoch 6435 - lr: 0.01000 - Train loss: 0.83812 - Test loss: 0.88783\n",
      "Epoch 6436 - lr: 0.01000 - Train loss: 0.90393 - Test loss: 0.89333\n",
      "Epoch 6437 - lr: 0.01000 - Train loss: 0.91709 - Test loss: 0.89341\n",
      "Epoch 6438 - lr: 0.01000 - Train loss: 0.97496 - Test loss: 0.88461\n",
      "Epoch 6439 - lr: 0.01000 - Train loss: 0.89731 - Test loss: 0.91340\n",
      "Epoch 6440 - lr: 0.01000 - Train loss: 0.89604 - Test loss: 0.92068\n",
      "Epoch 6441 - lr: 0.01000 - Train loss: 0.92217 - Test loss: 0.91592\n",
      "Epoch 6442 - lr: 0.01000 - Train loss: 0.83826 - Test loss: 0.88910\n",
      "Epoch 6443 - lr: 0.01000 - Train loss: 0.88037 - Test loss: 0.88280\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6444 - lr: 0.01000 - Train loss: 0.91608 - Test loss: 0.88915\n",
      "Epoch 6445 - lr: 0.01000 - Train loss: 0.91706 - Test loss: 0.88696\n",
      "Epoch 6446 - lr: 0.01000 - Train loss: 0.89629 - Test loss: 0.89731\n",
      "Epoch 6447 - lr: 0.01000 - Train loss: 0.83557 - Test loss: 0.88800\n",
      "Epoch 6448 - lr: 0.01000 - Train loss: 0.94522 - Test loss: 0.88780\n",
      "Epoch 6449 - lr: 0.01000 - Train loss: 0.97998 - Test loss: 0.91350\n",
      "Epoch 6450 - lr: 0.01000 - Train loss: 0.89436 - Test loss: 0.89378\n",
      "Epoch 6451 - lr: 0.01000 - Train loss: 0.92027 - Test loss: 0.90945\n",
      "Epoch 6452 - lr: 0.01000 - Train loss: 0.83635 - Test loss: 0.89091\n",
      "Epoch 6453 - lr: 0.01000 - Train loss: 0.93166 - Test loss: 0.88567\n",
      "Epoch 6454 - lr: 0.01000 - Train loss: 0.97170 - Test loss: 0.91065\n",
      "Epoch 6455 - lr: 0.01000 - Train loss: 0.92940 - Test loss: 0.92054\n",
      "Epoch 6456 - lr: 0.01000 - Train loss: 0.86954 - Test loss: 0.89628\n",
      "Epoch 6457 - lr: 0.01000 - Train loss: 0.92103 - Test loss: 0.89575\n",
      "Epoch 6458 - lr: 0.01000 - Train loss: 0.92371 - Test loss: 0.91437\n",
      "Epoch 6459 - lr: 0.01000 - Train loss: 0.91566 - Test loss: 0.88954\n",
      "Epoch 6460 - lr: 0.01000 - Train loss: 0.97368 - Test loss: 0.91430\n",
      "Epoch 6461 - lr: 0.01000 - Train loss: 0.90692 - Test loss: 0.88976\n",
      "Epoch 6462 - lr: 0.01000 - Train loss: 0.91574 - Test loss: 0.91477\n",
      "Epoch 6463 - lr: 0.01000 - Train loss: 0.92428 - Test loss: 0.92172\n",
      "Epoch 6464 - lr: 0.01000 - Train loss: 0.92873 - Test loss: 0.90008\n",
      "Epoch 6465 - lr: 0.01000 - Train loss: 0.97823 - Test loss: 0.91872\n",
      "Epoch 6466 - lr: 0.01000 - Train loss: 0.92255 - Test loss: 0.92225\n",
      "Epoch 6467 - lr: 0.01000 - Train loss: 0.90432 - Test loss: 0.92335\n",
      "Epoch 6468 - lr: 0.01000 - Train loss: 0.92076 - Test loss: 0.92277\n",
      "Epoch 6469 - lr: 0.01000 - Train loss: 0.91129 - Test loss: 0.92148\n",
      "Epoch 6470 - lr: 0.01000 - Train loss: 0.91624 - Test loss: 0.92020\n",
      "Epoch 6471 - lr: 0.01000 - Train loss: 0.93167 - Test loss: 0.89543\n",
      "Epoch 6472 - lr: 0.01000 - Train loss: 0.98604 - Test loss: 0.89570\n",
      "Epoch 6473 - lr: 0.01000 - Train loss: 0.86726 - Test loss: 0.88079\n",
      "Epoch 6474 - lr: 0.01000 - Train loss: 0.92036 - Test loss: 0.88916\n",
      "Epoch 6475 - lr: 0.01000 - Train loss: 0.93592 - Test loss: 0.88695\n",
      "Epoch 6476 - lr: 0.01000 - Train loss: 0.98420 - Test loss: 0.87944\n",
      "Epoch 6477 - lr: 0.01000 - Train loss: 0.91501 - Test loss: 0.88722\n",
      "Epoch 6478 - lr: 0.01000 - Train loss: 1.02352 - Test loss: 0.88793\n",
      "Epoch 6479 - lr: 0.01000 - Train loss: 0.94953 - Test loss: 0.91552\n",
      "Epoch 6480 - lr: 0.01000 - Train loss: 0.92079 - Test loss: 0.92162\n",
      "Epoch 6481 - lr: 0.01000 - Train loss: 0.88409 - Test loss: 0.91309\n",
      "Epoch 6482 - lr: 0.01000 - Train loss: 0.84612 - Test loss: 0.88506\n",
      "Epoch 6483 - lr: 0.01000 - Train loss: 0.87233 - Test loss: 0.88577\n",
      "Epoch 6484 - lr: 0.01000 - Train loss: 0.83800 - Test loss: 0.88283\n",
      "Epoch 6485 - lr: 0.01000 - Train loss: 0.94931 - Test loss: 0.88396\n",
      "Epoch 6486 - lr: 0.01000 - Train loss: 0.99873 - Test loss: 0.89302\n",
      "Epoch 6487 - lr: 0.01000 - Train loss: 0.99774 - Test loss: 0.89587\n",
      "Epoch 6488 - lr: 0.01000 - Train loss: 0.98756 - Test loss: 0.89606\n",
      "Epoch 6489 - lr: 0.01000 - Train loss: 0.92191 - Test loss: 0.88835\n",
      "Epoch 6490 - lr: 0.01000 - Train loss: 0.87168 - Test loss: 0.89212\n",
      "Epoch 6491 - lr: 0.01000 - Train loss: 0.83675 - Test loss: 0.89020\n",
      "Epoch 6492 - lr: 0.01000 - Train loss: 0.84276 - Test loss: 0.88914\n",
      "Epoch 6493 - lr: 0.01000 - Train loss: 0.90487 - Test loss: 0.91284\n",
      "Epoch 6494 - lr: 0.01000 - Train loss: 0.89681 - Test loss: 0.89206\n",
      "Epoch 6495 - lr: 0.01000 - Train loss: 0.91973 - Test loss: 0.90898\n",
      "Epoch 6496 - lr: 0.01000 - Train loss: 0.84482 - Test loss: 0.89052\n",
      "Epoch 6497 - lr: 0.01000 - Train loss: 0.92625 - Test loss: 0.91498\n",
      "Epoch 6498 - lr: 0.01000 - Train loss: 0.93022 - Test loss: 0.92276\n",
      "Epoch 6499 - lr: 0.01000 - Train loss: 0.92440 - Test loss: 0.92105\n",
      "Epoch 6500 - lr: 0.01000 - Train loss: 0.87804 - Test loss: 0.89231\n",
      "Epoch 6501 - lr: 0.01000 - Train loss: 0.92069 - Test loss: 0.89177\n",
      "Epoch 6502 - lr: 0.01000 - Train loss: 0.95558 - Test loss: 0.88794\n",
      "Epoch 6503 - lr: 0.01000 - Train loss: 0.91363 - Test loss: 0.89334\n",
      "Epoch 6504 - lr: 0.01000 - Train loss: 0.95696 - Test loss: 0.88970\n",
      "Epoch 6505 - lr: 0.01000 - Train loss: 0.92076 - Test loss: 0.89452\n",
      "Epoch 6506 - lr: 0.01000 - Train loss: 0.99914 - Test loss: 0.89102\n",
      "Epoch 6507 - lr: 0.01000 - Train loss: 0.97585 - Test loss: 0.91503\n",
      "Epoch 6508 - lr: 0.01000 - Train loss: 0.92266 - Test loss: 0.92199\n",
      "Epoch 6509 - lr: 0.01000 - Train loss: 0.83760 - Test loss: 0.89767\n",
      "Epoch 6510 - lr: 0.01000 - Train loss: 0.94621 - Test loss: 0.89542\n",
      "Epoch 6511 - lr: 0.01000 - Train loss: 0.96528 - Test loss: 0.91271\n",
      "Epoch 6512 - lr: 0.01000 - Train loss: 0.84199 - Test loss: 0.89636\n",
      "Epoch 6513 - lr: 0.01000 - Train loss: 0.93627 - Test loss: 0.91768\n",
      "Epoch 6514 - lr: 0.01000 - Train loss: 0.89039 - Test loss: 0.92485\n",
      "Epoch 6515 - lr: 0.01000 - Train loss: 0.96991 - Test loss: 0.90326\n",
      "Epoch 6516 - lr: 0.01000 - Train loss: 0.92404 - Test loss: 0.89115\n",
      "Epoch 6517 - lr: 0.01000 - Train loss: 0.92203 - Test loss: 0.89383\n",
      "Epoch 6518 - lr: 0.01000 - Train loss: 0.91408 - Test loss: 0.92206\n",
      "Epoch 6519 - lr: 0.01000 - Train loss: 0.92887 - Test loss: 0.92632\n",
      "Epoch 6520 - lr: 0.01000 - Train loss: 0.91812 - Test loss: 0.92510\n",
      "Epoch 6521 - lr: 0.01000 - Train loss: 0.92711 - Test loss: 0.92469\n",
      "Epoch 6522 - lr: 0.01000 - Train loss: 0.96482 - Test loss: 0.89643\n",
      "Epoch 6523 - lr: 0.01000 - Train loss: 0.97681 - Test loss: 0.91222\n",
      "Epoch 6524 - lr: 0.01000 - Train loss: 0.92523 - Test loss: 0.92171\n",
      "Epoch 6525 - lr: 0.01000 - Train loss: 0.89595 - Test loss: 0.89378\n",
      "Epoch 6526 - lr: 0.01000 - Train loss: 0.92609 - Test loss: 0.91694\n",
      "Epoch 6527 - lr: 0.01000 - Train loss: 0.86791 - Test loss: 0.89680\n",
      "Epoch 6528 - lr: 0.01000 - Train loss: 0.85399 - Test loss: 0.89106\n",
      "Epoch 6529 - lr: 0.01000 - Train loss: 0.85828 - Test loss: 0.88987\n",
      "Epoch 6530 - lr: 0.01000 - Train loss: 0.90912 - Test loss: 0.89393\n",
      "Epoch 6531 - lr: 0.01000 - Train loss: 0.95252 - Test loss: 0.88953\n",
      "Epoch 6532 - lr: 0.01000 - Train loss: 0.92713 - Test loss: 0.91625\n",
      "Epoch 6533 - lr: 0.01000 - Train loss: 0.87386 - Test loss: 0.89696\n",
      "Epoch 6534 - lr: 0.01000 - Train loss: 0.92177 - Test loss: 0.89753\n",
      "Epoch 6535 - lr: 0.01000 - Train loss: 0.97868 - Test loss: 0.88967\n",
      "Epoch 6536 - lr: 0.01000 - Train loss: 0.96491 - Test loss: 0.89908\n",
      "Epoch 6537 - lr: 0.01000 - Train loss: 0.86524 - Test loss: 0.89372\n",
      "Epoch 6538 - lr: 0.01000 - Train loss: 0.90830 - Test loss: 0.89778\n",
      "Epoch 6539 - lr: 0.01000 - Train loss: 0.97099 - Test loss: 0.89349\n",
      "Epoch 6540 - lr: 0.01000 - Train loss: 0.91164 - Test loss: 0.89948\n",
      "Epoch 6541 - lr: 0.01000 - Train loss: 0.95273 - Test loss: 0.89501\n",
      "Epoch 6542 - lr: 0.01000 - Train loss: 0.92481 - Test loss: 0.92300\n",
      "Epoch 6543 - lr: 0.01000 - Train loss: 0.88670 - Test loss: 0.92623\n",
      "Epoch 6544 - lr: 0.01000 - Train loss: 0.92115 - Test loss: 0.89581\n",
      "Epoch 6545 - lr: 0.01000 - Train loss: 0.95366 - Test loss: 0.89530\n",
      "Epoch 6546 - lr: 0.01000 - Train loss: 0.92449 - Test loss: 0.92257\n",
      "Epoch 6547 - lr: 0.01000 - Train loss: 0.87402 - Test loss: 0.91775\n",
      "Epoch 6548 - lr: 0.01000 - Train loss: 0.85260 - Test loss: 0.89714\n",
      "Epoch 6549 - lr: 0.01000 - Train loss: 0.85885 - Test loss: 0.89321\n",
      "Epoch 6550 - lr: 0.01000 - Train loss: 0.96580 - Test loss: 0.89517\n",
      "Epoch 6551 - lr: 0.01000 - Train loss: 0.95999 - Test loss: 0.89453\n",
      "Epoch 6552 - lr: 0.01000 - Train loss: 0.92294 - Test loss: 0.92246\n",
      "Epoch 6553 - lr: 0.01000 - Train loss: 0.89733 - Test loss: 0.92815\n",
      "Epoch 6554 - lr: 0.01000 - Train loss: 0.91247 - Test loss: 0.90395\n",
      "Epoch 6555 - lr: 0.01000 - Train loss: 0.96885 - Test loss: 0.89903\n",
      "Epoch 6556 - lr: 0.01000 - Train loss: 0.97251 - Test loss: 0.89745\n",
      "Epoch 6557 - lr: 0.01000 - Train loss: 0.99238 - Test loss: 0.91889\n",
      "Epoch 6558 - lr: 0.01000 - Train loss: 0.91142 - Test loss: 0.92778\n",
      "Epoch 6559 - lr: 0.01000 - Train loss: 0.92158 - Test loss: 0.92777\n",
      "Epoch 6560 - lr: 0.01000 - Train loss: 0.89673 - Test loss: 0.92628\n",
      "Epoch 6561 - lr: 0.01000 - Train loss: 0.88008 - Test loss: 0.92035\n",
      "Epoch 6562 - lr: 0.01000 - Train loss: 0.89731 - Test loss: 0.91403\n",
      "Epoch 6563 - lr: 0.01000 - Train loss: 0.91624 - Test loss: 0.92072\n",
      "Epoch 6564 - lr: 0.01000 - Train loss: 0.91766 - Test loss: 0.92101\n",
      "Epoch 6565 - lr: 0.01000 - Train loss: 0.91601 - Test loss: 0.92016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6566 - lr: 0.01000 - Train loss: 0.91794 - Test loss: 0.91886\n",
      "Epoch 6567 - lr: 0.01000 - Train loss: 0.91640 - Test loss: 0.91771\n",
      "Epoch 6568 - lr: 0.01000 - Train loss: 0.91768 - Test loss: 0.91649\n",
      "Epoch 6569 - lr: 0.01000 - Train loss: 0.91711 - Test loss: 0.91535\n",
      "Epoch 6570 - lr: 0.01000 - Train loss: 0.91745 - Test loss: 0.91425\n",
      "Epoch 6571 - lr: 0.01000 - Train loss: 0.91749 - Test loss: 0.91320\n",
      "Epoch 6572 - lr: 0.01000 - Train loss: 0.91760 - Test loss: 0.91219\n",
      "Epoch 6573 - lr: 0.01000 - Train loss: 0.91770 - Test loss: 0.91123\n",
      "Epoch 6574 - lr: 0.01000 - Train loss: 0.91775 - Test loss: 0.91032\n",
      "Epoch 6575 - lr: 0.01000 - Train loss: 0.91761 - Test loss: 0.90950\n",
      "Epoch 6576 - lr: 0.01000 - Train loss: 0.91669 - Test loss: 0.90880\n",
      "Epoch 6577 - lr: 0.01000 - Train loss: 0.91185 - Test loss: 0.90818\n",
      "Epoch 6578 - lr: 0.01000 - Train loss: 0.89999 - Test loss: 0.90706\n",
      "Epoch 6579 - lr: 0.01000 - Train loss: 0.91432 - Test loss: 0.90616\n",
      "Epoch 6580 - lr: 0.01000 - Train loss: 0.87564 - Test loss: 0.89796\n",
      "Epoch 6581 - lr: 0.01000 - Train loss: 0.86176 - Test loss: 0.89005\n",
      "Epoch 6582 - lr: 0.01000 - Train loss: 0.94575 - Test loss: 0.86521\n",
      "Epoch 6583 - lr: 0.01000 - Train loss: 1.00232 - Test loss: 0.86851\n",
      "Epoch 6584 - lr: 0.01000 - Train loss: 0.92653 - Test loss: 0.89689\n",
      "Epoch 6585 - lr: 0.01000 - Train loss: 0.92283 - Test loss: 0.90413\n",
      "Epoch 6586 - lr: 0.01000 - Train loss: 0.90045 - Test loss: 0.90529\n",
      "Epoch 6587 - lr: 0.01000 - Train loss: 0.90084 - Test loss: 0.90479\n",
      "Epoch 6588 - lr: 0.01000 - Train loss: 0.93076 - Test loss: 0.88052\n",
      "Epoch 6589 - lr: 0.01000 - Train loss: 0.94371 - Test loss: 0.89762\n",
      "Epoch 6590 - lr: 0.01000 - Train loss: 0.92746 - Test loss: 0.90080\n",
      "Epoch 6591 - lr: 0.01000 - Train loss: 0.92778 - Test loss: 0.89810\n",
      "Epoch 6592 - lr: 0.01000 - Train loss: 0.92293 - Test loss: 0.86549\n",
      "Epoch 6593 - lr: 0.01000 - Train loss: 0.92459 - Test loss: 0.88911\n",
      "Epoch 6594 - lr: 0.01000 - Train loss: 0.90997 - Test loss: 0.88991\n",
      "Epoch 6595 - lr: 0.01000 - Train loss: 0.92481 - Test loss: 0.89675\n",
      "Epoch 6596 - lr: 0.01000 - Train loss: 0.89750 - Test loss: 0.86781\n",
      "Epoch 6597 - lr: 0.01000 - Train loss: 0.90273 - Test loss: 0.89558\n",
      "Epoch 6598 - lr: 0.01000 - Train loss: 0.92671 - Test loss: 0.90143\n",
      "Epoch 6599 - lr: 0.01000 - Train loss: 0.92782 - Test loss: 0.89968\n",
      "Epoch 6600 - lr: 0.01000 - Train loss: 0.87305 - Test loss: 0.87371\n",
      "Epoch 6601 - lr: 0.01000 - Train loss: 0.89516 - Test loss: 0.87905\n",
      "Epoch 6602 - lr: 0.01000 - Train loss: 0.89901 - Test loss: 0.88951\n",
      "Epoch 6603 - lr: 0.01000 - Train loss: 0.92678 - Test loss: 0.87559\n",
      "Epoch 6604 - lr: 0.01000 - Train loss: 0.97952 - Test loss: 0.86753\n",
      "Epoch 6605 - lr: 0.01000 - Train loss: 0.92560 - Test loss: 0.89118\n",
      "Epoch 6606 - lr: 0.01000 - Train loss: 0.95220 - Test loss: 0.87123\n",
      "Epoch 6607 - lr: 0.01000 - Train loss: 0.95512 - Test loss: 0.87278\n",
      "Epoch 6608 - lr: 0.01000 - Train loss: 0.93092 - Test loss: 0.88384\n",
      "Epoch 6609 - lr: 0.01000 - Train loss: 0.93801 - Test loss: 0.89801\n",
      "Epoch 6610 - lr: 0.01000 - Train loss: 0.91207 - Test loss: 0.88216\n",
      "Epoch 6611 - lr: 0.01000 - Train loss: 1.00670 - Test loss: 0.90044\n",
      "Epoch 6612 - lr: 0.01000 - Train loss: 0.90773 - Test loss: 0.90957\n",
      "Epoch 6613 - lr: 0.01000 - Train loss: 0.92966 - Test loss: 0.88778\n",
      "Epoch 6614 - lr: 0.01000 - Train loss: 0.93219 - Test loss: 0.90340\n",
      "Epoch 6615 - lr: 0.01000 - Train loss: 0.92260 - Test loss: 0.90635\n",
      "Epoch 6616 - lr: 0.01000 - Train loss: 0.88157 - Test loss: 0.90601\n",
      "Epoch 6617 - lr: 0.01000 - Train loss: 0.89123 - Test loss: 0.89350\n",
      "Epoch 6618 - lr: 0.01000 - Train loss: 0.92134 - Test loss: 0.90427\n",
      "Epoch 6619 - lr: 0.01000 - Train loss: 0.92613 - Test loss: 0.88012\n",
      "Epoch 6620 - lr: 0.01000 - Train loss: 0.98786 - Test loss: 0.89636\n",
      "Epoch 6621 - lr: 0.01000 - Train loss: 0.92449 - Test loss: 0.90132\n",
      "Epoch 6622 - lr: 0.01000 - Train loss: 0.88344 - Test loss: 0.87517\n",
      "Epoch 6623 - lr: 0.01000 - Train loss: 0.92458 - Test loss: 0.89718\n",
      "Epoch 6624 - lr: 0.01000 - Train loss: 0.88858 - Test loss: 0.89128\n",
      "Epoch 6625 - lr: 0.01000 - Train loss: 0.92668 - Test loss: 0.90017\n",
      "Epoch 6626 - lr: 0.01000 - Train loss: 0.92169 - Test loss: 0.87571\n",
      "Epoch 6627 - lr: 0.01000 - Train loss: 0.98976 - Test loss: 0.87472\n",
      "Epoch 6628 - lr: 0.01000 - Train loss: 1.00706 - Test loss: 0.88429\n",
      "Epoch 6629 - lr: 0.01000 - Train loss: 0.88683 - Test loss: 0.89476\n",
      "Epoch 6630 - lr: 0.01000 - Train loss: 0.89112 - Test loss: 0.90313\n",
      "Epoch 6631 - lr: 0.01000 - Train loss: 0.93245 - Test loss: 0.88004\n",
      "Epoch 6632 - lr: 0.01000 - Train loss: 0.98881 - Test loss: 0.89951\n",
      "Epoch 6633 - lr: 0.01000 - Train loss: 0.89510 - Test loss: 0.89202\n",
      "Epoch 6634 - lr: 0.01000 - Train loss: 0.88066 - Test loss: 0.89806\n",
      "Epoch 6635 - lr: 0.01000 - Train loss: 0.89593 - Test loss: 0.88224\n",
      "Epoch 6636 - lr: 0.01000 - Train loss: 0.91628 - Test loss: 0.86784\n",
      "Epoch 6637 - lr: 0.01000 - Train loss: 0.92613 - Test loss: 0.89498\n",
      "Epoch 6638 - lr: 0.01000 - Train loss: 0.92677 - Test loss: 0.88777\n",
      "Epoch 6639 - lr: 0.01000 - Train loss: 0.87267 - Test loss: 0.87261\n",
      "Epoch 6640 - lr: 0.01000 - Train loss: 0.92473 - Test loss: 0.89978\n",
      "Epoch 6641 - lr: 0.01000 - Train loss: 0.91677 - Test loss: 0.90694\n",
      "Epoch 6642 - lr: 0.01000 - Train loss: 0.93035 - Test loss: 0.89030\n",
      "Epoch 6643 - lr: 0.01000 - Train loss: 0.91462 - Test loss: 0.90294\n",
      "Epoch 6644 - lr: 0.01000 - Train loss: 0.93050 - Test loss: 0.88229\n",
      "Epoch 6645 - lr: 0.01000 - Train loss: 1.00868 - Test loss: 0.89742\n",
      "Epoch 6646 - lr: 0.01000 - Train loss: 0.92422 - Test loss: 0.89976\n",
      "Epoch 6647 - lr: 0.01000 - Train loss: 0.94666 - Test loss: 0.86929\n",
      "Epoch 6648 - lr: 0.01000 - Train loss: 0.92794 - Test loss: 0.86973\n",
      "Epoch 6649 - lr: 0.01000 - Train loss: 0.90862 - Test loss: 0.86840\n",
      "Epoch 6650 - lr: 0.01000 - Train loss: 0.89532 - Test loss: 0.89218\n",
      "Epoch 6651 - lr: 0.01000 - Train loss: 0.90191 - Test loss: 0.90644\n",
      "Epoch 6652 - lr: 0.01000 - Train loss: 0.92592 - Test loss: 0.90434\n",
      "Epoch 6653 - lr: 0.01000 - Train loss: 0.92902 - Test loss: 0.87325\n",
      "Epoch 6654 - lr: 0.01000 - Train loss: 0.99704 - Test loss: 0.88042\n",
      "Epoch 6655 - lr: 0.01000 - Train loss: 1.00900 - Test loss: 0.87175\n",
      "Epoch 6656 - lr: 0.01000 - Train loss: 0.85495 - Test loss: 0.87560\n",
      "Epoch 6657 - lr: 0.01000 - Train loss: 0.87566 - Test loss: 0.87898\n",
      "Epoch 6658 - lr: 0.01000 - Train loss: 0.90054 - Test loss: 0.89076\n",
      "Epoch 6659 - lr: 0.01000 - Train loss: 0.83734 - Test loss: 0.88029\n",
      "Epoch 6660 - lr: 0.01000 - Train loss: 0.91368 - Test loss: 0.87700\n",
      "Epoch 6661 - lr: 0.01000 - Train loss: 0.83558 - Test loss: 0.87937\n",
      "Epoch 6662 - lr: 0.01000 - Train loss: 0.94722 - Test loss: 0.88014\n",
      "Epoch 6663 - lr: 0.01000 - Train loss: 0.98939 - Test loss: 0.88932\n",
      "Epoch 6664 - lr: 0.01000 - Train loss: 0.90580 - Test loss: 0.91358\n",
      "Epoch 6665 - lr: 0.01000 - Train loss: 0.91356 - Test loss: 0.91905\n",
      "Epoch 6666 - lr: 0.01000 - Train loss: 0.89648 - Test loss: 0.91632\n",
      "Epoch 6667 - lr: 0.01000 - Train loss: 0.91858 - Test loss: 0.91358\n",
      "Epoch 6668 - lr: 0.01000 - Train loss: 0.90177 - Test loss: 0.91491\n",
      "Epoch 6669 - lr: 0.01000 - Train loss: 0.92374 - Test loss: 0.91068\n",
      "Epoch 6670 - lr: 0.01000 - Train loss: 0.91333 - Test loss: 0.87793\n",
      "Epoch 6671 - lr: 0.01000 - Train loss: 0.92925 - Test loss: 0.88559\n",
      "Epoch 6672 - lr: 0.01000 - Train loss: 0.99735 - Test loss: 0.88831\n",
      "Epoch 6673 - lr: 0.01000 - Train loss: 0.97485 - Test loss: 0.90010\n",
      "Epoch 6674 - lr: 0.01000 - Train loss: 0.84027 - Test loss: 0.88240\n",
      "Epoch 6675 - lr: 0.01000 - Train loss: 0.83789 - Test loss: 0.87978\n",
      "Epoch 6676 - lr: 0.01000 - Train loss: 0.88592 - Test loss: 0.87779\n",
      "Epoch 6677 - lr: 0.01000 - Train loss: 0.92392 - Test loss: 0.89634\n",
      "Epoch 6678 - lr: 0.01000 - Train loss: 0.87015 - Test loss: 0.88671\n",
      "Epoch 6679 - lr: 0.01000 - Train loss: 0.89033 - Test loss: 0.87992\n",
      "Epoch 6680 - lr: 0.01000 - Train loss: 0.93086 - Test loss: 0.90663\n",
      "Epoch 6681 - lr: 0.01000 - Train loss: 0.91527 - Test loss: 0.91714\n",
      "Epoch 6682 - lr: 0.01000 - Train loss: 0.92806 - Test loss: 0.89737\n",
      "Epoch 6683 - lr: 0.01000 - Train loss: 0.83968 - Test loss: 0.88268\n",
      "Epoch 6684 - lr: 0.01000 - Train loss: 0.84230 - Test loss: 0.87902\n",
      "Epoch 6685 - lr: 0.01000 - Train loss: 0.91405 - Test loss: 0.87877\n",
      "Epoch 6686 - lr: 0.01000 - Train loss: 0.91945 - Test loss: 0.90911\n",
      "Epoch 6687 - lr: 0.01000 - Train loss: 0.92958 - Test loss: 0.91603\n",
      "Epoch 6688 - lr: 0.01000 - Train loss: 0.92239 - Test loss: 0.91208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6689 - lr: 0.01000 - Train loss: 0.84132 - Test loss: 0.88516\n",
      "Epoch 6690 - lr: 0.01000 - Train loss: 0.84150 - Test loss: 0.87937\n",
      "Epoch 6691 - lr: 0.01000 - Train loss: 0.92694 - Test loss: 0.87759\n",
      "Epoch 6692 - lr: 0.01000 - Train loss: 0.89237 - Test loss: 0.87924\n",
      "Epoch 6693 - lr: 0.01000 - Train loss: 0.92040 - Test loss: 0.90053\n",
      "Epoch 6694 - lr: 0.01000 - Train loss: 0.83753 - Test loss: 0.88635\n",
      "Epoch 6695 - lr: 0.01000 - Train loss: 0.83588 - Test loss: 0.88397\n",
      "Epoch 6696 - lr: 0.01000 - Train loss: 0.83545 - Test loss: 0.88383\n",
      "Epoch 6697 - lr: 0.01000 - Train loss: 0.83733 - Test loss: 0.88359\n",
      "Epoch 6698 - lr: 0.01000 - Train loss: 0.92275 - Test loss: 0.88111\n",
      "Epoch 6699 - lr: 0.01000 - Train loss: 0.90738 - Test loss: 0.89155\n",
      "Epoch 6700 - lr: 0.01000 - Train loss: 0.92547 - Test loss: 0.91330\n",
      "Epoch 6701 - lr: 0.01000 - Train loss: 0.86900 - Test loss: 0.89336\n",
      "Epoch 6702 - lr: 0.01000 - Train loss: 0.93304 - Test loss: 0.88453\n",
      "Epoch 6703 - lr: 0.01000 - Train loss: 0.97046 - Test loss: 0.90941\n",
      "Epoch 6704 - lr: 0.01000 - Train loss: 0.92961 - Test loss: 0.91942\n",
      "Epoch 6705 - lr: 0.01000 - Train loss: 0.97360 - Test loss: 0.89589\n",
      "Epoch 6706 - lr: 0.01000 - Train loss: 0.98551 - Test loss: 0.89427\n",
      "Epoch 6707 - lr: 0.01000 - Train loss: 0.93909 - Test loss: 0.88926\n",
      "Epoch 6708 - lr: 0.01000 - Train loss: 0.92510 - Test loss: 0.91411\n",
      "Epoch 6709 - lr: 0.01000 - Train loss: 0.87742 - Test loss: 0.89282\n",
      "Epoch 6710 - lr: 0.01000 - Train loss: 0.90264 - Test loss: 0.91879\n",
      "Epoch 6711 - lr: 0.01000 - Train loss: 0.92325 - Test loss: 0.92008\n",
      "Epoch 6712 - lr: 0.01000 - Train loss: 0.86411 - Test loss: 0.89213\n",
      "Epoch 6713 - lr: 0.01000 - Train loss: 0.92215 - Test loss: 0.91122\n",
      "Epoch 6714 - lr: 0.01000 - Train loss: 0.83634 - Test loss: 0.89130\n",
      "Epoch 6715 - lr: 0.01000 - Train loss: 0.94479 - Test loss: 0.88988\n",
      "Epoch 6716 - lr: 0.01000 - Train loss: 0.95623 - Test loss: 0.91591\n",
      "Epoch 6717 - lr: 0.01000 - Train loss: 0.91504 - Test loss: 0.89944\n",
      "Epoch 6718 - lr: 0.01000 - Train loss: 0.95638 - Test loss: 0.89007\n",
      "Epoch 6719 - lr: 0.01000 - Train loss: 0.92218 - Test loss: 0.91687\n",
      "Epoch 6720 - lr: 0.01000 - Train loss: 0.92281 - Test loss: 0.91809\n",
      "Epoch 6721 - lr: 0.01000 - Train loss: 0.83838 - Test loss: 0.89212\n",
      "Epoch 6722 - lr: 0.01000 - Train loss: 0.94568 - Test loss: 0.89061\n",
      "Epoch 6723 - lr: 0.01000 - Train loss: 0.85133 - Test loss: 0.88841\n",
      "Epoch 6724 - lr: 0.01000 - Train loss: 0.91941 - Test loss: 0.88591\n",
      "Epoch 6725 - lr: 0.01000 - Train loss: 0.91213 - Test loss: 0.88641\n",
      "Epoch 6726 - lr: 0.01000 - Train loss: 0.91838 - Test loss: 0.88831\n",
      "Epoch 6727 - lr: 0.01000 - Train loss: 0.90115 - Test loss: 0.88977\n",
      "Epoch 6728 - lr: 0.01000 - Train loss: 0.92044 - Test loss: 0.89767\n",
      "Epoch 6729 - lr: 0.01000 - Train loss: 0.96522 - Test loss: 0.89201\n",
      "Epoch 6730 - lr: 0.01000 - Train loss: 0.89646 - Test loss: 0.89378\n",
      "Epoch 6731 - lr: 0.01000 - Train loss: 0.88447 - Test loss: 0.89506\n",
      "Epoch 6732 - lr: 0.01000 - Train loss: 0.92558 - Test loss: 0.92423\n",
      "Epoch 6733 - lr: 0.01000 - Train loss: 0.91711 - Test loss: 0.93173\n",
      "Epoch 6734 - lr: 0.01000 - Train loss: 0.92653 - Test loss: 0.92910\n",
      "Epoch 6735 - lr: 0.01000 - Train loss: 0.90760 - Test loss: 0.89692\n",
      "Epoch 6736 - lr: 0.01000 - Train loss: 0.93224 - Test loss: 0.91962\n",
      "Epoch 6737 - lr: 0.01000 - Train loss: 0.92121 - Test loss: 0.92798\n",
      "Epoch 6738 - lr: 0.01000 - Train loss: 0.91419 - Test loss: 0.92934\n",
      "Epoch 6739 - lr: 0.01000 - Train loss: 0.92621 - Test loss: 0.92706\n",
      "Epoch 6740 - lr: 0.01000 - Train loss: 0.91448 - Test loss: 0.90185\n",
      "Epoch 6741 - lr: 0.01000 - Train loss: 0.97242 - Test loss: 0.89066\n",
      "Epoch 6742 - lr: 0.01000 - Train loss: 0.91005 - Test loss: 0.91880\n",
      "Epoch 6743 - lr: 0.01000 - Train loss: 0.92566 - Test loss: 0.92368\n",
      "Epoch 6744 - lr: 0.01000 - Train loss: 0.92560 - Test loss: 0.90206\n",
      "Epoch 6745 - lr: 0.01000 - Train loss: 0.94792 - Test loss: 0.90505\n",
      "Epoch 6746 - lr: 0.01000 - Train loss: 0.92588 - Test loss: 0.91837\n",
      "Epoch 6747 - lr: 0.01000 - Train loss: 0.91704 - Test loss: 0.91940\n",
      "Epoch 6748 - lr: 0.01000 - Train loss: 0.92461 - Test loss: 0.92105\n",
      "Epoch 6749 - lr: 0.01000 - Train loss: 0.86954 - Test loss: 0.91208\n",
      "Epoch 6750 - lr: 0.01000 - Train loss: 0.89388 - Test loss: 0.90223\n",
      "Epoch 6751 - lr: 0.01000 - Train loss: 0.89926 - Test loss: 0.90624\n",
      "Epoch 6752 - lr: 0.01000 - Train loss: 0.90994 - Test loss: 0.91670\n",
      "Epoch 6753 - lr: 0.01000 - Train loss: 0.91724 - Test loss: 0.91828\n",
      "Epoch 6754 - lr: 0.01000 - Train loss: 0.91379 - Test loss: 0.91773\n",
      "Epoch 6755 - lr: 0.01000 - Train loss: 0.90229 - Test loss: 0.91580\n",
      "Epoch 6756 - lr: 0.01000 - Train loss: 0.92343 - Test loss: 0.91288\n",
      "Epoch 6757 - lr: 0.01000 - Train loss: 0.90645 - Test loss: 0.88643\n",
      "Epoch 6758 - lr: 0.01000 - Train loss: 0.97275 - Test loss: 0.88177\n",
      "Epoch 6759 - lr: 0.01000 - Train loss: 0.98983 - Test loss: 0.88072\n",
      "Epoch 6760 - lr: 0.01000 - Train loss: 1.02281 - Test loss: 0.87833\n",
      "Epoch 6761 - lr: 0.01000 - Train loss: 0.98205 - Test loss: 0.90580\n",
      "Epoch 6762 - lr: 0.01000 - Train loss: 0.93337 - Test loss: 0.90917\n",
      "Epoch 6763 - lr: 0.01000 - Train loss: 0.93005 - Test loss: 0.89513\n",
      "Epoch 6764 - lr: 0.01000 - Train loss: 0.93861 - Test loss: 0.91051\n",
      "Epoch 6765 - lr: 0.01000 - Train loss: 0.91725 - Test loss: 0.88819\n",
      "Epoch 6766 - lr: 0.01000 - Train loss: 0.99157 - Test loss: 0.88665\n",
      "Epoch 6767 - lr: 0.01000 - Train loss: 1.02347 - Test loss: 0.88167\n",
      "Epoch 6768 - lr: 0.01000 - Train loss: 0.93785 - Test loss: 0.88300\n",
      "Epoch 6769 - lr: 0.01000 - Train loss: 0.92439 - Test loss: 0.90945\n",
      "Epoch 6770 - lr: 0.01000 - Train loss: 0.92357 - Test loss: 0.91837\n",
      "Epoch 6771 - lr: 0.01000 - Train loss: 0.90435 - Test loss: 0.92027\n",
      "Epoch 6772 - lr: 0.01000 - Train loss: 0.91890 - Test loss: 0.91972\n",
      "Epoch 6773 - lr: 0.01000 - Train loss: 0.91965 - Test loss: 0.91850\n",
      "Epoch 6774 - lr: 0.01000 - Train loss: 0.92024 - Test loss: 0.91719\n",
      "Epoch 6775 - lr: 0.01000 - Train loss: 0.92090 - Test loss: 0.91588\n",
      "Epoch 6776 - lr: 0.01000 - Train loss: 0.92215 - Test loss: 0.91445\n",
      "Epoch 6777 - lr: 0.01000 - Train loss: 0.92376 - Test loss: 0.91235\n",
      "Epoch 6778 - lr: 0.01000 - Train loss: 0.89059 - Test loss: 0.91171\n",
      "Epoch 6779 - lr: 0.01000 - Train loss: 0.92224 - Test loss: 0.90999\n",
      "Epoch 6780 - lr: 0.01000 - Train loss: 0.88824 - Test loss: 0.90936\n",
      "Epoch 6781 - lr: 0.01000 - Train loss: 0.91869 - Test loss: 0.90856\n",
      "Epoch 6782 - lr: 0.01000 - Train loss: 0.92167 - Test loss: 0.90737\n",
      "Epoch 6783 - lr: 0.01000 - Train loss: 0.91027 - Test loss: 0.90780\n",
      "Epoch 6784 - lr: 0.01000 - Train loss: 0.88066 - Test loss: 0.90195\n",
      "Epoch 6785 - lr: 0.01000 - Train loss: 0.88210 - Test loss: 0.89616\n",
      "Epoch 6786 - lr: 0.01000 - Train loss: 0.92031 - Test loss: 0.90332\n",
      "Epoch 6787 - lr: 0.01000 - Train loss: 0.91064 - Test loss: 0.90497\n",
      "Epoch 6788 - lr: 0.01000 - Train loss: 0.93707 - Test loss: 0.88000\n",
      "Epoch 6789 - lr: 0.01000 - Train loss: 0.96330 - Test loss: 0.89902\n",
      "Epoch 6790 - lr: 0.01000 - Train loss: 0.90394 - Test loss: 0.90325\n",
      "Epoch 6791 - lr: 0.01000 - Train loss: 0.89473 - Test loss: 0.89345\n",
      "Epoch 6792 - lr: 0.01000 - Train loss: 0.85552 - Test loss: 0.86884\n",
      "Epoch 6793 - lr: 0.01000 - Train loss: 0.85061 - Test loss: 0.87671\n",
      "Epoch 6794 - lr: 0.01000 - Train loss: 0.89637 - Test loss: 0.88778\n",
      "Epoch 6795 - lr: 0.01000 - Train loss: 0.89726 - Test loss: 0.89157\n",
      "Epoch 6796 - lr: 0.01000 - Train loss: 0.89738 - Test loss: 0.87777\n",
      "Epoch 6797 - lr: 0.01000 - Train loss: 0.90347 - Test loss: 0.88820\n",
      "Epoch 6798 - lr: 0.01000 - Train loss: 0.90214 - Test loss: 0.89339\n",
      "Epoch 6799 - lr: 0.01000 - Train loss: 0.95242 - Test loss: 0.87012\n",
      "Epoch 6800 - lr: 0.01000 - Train loss: 0.85661 - Test loss: 0.86554\n",
      "Epoch 6801 - lr: 0.01000 - Train loss: 0.97003 - Test loss: 0.86518\n",
      "Epoch 6802 - lr: 0.01000 - Train loss: 0.95362 - Test loss: 0.89256\n",
      "Epoch 6803 - lr: 0.01000 - Train loss: 0.88855 - Test loss: 0.89384\n",
      "Epoch 6804 - lr: 0.01000 - Train loss: 0.89863 - Test loss: 0.88524\n",
      "Epoch 6805 - lr: 0.01000 - Train loss: 0.90438 - Test loss: 0.89020\n",
      "Epoch 6806 - lr: 0.01000 - Train loss: 0.92446 - Test loss: 0.89917\n",
      "Epoch 6807 - lr: 0.01000 - Train loss: 0.91192 - Test loss: 0.90002\n",
      "Epoch 6808 - lr: 0.01000 - Train loss: 0.90765 - Test loss: 0.90432\n",
      "Epoch 6809 - lr: 0.01000 - Train loss: 0.89689 - Test loss: 0.89492\n",
      "Epoch 6810 - lr: 0.01000 - Train loss: 0.91480 - Test loss: 0.88935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6811 - lr: 0.01000 - Train loss: 0.88099 - Test loss: 0.88883\n",
      "Epoch 6812 - lr: 0.01000 - Train loss: 0.98643 - Test loss: 0.86977\n",
      "Epoch 6813 - lr: 0.01000 - Train loss: 0.93861 - Test loss: 0.88797\n",
      "Epoch 6814 - lr: 0.01000 - Train loss: 0.88835 - Test loss: 0.89191\n",
      "Epoch 6815 - lr: 0.01000 - Train loss: 0.92544 - Test loss: 0.87553\n",
      "Epoch 6816 - lr: 0.01000 - Train loss: 0.98562 - Test loss: 0.88237\n",
      "Epoch 6817 - lr: 0.01000 - Train loss: 0.92759 - Test loss: 0.89806\n",
      "Epoch 6818 - lr: 0.01000 - Train loss: 0.90728 - Test loss: 0.90315\n",
      "Epoch 6819 - lr: 0.01000 - Train loss: 0.90205 - Test loss: 0.89511\n",
      "Epoch 6820 - lr: 0.01000 - Train loss: 0.90968 - Test loss: 0.88855\n",
      "Epoch 6821 - lr: 0.01000 - Train loss: 0.92473 - Test loss: 0.87177\n",
      "Epoch 6822 - lr: 0.01000 - Train loss: 0.96537 - Test loss: 0.88607\n",
      "Epoch 6823 - lr: 0.01000 - Train loss: 0.90119 - Test loss: 0.89073\n",
      "Epoch 6824 - lr: 0.01000 - Train loss: 0.90232 - Test loss: 0.90173\n",
      "Epoch 6825 - lr: 0.01000 - Train loss: 0.92627 - Test loss: 0.87674\n",
      "Epoch 6826 - lr: 0.01000 - Train loss: 1.00979 - Test loss: 0.89392\n",
      "Epoch 6827 - lr: 0.01000 - Train loss: 0.89875 - Test loss: 0.89060\n",
      "Epoch 6828 - lr: 0.01000 - Train loss: 0.91038 - Test loss: 0.87427\n",
      "Epoch 6829 - lr: 0.01000 - Train loss: 1.00125 - Test loss: 0.86536\n",
      "Epoch 6830 - lr: 0.01000 - Train loss: 0.92663 - Test loss: 0.89053\n",
      "Epoch 6831 - lr: 0.01000 - Train loss: 0.94167 - Test loss: 0.87525\n",
      "Epoch 6832 - lr: 0.01000 - Train loss: 0.92263 - Test loss: 0.87343\n",
      "Epoch 6833 - lr: 0.01000 - Train loss: 0.94186 - Test loss: 0.87151\n",
      "Epoch 6834 - lr: 0.01000 - Train loss: 0.93967 - Test loss: 0.86933\n",
      "Epoch 6835 - lr: 0.01000 - Train loss: 0.92155 - Test loss: 0.87324\n",
      "Epoch 6836 - lr: 0.01000 - Train loss: 0.92061 - Test loss: 0.87157\n",
      "Epoch 6837 - lr: 0.01000 - Train loss: 0.96409 - Test loss: 0.89523\n",
      "Epoch 6838 - lr: 0.01000 - Train loss: 0.91256 - Test loss: 0.91108\n",
      "Epoch 6839 - lr: 0.01000 - Train loss: 0.89363 - Test loss: 0.91152\n",
      "Epoch 6840 - lr: 0.01000 - Train loss: 0.92706 - Test loss: 0.90921\n",
      "Epoch 6841 - lr: 0.01000 - Train loss: 0.92997 - Test loss: 0.89025\n",
      "Epoch 6842 - lr: 0.01000 - Train loss: 0.93506 - Test loss: 0.88597\n",
      "Epoch 6843 - lr: 0.01000 - Train loss: 0.97965 - Test loss: 0.88626\n",
      "Epoch 6844 - lr: 0.01000 - Train loss: 0.93328 - Test loss: 0.89888\n",
      "Epoch 6845 - lr: 0.01000 - Train loss: 0.83923 - Test loss: 0.87910\n",
      "Epoch 6846 - lr: 0.01000 - Train loss: 0.83996 - Test loss: 0.87495\n",
      "Epoch 6847 - lr: 0.01000 - Train loss: 0.93316 - Test loss: 0.88082\n",
      "Epoch 6848 - lr: 0.01000 - Train loss: 0.92087 - Test loss: 0.89948\n",
      "Epoch 6849 - lr: 0.01000 - Train loss: 0.83997 - Test loss: 0.88001\n",
      "Epoch 6850 - lr: 0.01000 - Train loss: 0.83620 - Test loss: 0.87750\n",
      "Epoch 6851 - lr: 0.01000 - Train loss: 0.94102 - Test loss: 0.88076\n",
      "Epoch 6852 - lr: 0.01000 - Train loss: 0.90546 - Test loss: 0.87785\n",
      "Epoch 6853 - lr: 0.01000 - Train loss: 0.92055 - Test loss: 0.89829\n",
      "Epoch 6854 - lr: 0.01000 - Train loss: 0.83990 - Test loss: 0.88193\n",
      "Epoch 6855 - lr: 0.01000 - Train loss: 0.94629 - Test loss: 0.88360\n",
      "Epoch 6856 - lr: 0.01000 - Train loss: 0.93543 - Test loss: 0.87894\n",
      "Epoch 6857 - lr: 0.01000 - Train loss: 0.96710 - Test loss: 0.90467\n",
      "Epoch 6858 - lr: 0.01000 - Train loss: 0.92819 - Test loss: 0.91426\n",
      "Epoch 6859 - lr: 0.01000 - Train loss: 0.91713 - Test loss: 0.88478\n",
      "Epoch 6860 - lr: 0.01000 - Train loss: 0.93431 - Test loss: 0.90459\n",
      "Epoch 6861 - lr: 0.01000 - Train loss: 0.89027 - Test loss: 0.88611\n",
      "Epoch 6862 - lr: 0.01000 - Train loss: 0.92981 - Test loss: 0.89296\n",
      "Epoch 6863 - lr: 0.01000 - Train loss: 0.97180 - Test loss: 0.89171\n",
      "Epoch 6864 - lr: 0.01000 - Train loss: 0.97098 - Test loss: 0.88398\n",
      "Epoch 6865 - lr: 0.01000 - Train loss: 0.86516 - Test loss: 0.88657\n",
      "Epoch 6866 - lr: 0.01000 - Train loss: 0.92569 - Test loss: 0.89501\n",
      "Epoch 6867 - lr: 0.01000 - Train loss: 1.00576 - Test loss: 0.89585\n",
      "Epoch 6868 - lr: 0.01000 - Train loss: 0.91228 - Test loss: 0.89697\n",
      "Epoch 6869 - lr: 0.01000 - Train loss: 0.96979 - Test loss: 0.88861\n",
      "Epoch 6870 - lr: 0.01000 - Train loss: 0.83849 - Test loss: 0.89170\n",
      "Epoch 6871 - lr: 0.01000 - Train loss: 0.96586 - Test loss: 0.90013\n",
      "Epoch 6872 - lr: 0.01000 - Train loss: 0.87472 - Test loss: 0.89372\n",
      "Epoch 6873 - lr: 0.01000 - Train loss: 0.92664 - Test loss: 0.92065\n",
      "Epoch 6874 - lr: 0.01000 - Train loss: 0.92430 - Test loss: 0.90553\n",
      "Epoch 6875 - lr: 0.01000 - Train loss: 0.95385 - Test loss: 0.89596\n",
      "Epoch 6876 - lr: 0.01000 - Train loss: 0.92338 - Test loss: 0.92227\n",
      "Epoch 6877 - lr: 0.01000 - Train loss: 0.92271 - Test loss: 0.92761\n",
      "Epoch 6878 - lr: 0.01000 - Train loss: 0.89088 - Test loss: 0.92660\n",
      "Epoch 6879 - lr: 0.01000 - Train loss: 0.88562 - Test loss: 0.91592\n",
      "Epoch 6880 - lr: 0.01000 - Train loss: 0.90312 - Test loss: 0.89056\n",
      "Epoch 6881 - lr: 0.01000 - Train loss: 0.91123 - Test loss: 0.89078\n",
      "Epoch 6882 - lr: 0.01000 - Train loss: 0.91724 - Test loss: 0.89976\n",
      "Epoch 6883 - lr: 0.01000 - Train loss: 0.91643 - Test loss: 0.88832\n",
      "Epoch 6884 - lr: 0.01000 - Train loss: 0.96078 - Test loss: 0.88774\n",
      "Epoch 6885 - lr: 0.01000 - Train loss: 0.91019 - Test loss: 0.89006\n",
      "Epoch 6886 - lr: 0.01000 - Train loss: 0.91144 - Test loss: 0.89225\n",
      "Epoch 6887 - lr: 0.01000 - Train loss: 0.87096 - Test loss: 0.89788\n",
      "Epoch 6888 - lr: 0.01000 - Train loss: 0.90776 - Test loss: 0.89475\n",
      "Epoch 6889 - lr: 0.01000 - Train loss: 0.87501 - Test loss: 0.89915\n",
      "Epoch 6890 - lr: 0.01000 - Train loss: 0.91417 - Test loss: 0.90157\n",
      "Epoch 6891 - lr: 0.01000 - Train loss: 0.94581 - Test loss: 0.89748\n",
      "Epoch 6892 - lr: 0.01000 - Train loss: 0.92183 - Test loss: 0.92550\n",
      "Epoch 6893 - lr: 0.01000 - Train loss: 0.91541 - Test loss: 0.93210\n",
      "Epoch 6894 - lr: 0.01000 - Train loss: 0.92281 - Test loss: 0.93122\n",
      "Epoch 6895 - lr: 0.01000 - Train loss: 0.90912 - Test loss: 0.90547\n",
      "Epoch 6896 - lr: 0.01000 - Train loss: 0.98104 - Test loss: 0.89686\n",
      "Epoch 6897 - lr: 0.01000 - Train loss: 0.90005 - Test loss: 0.89406\n",
      "Epoch 6898 - lr: 0.01000 - Train loss: 0.90668 - Test loss: 0.90665\n",
      "Epoch 6899 - lr: 0.01000 - Train loss: 0.91550 - Test loss: 0.90205\n",
      "Epoch 6900 - lr: 0.01000 - Train loss: 0.93920 - Test loss: 0.89837\n",
      "Epoch 6901 - lr: 0.01000 - Train loss: 0.91525 - Test loss: 0.90043\n",
      "Epoch 6902 - lr: 0.01000 - Train loss: 0.93352 - Test loss: 0.89878\n",
      "Epoch 6903 - lr: 0.01000 - Train loss: 0.89256 - Test loss: 0.89487\n",
      "Epoch 6904 - lr: 0.01000 - Train loss: 0.88362 - Test loss: 0.91687\n",
      "Epoch 6905 - lr: 0.01000 - Train loss: 0.89338 - Test loss: 0.91406\n",
      "Epoch 6906 - lr: 0.01000 - Train loss: 0.89420 - Test loss: 0.91414\n",
      "Epoch 6907 - lr: 0.01000 - Train loss: 0.89893 - Test loss: 0.91726\n",
      "Epoch 6908 - lr: 0.01000 - Train loss: 0.91496 - Test loss: 0.90231\n",
      "Epoch 6909 - lr: 0.01000 - Train loss: 0.96285 - Test loss: 0.89888\n",
      "Epoch 6910 - lr: 0.01000 - Train loss: 0.97140 - Test loss: 0.89794\n",
      "Epoch 6911 - lr: 0.01000 - Train loss: 0.98927 - Test loss: 0.91945\n",
      "Epoch 6912 - lr: 0.01000 - Train loss: 0.91050 - Test loss: 0.92805\n",
      "Epoch 6913 - lr: 0.01000 - Train loss: 0.91925 - Test loss: 0.92818\n",
      "Epoch 6914 - lr: 0.01000 - Train loss: 0.90517 - Test loss: 0.92731\n",
      "Epoch 6915 - lr: 0.01000 - Train loss: 0.91860 - Test loss: 0.92551\n",
      "Epoch 6916 - lr: 0.01000 - Train loss: 0.91032 - Test loss: 0.92441\n",
      "Epoch 6917 - lr: 0.01000 - Train loss: 0.91885 - Test loss: 0.92268\n",
      "Epoch 6918 - lr: 0.01000 - Train loss: 0.91415 - Test loss: 0.92143\n",
      "Epoch 6919 - lr: 0.01000 - Train loss: 0.91801 - Test loss: 0.91991\n",
      "Epoch 6920 - lr: 0.01000 - Train loss: 0.91503 - Test loss: 0.91874\n",
      "Epoch 6921 - lr: 0.01000 - Train loss: 0.91749 - Test loss: 0.91742\n",
      "Epoch 6922 - lr: 0.01000 - Train loss: 0.91616 - Test loss: 0.91626\n",
      "Epoch 6923 - lr: 0.01000 - Train loss: 0.91692 - Test loss: 0.91510\n",
      "Epoch 6924 - lr: 0.01000 - Train loss: 0.91674 - Test loss: 0.91401\n",
      "Epoch 6925 - lr: 0.01000 - Train loss: 0.91689 - Test loss: 0.91297\n",
      "Epoch 6926 - lr: 0.01000 - Train loss: 0.91699 - Test loss: 0.91197\n",
      "Epoch 6927 - lr: 0.01000 - Train loss: 0.91707 - Test loss: 0.91103\n",
      "Epoch 6928 - lr: 0.01000 - Train loss: 0.91707 - Test loss: 0.91014\n",
      "Epoch 6929 - lr: 0.01000 - Train loss: 0.91672 - Test loss: 0.90935\n",
      "Epoch 6930 - lr: 0.01000 - Train loss: 0.91483 - Test loss: 0.90871\n",
      "Epoch 6931 - lr: 0.01000 - Train loss: 0.90552 - Test loss: 0.90801\n",
      "Epoch 6932 - lr: 0.01000 - Train loss: 0.89652 - Test loss: 0.90625\n",
      "Epoch 6933 - lr: 0.01000 - Train loss: 0.91991 - Test loss: 0.90460\n",
      "Epoch 6934 - lr: 0.01000 - Train loss: 0.91951 - Test loss: 0.90374\n",
      "Epoch 6935 - lr: 0.01000 - Train loss: 0.91955 - Test loss: 0.90309\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6936 - lr: 0.01000 - Train loss: 0.91959 - Test loss: 0.90254\n",
      "Epoch 6937 - lr: 0.01000 - Train loss: 0.91949 - Test loss: 0.90208\n",
      "Epoch 6938 - lr: 0.01000 - Train loss: 0.91831 - Test loss: 0.90189\n",
      "Epoch 6939 - lr: 0.01000 - Train loss: 0.90292 - Test loss: 0.90184\n",
      "Epoch 6940 - lr: 0.01000 - Train loss: 0.92613 - Test loss: 0.87547\n",
      "Epoch 6941 - lr: 0.01000 - Train loss: 0.98772 - Test loss: 0.86864\n",
      "Epoch 6942 - lr: 0.01000 - Train loss: 0.92540 - Test loss: 0.89134\n",
      "Epoch 6943 - lr: 0.01000 - Train loss: 0.93901 - Test loss: 0.87606\n",
      "Epoch 6944 - lr: 0.01000 - Train loss: 0.99547 - Test loss: 0.86821\n",
      "Epoch 6945 - lr: 0.01000 - Train loss: 0.92413 - Test loss: 0.88941\n",
      "Epoch 6946 - lr: 0.01000 - Train loss: 0.92100 - Test loss: 0.86600\n",
      "Epoch 6947 - lr: 0.01000 - Train loss: 0.93690 - Test loss: 0.89307\n",
      "Epoch 6948 - lr: 0.01000 - Train loss: 0.92701 - Test loss: 0.88222\n",
      "Epoch 6949 - lr: 0.01000 - Train loss: 0.87543 - Test loss: 0.86762\n",
      "Epoch 6950 - lr: 0.01000 - Train loss: 0.89658 - Test loss: 0.89661\n",
      "Epoch 6951 - lr: 0.01000 - Train loss: 0.87943 - Test loss: 0.89758\n",
      "Epoch 6952 - lr: 0.01000 - Train loss: 0.84910 - Test loss: 0.87008\n",
      "Epoch 6953 - lr: 0.01000 - Train loss: 0.91945 - Test loss: 0.87555\n",
      "Epoch 6954 - lr: 0.01000 - Train loss: 0.93521 - Test loss: 0.89112\n",
      "Epoch 6955 - lr: 0.01000 - Train loss: 0.84084 - Test loss: 0.87050\n",
      "Epoch 6956 - lr: 0.01000 - Train loss: 0.86273 - Test loss: 0.86760\n",
      "Epoch 6957 - lr: 0.01000 - Train loss: 0.96532 - Test loss: 0.86853\n",
      "Epoch 6958 - lr: 0.01000 - Train loss: 1.00233 - Test loss: 0.87404\n",
      "Epoch 6959 - lr: 0.01000 - Train loss: 0.98099 - Test loss: 0.86911\n",
      "Epoch 6960 - lr: 0.01000 - Train loss: 0.92364 - Test loss: 0.87725\n",
      "Epoch 6961 - lr: 0.01000 - Train loss: 0.98576 - Test loss: 0.87270\n",
      "Epoch 6962 - lr: 0.01000 - Train loss: 0.90951 - Test loss: 0.89647\n",
      "Epoch 6963 - lr: 0.01000 - Train loss: 0.92277 - Test loss: 0.90719\n",
      "Epoch 6964 - lr: 0.01000 - Train loss: 0.90214 - Test loss: 0.88054\n",
      "Epoch 6965 - lr: 0.01000 - Train loss: 0.94368 - Test loss: 0.87752\n",
      "Epoch 6966 - lr: 0.01000 - Train loss: 0.99258 - Test loss: 0.87973\n",
      "Epoch 6967 - lr: 0.01000 - Train loss: 0.97518 - Test loss: 0.90790\n",
      "Epoch 6968 - lr: 0.01000 - Train loss: 0.91652 - Test loss: 0.88468\n",
      "Epoch 6969 - lr: 0.01000 - Train loss: 0.87074 - Test loss: 0.88782\n",
      "Epoch 6970 - lr: 0.01000 - Train loss: 0.93265 - Test loss: 0.89014\n",
      "Epoch 6971 - lr: 0.01000 - Train loss: 0.93542 - Test loss: 0.90972\n",
      "Epoch 6972 - lr: 0.01000 - Train loss: 0.92435 - Test loss: 0.91745\n",
      "Epoch 6973 - lr: 0.01000 - Train loss: 0.92550 - Test loss: 0.92142\n",
      "Epoch 6974 - lr: 0.01000 - Train loss: 0.92375 - Test loss: 0.91944\n",
      "Epoch 6975 - lr: 0.01000 - Train loss: 0.90150 - Test loss: 0.91553\n",
      "Epoch 6976 - lr: 0.01000 - Train loss: 0.89848 - Test loss: 0.91331\n",
      "Epoch 6977 - lr: 0.01000 - Train loss: 0.87001 - Test loss: 0.88820\n",
      "Epoch 6978 - lr: 0.01000 - Train loss: 0.86292 - Test loss: 0.88239\n",
      "Epoch 6979 - lr: 0.01000 - Train loss: 0.89774 - Test loss: 0.90437\n",
      "Epoch 6980 - lr: 0.01000 - Train loss: 0.90397 - Test loss: 0.90251\n",
      "Epoch 6981 - lr: 0.01000 - Train loss: 0.89276 - Test loss: 0.90691\n",
      "Epoch 6982 - lr: 0.01000 - Train loss: 0.92101 - Test loss: 0.91537\n",
      "Epoch 6983 - lr: 0.01000 - Train loss: 0.91428 - Test loss: 0.91675\n",
      "Epoch 6984 - lr: 0.01000 - Train loss: 0.91994 - Test loss: 0.91594\n",
      "Epoch 6985 - lr: 0.01000 - Train loss: 0.91930 - Test loss: 0.91458\n",
      "Epoch 6986 - lr: 0.01000 - Train loss: 0.91515 - Test loss: 0.91391\n",
      "Epoch 6987 - lr: 0.01000 - Train loss: 0.91933 - Test loss: 0.91292\n",
      "Epoch 6988 - lr: 0.01000 - Train loss: 0.91957 - Test loss: 0.91165\n",
      "Epoch 6989 - lr: 0.01000 - Train loss: 0.91775 - Test loss: 0.91070\n",
      "Epoch 6990 - lr: 0.01000 - Train loss: 0.91877 - Test loss: 0.90975\n",
      "Epoch 6991 - lr: 0.01000 - Train loss: 0.91812 - Test loss: 0.90892\n",
      "Epoch 6992 - lr: 0.01000 - Train loss: 0.91853 - Test loss: 0.90811\n",
      "Epoch 6993 - lr: 0.01000 - Train loss: 0.91851 - Test loss: 0.90733\n",
      "Epoch 6994 - lr: 0.01000 - Train loss: 0.91856 - Test loss: 0.90660\n",
      "Epoch 6995 - lr: 0.01000 - Train loss: 0.91853 - Test loss: 0.90593\n",
      "Epoch 6996 - lr: 0.01000 - Train loss: 0.91801 - Test loss: 0.90542\n",
      "Epoch 6997 - lr: 0.01000 - Train loss: 0.91370 - Test loss: 0.90519\n",
      "Epoch 6998 - lr: 0.01000 - Train loss: 0.87920 - Test loss: 0.89905\n",
      "Epoch 6999 - lr: 0.01000 - Train loss: 0.86552 - Test loss: 0.89026\n",
      "Epoch 7000 - lr: 0.01000 - Train loss: 0.86796 - Test loss: 0.86811\n",
      "Epoch 7001 - lr: 0.01000 - Train loss: 0.88729 - Test loss: 0.86483\n",
      "Epoch 7002 - lr: 0.01000 - Train loss: 0.91734 - Test loss: 0.89474\n",
      "Epoch 7003 - lr: 0.01000 - Train loss: 0.89431 - Test loss: 0.90138\n",
      "Epoch 7004 - lr: 0.01000 - Train loss: 0.91773 - Test loss: 0.90201\n",
      "Epoch 7005 - lr: 0.01000 - Train loss: 0.91932 - Test loss: 0.90204\n",
      "Epoch 7006 - lr: 0.01000 - Train loss: 0.91284 - Test loss: 0.90218\n",
      "Epoch 7007 - lr: 0.01000 - Train loss: 0.92080 - Test loss: 0.87435\n",
      "Epoch 7008 - lr: 0.01000 - Train loss: 1.00170 - Test loss: 0.87782\n",
      "Epoch 7009 - lr: 0.01000 - Train loss: 0.89266 - Test loss: 0.86416\n",
      "Epoch 7010 - lr: 0.01000 - Train loss: 0.90344 - Test loss: 0.89342\n",
      "Epoch 7011 - lr: 0.01000 - Train loss: 0.90142 - Test loss: 0.90089\n",
      "Epoch 7012 - lr: 0.01000 - Train loss: 0.94220 - Test loss: 0.87945\n",
      "Epoch 7013 - lr: 0.01000 - Train loss: 0.90414 - Test loss: 0.89328\n",
      "Epoch 7014 - lr: 0.01000 - Train loss: 0.91763 - Test loss: 0.86426\n",
      "Epoch 7015 - lr: 0.01000 - Train loss: 0.93002 - Test loss: 0.88976\n",
      "Epoch 7016 - lr: 0.01000 - Train loss: 0.87378 - Test loss: 0.87070\n",
      "Epoch 7017 - lr: 0.01000 - Train loss: 0.88752 - Test loss: 0.86482\n",
      "Epoch 7018 - lr: 0.01000 - Train loss: 0.91701 - Test loss: 0.89486\n",
      "Epoch 7019 - lr: 0.01000 - Train loss: 0.92661 - Test loss: 0.88710\n",
      "Epoch 7020 - lr: 0.01000 - Train loss: 0.88517 - Test loss: 0.89588\n",
      "Epoch 7021 - lr: 0.01000 - Train loss: 0.92488 - Test loss: 0.86496\n",
      "Epoch 7022 - lr: 0.01000 - Train loss: 0.92398 - Test loss: 0.89256\n",
      "Epoch 7023 - lr: 0.01000 - Train loss: 0.92503 - Test loss: 0.89743\n",
      "Epoch 7024 - lr: 0.01000 - Train loss: 0.88951 - Test loss: 0.86917\n",
      "Epoch 7025 - lr: 0.01000 - Train loss: 0.91723 - Test loss: 0.89542\n",
      "Epoch 7026 - lr: 0.01000 - Train loss: 0.87607 - Test loss: 0.89469\n",
      "Epoch 7027 - lr: 0.01000 - Train loss: 0.92851 - Test loss: 0.86526\n",
      "Epoch 7028 - lr: 0.01000 - Train loss: 0.93107 - Test loss: 0.88283\n",
      "Epoch 7029 - lr: 0.01000 - Train loss: 0.96472 - Test loss: 0.86776\n",
      "Epoch 7030 - lr: 0.01000 - Train loss: 0.96484 - Test loss: 0.88700\n",
      "Epoch 7031 - lr: 0.01000 - Train loss: 0.91830 - Test loss: 0.86804\n",
      "Epoch 7032 - lr: 0.01000 - Train loss: 0.93070 - Test loss: 0.89700\n",
      "Epoch 7033 - lr: 0.01000 - Train loss: 0.92885 - Test loss: 0.90373\n",
      "Epoch 7034 - lr: 0.01000 - Train loss: 0.90775 - Test loss: 0.90659\n",
      "Epoch 7035 - lr: 0.01000 - Train loss: 0.91457 - Test loss: 0.89294\n",
      "Epoch 7036 - lr: 0.01000 - Train loss: 0.92478 - Test loss: 0.89803\n",
      "Epoch 7037 - lr: 0.01000 - Train loss: 0.85271 - Test loss: 0.87161\n",
      "Epoch 7038 - lr: 0.01000 - Train loss: 0.90318 - Test loss: 0.86708\n",
      "Epoch 7039 - lr: 0.01000 - Train loss: 0.92999 - Test loss: 0.89279\n",
      "Epoch 7040 - lr: 0.01000 - Train loss: 0.85374 - Test loss: 0.87369\n",
      "Epoch 7041 - lr: 0.01000 - Train loss: 0.83702 - Test loss: 0.87046\n",
      "Epoch 7042 - lr: 0.01000 - Train loss: 0.83754 - Test loss: 0.87027\n",
      "Epoch 7043 - lr: 0.01000 - Train loss: 0.83721 - Test loss: 0.87083\n",
      "Epoch 7044 - lr: 0.01000 - Train loss: 0.83694 - Test loss: 0.87156\n",
      "Epoch 7045 - lr: 0.01000 - Train loss: 0.83656 - Test loss: 0.87234\n",
      "Epoch 7046 - lr: 0.01000 - Train loss: 0.83619 - Test loss: 0.87310\n",
      "Epoch 7047 - lr: 0.01000 - Train loss: 0.83581 - Test loss: 0.87384\n",
      "Epoch 7048 - lr: 0.01000 - Train loss: 0.83541 - Test loss: 0.87455\n",
      "Epoch 7049 - lr: 0.01000 - Train loss: 0.83497 - Test loss: 0.87515\n",
      "Epoch 7050 - lr: 0.01000 - Train loss: 0.88807 - Test loss: 0.87341\n",
      "Epoch 7051 - lr: 0.01000 - Train loss: 0.93414 - Test loss: 0.88186\n",
      "Epoch 7052 - lr: 0.01000 - Train loss: 0.97910 - Test loss: 0.90180\n",
      "Epoch 7053 - lr: 0.01000 - Train loss: 0.83778 - Test loss: 0.88203\n",
      "Epoch 7054 - lr: 0.01000 - Train loss: 0.83543 - Test loss: 0.87869\n",
      "Epoch 7055 - lr: 0.01000 - Train loss: 0.89963 - Test loss: 0.88971\n",
      "Epoch 7056 - lr: 0.01000 - Train loss: 0.83564 - Test loss: 0.88079\n",
      "Epoch 7057 - lr: 0.01000 - Train loss: 0.84152 - Test loss: 0.87854\n",
      "Epoch 7058 - lr: 0.01000 - Train loss: 0.93561 - Test loss: 0.88769\n",
      "Epoch 7059 - lr: 0.01000 - Train loss: 0.92978 - Test loss: 0.90607\n",
      "Epoch 7060 - lr: 0.01000 - Train loss: 0.83980 - Test loss: 0.88377\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7061 - lr: 0.01000 - Train loss: 0.83749 - Test loss: 0.87970\n",
      "Epoch 7062 - lr: 0.01000 - Train loss: 0.91015 - Test loss: 0.87722\n",
      "Epoch 7063 - lr: 0.01000 - Train loss: 0.94247 - Test loss: 0.88826\n",
      "Epoch 7064 - lr: 0.01000 - Train loss: 0.97223 - Test loss: 0.90644\n",
      "Epoch 7065 - lr: 0.01000 - Train loss: 0.84855 - Test loss: 0.88341\n",
      "Epoch 7066 - lr: 0.01000 - Train loss: 0.92413 - Test loss: 0.88682\n",
      "Epoch 7067 - lr: 0.01000 - Train loss: 0.91936 - Test loss: 0.88442\n",
      "Epoch 7068 - lr: 0.01000 - Train loss: 0.92818 - Test loss: 0.88158\n",
      "Epoch 7069 - lr: 0.01000 - Train loss: 0.94384 - Test loss: 0.88375\n",
      "Epoch 7070 - lr: 0.01000 - Train loss: 0.92274 - Test loss: 0.90825\n",
      "Epoch 7071 - lr: 0.01000 - Train loss: 0.84931 - Test loss: 0.88855\n",
      "Epoch 7072 - lr: 0.01000 - Train loss: 0.91564 - Test loss: 0.89328\n",
      "Epoch 7073 - lr: 0.01000 - Train loss: 0.94703 - Test loss: 0.88721\n",
      "Epoch 7074 - lr: 0.01000 - Train loss: 0.92686 - Test loss: 0.91425\n",
      "Epoch 7075 - lr: 0.01000 - Train loss: 0.86863 - Test loss: 0.89628\n",
      "Epoch 7076 - lr: 0.01000 - Train loss: 0.84627 - Test loss: 0.89068\n",
      "Epoch 7077 - lr: 0.01000 - Train loss: 0.87899 - Test loss: 0.88818\n",
      "Epoch 7078 - lr: 0.01000 - Train loss: 0.92217 - Test loss: 0.91653\n",
      "Epoch 7079 - lr: 0.01000 - Train loss: 0.92227 - Test loss: 0.91753\n",
      "Epoch 7080 - lr: 0.01000 - Train loss: 0.84613 - Test loss: 0.89047\n",
      "Epoch 7081 - lr: 0.01000 - Train loss: 0.92571 - Test loss: 0.91376\n",
      "Epoch 7082 - lr: 0.01000 - Train loss: 0.92898 - Test loss: 0.92118\n",
      "Epoch 7083 - lr: 0.01000 - Train loss: 0.91733 - Test loss: 0.92235\n",
      "Epoch 7084 - lr: 0.01000 - Train loss: 0.92703 - Test loss: 0.91865\n",
      "Epoch 7085 - lr: 0.01000 - Train loss: 0.91374 - Test loss: 0.88610\n",
      "Epoch 7086 - lr: 0.01000 - Train loss: 0.92850 - Test loss: 0.89039\n",
      "Epoch 7087 - lr: 0.01000 - Train loss: 0.93947 - Test loss: 0.88941\n",
      "Epoch 7088 - lr: 0.01000 - Train loss: 0.91093 - Test loss: 0.88515\n",
      "Epoch 7089 - lr: 0.01000 - Train loss: 0.93133 - Test loss: 0.89073\n",
      "Epoch 7090 - lr: 0.01000 - Train loss: 0.92594 - Test loss: 0.88966\n",
      "Epoch 7091 - lr: 0.01000 - Train loss: 0.93146 - Test loss: 0.88708\n",
      "Epoch 7092 - lr: 0.01000 - Train loss: 0.98387 - Test loss: 0.88968\n",
      "Epoch 7093 - lr: 0.01000 - Train loss: 0.94920 - Test loss: 0.91797\n",
      "Epoch 7094 - lr: 0.01000 - Train loss: 0.92205 - Test loss: 0.91137\n",
      "Epoch 7095 - lr: 0.01000 - Train loss: 0.84740 - Test loss: 0.89584\n",
      "Epoch 7096 - lr: 0.01000 - Train loss: 0.86810 - Test loss: 0.89159\n",
      "Epoch 7097 - lr: 0.01000 - Train loss: 0.90250 - Test loss: 0.91929\n",
      "Epoch 7098 - lr: 0.01000 - Train loss: 0.91880 - Test loss: 0.92571\n",
      "Epoch 7099 - lr: 0.01000 - Train loss: 0.92378 - Test loss: 0.92439\n",
      "Epoch 7100 - lr: 0.01000 - Train loss: 0.90249 - Test loss: 0.92044\n",
      "Epoch 7101 - lr: 0.01000 - Train loss: 0.91424 - Test loss: 0.92017\n",
      "Epoch 7102 - lr: 0.01000 - Train loss: 0.92319 - Test loss: 0.92122\n",
      "Epoch 7103 - lr: 0.01000 - Train loss: 0.92094 - Test loss: 0.89844\n",
      "Epoch 7104 - lr: 0.01000 - Train loss: 0.95571 - Test loss: 0.91594\n",
      "Epoch 7105 - lr: 0.01000 - Train loss: 0.90820 - Test loss: 0.92026\n",
      "Epoch 7106 - lr: 0.01000 - Train loss: 0.91845 - Test loss: 0.92030\n",
      "Epoch 7107 - lr: 0.01000 - Train loss: 0.91893 - Test loss: 0.91922\n",
      "Epoch 7108 - lr: 0.01000 - Train loss: 0.91797 - Test loss: 0.91808\n",
      "Epoch 7109 - lr: 0.01000 - Train loss: 0.91002 - Test loss: 0.91697\n",
      "Epoch 7110 - lr: 0.01000 - Train loss: 0.90134 - Test loss: 0.91477\n",
      "Epoch 7111 - lr: 0.01000 - Train loss: 0.92368 - Test loss: 0.91148\n",
      "Epoch 7112 - lr: 0.01000 - Train loss: 0.93008 - Test loss: 0.89190\n",
      "Epoch 7113 - lr: 0.01000 - Train loss: 0.89938 - Test loss: 0.89479\n",
      "Epoch 7114 - lr: 0.01000 - Train loss: 0.85202 - Test loss: 0.87895\n",
      "Epoch 7115 - lr: 0.01000 - Train loss: 0.87758 - Test loss: 0.89575\n",
      "Epoch 7116 - lr: 0.01000 - Train loss: 0.90982 - Test loss: 0.88346\n",
      "Epoch 7117 - lr: 0.01000 - Train loss: 0.99923 - Test loss: 0.88551\n",
      "Epoch 7118 - lr: 0.01000 - Train loss: 0.95318 - Test loss: 0.89285\n",
      "Epoch 7119 - lr: 0.01000 - Train loss: 0.89750 - Test loss: 0.90862\n",
      "Epoch 7120 - lr: 0.01000 - Train loss: 0.91161 - Test loss: 0.91212\n",
      "Epoch 7121 - lr: 0.01000 - Train loss: 0.93689 - Test loss: 0.88769\n",
      "Epoch 7122 - lr: 0.01000 - Train loss: 0.97735 - Test loss: 0.90595\n",
      "Epoch 7123 - lr: 0.01000 - Train loss: 0.92943 - Test loss: 0.88810\n",
      "Epoch 7124 - lr: 0.01000 - Train loss: 0.93441 - Test loss: 0.89367\n",
      "Epoch 7125 - lr: 0.01000 - Train loss: 0.85148 - Test loss: 0.87513\n",
      "Epoch 7126 - lr: 0.01000 - Train loss: 0.92484 - Test loss: 0.89903\n",
      "Epoch 7127 - lr: 0.01000 - Train loss: 0.91323 - Test loss: 0.87477\n",
      "Epoch 7128 - lr: 0.01000 - Train loss: 0.92474 - Test loss: 0.90121\n",
      "Epoch 7129 - lr: 0.01000 - Train loss: 0.91622 - Test loss: 0.88507\n",
      "Epoch 7130 - lr: 0.01000 - Train loss: 0.95036 - Test loss: 0.87854\n",
      "Epoch 7131 - lr: 0.01000 - Train loss: 0.90672 - Test loss: 0.89612\n",
      "Epoch 7132 - lr: 0.01000 - Train loss: 0.92394 - Test loss: 0.90647\n",
      "Epoch 7133 - lr: 0.01000 - Train loss: 0.90599 - Test loss: 0.87820\n",
      "Epoch 7134 - lr: 0.01000 - Train loss: 0.92497 - Test loss: 0.90166\n",
      "Epoch 7135 - lr: 0.01000 - Train loss: 0.85418 - Test loss: 0.88071\n",
      "Epoch 7136 - lr: 0.01000 - Train loss: 0.90061 - Test loss: 0.89775\n",
      "Epoch 7137 - lr: 0.01000 - Train loss: 0.92612 - Test loss: 0.90942\n",
      "Epoch 7138 - lr: 0.01000 - Train loss: 0.90085 - Test loss: 0.91276\n",
      "Epoch 7139 - lr: 0.01000 - Train loss: 0.88810 - Test loss: 0.89673\n",
      "Epoch 7140 - lr: 0.01000 - Train loss: 0.92680 - Test loss: 0.90563\n",
      "Epoch 7141 - lr: 0.01000 - Train loss: 0.92311 - Test loss: 0.88200\n",
      "Epoch 7142 - lr: 0.01000 - Train loss: 0.96680 - Test loss: 0.87471\n",
      "Epoch 7143 - lr: 0.01000 - Train loss: 0.92467 - Test loss: 0.90149\n",
      "Epoch 7144 - lr: 0.01000 - Train loss: 0.92672 - Test loss: 0.88978\n",
      "Epoch 7145 - lr: 0.01000 - Train loss: 0.87362 - Test loss: 0.87914\n",
      "Epoch 7146 - lr: 0.01000 - Train loss: 0.89492 - Test loss: 0.89680\n",
      "Epoch 7147 - lr: 0.01000 - Train loss: 0.90955 - Test loss: 0.90983\n",
      "Epoch 7148 - lr: 0.01000 - Train loss: 0.88701 - Test loss: 0.89572\n",
      "Epoch 7149 - lr: 0.01000 - Train loss: 0.92627 - Test loss: 0.90428\n",
      "Epoch 7150 - lr: 0.01000 - Train loss: 0.94911 - Test loss: 0.87658\n",
      "Epoch 7151 - lr: 0.01000 - Train loss: 0.94553 - Test loss: 0.90029\n",
      "Epoch 7152 - lr: 0.01000 - Train loss: 0.92616 - Test loss: 0.89486\n",
      "Epoch 7153 - lr: 0.01000 - Train loss: 0.90879 - Test loss: 0.88367\n",
      "Epoch 7154 - lr: 0.01000 - Train loss: 0.92751 - Test loss: 0.90330\n",
      "Epoch 7155 - lr: 0.01000 - Train loss: 0.92028 - Test loss: 0.89106\n",
      "Epoch 7156 - lr: 0.01000 - Train loss: 0.89427 - Test loss: 0.87760\n",
      "Epoch 7157 - lr: 0.01000 - Train loss: 0.92702 - Test loss: 0.90248\n",
      "Epoch 7158 - lr: 0.01000 - Train loss: 0.92270 - Test loss: 0.90707\n",
      "Epoch 7159 - lr: 0.01000 - Train loss: 0.92511 - Test loss: 0.88937\n",
      "Epoch 7160 - lr: 0.01000 - Train loss: 0.90349 - Test loss: 0.89426\n",
      "Epoch 7161 - lr: 0.01000 - Train loss: 0.92387 - Test loss: 0.88803\n",
      "Epoch 7162 - lr: 0.01000 - Train loss: 0.90297 - Test loss: 0.88458\n",
      "Epoch 7163 - lr: 0.01000 - Train loss: 0.89975 - Test loss: 0.87339\n",
      "Epoch 7164 - lr: 0.01000 - Train loss: 0.92419 - Test loss: 0.90196\n",
      "Epoch 7165 - lr: 0.01000 - Train loss: 0.92281 - Test loss: 0.90858\n",
      "Epoch 7166 - lr: 0.01000 - Train loss: 0.92411 - Test loss: 0.90807\n",
      "Epoch 7167 - lr: 0.01000 - Train loss: 0.90845 - Test loss: 0.88256\n",
      "Epoch 7168 - lr: 0.01000 - Train loss: 0.98164 - Test loss: 0.87872\n",
      "Epoch 7169 - lr: 0.01000 - Train loss: 0.99693 - Test loss: 0.89996\n",
      "Epoch 7170 - lr: 0.01000 - Train loss: 0.90340 - Test loss: 0.90886\n",
      "Epoch 7171 - lr: 0.01000 - Train loss: 0.91948 - Test loss: 0.90997\n",
      "Epoch 7172 - lr: 0.01000 - Train loss: 0.88740 - Test loss: 0.90594\n",
      "Epoch 7173 - lr: 0.01000 - Train loss: 0.89408 - Test loss: 0.87416\n",
      "Epoch 7174 - lr: 0.01000 - Train loss: 0.92776 - Test loss: 0.89905\n",
      "Epoch 7175 - lr: 0.01000 - Train loss: 0.89503 - Test loss: 0.90634\n",
      "Epoch 7176 - lr: 0.01000 - Train loss: 0.87797 - Test loss: 0.90033\n",
      "Epoch 7177 - lr: 0.01000 - Train loss: 0.92801 - Test loss: 0.87832\n",
      "Epoch 7178 - lr: 0.01000 - Train loss: 0.92599 - Test loss: 0.89731\n",
      "Epoch 7179 - lr: 0.01000 - Train loss: 0.93577 - Test loss: 0.87296\n",
      "Epoch 7180 - lr: 0.01000 - Train loss: 0.96174 - Test loss: 0.87023\n",
      "Epoch 7181 - lr: 0.01000 - Train loss: 1.00165 - Test loss: 0.87695\n",
      "Epoch 7182 - lr: 0.01000 - Train loss: 1.02435 - Test loss: 0.88367\n",
      "Epoch 7183 - lr: 0.01000 - Train loss: 0.92408 - Test loss: 0.90559\n",
      "Epoch 7184 - lr: 0.01000 - Train loss: 0.92486 - Test loss: 0.90891\n",
      "Epoch 7185 - lr: 0.01000 - Train loss: 0.90994 - Test loss: 0.88452\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7186 - lr: 0.01000 - Train loss: 1.00001 - Test loss: 0.88135\n",
      "Epoch 7187 - lr: 0.01000 - Train loss: 1.01010 - Test loss: 0.89952\n",
      "Epoch 7188 - lr: 0.01000 - Train loss: 0.91530 - Test loss: 0.88596\n",
      "Epoch 7189 - lr: 0.01000 - Train loss: 1.00170 - Test loss: 0.88719\n",
      "Epoch 7190 - lr: 0.01000 - Train loss: 0.94733 - Test loss: 0.88443\n",
      "Epoch 7191 - lr: 0.01000 - Train loss: 0.99409 - Test loss: 0.87604\n",
      "Epoch 7192 - lr: 0.01000 - Train loss: 0.89560 - Test loss: 0.89983\n",
      "Epoch 7193 - lr: 0.01000 - Train loss: 0.89329 - Test loss: 0.89264\n",
      "Epoch 7194 - lr: 0.01000 - Train loss: 0.85070 - Test loss: 0.88214\n",
      "Epoch 7195 - lr: 0.01000 - Train loss: 0.90307 - Test loss: 0.89333\n",
      "Epoch 7196 - lr: 0.01000 - Train loss: 0.89945 - Test loss: 0.89926\n",
      "Epoch 7197 - lr: 0.01000 - Train loss: 0.86431 - Test loss: 0.88206\n",
      "Epoch 7198 - lr: 0.01000 - Train loss: 0.92199 - Test loss: 0.88529\n",
      "Epoch 7199 - lr: 0.01000 - Train loss: 1.00313 - Test loss: 0.90331\n",
      "Epoch 7200 - lr: 0.01000 - Train loss: 0.89528 - Test loss: 0.88355\n",
      "Epoch 7201 - lr: 0.01000 - Train loss: 0.92459 - Test loss: 0.90960\n",
      "Epoch 7202 - lr: 0.01000 - Train loss: 0.92034 - Test loss: 0.89381\n",
      "Epoch 7203 - lr: 0.01000 - Train loss: 0.97424 - Test loss: 0.91339\n",
      "Epoch 7204 - lr: 0.01000 - Train loss: 0.92236 - Test loss: 0.91660\n",
      "Epoch 7205 - lr: 0.01000 - Train loss: 0.88824 - Test loss: 0.91589\n",
      "Epoch 7206 - lr: 0.01000 - Train loss: 0.90602 - Test loss: 0.88978\n",
      "Epoch 7207 - lr: 0.01000 - Train loss: 0.95375 - Test loss: 0.90883\n",
      "Epoch 7208 - lr: 0.01000 - Train loss: 0.90809 - Test loss: 0.88856\n",
      "Epoch 7209 - lr: 0.01000 - Train loss: 0.97936 - Test loss: 0.88661\n",
      "Epoch 7210 - lr: 0.01000 - Train loss: 0.97786 - Test loss: 0.90908\n",
      "Epoch 7211 - lr: 0.01000 - Train loss: 0.92285 - Test loss: 0.91304\n",
      "Epoch 7212 - lr: 0.01000 - Train loss: 0.91043 - Test loss: 0.88893\n",
      "Epoch 7213 - lr: 0.01000 - Train loss: 0.99555 - Test loss: 0.88804\n",
      "Epoch 7214 - lr: 0.01000 - Train loss: 0.96812 - Test loss: 0.91044\n",
      "Epoch 7215 - lr: 0.01000 - Train loss: 0.90949 - Test loss: 0.91494\n",
      "Epoch 7216 - lr: 0.01000 - Train loss: 0.90263 - Test loss: 0.91418\n",
      "Epoch 7217 - lr: 0.01000 - Train loss: 0.92561 - Test loss: 0.91102\n",
      "Epoch 7218 - lr: 0.01000 - Train loss: 0.92955 - Test loss: 0.89159\n",
      "Epoch 7219 - lr: 0.01000 - Train loss: 0.91773 - Test loss: 0.88289\n",
      "Epoch 7220 - lr: 0.01000 - Train loss: 0.92707 - Test loss: 0.90361\n",
      "Epoch 7221 - lr: 0.01000 - Train loss: 0.93156 - Test loss: 0.88684\n",
      "Epoch 7222 - lr: 0.01000 - Train loss: 0.96754 - Test loss: 0.90755\n",
      "Epoch 7223 - lr: 0.01000 - Train loss: 0.92644 - Test loss: 0.88999\n",
      "Epoch 7224 - lr: 0.01000 - Train loss: 0.91695 - Test loss: 0.87310\n",
      "Epoch 7225 - lr: 0.01000 - Train loss: 0.98821 - Test loss: 0.88401\n",
      "Epoch 7226 - lr: 0.01000 - Train loss: 0.95133 - Test loss: 0.90690\n",
      "Epoch 7227 - lr: 0.01000 - Train loss: 0.92654 - Test loss: 0.90730\n",
      "Epoch 7228 - lr: 0.01000 - Train loss: 0.85615 - Test loss: 0.87897\n",
      "Epoch 7229 - lr: 0.01000 - Train loss: 0.85370 - Test loss: 0.87623\n",
      "Epoch 7230 - lr: 0.01000 - Train loss: 0.83650 - Test loss: 0.87601\n",
      "Epoch 7231 - lr: 0.01000 - Train loss: 0.83839 - Test loss: 0.87567\n",
      "Epoch 7232 - lr: 0.01000 - Train loss: 0.94955 - Test loss: 0.87700\n",
      "Epoch 7233 - lr: 0.01000 - Train loss: 1.00165 - Test loss: 0.88654\n",
      "Epoch 7234 - lr: 0.01000 - Train loss: 0.97755 - Test loss: 0.90507\n",
      "Epoch 7235 - lr: 0.01000 - Train loss: 0.84164 - Test loss: 0.88362\n",
      "Epoch 7236 - lr: 0.01000 - Train loss: 0.93144 - Test loss: 0.87836\n",
      "Epoch 7237 - lr: 0.01000 - Train loss: 0.97611 - Test loss: 0.89789\n",
      "Epoch 7238 - lr: 0.01000 - Train loss: 0.83978 - Test loss: 0.88517\n",
      "Epoch 7239 - lr: 0.01000 - Train loss: 0.86870 - Test loss: 0.88275\n",
      "Epoch 7240 - lr: 0.01000 - Train loss: 0.89987 - Test loss: 0.91157\n",
      "Epoch 7241 - lr: 0.01000 - Train loss: 0.92317 - Test loss: 0.91422\n",
      "Epoch 7242 - lr: 0.01000 - Train loss: 0.84279 - Test loss: 0.88802\n",
      "Epoch 7243 - lr: 0.01000 - Train loss: 0.93313 - Test loss: 0.88115\n",
      "Epoch 7244 - lr: 0.01000 - Train loss: 0.98008 - Test loss: 0.90391\n",
      "Epoch 7245 - lr: 0.01000 - Train loss: 0.92044 - Test loss: 0.89348\n",
      "Epoch 7246 - lr: 0.01000 - Train loss: 1.02199 - Test loss: 0.88835\n",
      "Epoch 7247 - lr: 0.01000 - Train loss: 0.98211 - Test loss: 0.89326\n",
      "Epoch 7248 - lr: 0.01000 - Train loss: 0.95960 - Test loss: 0.88782\n",
      "Epoch 7249 - lr: 0.01000 - Train loss: 0.92902 - Test loss: 0.91541\n",
      "Epoch 7250 - lr: 0.01000 - Train loss: 0.92665 - Test loss: 0.92236\n",
      "Epoch 7251 - lr: 0.01000 - Train loss: 0.90845 - Test loss: 0.89242\n",
      "Epoch 7252 - lr: 0.01000 - Train loss: 0.92986 - Test loss: 0.91608\n",
      "Epoch 7253 - lr: 0.01000 - Train loss: 0.92480 - Test loss: 0.92245\n",
      "Epoch 7254 - lr: 0.01000 - Train loss: 0.87244 - Test loss: 0.89575\n",
      "Epoch 7255 - lr: 0.01000 - Train loss: 0.92942 - Test loss: 0.90206\n",
      "Epoch 7256 - lr: 0.01000 - Train loss: 0.88840 - Test loss: 0.90558\n",
      "Epoch 7257 - lr: 0.01000 - Train loss: 0.92461 - Test loss: 0.91872\n",
      "Epoch 7258 - lr: 0.01000 - Train loss: 0.93088 - Test loss: 0.91696\n",
      "Epoch 7259 - lr: 0.01000 - Train loss: 0.91618 - Test loss: 0.89591\n",
      "Epoch 7260 - lr: 0.01000 - Train loss: 0.99045 - Test loss: 0.90534\n",
      "Epoch 7261 - lr: 0.01000 - Train loss: 0.89641 - Test loss: 0.90725\n",
      "Epoch 7262 - lr: 0.01000 - Train loss: 0.90257 - Test loss: 0.90883\n",
      "Epoch 7263 - lr: 0.01000 - Train loss: 0.87170 - Test loss: 0.88943\n",
      "Epoch 7264 - lr: 0.01000 - Train loss: 0.91266 - Test loss: 0.91570\n",
      "Epoch 7265 - lr: 0.01000 - Train loss: 0.92026 - Test loss: 0.92090\n",
      "Epoch 7266 - lr: 0.01000 - Train loss: 0.90373 - Test loss: 0.92158\n",
      "Epoch 7267 - lr: 0.01000 - Train loss: 0.91930 - Test loss: 0.92030\n",
      "Epoch 7268 - lr: 0.01000 - Train loss: 0.91697 - Test loss: 0.91890\n",
      "Epoch 7269 - lr: 0.01000 - Train loss: 0.91645 - Test loss: 0.91772\n",
      "Epoch 7270 - lr: 0.01000 - Train loss: 0.91720 - Test loss: 0.91650\n",
      "Epoch 7271 - lr: 0.01000 - Train loss: 0.91624 - Test loss: 0.91545\n",
      "Epoch 7272 - lr: 0.01000 - Train loss: 0.91732 - Test loss: 0.91432\n",
      "Epoch 7273 - lr: 0.01000 - Train loss: 0.91657 - Test loss: 0.91331\n",
      "Epoch 7274 - lr: 0.01000 - Train loss: 0.91711 - Test loss: 0.91230\n",
      "Epoch 7275 - lr: 0.01000 - Train loss: 0.91702 - Test loss: 0.91134\n",
      "Epoch 7276 - lr: 0.01000 - Train loss: 0.91711 - Test loss: 0.91042\n",
      "Epoch 7277 - lr: 0.01000 - Train loss: 0.91717 - Test loss: 0.90956\n",
      "Epoch 7278 - lr: 0.01000 - Train loss: 0.91713 - Test loss: 0.90876\n",
      "Epoch 7279 - lr: 0.01000 - Train loss: 0.91662 - Test loss: 0.90808\n",
      "Epoch 7280 - lr: 0.01000 - Train loss: 0.91336 - Test loss: 0.90761\n",
      "Epoch 7281 - lr: 0.01000 - Train loss: 0.89483 - Test loss: 0.90579\n",
      "Epoch 7282 - lr: 0.01000 - Train loss: 0.91475 - Test loss: 0.90488\n",
      "Epoch 7283 - lr: 0.01000 - Train loss: 0.91263 - Test loss: 0.90482\n",
      "Epoch 7284 - lr: 0.01000 - Train loss: 0.91040 - Test loss: 0.88434\n",
      "Epoch 7285 - lr: 0.01000 - Train loss: 0.88907 - Test loss: 0.89832\n",
      "Epoch 7286 - lr: 0.01000 - Train loss: 0.91754 - Test loss: 0.90154\n",
      "Epoch 7287 - lr: 0.01000 - Train loss: 0.92657 - Test loss: 0.87878\n",
      "Epoch 7288 - lr: 0.01000 - Train loss: 0.88067 - Test loss: 0.86591\n",
      "Epoch 7289 - lr: 0.01000 - Train loss: 0.88134 - Test loss: 0.86459\n",
      "Epoch 7290 - lr: 0.01000 - Train loss: 0.92612 - Test loss: 0.86831\n",
      "Epoch 7291 - lr: 0.01000 - Train loss: 0.98138 - Test loss: 0.89619\n",
      "Epoch 7292 - lr: 0.01000 - Train loss: 0.89280 - Test loss: 0.90157\n",
      "Epoch 7293 - lr: 0.01000 - Train loss: 0.90202 - Test loss: 0.90325\n",
      "Epoch 7294 - lr: 0.01000 - Train loss: 0.93640 - Test loss: 0.88387\n",
      "Epoch 7295 - lr: 0.01000 - Train loss: 0.88601 - Test loss: 0.86681\n",
      "Epoch 7296 - lr: 0.01000 - Train loss: 0.90325 - Test loss: 0.89475\n",
      "Epoch 7297 - lr: 0.01000 - Train loss: 0.94206 - Test loss: 0.88049\n",
      "Epoch 7298 - lr: 0.01000 - Train loss: 0.88991 - Test loss: 0.88797\n",
      "Epoch 7299 - lr: 0.01000 - Train loss: 0.90129 - Test loss: 0.88971\n",
      "Epoch 7300 - lr: 0.01000 - Train loss: 0.91984 - Test loss: 0.90012\n",
      "Epoch 7301 - lr: 0.01000 - Train loss: 0.91927 - Test loss: 0.88114\n",
      "Epoch 7302 - lr: 0.01000 - Train loss: 0.87308 - Test loss: 0.86944\n",
      "Epoch 7303 - lr: 0.01000 - Train loss: 0.89627 - Test loss: 0.88588\n",
      "Epoch 7304 - lr: 0.01000 - Train loss: 0.88787 - Test loss: 0.89675\n",
      "Epoch 7305 - lr: 0.01000 - Train loss: 0.89614 - Test loss: 0.89470\n",
      "Epoch 7306 - lr: 0.01000 - Train loss: 0.93581 - Test loss: 0.89100\n",
      "Epoch 7307 - lr: 0.01000 - Train loss: 0.88703 - Test loss: 0.86695\n",
      "Epoch 7308 - lr: 0.01000 - Train loss: 0.92423 - Test loss: 0.89311\n",
      "Epoch 7309 - lr: 0.01000 - Train loss: 0.91036 - Test loss: 0.90078\n",
      "Epoch 7310 - lr: 0.01000 - Train loss: 0.91768 - Test loss: 0.88135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7311 - lr: 0.01000 - Train loss: 0.95868 - Test loss: 0.86506\n",
      "Epoch 7312 - lr: 0.01000 - Train loss: 0.94921 - Test loss: 0.89175\n",
      "Epoch 7313 - lr: 0.01000 - Train loss: 0.87080 - Test loss: 0.87232\n",
      "Epoch 7314 - lr: 0.01000 - Train loss: 0.86329 - Test loss: 0.88581\n",
      "Epoch 7315 - lr: 0.01000 - Train loss: 0.87468 - Test loss: 0.87093\n",
      "Epoch 7316 - lr: 0.01000 - Train loss: 0.92829 - Test loss: 0.87221\n",
      "Epoch 7317 - lr: 0.01000 - Train loss: 0.97625 - Test loss: 0.87946\n",
      "Epoch 7318 - lr: 0.01000 - Train loss: 0.90151 - Test loss: 0.89468\n",
      "Epoch 7319 - lr: 0.01000 - Train loss: 0.90649 - Test loss: 0.88709\n",
      "Epoch 7320 - lr: 0.01000 - Train loss: 0.91330 - Test loss: 0.90146\n",
      "Epoch 7321 - lr: 0.01000 - Train loss: 0.92238 - Test loss: 0.87714\n",
      "Epoch 7322 - lr: 0.01000 - Train loss: 0.99027 - Test loss: 0.89593\n",
      "Epoch 7323 - lr: 0.01000 - Train loss: 0.90619 - Test loss: 0.88694\n",
      "Epoch 7324 - lr: 0.01000 - Train loss: 0.87848 - Test loss: 0.89440\n",
      "Epoch 7325 - lr: 0.01000 - Train loss: 0.90555 - Test loss: 0.88884\n",
      "Epoch 7326 - lr: 0.01000 - Train loss: 0.90252 - Test loss: 0.89554\n",
      "Epoch 7327 - lr: 0.01000 - Train loss: 0.88016 - Test loss: 0.89946\n",
      "Epoch 7328 - lr: 0.01000 - Train loss: 0.90597 - Test loss: 0.87427\n",
      "Epoch 7329 - lr: 0.01000 - Train loss: 0.95102 - Test loss: 0.89428\n",
      "Epoch 7330 - lr: 0.01000 - Train loss: 0.92442 - Test loss: 0.87279\n",
      "Epoch 7331 - lr: 0.01000 - Train loss: 0.99116 - Test loss: 0.89424\n",
      "Epoch 7332 - lr: 0.01000 - Train loss: 0.87968 - Test loss: 0.89907\n",
      "Epoch 7333 - lr: 0.01000 - Train loss: 0.91802 - Test loss: 0.87635\n",
      "Epoch 7334 - lr: 0.01000 - Train loss: 0.99106 - Test loss: 0.89485\n",
      "Epoch 7335 - lr: 0.01000 - Train loss: 0.91355 - Test loss: 0.88331\n",
      "Epoch 7336 - lr: 0.01000 - Train loss: 0.86916 - Test loss: 0.89294\n",
      "Epoch 7337 - lr: 0.01000 - Train loss: 0.89920 - Test loss: 0.89125\n",
      "Epoch 7338 - lr: 0.01000 - Train loss: 0.92987 - Test loss: 0.87447\n",
      "Epoch 7339 - lr: 0.01000 - Train loss: 0.98233 - Test loss: 0.89595\n",
      "Epoch 7340 - lr: 0.01000 - Train loss: 0.92177 - Test loss: 0.88504\n",
      "Epoch 7341 - lr: 0.01000 - Train loss: 0.92761 - Test loss: 0.89632\n",
      "Epoch 7342 - lr: 0.01000 - Train loss: 0.88925 - Test loss: 0.90000\n",
      "Epoch 7343 - lr: 0.01000 - Train loss: 0.92703 - Test loss: 0.87826\n",
      "Epoch 7344 - lr: 0.01000 - Train loss: 0.90876 - Test loss: 0.88348\n",
      "Epoch 7345 - lr: 0.01000 - Train loss: 0.87745 - Test loss: 0.88573\n",
      "Epoch 7346 - lr: 0.01000 - Train loss: 0.92965 - Test loss: 0.89325\n",
      "Epoch 7347 - lr: 0.01000 - Train loss: 0.90029 - Test loss: 0.89255\n",
      "Epoch 7348 - lr: 0.01000 - Train loss: 0.94492 - Test loss: 0.86953\n",
      "Epoch 7349 - lr: 0.01000 - Train loss: 0.88037 - Test loss: 0.88268\n",
      "Epoch 7350 - lr: 0.01000 - Train loss: 0.98713 - Test loss: 0.86672\n",
      "Epoch 7351 - lr: 0.01000 - Train loss: 1.00596 - Test loss: 0.87423\n",
      "Epoch 7352 - lr: 0.01000 - Train loss: 0.91654 - Test loss: 0.88050\n",
      "Epoch 7353 - lr: 0.01000 - Train loss: 0.92593 - Test loss: 0.89509\n",
      "Epoch 7354 - lr: 0.01000 - Train loss: 0.92700 - Test loss: 0.88384\n",
      "Epoch 7355 - lr: 0.01000 - Train loss: 0.91231 - Test loss: 0.88895\n",
      "Epoch 7356 - lr: 0.01000 - Train loss: 0.91869 - Test loss: 0.89949\n",
      "Epoch 7357 - lr: 0.01000 - Train loss: 0.92414 - Test loss: 0.90063\n",
      "Epoch 7358 - lr: 0.01000 - Train loss: 0.86954 - Test loss: 0.89711\n",
      "Epoch 7359 - lr: 0.01000 - Train loss: 0.91377 - Test loss: 0.88881\n",
      "Epoch 7360 - lr: 0.01000 - Train loss: 0.93029 - Test loss: 0.87583\n",
      "Epoch 7361 - lr: 0.01000 - Train loss: 0.86889 - Test loss: 0.88653\n",
      "Epoch 7362 - lr: 0.01000 - Train loss: 0.90666 - Test loss: 0.87248\n",
      "Epoch 7363 - lr: 0.01000 - Train loss: 0.98145 - Test loss: 0.87111\n",
      "Epoch 7364 - lr: 0.01000 - Train loss: 0.94351 - Test loss: 0.89047\n",
      "Epoch 7365 - lr: 0.01000 - Train loss: 0.90142 - Test loss: 0.89138\n",
      "Epoch 7366 - lr: 0.01000 - Train loss: 0.89726 - Test loss: 0.90054\n",
      "Epoch 7367 - lr: 0.01000 - Train loss: 0.92373 - Test loss: 0.90121\n",
      "Epoch 7368 - lr: 0.01000 - Train loss: 0.92514 - Test loss: 0.89981\n",
      "Epoch 7369 - lr: 0.01000 - Train loss: 0.89972 - Test loss: 0.88092\n",
      "Epoch 7370 - lr: 0.01000 - Train loss: 0.85964 - Test loss: 0.88678\n",
      "Epoch 7371 - lr: 0.01000 - Train loss: 0.90212 - Test loss: 0.88752\n",
      "Epoch 7372 - lr: 0.01000 - Train loss: 0.91260 - Test loss: 0.89836\n",
      "Epoch 7373 - lr: 0.01000 - Train loss: 0.90152 - Test loss: 0.89112\n",
      "Epoch 7374 - lr: 0.01000 - Train loss: 0.90873 - Test loss: 0.88657\n",
      "Epoch 7375 - lr: 0.01000 - Train loss: 0.92495 - Test loss: 0.89422\n",
      "Epoch 7376 - lr: 0.01000 - Train loss: 0.87565 - Test loss: 0.88837\n",
      "Epoch 7377 - lr: 0.01000 - Train loss: 0.90573 - Test loss: 0.88752\n",
      "Epoch 7378 - lr: 0.01000 - Train loss: 0.94129 - Test loss: 0.87603\n",
      "Epoch 7379 - lr: 0.01000 - Train loss: 0.88009 - Test loss: 0.88461\n",
      "Epoch 7380 - lr: 0.01000 - Train loss: 0.91808 - Test loss: 0.89611\n",
      "Epoch 7381 - lr: 0.01000 - Train loss: 0.90345 - Test loss: 0.89017\n",
      "Epoch 7382 - lr: 0.01000 - Train loss: 0.90781 - Test loss: 0.87115\n",
      "Epoch 7383 - lr: 0.01000 - Train loss: 0.92615 - Test loss: 0.88883\n",
      "Epoch 7384 - lr: 0.01000 - Train loss: 0.93310 - Test loss: 0.89167\n",
      "Epoch 7385 - lr: 0.01000 - Train loss: 0.89368 - Test loss: 0.88937\n",
      "Epoch 7386 - lr: 0.01000 - Train loss: 0.90644 - Test loss: 0.88098\n",
      "Epoch 7387 - lr: 0.01000 - Train loss: 0.91465 - Test loss: 0.88033\n",
      "Epoch 7388 - lr: 0.01000 - Train loss: 0.91068 - Test loss: 0.88552\n",
      "Epoch 7389 - lr: 0.01000 - Train loss: 0.89183 - Test loss: 0.89500\n",
      "Epoch 7390 - lr: 0.01000 - Train loss: 0.92284 - Test loss: 0.89558\n",
      "Epoch 7391 - lr: 0.01000 - Train loss: 0.91078 - Test loss: 0.89738\n",
      "Epoch 7392 - lr: 0.01000 - Train loss: 0.90382 - Test loss: 0.89277\n",
      "Epoch 7393 - lr: 0.01000 - Train loss: 0.90876 - Test loss: 0.88459\n",
      "Epoch 7394 - lr: 0.01000 - Train loss: 0.92538 - Test loss: 0.89201\n",
      "Epoch 7395 - lr: 0.01000 - Train loss: 0.92561 - Test loss: 0.87436\n",
      "Epoch 7396 - lr: 0.01000 - Train loss: 0.85825 - Test loss: 0.87734\n",
      "Epoch 7397 - lr: 0.01000 - Train loss: 0.89109 - Test loss: 0.88378\n",
      "Epoch 7398 - lr: 0.01000 - Train loss: 0.89882 - Test loss: 0.88894\n",
      "Epoch 7399 - lr: 0.01000 - Train loss: 0.90456 - Test loss: 0.88487\n",
      "Epoch 7400 - lr: 0.01000 - Train loss: 0.92569 - Test loss: 0.86826\n",
      "Epoch 7401 - lr: 0.01000 - Train loss: 0.99803 - Test loss: 0.88808\n",
      "Epoch 7402 - lr: 0.01000 - Train loss: 0.91869 - Test loss: 0.87005\n",
      "Epoch 7403 - lr: 0.01000 - Train loss: 0.99657 - Test loss: 0.88969\n",
      "Epoch 7404 - lr: 0.01000 - Train loss: 0.90073 - Test loss: 0.89696\n",
      "Epoch 7405 - lr: 0.01000 - Train loss: 0.96001 - Test loss: 0.87078\n",
      "Epoch 7406 - lr: 0.01000 - Train loss: 0.96028 - Test loss: 0.88325\n",
      "Epoch 7407 - lr: 0.01000 - Train loss: 0.91065 - Test loss: 0.88234\n",
      "Epoch 7408 - lr: 0.01000 - Train loss: 0.91438 - Test loss: 0.86516\n",
      "Epoch 7409 - lr: 0.01000 - Train loss: 0.92791 - Test loss: 0.89098\n",
      "Epoch 7410 - lr: 0.01000 - Train loss: 0.92733 - Test loss: 0.89546\n",
      "Epoch 7411 - lr: 0.01000 - Train loss: 0.93560 - Test loss: 0.87357\n",
      "Epoch 7412 - lr: 0.01000 - Train loss: 0.94417 - Test loss: 0.89178\n",
      "Epoch 7413 - lr: 0.01000 - Train loss: 0.92772 - Test loss: 0.87095\n",
      "Epoch 7414 - lr: 0.01000 - Train loss: 1.01109 - Test loss: 0.86932\n",
      "Epoch 7415 - lr: 0.01000 - Train loss: 1.01451 - Test loss: 0.87688\n",
      "Epoch 7416 - lr: 0.01000 - Train loss: 0.89072 - Test loss: 0.88475\n",
      "Epoch 7417 - lr: 0.01000 - Train loss: 0.92712 - Test loss: 0.89743\n",
      "Epoch 7418 - lr: 0.01000 - Train loss: 0.88437 - Test loss: 0.88856\n",
      "Epoch 7419 - lr: 0.01000 - Train loss: 0.92551 - Test loss: 0.89685\n",
      "Epoch 7420 - lr: 0.01000 - Train loss: 0.91472 - Test loss: 0.89772\n",
      "Epoch 7421 - lr: 0.01000 - Train loss: 0.93206 - Test loss: 0.87641\n",
      "Epoch 7422 - lr: 0.01000 - Train loss: 0.98195 - Test loss: 0.89679\n",
      "Epoch 7423 - lr: 0.01000 - Train loss: 0.89547 - Test loss: 0.89057\n",
      "Epoch 7424 - lr: 0.01000 - Train loss: 0.93404 - Test loss: 0.89472\n",
      "Epoch 7425 - lr: 0.01000 - Train loss: 0.91830 - Test loss: 0.88021\n",
      "Epoch 7426 - lr: 0.01000 - Train loss: 0.92223 - Test loss: 0.89356\n",
      "Epoch 7427 - lr: 0.01000 - Train loss: 0.90337 - Test loss: 0.88975\n",
      "Epoch 7428 - lr: 0.01000 - Train loss: 0.91635 - Test loss: 0.86387\n",
      "Epoch 7429 - lr: 0.01000 - Train loss: 0.92855 - Test loss: 0.88898\n",
      "Epoch 7430 - lr: 0.01000 - Train loss: 0.88208 - Test loss: 0.86595\n",
      "Epoch 7431 - lr: 0.01000 - Train loss: 0.92576 - Test loss: 0.89049\n",
      "Epoch 7432 - lr: 0.01000 - Train loss: 0.90708 - Test loss: 0.86650\n",
      "Epoch 7433 - lr: 0.01000 - Train loss: 0.92998 - Test loss: 0.89310\n",
      "Epoch 7434 - lr: 0.01000 - Train loss: 0.92915 - Test loss: 0.88261\n",
      "Epoch 7435 - lr: 0.01000 - Train loss: 0.91243 - Test loss: 0.86689\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7436 - lr: 0.01000 - Train loss: 0.92692 - Test loss: 0.89276\n",
      "Epoch 7437 - lr: 0.01000 - Train loss: 0.88489 - Test loss: 0.87009\n",
      "Epoch 7438 - lr: 0.01000 - Train loss: 0.92749 - Test loss: 0.89505\n",
      "Epoch 7439 - lr: 0.01000 - Train loss: 0.87380 - Test loss: 0.87477\n",
      "Epoch 7440 - lr: 0.01000 - Train loss: 0.91095 - Test loss: 0.89075\n",
      "Epoch 7441 - lr: 0.01000 - Train loss: 0.92267 - Test loss: 0.90229\n",
      "Epoch 7442 - lr: 0.01000 - Train loss: 0.92506 - Test loss: 0.88492\n",
      "Epoch 7443 - lr: 0.01000 - Train loss: 0.89951 - Test loss: 0.90156\n",
      "Epoch 7444 - lr: 0.01000 - Train loss: 0.92458 - Test loss: 0.90405\n",
      "Epoch 7445 - lr: 0.01000 - Train loss: 0.87927 - Test loss: 0.90320\n",
      "Epoch 7446 - lr: 0.01000 - Train loss: 0.90150 - Test loss: 0.88926\n",
      "Epoch 7447 - lr: 0.01000 - Train loss: 0.92649 - Test loss: 0.87698\n",
      "Epoch 7448 - lr: 0.01000 - Train loss: 0.98865 - Test loss: 0.89519\n",
      "Epoch 7449 - lr: 0.01000 - Train loss: 0.90290 - Test loss: 0.86962\n",
      "Epoch 7450 - lr: 0.01000 - Train loss: 0.92251 - Test loss: 0.89759\n",
      "Epoch 7451 - lr: 0.01000 - Train loss: 0.87566 - Test loss: 0.89638\n",
      "Epoch 7452 - lr: 0.01000 - Train loss: 0.95403 - Test loss: 0.87085\n",
      "Epoch 7453 - lr: 0.01000 - Train loss: 0.98580 - Test loss: 0.89341\n",
      "Epoch 7454 - lr: 0.01000 - Train loss: 0.91753 - Test loss: 0.87961\n",
      "Epoch 7455 - lr: 0.01000 - Train loss: 0.92732 - Test loss: 0.89985\n",
      "Epoch 7456 - lr: 0.01000 - Train loss: 0.92145 - Test loss: 0.90378\n",
      "Epoch 7457 - lr: 0.01000 - Train loss: 0.90849 - Test loss: 0.90603\n",
      "Epoch 7458 - lr: 0.01000 - Train loss: 0.90534 - Test loss: 0.89232\n",
      "Epoch 7459 - lr: 0.01000 - Train loss: 0.92724 - Test loss: 0.89914\n",
      "Epoch 7460 - lr: 0.01000 - Train loss: 0.93675 - Test loss: 0.89430\n",
      "Epoch 7461 - lr: 0.01000 - Train loss: 0.89365 - Test loss: 0.86982\n",
      "Epoch 7462 - lr: 0.01000 - Train loss: 0.92749 - Test loss: 0.89407\n",
      "Epoch 7463 - lr: 0.01000 - Train loss: 0.87394 - Test loss: 0.87397\n",
      "Epoch 7464 - lr: 0.01000 - Train loss: 0.92830 - Test loss: 0.88135\n",
      "Epoch 7465 - lr: 0.01000 - Train loss: 0.89978 - Test loss: 0.87230\n",
      "Epoch 7466 - lr: 0.01000 - Train loss: 0.91644 - Test loss: 0.89957\n",
      "Epoch 7467 - lr: 0.01000 - Train loss: 0.93296 - Test loss: 0.88117\n",
      "Epoch 7468 - lr: 0.01000 - Train loss: 0.97829 - Test loss: 0.90142\n",
      "Epoch 7469 - lr: 0.01000 - Train loss: 0.92615 - Test loss: 0.88044\n",
      "Epoch 7470 - lr: 0.01000 - Train loss: 1.00593 - Test loss: 0.87609\n",
      "Epoch 7471 - lr: 0.01000 - Train loss: 0.99819 - Test loss: 0.87615\n",
      "Epoch 7472 - lr: 0.01000 - Train loss: 0.93839 - Test loss: 0.87319\n",
      "Epoch 7473 - lr: 0.01000 - Train loss: 0.98282 - Test loss: 0.87770\n",
      "Epoch 7474 - lr: 0.01000 - Train loss: 0.93081 - Test loss: 0.90470\n",
      "Epoch 7475 - lr: 0.01000 - Train loss: 0.92653 - Test loss: 0.90771\n",
      "Epoch 7476 - lr: 0.01000 - Train loss: 0.86955 - Test loss: 0.88250\n",
      "Epoch 7477 - lr: 0.01000 - Train loss: 0.91506 - Test loss: 0.87279\n",
      "Epoch 7478 - lr: 0.01000 - Train loss: 0.95913 - Test loss: 0.88074\n",
      "Epoch 7479 - lr: 0.01000 - Train loss: 1.00687 - Test loss: 0.87474\n",
      "Epoch 7480 - lr: 0.01000 - Train loss: 0.93999 - Test loss: 0.87784\n",
      "Epoch 7481 - lr: 0.01000 - Train loss: 0.97121 - Test loss: 0.88690\n",
      "Epoch 7482 - lr: 0.01000 - Train loss: 0.89489 - Test loss: 0.88070\n",
      "Epoch 7483 - lr: 0.01000 - Train loss: 0.92467 - Test loss: 0.90942\n",
      "Epoch 7484 - lr: 0.01000 - Train loss: 0.89240 - Test loss: 0.91695\n",
      "Epoch 7485 - lr: 0.01000 - Train loss: 0.92288 - Test loss: 0.91634\n",
      "Epoch 7486 - lr: 0.01000 - Train loss: 0.92901 - Test loss: 0.89420\n",
      "Epoch 7487 - lr: 0.01000 - Train loss: 0.97462 - Test loss: 0.90778\n",
      "Epoch 7488 - lr: 0.01000 - Train loss: 0.83884 - Test loss: 0.88496\n",
      "Epoch 7489 - lr: 0.01000 - Train loss: 0.94597 - Test loss: 0.88066\n",
      "Epoch 7490 - lr: 0.01000 - Train loss: 0.97767 - Test loss: 0.89536\n",
      "Epoch 7491 - lr: 0.01000 - Train loss: 0.83602 - Test loss: 0.88538\n",
      "Epoch 7492 - lr: 0.01000 - Train loss: 0.92174 - Test loss: 0.88167\n",
      "Epoch 7493 - lr: 0.01000 - Train loss: 0.97009 - Test loss: 0.90860\n",
      "Epoch 7494 - lr: 0.01000 - Train loss: 0.85114 - Test loss: 0.88841\n",
      "Epoch 7495 - lr: 0.01000 - Train loss: 0.86307 - Test loss: 0.88451\n",
      "Epoch 7496 - lr: 0.01000 - Train loss: 0.92948 - Test loss: 0.89289\n",
      "Epoch 7497 - lr: 0.01000 - Train loss: 0.97657 - Test loss: 0.89637\n",
      "Epoch 7498 - lr: 0.01000 - Train loss: 0.92721 - Test loss: 0.89289\n",
      "Epoch 7499 - lr: 0.01000 - Train loss: 0.92714 - Test loss: 0.88636\n",
      "Epoch 7500 - lr: 0.01000 - Train loss: 0.91883 - Test loss: 0.89370\n",
      "Epoch 7501 - lr: 0.01000 - Train loss: 0.97287 - Test loss: 0.88795\n",
      "Epoch 7502 - lr: 0.01000 - Train loss: 0.86295 - Test loss: 0.88892\n",
      "Epoch 7503 - lr: 0.01000 - Train loss: 0.92614 - Test loss: 0.91610\n",
      "Epoch 7504 - lr: 0.01000 - Train loss: 0.92556 - Test loss: 0.90140\n",
      "Epoch 7505 - lr: 0.01000 - Train loss: 0.98982 - Test loss: 0.89826\n",
      "Epoch 7506 - lr: 0.01000 - Train loss: 0.97743 - Test loss: 0.89860\n",
      "Epoch 7507 - lr: 0.01000 - Train loss: 0.91775 - Test loss: 0.89269\n",
      "Epoch 7508 - lr: 0.01000 - Train loss: 0.89749 - Test loss: 0.89154\n",
      "Epoch 7509 - lr: 0.01000 - Train loss: 0.92543 - Test loss: 0.91922\n",
      "Epoch 7510 - lr: 0.01000 - Train loss: 0.89469 - Test loss: 0.92567\n",
      "Epoch 7511 - lr: 0.01000 - Train loss: 0.90138 - Test loss: 0.92666\n",
      "Epoch 7512 - lr: 0.01000 - Train loss: 0.92698 - Test loss: 0.92226\n",
      "Epoch 7513 - lr: 0.01000 - Train loss: 0.90657 - Test loss: 0.88946\n",
      "Epoch 7514 - lr: 0.01000 - Train loss: 0.92053 - Test loss: 0.91396\n",
      "Epoch 7515 - lr: 0.01000 - Train loss: 0.92678 - Test loss: 0.91960\n",
      "Epoch 7516 - lr: 0.01000 - Train loss: 0.90113 - Test loss: 0.89017\n",
      "Epoch 7517 - lr: 0.01000 - Train loss: 0.91906 - Test loss: 0.91601\n",
      "Epoch 7518 - lr: 0.01000 - Train loss: 0.92433 - Test loss: 0.91867\n",
      "Epoch 7519 - lr: 0.01000 - Train loss: 0.90147 - Test loss: 0.88784\n",
      "Epoch 7520 - lr: 0.01000 - Train loss: 0.91280 - Test loss: 0.89226\n",
      "Epoch 7521 - lr: 0.01000 - Train loss: 0.96728 - Test loss: 0.88583\n",
      "Epoch 7522 - lr: 0.01000 - Train loss: 0.90675 - Test loss: 0.91549\n",
      "Epoch 7523 - lr: 0.01000 - Train loss: 0.92641 - Test loss: 0.92025\n",
      "Epoch 7524 - lr: 0.01000 - Train loss: 0.96565 - Test loss: 0.89863\n",
      "Epoch 7525 - lr: 0.01000 - Train loss: 0.93336 - Test loss: 0.88712\n",
      "Epoch 7526 - lr: 0.01000 - Train loss: 0.91567 - Test loss: 0.88776\n",
      "Epoch 7527 - lr: 0.01000 - Train loss: 0.90053 - Test loss: 0.88850\n",
      "Epoch 7528 - lr: 0.01000 - Train loss: 0.91443 - Test loss: 0.91797\n",
      "Epoch 7529 - lr: 0.01000 - Train loss: 0.92593 - Test loss: 0.92311\n",
      "Epoch 7530 - lr: 0.01000 - Train loss: 0.89144 - Test loss: 0.90793\n",
      "Epoch 7531 - lr: 0.01000 - Train loss: 0.92090 - Test loss: 0.91970\n",
      "Epoch 7532 - lr: 0.01000 - Train loss: 0.87327 - Test loss: 0.91549\n",
      "Epoch 7533 - lr: 0.01000 - Train loss: 0.90684 - Test loss: 0.88660\n",
      "Epoch 7534 - lr: 0.01000 - Train loss: 0.92636 - Test loss: 0.89122\n",
      "Epoch 7535 - lr: 0.01000 - Train loss: 0.97944 - Test loss: 0.88525\n",
      "Epoch 7536 - lr: 0.01000 - Train loss: 0.92116 - Test loss: 0.89310\n",
      "Epoch 7537 - lr: 0.01000 - Train loss: 0.96052 - Test loss: 0.88897\n",
      "Epoch 7538 - lr: 0.01000 - Train loss: 0.92410 - Test loss: 0.91590\n",
      "Epoch 7539 - lr: 0.01000 - Train loss: 0.92178 - Test loss: 0.90087\n",
      "Epoch 7540 - lr: 0.01000 - Train loss: 0.95460 - Test loss: 0.91951\n",
      "Epoch 7541 - lr: 0.01000 - Train loss: 0.91498 - Test loss: 0.92343\n",
      "Epoch 7542 - lr: 0.01000 - Train loss: 0.92344 - Test loss: 0.92179\n",
      "Epoch 7543 - lr: 0.01000 - Train loss: 0.92427 - Test loss: 0.89865\n",
      "Epoch 7544 - lr: 0.01000 - Train loss: 0.97542 - Test loss: 0.91499\n",
      "Epoch 7545 - lr: 0.01000 - Train loss: 0.92846 - Test loss: 0.91240\n",
      "Epoch 7546 - lr: 0.01000 - Train loss: 0.88126 - Test loss: 0.88734\n",
      "Epoch 7547 - lr: 0.01000 - Train loss: 0.92346 - Test loss: 0.91228\n",
      "Epoch 7548 - lr: 0.01000 - Train loss: 0.92361 - Test loss: 0.89684\n",
      "Epoch 7549 - lr: 0.01000 - Train loss: 0.96519 - Test loss: 0.91652\n",
      "Epoch 7550 - lr: 0.01000 - Train loss: 0.92231 - Test loss: 0.91913\n",
      "Epoch 7551 - lr: 0.01000 - Train loss: 0.91491 - Test loss: 0.91899\n",
      "Epoch 7552 - lr: 0.01000 - Train loss: 0.92172 - Test loss: 0.91712\n",
      "Epoch 7553 - lr: 0.01000 - Train loss: 0.90204 - Test loss: 0.89749\n",
      "Epoch 7554 - lr: 0.01000 - Train loss: 0.91468 - Test loss: 0.90892\n",
      "Epoch 7555 - lr: 0.01000 - Train loss: 0.91931 - Test loss: 0.91406\n",
      "Epoch 7556 - lr: 0.01000 - Train loss: 0.91957 - Test loss: 0.91357\n",
      "Epoch 7557 - lr: 0.01000 - Train loss: 0.90080 - Test loss: 0.91381\n",
      "Epoch 7558 - lr: 0.01000 - Train loss: 0.90139 - Test loss: 0.91282\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7559 - lr: 0.01000 - Train loss: 0.91754 - Test loss: 0.91135\n",
      "Epoch 7560 - lr: 0.01000 - Train loss: 0.91691 - Test loss: 0.91027\n",
      "Epoch 7561 - lr: 0.01000 - Train loss: 0.91257 - Test loss: 0.90963\n",
      "Epoch 7562 - lr: 0.01000 - Train loss: 0.92374 - Test loss: 0.88572\n",
      "Epoch 7563 - lr: 0.01000 - Train loss: 0.90139 - Test loss: 0.86986\n",
      "Epoch 7564 - lr: 0.01000 - Train loss: 0.92361 - Test loss: 0.89770\n",
      "Epoch 7565 - lr: 0.01000 - Train loss: 0.91089 - Test loss: 0.90495\n",
      "Epoch 7566 - lr: 0.01000 - Train loss: 0.91248 - Test loss: 0.90618\n",
      "Epoch 7567 - lr: 0.01000 - Train loss: 0.92205 - Test loss: 0.87930\n",
      "Epoch 7568 - lr: 0.01000 - Train loss: 0.97139 - Test loss: 0.89974\n",
      "Epoch 7569 - lr: 0.01000 - Train loss: 0.86658 - Test loss: 0.88975\n",
      "Epoch 7570 - lr: 0.01000 - Train loss: 0.87335 - Test loss: 0.86421\n",
      "Epoch 7571 - lr: 0.01000 - Train loss: 0.91567 - Test loss: 0.89315\n",
      "Epoch 7572 - lr: 0.01000 - Train loss: 0.91379 - Test loss: 0.89813\n",
      "Epoch 7573 - lr: 0.01000 - Train loss: 0.90495 - Test loss: 0.89931\n",
      "Epoch 7574 - lr: 0.01000 - Train loss: 0.87961 - Test loss: 0.89596\n",
      "Epoch 7575 - lr: 0.01000 - Train loss: 0.86770 - Test loss: 0.87892\n",
      "Epoch 7576 - lr: 0.01000 - Train loss: 0.87704 - Test loss: 0.89155\n",
      "Epoch 7577 - lr: 0.01000 - Train loss: 0.85733 - Test loss: 0.87909\n",
      "Epoch 7578 - lr: 0.01000 - Train loss: 0.83874 - Test loss: 0.85098\n",
      "Epoch 7579 - lr: 0.01000 - Train loss: 0.87603 - Test loss: 0.84346\n",
      "Epoch 7580 - lr: 0.01000 - Train loss: 0.90972 - Test loss: 0.88023\n",
      "Epoch 7581 - lr: 0.01000 - Train loss: 0.89871 - Test loss: 0.88264\n",
      "Epoch 7582 - lr: 0.01000 - Train loss: 0.89209 - Test loss: 0.88094\n",
      "Epoch 7583 - lr: 0.01000 - Train loss: 0.91864 - Test loss: 0.84142\n",
      "Epoch 7584 - lr: 0.01000 - Train loss: 0.88771 - Test loss: 0.86988\n",
      "Epoch 7585 - lr: 0.01000 - Train loss: 0.87672 - Test loss: 0.86063\n",
      "Epoch 7586 - lr: 0.01000 - Train loss: 0.89986 - Test loss: 0.83528\n",
      "Epoch 7587 - lr: 0.01000 - Train loss: 0.90321 - Test loss: 0.85124\n",
      "Epoch 7588 - lr: 0.01000 - Train loss: 0.86807 - Test loss: 0.85061\n",
      "Epoch 7589 - lr: 0.01000 - Train loss: 0.86318 - Test loss: 0.82552\n",
      "Epoch 7590 - lr: 0.01000 - Train loss: 0.85526 - Test loss: 0.80773\n",
      "Epoch 7591 - lr: 0.01000 - Train loss: 0.86025 - Test loss: 0.83383\n",
      "Epoch 7592 - lr: 0.01000 - Train loss: 0.91269 - Test loss: 0.82716\n",
      "Epoch 7593 - lr: 0.01000 - Train loss: 0.86292 - Test loss: 0.81458\n",
      "Epoch 7594 - lr: 0.01000 - Train loss: 0.82299 - Test loss: 0.81034\n",
      "Epoch 7595 - lr: 0.01000 - Train loss: 0.81429 - Test loss: 0.80639\n",
      "Epoch 7596 - lr: 0.01000 - Train loss: 0.89051 - Test loss: 0.79886\n",
      "Epoch 7597 - lr: 0.01000 - Train loss: 0.95635 - Test loss: 0.79314\n",
      "Epoch 7598 - lr: 0.01000 - Train loss: 0.86465 - Test loss: 0.80309\n",
      "Epoch 7599 - lr: 0.01000 - Train loss: 0.82736 - Test loss: 0.79064\n",
      "Epoch 7600 - lr: 0.01000 - Train loss: 0.92051 - Test loss: 0.79267\n",
      "Epoch 7601 - lr: 0.01000 - Train loss: 0.81475 - Test loss: 0.78226\n",
      "Epoch 7602 - lr: 0.01000 - Train loss: 0.91896 - Test loss: 0.79159\n",
      "Epoch 7603 - lr: 0.01000 - Train loss: 0.80601 - Test loss: 0.77530\n",
      "Epoch 7604 - lr: 0.01000 - Train loss: 0.93473 - Test loss: 0.78909\n",
      "Epoch 7605 - lr: 0.01000 - Train loss: 0.78782 - Test loss: 0.76783\n",
      "Epoch 7606 - lr: 0.01000 - Train loss: 0.78268 - Test loss: 0.76676\n",
      "Epoch 7607 - lr: 0.01000 - Train loss: 0.84422 - Test loss: 0.76195\n",
      "Epoch 7608 - lr: 0.01000 - Train loss: 0.90658 - Test loss: 0.77985\n",
      "Epoch 7609 - lr: 0.01000 - Train loss: 0.77785 - Test loss: 0.75912\n",
      "Epoch 7610 - lr: 0.01000 - Train loss: 0.75879 - Test loss: 0.75647\n",
      "Epoch 7611 - lr: 0.01000 - Train loss: 0.75492 - Test loss: 0.75573\n",
      "Epoch 7612 - lr: 0.01000 - Train loss: 0.76286 - Test loss: 0.75581\n",
      "Epoch 7613 - lr: 0.01000 - Train loss: 0.78388 - Test loss: 0.75472\n",
      "Epoch 7614 - lr: 0.01000 - Train loss: 0.75576 - Test loss: 0.75502\n",
      "Epoch 7615 - lr: 0.01000 - Train loss: 0.74778 - Test loss: 0.75506\n",
      "Epoch 7616 - lr: 0.01000 - Train loss: 0.73988 - Test loss: 0.75549\n",
      "Epoch 7617 - lr: 0.01000 - Train loss: 0.74050 - Test loss: 0.75514\n",
      "Epoch 7618 - lr: 0.01000 - Train loss: 0.74247 - Test loss: 0.75679\n",
      "Epoch 7619 - lr: 0.01000 - Train loss: 0.73914 - Test loss: 0.75792\n",
      "Epoch 7620 - lr: 0.01000 - Train loss: 0.73835 - Test loss: 0.75849\n",
      "Epoch 7621 - lr: 0.01000 - Train loss: 0.73533 - Test loss: 0.75968\n",
      "Epoch 7622 - lr: 0.01000 - Train loss: 0.73590 - Test loss: 0.75987\n",
      "Epoch 7623 - lr: 0.01000 - Train loss: 0.73508 - Test loss: 0.76052\n",
      "Epoch 7624 - lr: 0.01000 - Train loss: 0.73078 - Test loss: 0.76188\n",
      "Epoch 7625 - lr: 0.01000 - Train loss: 0.73168 - Test loss: 0.76201\n",
      "Epoch 7626 - lr: 0.01000 - Train loss: 0.73040 - Test loss: 0.76273\n",
      "Epoch 7627 - lr: 0.01000 - Train loss: 0.72666 - Test loss: 0.76395\n",
      "Epoch 7628 - lr: 0.01000 - Train loss: 0.72826 - Test loss: 0.76388\n",
      "Epoch 7629 - lr: 0.01000 - Train loss: 0.72689 - Test loss: 0.76453\n",
      "Epoch 7630 - lr: 0.01000 - Train loss: 0.72385 - Test loss: 0.76551\n",
      "Epoch 7631 - lr: 0.01000 - Train loss: 0.72516 - Test loss: 0.76551\n",
      "Epoch 7632 - lr: 0.01000 - Train loss: 0.72243 - Test loss: 0.76639\n",
      "Epoch 7633 - lr: 0.01000 - Train loss: 0.72364 - Test loss: 0.76635\n",
      "Epoch 7634 - lr: 0.01000 - Train loss: 0.72094 - Test loss: 0.76718\n",
      "Epoch 7635 - lr: 0.01000 - Train loss: 0.72232 - Test loss: 0.76709\n",
      "Epoch 7636 - lr: 0.01000 - Train loss: 0.71965 - Test loss: 0.76785\n",
      "Epoch 7637 - lr: 0.01000 - Train loss: 0.72119 - Test loss: 0.76772\n",
      "Epoch 7638 - lr: 0.01000 - Train loss: 0.71857 - Test loss: 0.76843\n",
      "Epoch 7639 - lr: 0.01000 - Train loss: 0.72023 - Test loss: 0.76827\n",
      "Epoch 7640 - lr: 0.01000 - Train loss: 0.71767 - Test loss: 0.76893\n",
      "Epoch 7641 - lr: 0.01000 - Train loss: 0.71941 - Test loss: 0.76874\n",
      "Epoch 7642 - lr: 0.01000 - Train loss: 0.71692 - Test loss: 0.76934\n",
      "Epoch 7643 - lr: 0.01000 - Train loss: 0.71873 - Test loss: 0.76914\n",
      "Epoch 7644 - lr: 0.01000 - Train loss: 0.71632 - Test loss: 0.76969\n",
      "Epoch 7645 - lr: 0.01000 - Train loss: 0.71816 - Test loss: 0.76948\n",
      "Epoch 7646 - lr: 0.01000 - Train loss: 0.71584 - Test loss: 0.76998\n",
      "Epoch 7647 - lr: 0.01000 - Train loss: 0.71769 - Test loss: 0.76975\n",
      "Epoch 7648 - lr: 0.01000 - Train loss: 0.71548 - Test loss: 0.77020\n",
      "Epoch 7649 - lr: 0.01000 - Train loss: 0.71732 - Test loss: 0.76996\n",
      "Epoch 7650 - lr: 0.01000 - Train loss: 0.71521 - Test loss: 0.77037\n",
      "Epoch 7651 - lr: 0.01000 - Train loss: 0.71702 - Test loss: 0.77012\n",
      "Epoch 7652 - lr: 0.01000 - Train loss: 0.71502 - Test loss: 0.77047\n",
      "Epoch 7653 - lr: 0.01000 - Train loss: 0.71680 - Test loss: 0.77022\n",
      "Epoch 7654 - lr: 0.01000 - Train loss: 0.71491 - Test loss: 0.77053\n",
      "Epoch 7655 - lr: 0.01000 - Train loss: 0.71664 - Test loss: 0.77026\n",
      "Epoch 7656 - lr: 0.01000 - Train loss: 0.71486 - Test loss: 0.77052\n",
      "Epoch 7657 - lr: 0.01000 - Train loss: 0.71654 - Test loss: 0.77024\n",
      "Epoch 7658 - lr: 0.01000 - Train loss: 0.71487 - Test loss: 0.77045\n",
      "Epoch 7659 - lr: 0.01000 - Train loss: 0.71648 - Test loss: 0.77016\n",
      "Epoch 7660 - lr: 0.01000 - Train loss: 0.71491 - Test loss: 0.77033\n",
      "Epoch 7661 - lr: 0.01000 - Train loss: 0.71647 - Test loss: 0.77001\n",
      "Epoch 7662 - lr: 0.01000 - Train loss: 0.71500 - Test loss: 0.77013\n",
      "Epoch 7663 - lr: 0.01000 - Train loss: 0.71650 - Test loss: 0.76980\n",
      "Epoch 7664 - lr: 0.01000 - Train loss: 0.71512 - Test loss: 0.76987\n",
      "Epoch 7665 - lr: 0.01000 - Train loss: 0.71655 - Test loss: 0.76951\n",
      "Epoch 7666 - lr: 0.01000 - Train loss: 0.71525 - Test loss: 0.76953\n",
      "Epoch 7667 - lr: 0.01000 - Train loss: 0.71663 - Test loss: 0.76914\n",
      "Epoch 7668 - lr: 0.01000 - Train loss: 0.71541 - Test loss: 0.76912\n",
      "Epoch 7669 - lr: 0.01000 - Train loss: 0.71673 - Test loss: 0.76869\n",
      "Epoch 7670 - lr: 0.01000 - Train loss: 0.71557 - Test loss: 0.76862\n",
      "Epoch 7671 - lr: 0.01000 - Train loss: 0.71683 - Test loss: 0.76815\n",
      "Epoch 7672 - lr: 0.01000 - Train loss: 0.71573 - Test loss: 0.76803\n",
      "Epoch 7673 - lr: 0.01000 - Train loss: 0.71695 - Test loss: 0.76752\n",
      "Epoch 7674 - lr: 0.01000 - Train loss: 0.71589 - Test loss: 0.76734\n",
      "Epoch 7675 - lr: 0.01000 - Train loss: 0.71706 - Test loss: 0.76678\n",
      "Epoch 7676 - lr: 0.01000 - Train loss: 0.71604 - Test loss: 0.76655\n",
      "Epoch 7677 - lr: 0.01000 - Train loss: 0.71717 - Test loss: 0.76594\n",
      "Epoch 7678 - lr: 0.01000 - Train loss: 0.71617 - Test loss: 0.76565\n",
      "Epoch 7679 - lr: 0.01000 - Train loss: 0.71727 - Test loss: 0.76498\n",
      "Epoch 7680 - lr: 0.01000 - Train loss: 0.71628 - Test loss: 0.76463\n",
      "Epoch 7681 - lr: 0.01000 - Train loss: 0.71736 - Test loss: 0.76390\n",
      "Epoch 7682 - lr: 0.01000 - Train loss: 0.71637 - Test loss: 0.76350\n",
      "Epoch 7683 - lr: 0.01000 - Train loss: 0.71742 - Test loss: 0.76270\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7684 - lr: 0.01000 - Train loss: 0.71642 - Test loss: 0.76223\n",
      "Epoch 7685 - lr: 0.01000 - Train loss: 0.71745 - Test loss: 0.76137\n",
      "Epoch 7686 - lr: 0.01000 - Train loss: 0.71644 - Test loss: 0.76084\n",
      "Epoch 7687 - lr: 0.01000 - Train loss: 0.71746 - Test loss: 0.75990\n",
      "Epoch 7688 - lr: 0.01000 - Train loss: 0.71641 - Test loss: 0.75930\n",
      "Epoch 7689 - lr: 0.01000 - Train loss: 0.71742 - Test loss: 0.75829\n",
      "Epoch 7690 - lr: 0.01000 - Train loss: 0.71634 - Test loss: 0.75763\n",
      "Epoch 7691 - lr: 0.01000 - Train loss: 0.71734 - Test loss: 0.75653\n",
      "Epoch 7692 - lr: 0.01000 - Train loss: 0.71623 - Test loss: 0.75580\n",
      "Epoch 7693 - lr: 0.01000 - Train loss: 0.71721 - Test loss: 0.75462\n",
      "Epoch 7694 - lr: 0.01000 - Train loss: 0.71606 - Test loss: 0.75382\n",
      "Epoch 7695 - lr: 0.01000 - Train loss: 0.71703 - Test loss: 0.75256\n",
      "Epoch 7696 - lr: 0.01000 - Train loss: 0.71583 - Test loss: 0.75168\n",
      "Epoch 7697 - lr: 0.01000 - Train loss: 0.71679 - Test loss: 0.75032\n",
      "Epoch 7698 - lr: 0.01000 - Train loss: 0.71554 - Test loss: 0.74937\n",
      "Epoch 7699 - lr: 0.01000 - Train loss: 0.71647 - Test loss: 0.74792\n",
      "Epoch 7700 - lr: 0.01000 - Train loss: 0.71518 - Test loss: 0.74689\n",
      "Epoch 7701 - lr: 0.01000 - Train loss: 0.71608 - Test loss: 0.74534\n",
      "Epoch 7702 - lr: 0.01000 - Train loss: 0.71475 - Test loss: 0.74423\n",
      "Epoch 7703 - lr: 0.01000 - Train loss: 0.71561 - Test loss: 0.74258\n",
      "Epoch 7704 - lr: 0.01000 - Train loss: 0.71424 - Test loss: 0.74138\n",
      "Epoch 7705 - lr: 0.01000 - Train loss: 0.71504 - Test loss: 0.73963\n",
      "Epoch 7706 - lr: 0.01000 - Train loss: 0.71365 - Test loss: 0.73834\n",
      "Epoch 7707 - lr: 0.01000 - Train loss: 0.71438 - Test loss: 0.73649\n",
      "Epoch 7708 - lr: 0.01000 - Train loss: 0.71297 - Test loss: 0.73511\n",
      "Epoch 7709 - lr: 0.01000 - Train loss: 0.71361 - Test loss: 0.73317\n",
      "Epoch 7710 - lr: 0.01000 - Train loss: 0.71219 - Test loss: 0.73170\n",
      "Epoch 7711 - lr: 0.01000 - Train loss: 0.71273 - Test loss: 0.72967\n",
      "Epoch 7712 - lr: 0.01000 - Train loss: 0.71131 - Test loss: 0.72814\n",
      "Epoch 7713 - lr: 0.01000 - Train loss: 0.71175 - Test loss: 0.72603\n",
      "Epoch 7714 - lr: 0.01000 - Train loss: 0.71033 - Test loss: 0.72445\n",
      "Epoch 7715 - lr: 0.01000 - Train loss: 0.71067 - Test loss: 0.72229\n",
      "Epoch 7716 - lr: 0.01000 - Train loss: 0.70926 - Test loss: 0.72068\n",
      "Epoch 7717 - lr: 0.01000 - Train loss: 0.70951 - Test loss: 0.71851\n",
      "Epoch 7718 - lr: 0.01000 - Train loss: 0.70812 - Test loss: 0.71692\n",
      "Epoch 7719 - lr: 0.01000 - Train loss: 0.70828 - Test loss: 0.71478\n",
      "Epoch 7720 - lr: 0.01000 - Train loss: 0.70693 - Test loss: 0.71322\n",
      "Epoch 7721 - lr: 0.01000 - Train loss: 0.70702 - Test loss: 0.71115\n",
      "Epoch 7722 - lr: 0.01000 - Train loss: 0.70573 - Test loss: 0.70967\n",
      "Epoch 7723 - lr: 0.01000 - Train loss: 0.70576 - Test loss: 0.70769\n",
      "Epoch 7724 - lr: 0.01000 - Train loss: 0.70455 - Test loss: 0.70631\n",
      "Epoch 7725 - lr: 0.01000 - Train loss: 0.70454 - Test loss: 0.70446\n",
      "Epoch 7726 - lr: 0.01000 - Train loss: 0.70340 - Test loss: 0.70317\n",
      "Epoch 7727 - lr: 0.01000 - Train loss: 0.70337 - Test loss: 0.70145\n",
      "Epoch 7728 - lr: 0.01000 - Train loss: 0.70231 - Test loss: 0.70026\n",
      "Epoch 7729 - lr: 0.01000 - Train loss: 0.70226 - Test loss: 0.69867\n",
      "Epoch 7730 - lr: 0.01000 - Train loss: 0.70128 - Test loss: 0.69758\n",
      "Epoch 7731 - lr: 0.01000 - Train loss: 0.70122 - Test loss: 0.69611\n",
      "Epoch 7732 - lr: 0.01000 - Train loss: 0.70032 - Test loss: 0.69510\n",
      "Epoch 7733 - lr: 0.01000 - Train loss: 0.70025 - Test loss: 0.69374\n",
      "Epoch 7734 - lr: 0.01000 - Train loss: 0.69942 - Test loss: 0.69280\n",
      "Epoch 7735 - lr: 0.01000 - Train loss: 0.69934 - Test loss: 0.69154\n",
      "Epoch 7736 - lr: 0.01000 - Train loss: 0.69858 - Test loss: 0.69066\n",
      "Epoch 7737 - lr: 0.01000 - Train loss: 0.69850 - Test loss: 0.68949\n",
      "Epoch 7738 - lr: 0.01000 - Train loss: 0.69779 - Test loss: 0.68866\n",
      "Epoch 7739 - lr: 0.01000 - Train loss: 0.69770 - Test loss: 0.68757\n",
      "Epoch 7740 - lr: 0.01000 - Train loss: 0.69705 - Test loss: 0.68678\n",
      "Epoch 7741 - lr: 0.01000 - Train loss: 0.69695 - Test loss: 0.68577\n",
      "Epoch 7742 - lr: 0.01000 - Train loss: 0.69634 - Test loss: 0.68501\n",
      "Epoch 7743 - lr: 0.01000 - Train loss: 0.69624 - Test loss: 0.68406\n",
      "Epoch 7744 - lr: 0.01000 - Train loss: 0.69568 - Test loss: 0.68333\n",
      "Epoch 7745 - lr: 0.01000 - Train loss: 0.69556 - Test loss: 0.68244\n",
      "Epoch 7746 - lr: 0.01000 - Train loss: 0.69504 - Test loss: 0.68174\n",
      "Epoch 7747 - lr: 0.01000 - Train loss: 0.69492 - Test loss: 0.68089\n",
      "Epoch 7748 - lr: 0.01000 - Train loss: 0.69443 - Test loss: 0.68022\n",
      "Epoch 7749 - lr: 0.01000 - Train loss: 0.69430 - Test loss: 0.67941\n",
      "Epoch 7750 - lr: 0.01000 - Train loss: 0.69385 - Test loss: 0.67876\n",
      "Epoch 7751 - lr: 0.01000 - Train loss: 0.69372 - Test loss: 0.67800\n",
      "Epoch 7752 - lr: 0.01000 - Train loss: 0.69330 - Test loss: 0.67736\n",
      "Epoch 7753 - lr: 0.01000 - Train loss: 0.69315 - Test loss: 0.67663\n",
      "Epoch 7754 - lr: 0.01000 - Train loss: 0.69276 - Test loss: 0.67601\n",
      "Epoch 7755 - lr: 0.01000 - Train loss: 0.69261 - Test loss: 0.67532\n",
      "Epoch 7756 - lr: 0.01000 - Train loss: 0.69225 - Test loss: 0.67471\n",
      "Epoch 7757 - lr: 0.01000 - Train loss: 0.69208 - Test loss: 0.67405\n",
      "Epoch 7758 - lr: 0.01000 - Train loss: 0.69176 - Test loss: 0.67346\n",
      "Epoch 7759 - lr: 0.01000 - Train loss: 0.69158 - Test loss: 0.67283\n",
      "Epoch 7760 - lr: 0.01000 - Train loss: 0.69128 - Test loss: 0.67225\n",
      "Epoch 7761 - lr: 0.01000 - Train loss: 0.69109 - Test loss: 0.67164\n",
      "Epoch 7762 - lr: 0.01000 - Train loss: 0.69082 - Test loss: 0.67107\n",
      "Epoch 7763 - lr: 0.01000 - Train loss: 0.69062 - Test loss: 0.67049\n",
      "Epoch 7764 - lr: 0.01000 - Train loss: 0.69038 - Test loss: 0.66994\n",
      "Epoch 7765 - lr: 0.01000 - Train loss: 0.69017 - Test loss: 0.66937\n",
      "Epoch 7766 - lr: 0.01000 - Train loss: 0.68996 - Test loss: 0.66883\n",
      "Epoch 7767 - lr: 0.01000 - Train loss: 0.68975 - Test loss: 0.66828\n",
      "Epoch 7768 - lr: 0.01000 - Train loss: 0.68955 - Test loss: 0.66776\n",
      "Epoch 7769 - lr: 0.01000 - Train loss: 0.68934 - Test loss: 0.66723\n",
      "Epoch 7770 - lr: 0.01000 - Train loss: 0.68916 - Test loss: 0.66672\n",
      "Epoch 7771 - lr: 0.01000 - Train loss: 0.68895 - Test loss: 0.66620\n",
      "Epoch 7772 - lr: 0.01000 - Train loss: 0.68878 - Test loss: 0.66570\n",
      "Epoch 7773 - lr: 0.01000 - Train loss: 0.68859 - Test loss: 0.66521\n",
      "Epoch 7774 - lr: 0.01000 - Train loss: 0.68842 - Test loss: 0.66472\n",
      "Epoch 7775 - lr: 0.01000 - Train loss: 0.68823 - Test loss: 0.66424\n",
      "Epoch 7776 - lr: 0.01000 - Train loss: 0.68807 - Test loss: 0.66376\n",
      "Epoch 7777 - lr: 0.01000 - Train loss: 0.68790 - Test loss: 0.66329\n",
      "Epoch 7778 - lr: 0.01000 - Train loss: 0.68773 - Test loss: 0.66282\n",
      "Epoch 7779 - lr: 0.01000 - Train loss: 0.68757 - Test loss: 0.66236\n",
      "Epoch 7780 - lr: 0.01000 - Train loss: 0.68741 - Test loss: 0.66191\n",
      "Epoch 7781 - lr: 0.01000 - Train loss: 0.68726 - Test loss: 0.66146\n",
      "Epoch 7782 - lr: 0.01000 - Train loss: 0.68710 - Test loss: 0.66102\n",
      "Epoch 7783 - lr: 0.01000 - Train loss: 0.68696 - Test loss: 0.66058\n",
      "Epoch 7784 - lr: 0.01000 - Train loss: 0.68681 - Test loss: 0.66015\n",
      "Epoch 7785 - lr: 0.01000 - Train loss: 0.68667 - Test loss: 0.65972\n",
      "Epoch 7786 - lr: 0.01000 - Train loss: 0.68653 - Test loss: 0.65930\n",
      "Epoch 7787 - lr: 0.01000 - Train loss: 0.68639 - Test loss: 0.65889\n",
      "Epoch 7788 - lr: 0.01000 - Train loss: 0.68626 - Test loss: 0.65847\n",
      "Epoch 7789 - lr: 0.01000 - Train loss: 0.68614 - Test loss: 0.65807\n",
      "Epoch 7790 - lr: 0.01000 - Train loss: 0.68601 - Test loss: 0.65766\n",
      "Epoch 7791 - lr: 0.01000 - Train loss: 0.68590 - Test loss: 0.65727\n",
      "Epoch 7792 - lr: 0.01000 - Train loss: 0.68578 - Test loss: 0.65687\n",
      "Epoch 7793 - lr: 0.01000 - Train loss: 0.68568 - Test loss: 0.65649\n",
      "Epoch 7794 - lr: 0.01000 - Train loss: 0.68558 - Test loss: 0.65610\n",
      "Epoch 7795 - lr: 0.01000 - Train loss: 0.68549 - Test loss: 0.65573\n",
      "Epoch 7796 - lr: 0.01000 - Train loss: 0.68541 - Test loss: 0.65536\n",
      "Epoch 7797 - lr: 0.01000 - Train loss: 0.68534 - Test loss: 0.65499\n",
      "Epoch 7798 - lr: 0.01000 - Train loss: 0.68530 - Test loss: 0.65463\n",
      "Epoch 7799 - lr: 0.01000 - Train loss: 0.68530 - Test loss: 0.65429\n",
      "Epoch 7800 - lr: 0.01000 - Train loss: 0.68536 - Test loss: 0.65396\n",
      "Epoch 7801 - lr: 0.01000 - Train loss: 0.68558 - Test loss: 0.65366\n",
      "Epoch 7802 - lr: 0.01000 - Train loss: 0.68618 - Test loss: 0.65347\n",
      "Epoch 7803 - lr: 0.01000 - Train loss: 0.68766 - Test loss: 0.65347\n",
      "Epoch 7804 - lr: 0.01000 - Train loss: 0.68891 - Test loss: 0.65196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7805 - lr: 0.01000 - Train loss: 0.68393 - Test loss: 0.65227\n",
      "Epoch 7806 - lr: 0.01000 - Train loss: 0.69153 - Test loss: 0.65180\n",
      "Epoch 7807 - lr: 0.01000 - Train loss: 0.68383 - Test loss: 0.65153\n",
      "Epoch 7808 - lr: 0.01000 - Train loss: 0.68578 - Test loss: 0.65156\n",
      "Epoch 7809 - lr: 0.01000 - Train loss: 0.68727 - Test loss: 0.65160\n",
      "Epoch 7810 - lr: 0.01000 - Train loss: 0.69064 - Test loss: 0.65052\n",
      "Epoch 7811 - lr: 0.01000 - Train loss: 0.68364 - Test loss: 0.65037\n",
      "Epoch 7812 - lr: 0.01000 - Train loss: 0.68685 - Test loss: 0.65066\n",
      "Epoch 7813 - lr: 0.01000 - Train loss: 0.68628 - Test loss: 0.64887\n",
      "Epoch 7814 - lr: 0.01000 - Train loss: 0.67999 - Test loss: 0.64851\n",
      "Epoch 7815 - lr: 0.01000 - Train loss: 0.68476 - Test loss: 0.64921\n",
      "Epoch 7816 - lr: 0.01000 - Train loss: 0.68543 - Test loss: 0.64923\n",
      "Epoch 7817 - lr: 0.01000 - Train loss: 0.68667 - Test loss: 0.64913\n",
      "Epoch 7818 - lr: 0.01000 - Train loss: 0.68735 - Test loss: 0.64766\n",
      "Epoch 7819 - lr: 0.01000 - Train loss: 0.68255 - Test loss: 0.64813\n",
      "Epoch 7820 - lr: 0.01000 - Train loss: 0.69202 - Test loss: 0.64797\n",
      "Epoch 7821 - lr: 0.01000 - Train loss: 0.68196 - Test loss: 0.64729\n",
      "Epoch 7822 - lr: 0.01000 - Train loss: 0.68187 - Test loss: 0.64702\n",
      "Epoch 7823 - lr: 0.01000 - Train loss: 0.68173 - Test loss: 0.64670\n",
      "Epoch 7824 - lr: 0.01000 - Train loss: 0.68166 - Test loss: 0.64645\n",
      "Epoch 7825 - lr: 0.01000 - Train loss: 0.68154 - Test loss: 0.64615\n",
      "Epoch 7826 - lr: 0.01000 - Train loss: 0.68143 - Test loss: 0.64590\n",
      "Epoch 7827 - lr: 0.01000 - Train loss: 0.68134 - Test loss: 0.64560\n",
      "Epoch 7828 - lr: 0.01000 - Train loss: 0.68121 - Test loss: 0.64535\n",
      "Epoch 7829 - lr: 0.01000 - Train loss: 0.68113 - Test loss: 0.64507\n",
      "Epoch 7830 - lr: 0.01000 - Train loss: 0.68101 - Test loss: 0.64481\n",
      "Epoch 7831 - lr: 0.01000 - Train loss: 0.68094 - Test loss: 0.64454\n",
      "Epoch 7832 - lr: 0.01000 - Train loss: 0.68083 - Test loss: 0.64428\n",
      "Epoch 7833 - lr: 0.01000 - Train loss: 0.68076 - Test loss: 0.64402\n",
      "Epoch 7834 - lr: 0.01000 - Train loss: 0.68066 - Test loss: 0.64377\n",
      "Epoch 7835 - lr: 0.01000 - Train loss: 0.68058 - Test loss: 0.64351\n",
      "Epoch 7836 - lr: 0.01000 - Train loss: 0.68050 - Test loss: 0.64326\n",
      "Epoch 7837 - lr: 0.01000 - Train loss: 0.68042 - Test loss: 0.64301\n",
      "Epoch 7838 - lr: 0.01000 - Train loss: 0.68035 - Test loss: 0.64276\n",
      "Epoch 7839 - lr: 0.01000 - Train loss: 0.68028 - Test loss: 0.64251\n",
      "Epoch 7840 - lr: 0.01000 - Train loss: 0.68021 - Test loss: 0.64226\n",
      "Epoch 7841 - lr: 0.01000 - Train loss: 0.68014 - Test loss: 0.64202\n",
      "Epoch 7842 - lr: 0.01000 - Train loss: 0.68008 - Test loss: 0.64177\n",
      "Epoch 7843 - lr: 0.01000 - Train loss: 0.68002 - Test loss: 0.64153\n",
      "Epoch 7844 - lr: 0.01000 - Train loss: 0.67997 - Test loss: 0.64129\n",
      "Epoch 7845 - lr: 0.01000 - Train loss: 0.67992 - Test loss: 0.64106\n",
      "Epoch 7846 - lr: 0.01000 - Train loss: 0.67988 - Test loss: 0.64082\n",
      "Epoch 7847 - lr: 0.01000 - Train loss: 0.67985 - Test loss: 0.64059\n",
      "Epoch 7848 - lr: 0.01000 - Train loss: 0.67984 - Test loss: 0.64036\n",
      "Epoch 7849 - lr: 0.01000 - Train loss: 0.67984 - Test loss: 0.64014\n",
      "Epoch 7850 - lr: 0.01000 - Train loss: 0.67988 - Test loss: 0.63992\n",
      "Epoch 7851 - lr: 0.01000 - Train loss: 0.67999 - Test loss: 0.63972\n",
      "Epoch 7852 - lr: 0.01000 - Train loss: 0.68023 - Test loss: 0.63954\n",
      "Epoch 7853 - lr: 0.01000 - Train loss: 0.68087 - Test loss: 0.63946\n",
      "Epoch 7854 - lr: 0.01000 - Train loss: 0.68253 - Test loss: 0.63967\n",
      "Epoch 7855 - lr: 0.01000 - Train loss: 0.68910 - Test loss: 0.63909\n",
      "Epoch 7856 - lr: 0.01000 - Train loss: 0.67922 - Test loss: 0.63850\n",
      "Epoch 7857 - lr: 0.01000 - Train loss: 0.67892 - Test loss: 0.63821\n",
      "Epoch 7858 - lr: 0.01000 - Train loss: 0.67878 - Test loss: 0.63798\n",
      "Epoch 7859 - lr: 0.01000 - Train loss: 0.67871 - Test loss: 0.63776\n",
      "Epoch 7860 - lr: 0.01000 - Train loss: 0.67868 - Test loss: 0.63756\n",
      "Epoch 7861 - lr: 0.01000 - Train loss: 0.67865 - Test loss: 0.63736\n",
      "Epoch 7862 - lr: 0.01000 - Train loss: 0.67865 - Test loss: 0.63716\n",
      "Epoch 7863 - lr: 0.01000 - Train loss: 0.67866 - Test loss: 0.63697\n",
      "Epoch 7864 - lr: 0.01000 - Train loss: 0.67871 - Test loss: 0.63678\n",
      "Epoch 7865 - lr: 0.01000 - Train loss: 0.67882 - Test loss: 0.63660\n",
      "Epoch 7866 - lr: 0.01000 - Train loss: 0.67907 - Test loss: 0.63645\n",
      "Epoch 7867 - lr: 0.01000 - Train loss: 0.67969 - Test loss: 0.63639\n",
      "Epoch 7868 - lr: 0.01000 - Train loss: 0.68135 - Test loss: 0.63663\n",
      "Epoch 7869 - lr: 0.01000 - Train loss: 0.68792 - Test loss: 0.63604\n",
      "Epoch 7870 - lr: 0.01000 - Train loss: 0.67809 - Test loss: 0.63549\n",
      "Epoch 7871 - lr: 0.01000 - Train loss: 0.67785 - Test loss: 0.63524\n",
      "Epoch 7872 - lr: 0.01000 - Train loss: 0.67774 - Test loss: 0.63503\n",
      "Epoch 7873 - lr: 0.01000 - Train loss: 0.67769 - Test loss: 0.63485\n",
      "Epoch 7874 - lr: 0.01000 - Train loss: 0.67767 - Test loss: 0.63467\n",
      "Epoch 7875 - lr: 0.01000 - Train loss: 0.67767 - Test loss: 0.63449\n",
      "Epoch 7876 - lr: 0.01000 - Train loss: 0.67769 - Test loss: 0.63432\n",
      "Epoch 7877 - lr: 0.01000 - Train loss: 0.67775 - Test loss: 0.63415\n",
      "Epoch 7878 - lr: 0.01000 - Train loss: 0.67787 - Test loss: 0.63399\n",
      "Epoch 7879 - lr: 0.01000 - Train loss: 0.67814 - Test loss: 0.63386\n",
      "Epoch 7880 - lr: 0.01000 - Train loss: 0.67883 - Test loss: 0.63383\n",
      "Epoch 7881 - lr: 0.01000 - Train loss: 0.68058 - Test loss: 0.63411\n",
      "Epoch 7882 - lr: 0.01000 - Train loss: 0.69178 - Test loss: 0.63433\n",
      "Epoch 7883 - lr: 0.01000 - Train loss: 0.67609 - Test loss: 0.63183\n",
      "Epoch 7884 - lr: 0.01000 - Train loss: 0.66848 - Test loss: 0.63113\n",
      "Epoch 7885 - lr: 0.01000 - Train loss: 0.66877 - Test loss: 0.63116\n",
      "Epoch 7886 - lr: 0.01000 - Train loss: 0.67186 - Test loss: 0.63186\n",
      "Epoch 7887 - lr: 0.01000 - Train loss: 0.67680 - Test loss: 0.63274\n",
      "Epoch 7888 - lr: 0.01000 - Train loss: 0.67766 - Test loss: 0.63283\n",
      "Epoch 7889 - lr: 0.01000 - Train loss: 0.67852 - Test loss: 0.63285\n",
      "Epoch 7890 - lr: 0.01000 - Train loss: 0.68028 - Test loss: 0.63302\n",
      "Epoch 7891 - lr: 0.01000 - Train loss: 0.69418 - Test loss: 0.63366\n",
      "Epoch 7892 - lr: 0.01000 - Train loss: 0.70304 - Test loss: 0.63458\n",
      "Epoch 7893 - lr: 0.01000 - Train loss: 0.76320 - Test loss: 0.63455\n",
      "Epoch 7894 - lr: 0.01000 - Train loss: 0.82452 - Test loss: 0.63398\n",
      "Epoch 7895 - lr: 0.01000 - Train loss: 0.80364 - Test loss: 0.63558\n",
      "Epoch 7896 - lr: 0.01000 - Train loss: 0.82070 - Test loss: 0.63591\n",
      "Epoch 7897 - lr: 0.01000 - Train loss: 0.81490 - Test loss: 0.63618\n",
      "Epoch 7898 - lr: 0.01000 - Train loss: 0.80580 - Test loss: 0.63734\n",
      "Epoch 7899 - lr: 0.01000 - Train loss: 0.81382 - Test loss: 0.63766\n",
      "Epoch 7900 - lr: 0.01000 - Train loss: 0.80462 - Test loss: 0.63848\n",
      "Epoch 7901 - lr: 0.01000 - Train loss: 0.80856 - Test loss: 0.63889\n",
      "Epoch 7902 - lr: 0.01000 - Train loss: 0.80326 - Test loss: 0.63951\n",
      "Epoch 7903 - lr: 0.01000 - Train loss: 0.80378 - Test loss: 0.63998\n",
      "Epoch 7904 - lr: 0.01000 - Train loss: 0.80127 - Test loss: 0.64045\n",
      "Epoch 7905 - lr: 0.01000 - Train loss: 0.79997 - Test loss: 0.64087\n",
      "Epoch 7906 - lr: 0.01000 - Train loss: 0.79829 - Test loss: 0.64125\n",
      "Epoch 7907 - lr: 0.01000 - Train loss: 0.79665 - Test loss: 0.64158\n",
      "Epoch 7908 - lr: 0.01000 - Train loss: 0.79497 - Test loss: 0.64186\n",
      "Epoch 7909 - lr: 0.01000 - Train loss: 0.79326 - Test loss: 0.64211\n",
      "Epoch 7910 - lr: 0.01000 - Train loss: 0.79151 - Test loss: 0.64232\n",
      "Epoch 7911 - lr: 0.01000 - Train loss: 0.78972 - Test loss: 0.64250\n",
      "Epoch 7912 - lr: 0.01000 - Train loss: 0.78785 - Test loss: 0.64264\n",
      "Epoch 7913 - lr: 0.01000 - Train loss: 0.78584 - Test loss: 0.64276\n",
      "Epoch 7914 - lr: 0.01000 - Train loss: 0.78360 - Test loss: 0.64284\n",
      "Epoch 7915 - lr: 0.01000 - Train loss: 0.78092 - Test loss: 0.64289\n",
      "Epoch 7916 - lr: 0.01000 - Train loss: 0.77726 - Test loss: 0.64285\n",
      "Epoch 7917 - lr: 0.01000 - Train loss: 0.77060 - Test loss: 0.64255\n",
      "Epoch 7918 - lr: 0.01000 - Train loss: 0.74800 - Test loss: 0.64055\n",
      "Epoch 7919 - lr: 0.01000 - Train loss: 0.67640 - Test loss: 0.64038\n",
      "Epoch 7920 - lr: 0.01000 - Train loss: 0.67651 - Test loss: 0.64010\n",
      "Epoch 7921 - lr: 0.01000 - Train loss: 0.67778 - Test loss: 0.63954\n",
      "Epoch 7922 - lr: 0.01000 - Train loss: 0.67858 - Test loss: 0.63945\n",
      "Epoch 7923 - lr: 0.01000 - Train loss: 0.68301 - Test loss: 0.63900\n",
      "Epoch 7924 - lr: 0.01000 - Train loss: 0.67594 - Test loss: 0.63804\n",
      "Epoch 7925 - lr: 0.01000 - Train loss: 0.67490 - Test loss: 0.63746\n",
      "Epoch 7926 - lr: 0.01000 - Train loss: 0.67593 - Test loss: 0.63710\n",
      "Epoch 7927 - lr: 0.01000 - Train loss: 0.67489 - Test loss: 0.63667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7928 - lr: 0.01000 - Train loss: 0.67586 - Test loss: 0.63633\n",
      "Epoch 7929 - lr: 0.01000 - Train loss: 0.67492 - Test loss: 0.63596\n",
      "Epoch 7930 - lr: 0.01000 - Train loss: 0.67585 - Test loss: 0.63562\n",
      "Epoch 7931 - lr: 0.01000 - Train loss: 0.67517 - Test loss: 0.63532\n",
      "Epoch 7932 - lr: 0.01000 - Train loss: 0.67614 - Test loss: 0.63495\n",
      "Epoch 7933 - lr: 0.01000 - Train loss: 0.67638 - Test loss: 0.63486\n",
      "Epoch 7934 - lr: 0.01000 - Train loss: 0.67912 - Test loss: 0.63450\n",
      "Epoch 7935 - lr: 0.01000 - Train loss: 0.67497 - Test loss: 0.63400\n",
      "Epoch 7936 - lr: 0.01000 - Train loss: 0.67629 - Test loss: 0.63384\n",
      "Epoch 7937 - lr: 0.01000 - Train loss: 0.67411 - Test loss: 0.63329\n",
      "Epoch 7938 - lr: 0.01000 - Train loss: 0.67385 - Test loss: 0.63292\n",
      "Epoch 7939 - lr: 0.01000 - Train loss: 0.67478 - Test loss: 0.63269\n",
      "Epoch 7940 - lr: 0.01000 - Train loss: 0.67346 - Test loss: 0.63234\n",
      "Epoch 7941 - lr: 0.01000 - Train loss: 0.67445 - Test loss: 0.63216\n",
      "Epoch 7942 - lr: 0.01000 - Train loss: 0.67291 - Test loss: 0.63177\n",
      "Epoch 7943 - lr: 0.01000 - Train loss: 0.67405 - Test loss: 0.63165\n",
      "Epoch 7944 - lr: 0.01000 - Train loss: 0.67248 - Test loss: 0.63125\n",
      "Epoch 7945 - lr: 0.01000 - Train loss: 0.67375 - Test loss: 0.63117\n",
      "Epoch 7946 - lr: 0.01000 - Train loss: 0.67235 - Test loss: 0.63079\n",
      "Epoch 7947 - lr: 0.01000 - Train loss: 0.67362 - Test loss: 0.63071\n",
      "Epoch 7948 - lr: 0.01000 - Train loss: 0.67211 - Test loss: 0.63032\n",
      "Epoch 7949 - lr: 0.01000 - Train loss: 0.67343 - Test loss: 0.63026\n",
      "Epoch 7950 - lr: 0.01000 - Train loss: 0.67197 - Test loss: 0.62989\n",
      "Epoch 7951 - lr: 0.01000 - Train loss: 0.67329 - Test loss: 0.62983\n",
      "Epoch 7952 - lr: 0.01000 - Train loss: 0.67176 - Test loss: 0.62945\n",
      "Epoch 7953 - lr: 0.01000 - Train loss: 0.67312 - Test loss: 0.62940\n",
      "Epoch 7954 - lr: 0.01000 - Train loss: 0.67162 - Test loss: 0.62904\n",
      "Epoch 7955 - lr: 0.01000 - Train loss: 0.67299 - Test loss: 0.62899\n",
      "Epoch 7956 - lr: 0.01000 - Train loss: 0.67141 - Test loss: 0.62862\n",
      "Epoch 7957 - lr: 0.01000 - Train loss: 0.67282 - Test loss: 0.62859\n",
      "Epoch 7958 - lr: 0.01000 - Train loss: 0.67131 - Test loss: 0.62823\n",
      "Epoch 7959 - lr: 0.01000 - Train loss: 0.67271 - Test loss: 0.62820\n",
      "Epoch 7960 - lr: 0.01000 - Train loss: 0.67108 - Test loss: 0.62783\n",
      "Epoch 7961 - lr: 0.01000 - Train loss: 0.67253 - Test loss: 0.62781\n",
      "Epoch 7962 - lr: 0.01000 - Train loss: 0.67105 - Test loss: 0.62747\n",
      "Epoch 7963 - lr: 0.01000 - Train loss: 0.67246 - Test loss: 0.62744\n",
      "Epoch 7964 - lr: 0.01000 - Train loss: 0.67071 - Test loss: 0.62707\n",
      "Epoch 7965 - lr: 0.01000 - Train loss: 0.67224 - Test loss: 0.62706\n",
      "Epoch 7966 - lr: 0.01000 - Train loss: 0.67093 - Test loss: 0.62676\n",
      "Epoch 7967 - lr: 0.01000 - Train loss: 0.67229 - Test loss: 0.62672\n",
      "Epoch 7968 - lr: 0.01000 - Train loss: 0.67021 - Test loss: 0.62632\n",
      "Epoch 7969 - lr: 0.01000 - Train loss: 0.67192 - Test loss: 0.62634\n",
      "Epoch 7970 - lr: 0.01000 - Train loss: 0.67133 - Test loss: 0.62615\n",
      "Epoch 7971 - lr: 0.01000 - Train loss: 0.67244 - Test loss: 0.62604\n",
      "Epoch 7972 - lr: 0.01000 - Train loss: 0.66994 - Test loss: 0.62561\n",
      "Epoch 7973 - lr: 0.01000 - Train loss: 0.67168 - Test loss: 0.62563\n",
      "Epoch 7974 - lr: 0.01000 - Train loss: 0.67115 - Test loss: 0.62546\n",
      "Epoch 7975 - lr: 0.01000 - Train loss: 0.67224 - Test loss: 0.62535\n",
      "Epoch 7976 - lr: 0.01000 - Train loss: 0.66969 - Test loss: 0.62493\n",
      "Epoch 7977 - lr: 0.01000 - Train loss: 0.67145 - Test loss: 0.62496\n",
      "Epoch 7978 - lr: 0.01000 - Train loss: 0.67093 - Test loss: 0.62480\n",
      "Epoch 7979 - lr: 0.01000 - Train loss: 0.67203 - Test loss: 0.62468\n",
      "Epoch 7980 - lr: 0.01000 - Train loss: 0.66941 - Test loss: 0.62427\n",
      "Epoch 7981 - lr: 0.01000 - Train loss: 0.67123 - Test loss: 0.62431\n",
      "Epoch 7982 - lr: 0.01000 - Train loss: 0.67093 - Test loss: 0.62419\n",
      "Epoch 7983 - lr: 0.01000 - Train loss: 0.67198 - Test loss: 0.62405\n",
      "Epoch 7984 - lr: 0.01000 - Train loss: 0.66960 - Test loss: 0.62369\n",
      "Epoch 7985 - lr: 0.01000 - Train loss: 0.67113 - Test loss: 0.62369\n",
      "Epoch 7986 - lr: 0.01000 - Train loss: 0.66928 - Test loss: 0.62336\n",
      "Epoch 7987 - lr: 0.01000 - Train loss: 0.67096 - Test loss: 0.62338\n",
      "Epoch 7988 - lr: 0.01000 - Train loss: 0.66976 - Test loss: 0.62316\n",
      "Epoch 7989 - lr: 0.01000 - Train loss: 0.67111 - Test loss: 0.62312\n",
      "Epoch 7990 - lr: 0.01000 - Train loss: 0.66844 - Test loss: 0.62270\n",
      "Epoch 7991 - lr: 0.01000 - Train loss: 0.67076 - Test loss: 0.62281\n",
      "Epoch 7992 - lr: 0.01000 - Train loss: 0.67277 - Test loss: 0.62307\n",
      "Epoch 7993 - lr: 0.01000 - Train loss: 0.67857 - Test loss: 0.62310\n",
      "Epoch 7994 - lr: 0.01000 - Train loss: 0.67242 - Test loss: 0.62239\n",
      "Epoch 7995 - lr: 0.01000 - Train loss: 0.67335 - Test loss: 0.62255\n",
      "Epoch 7996 - lr: 0.01000 - Train loss: 0.68034 - Test loss: 0.62263\n",
      "Epoch 7997 - lr: 0.01000 - Train loss: 0.68002 - Test loss: 0.62226\n",
      "Epoch 7998 - lr: 0.01000 - Train loss: 0.67965 - Test loss: 0.62211\n",
      "Epoch 7999 - lr: 0.01000 - Train loss: 0.67901 - Test loss: 0.62179\n",
      "Epoch 8000 - lr: 0.01000 - Train loss: 0.67529 - Test loss: 0.62118\n",
      "Epoch 8001 - lr: 0.01000 - Train loss: 0.67033 - Test loss: 0.62067\n",
      "Epoch 8002 - lr: 0.01000 - Train loss: 0.67388 - Test loss: 0.62108\n",
      "Epoch 8003 - lr: 0.01000 - Train loss: 0.67914 - Test loss: 0.62077\n",
      "Epoch 8004 - lr: 0.01000 - Train loss: 0.67452 - Test loss: 0.62040\n",
      "Epoch 8005 - lr: 0.01000 - Train loss: 0.67116 - Test loss: 0.62012\n",
      "Epoch 8006 - lr: 0.01000 - Train loss: 0.67162 - Test loss: 0.62004\n",
      "Epoch 8007 - lr: 0.01000 - Train loss: 0.67012 - Test loss: 0.61975\n",
      "Epoch 8008 - lr: 0.01000 - Train loss: 0.67378 - Test loss: 0.62022\n",
      "Epoch 8009 - lr: 0.01000 - Train loss: 0.67869 - Test loss: 0.61987\n",
      "Epoch 8010 - lr: 0.01000 - Train loss: 0.67317 - Test loss: 0.61946\n",
      "Epoch 8011 - lr: 0.01000 - Train loss: 0.67327 - Test loss: 0.61961\n",
      "Epoch 8012 - lr: 0.01000 - Train loss: 0.67193 - Test loss: 0.61910\n",
      "Epoch 8013 - lr: 0.01000 - Train loss: 0.67326 - Test loss: 0.61943\n",
      "Epoch 8014 - lr: 0.01000 - Train loss: 0.67849 - Test loss: 0.61911\n",
      "Epoch 8015 - lr: 0.01000 - Train loss: 0.67300 - Test loss: 0.61871\n",
      "Epoch 8016 - lr: 0.01000 - Train loss: 0.67305 - Test loss: 0.61886\n",
      "Epoch 8017 - lr: 0.01000 - Train loss: 0.67152 - Test loss: 0.61836\n",
      "Epoch 8018 - lr: 0.01000 - Train loss: 0.67250 - Test loss: 0.61862\n",
      "Epoch 8019 - lr: 0.01000 - Train loss: 0.67938 - Test loss: 0.61871\n",
      "Epoch 8020 - lr: 0.01000 - Train loss: 0.67891 - Test loss: 0.61841\n",
      "Epoch 8021 - lr: 0.01000 - Train loss: 0.67809 - Test loss: 0.61829\n",
      "Epoch 8022 - lr: 0.01000 - Train loss: 0.67417 - Test loss: 0.61774\n",
      "Epoch 8023 - lr: 0.01000 - Train loss: 0.66964 - Test loss: 0.61734\n",
      "Epoch 8024 - lr: 0.01000 - Train loss: 0.67340 - Test loss: 0.61777\n",
      "Epoch 8025 - lr: 0.01000 - Train loss: 0.67919 - Test loss: 0.61780\n",
      "Epoch 8026 - lr: 0.01000 - Train loss: 0.67805 - Test loss: 0.61729\n",
      "Epoch 8027 - lr: 0.01000 - Train loss: 0.67258 - Test loss: 0.61688\n",
      "Epoch 8028 - lr: 0.01000 - Train loss: 0.67264 - Test loss: 0.61704\n",
      "Epoch 8029 - lr: 0.01000 - Train loss: 0.67092 - Test loss: 0.61655\n",
      "Epoch 8030 - lr: 0.01000 - Train loss: 0.67133 - Test loss: 0.61672\n",
      "Epoch 8031 - lr: 0.01000 - Train loss: 0.67755 - Test loss: 0.61686\n",
      "Epoch 8032 - lr: 0.01000 - Train loss: 0.67254 - Test loss: 0.61631\n",
      "Epoch 8033 - lr: 0.01000 - Train loss: 0.67206 - Test loss: 0.61635\n",
      "Epoch 8034 - lr: 0.01000 - Train loss: 0.66979 - Test loss: 0.61595\n",
      "Epoch 8035 - lr: 0.01000 - Train loss: 0.66631 - Test loss: 0.61546\n",
      "Epoch 8036 - lr: 0.01000 - Train loss: 0.66894 - Test loss: 0.61570\n",
      "Epoch 8037 - lr: 0.01000 - Train loss: 0.67181 - Test loss: 0.61619\n",
      "Epoch 8038 - lr: 0.01000 - Train loss: 0.67867 - Test loss: 0.61629\n",
      "Epoch 8039 - lr: 0.01000 - Train loss: 0.67842 - Test loss: 0.61609\n",
      "Epoch 8040 - lr: 0.01000 - Train loss: 0.67865 - Test loss: 0.61607\n",
      "Epoch 8041 - lr: 0.01000 - Train loss: 0.67717 - Test loss: 0.61544\n",
      "Epoch 8042 - lr: 0.01000 - Train loss: 0.67047 - Test loss: 0.61502\n",
      "Epoch 8043 - lr: 0.01000 - Train loss: 0.67046 - Test loss: 0.61514\n",
      "Epoch 8044 - lr: 0.01000 - Train loss: 0.67508 - Test loss: 0.61513\n",
      "Epoch 8045 - lr: 0.01000 - Train loss: 0.66883 - Test loss: 0.61473\n",
      "Epoch 8046 - lr: 0.01000 - Train loss: 0.66531 - Test loss: 0.61416\n",
      "Epoch 8047 - lr: 0.01000 - Train loss: 0.66936 - Test loss: 0.61455\n",
      "Epoch 8048 - lr: 0.01000 - Train loss: 0.67170 - Test loss: 0.61487\n",
      "Epoch 8049 - lr: 0.01000 - Train loss: 0.66936 - Test loss: 0.61447\n",
      "Epoch 8050 - lr: 0.01000 - Train loss: 0.66569 - Test loss: 0.61399\n",
      "Epoch 8051 - lr: 0.01000 - Train loss: 0.66844 - Test loss: 0.61423\n",
      "Epoch 8052 - lr: 0.01000 - Train loss: 0.67156 - Test loss: 0.61475\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8053 - lr: 0.01000 - Train loss: 0.67711 - Test loss: 0.61451\n",
      "Epoch 8054 - lr: 0.01000 - Train loss: 0.67162 - Test loss: 0.61415\n",
      "Epoch 8055 - lr: 0.01000 - Train loss: 0.67185 - Test loss: 0.61433\n",
      "Epoch 8056 - lr: 0.01000 - Train loss: 0.67067 - Test loss: 0.61384\n",
      "Epoch 8057 - lr: 0.01000 - Train loss: 0.67223 - Test loss: 0.61424\n",
      "Epoch 8058 - lr: 0.01000 - Train loss: 0.67445 - Test loss: 0.61341\n",
      "Epoch 8059 - lr: 0.01000 - Train loss: 0.66795 - Test loss: 0.61337\n",
      "Epoch 8060 - lr: 0.01000 - Train loss: 0.66876 - Test loss: 0.61347\n",
      "Epoch 8061 - lr: 0.01000 - Train loss: 0.66987 - Test loss: 0.61332\n",
      "Epoch 8062 - lr: 0.01000 - Train loss: 0.66993 - Test loss: 0.61351\n",
      "Epoch 8063 - lr: 0.01000 - Train loss: 0.67540 - Test loss: 0.61356\n",
      "Epoch 8064 - lr: 0.01000 - Train loss: 0.66873 - Test loss: 0.61304\n",
      "Epoch 8065 - lr: 0.01000 - Train loss: 0.66459 - Test loss: 0.61247\n",
      "Epoch 8066 - lr: 0.01000 - Train loss: 0.66872 - Test loss: 0.61284\n",
      "Epoch 8067 - lr: 0.01000 - Train loss: 0.67098 - Test loss: 0.61314\n",
      "Epoch 8068 - lr: 0.01000 - Train loss: 0.66859 - Test loss: 0.61275\n",
      "Epoch 8069 - lr: 0.01000 - Train loss: 0.66439 - Test loss: 0.61221\n",
      "Epoch 8070 - lr: 0.01000 - Train loss: 0.66866 - Test loss: 0.61260\n",
      "Epoch 8071 - lr: 0.01000 - Train loss: 0.67048 - Test loss: 0.61283\n",
      "Epoch 8072 - lr: 0.01000 - Train loss: 0.66807 - Test loss: 0.61250\n",
      "Epoch 8073 - lr: 0.01000 - Train loss: 0.66366 - Test loss: 0.61189\n",
      "Epoch 8074 - lr: 0.01000 - Train loss: 0.66989 - Test loss: 0.61248\n",
      "Epoch 8075 - lr: 0.01000 - Train loss: 0.66734 - Test loss: 0.61226\n",
      "Epoch 8076 - lr: 0.01000 - Train loss: 0.66454 - Test loss: 0.61185\n",
      "Epoch 8077 - lr: 0.01000 - Train loss: 0.66773 - Test loss: 0.61212\n",
      "Epoch 8078 - lr: 0.01000 - Train loss: 0.67155 - Test loss: 0.61272\n",
      "Epoch 8079 - lr: 0.01000 - Train loss: 0.67362 - Test loss: 0.61189\n",
      "Epoch 8080 - lr: 0.01000 - Train loss: 0.66727 - Test loss: 0.61186\n",
      "Epoch 8081 - lr: 0.01000 - Train loss: 0.66926 - Test loss: 0.61212\n",
      "Epoch 8082 - lr: 0.01000 - Train loss: 0.67490 - Test loss: 0.61217\n",
      "Epoch 8083 - lr: 0.01000 - Train loss: 0.66814 - Test loss: 0.61161\n",
      "Epoch 8084 - lr: 0.01000 - Train loss: 0.66422 - Test loss: 0.61110\n",
      "Epoch 8085 - lr: 0.01000 - Train loss: 0.66750 - Test loss: 0.61136\n",
      "Epoch 8086 - lr: 0.01000 - Train loss: 0.67135 - Test loss: 0.61194\n",
      "Epoch 8087 - lr: 0.01000 - Train loss: 0.67422 - Test loss: 0.61126\n",
      "Epoch 8088 - lr: 0.01000 - Train loss: 0.66723 - Test loss: 0.61113\n",
      "Epoch 8089 - lr: 0.01000 - Train loss: 0.66290 - Test loss: 0.61058\n",
      "Epoch 8090 - lr: 0.01000 - Train loss: 0.66992 - Test loss: 0.61126\n",
      "Epoch 8091 - lr: 0.01000 - Train loss: 0.66733 - Test loss: 0.61101\n",
      "Epoch 8092 - lr: 0.01000 - Train loss: 0.66257 - Test loss: 0.61039\n",
      "Epoch 8093 - lr: 0.01000 - Train loss: 0.67030 - Test loss: 0.61115\n",
      "Epoch 8094 - lr: 0.01000 - Train loss: 0.66779 - Test loss: 0.61080\n",
      "Epoch 8095 - lr: 0.01000 - Train loss: 0.66373 - Test loss: 0.61033\n",
      "Epoch 8096 - lr: 0.01000 - Train loss: 0.66731 - Test loss: 0.61061\n",
      "Epoch 8097 - lr: 0.01000 - Train loss: 0.67097 - Test loss: 0.61112\n",
      "Epoch 8098 - lr: 0.01000 - Train loss: 0.67673 - Test loss: 0.61114\n",
      "Epoch 8099 - lr: 0.01000 - Train loss: 0.67457 - Test loss: 0.61041\n",
      "Epoch 8100 - lr: 0.01000 - Train loss: 0.66743 - Test loss: 0.61010\n",
      "Epoch 8101 - lr: 0.01000 - Train loss: 0.66267 - Test loss: 0.60954\n",
      "Epoch 8102 - lr: 0.01000 - Train loss: 0.66906 - Test loss: 0.61013\n",
      "Epoch 8103 - lr: 0.01000 - Train loss: 0.66644 - Test loss: 0.60990\n",
      "Epoch 8104 - lr: 0.01000 - Train loss: 0.66280 - Test loss: 0.60942\n",
      "Epoch 8105 - lr: 0.01000 - Train loss: 0.66836 - Test loss: 0.60993\n",
      "Epoch 8106 - lr: 0.01000 - Train loss: 0.66642 - Test loss: 0.60975\n",
      "Epoch 8107 - lr: 0.01000 - Train loss: 0.66952 - Test loss: 0.61013\n",
      "Epoch 8108 - lr: 0.01000 - Train loss: 0.67587 - Test loss: 0.61002\n",
      "Epoch 8109 - lr: 0.01000 - Train loss: 0.67502 - Test loss: 0.60994\n",
      "Epoch 8110 - lr: 0.01000 - Train loss: 0.67053 - Test loss: 0.60938\n",
      "Epoch 8111 - lr: 0.01000 - Train loss: 0.66782 - Test loss: 0.60918\n",
      "Epoch 8112 - lr: 0.01000 - Train loss: 0.66647 - Test loss: 0.60893\n",
      "Epoch 8113 - lr: 0.01000 - Train loss: 0.67042 - Test loss: 0.60939\n",
      "Epoch 8114 - lr: 0.01000 - Train loss: 0.67455 - Test loss: 0.60895\n",
      "Epoch 8115 - lr: 0.01000 - Train loss: 0.66751 - Test loss: 0.60861\n",
      "Epoch 8116 - lr: 0.01000 - Train loss: 0.66523 - Test loss: 0.60842\n",
      "Epoch 8117 - lr: 0.01000 - Train loss: 0.66654 - Test loss: 0.60848\n",
      "Epoch 8118 - lr: 0.01000 - Train loss: 0.66161 - Test loss: 0.60795\n",
      "Epoch 8119 - lr: 0.01000 - Train loss: 0.66977 - Test loss: 0.60879\n",
      "Epoch 8120 - lr: 0.01000 - Train loss: 0.66781 - Test loss: 0.60843\n",
      "Epoch 8121 - lr: 0.01000 - Train loss: 0.66785 - Test loss: 0.60857\n",
      "Epoch 8122 - lr: 0.01000 - Train loss: 0.67294 - Test loss: 0.60856\n",
      "Epoch 8123 - lr: 0.01000 - Train loss: 0.66633 - Test loss: 0.60811\n",
      "Epoch 8124 - lr: 0.01000 - Train loss: 0.66142 - Test loss: 0.60747\n",
      "Epoch 8125 - lr: 0.01000 - Train loss: 0.66953 - Test loss: 0.60828\n",
      "Epoch 8126 - lr: 0.01000 - Train loss: 0.66742 - Test loss: 0.60792\n",
      "Epoch 8127 - lr: 0.01000 - Train loss: 0.66663 - Test loss: 0.60793\n",
      "Epoch 8128 - lr: 0.01000 - Train loss: 0.66800 - Test loss: 0.60774\n",
      "Epoch 8129 - lr: 0.01000 - Train loss: 0.66913 - Test loss: 0.60811\n",
      "Epoch 8130 - lr: 0.01000 - Train loss: 0.67407 - Test loss: 0.60774\n",
      "Epoch 8131 - lr: 0.01000 - Train loss: 0.66699 - Test loss: 0.60741\n",
      "Epoch 8132 - lr: 0.01000 - Train loss: 0.66444 - Test loss: 0.60718\n",
      "Epoch 8133 - lr: 0.01000 - Train loss: 0.66585 - Test loss: 0.60726\n",
      "Epoch 8134 - lr: 0.01000 - Train loss: 0.66089 - Test loss: 0.60673\n",
      "Epoch 8135 - lr: 0.01000 - Train loss: 0.66970 - Test loss: 0.60766\n",
      "Epoch 8136 - lr: 0.01000 - Train loss: 0.67147 - Test loss: 0.60748\n",
      "Epoch 8137 - lr: 0.01000 - Train loss: 0.66523 - Test loss: 0.60703\n",
      "Epoch 8138 - lr: 0.01000 - Train loss: 0.66200 - Test loss: 0.60657\n",
      "Epoch 8139 - lr: 0.01000 - Train loss: 0.66668 - Test loss: 0.60698\n",
      "Epoch 8140 - lr: 0.01000 - Train loss: 0.66728 - Test loss: 0.60707\n",
      "Epoch 8141 - lr: 0.01000 - Train loss: 0.66513 - Test loss: 0.60679\n",
      "Epoch 8142 - lr: 0.01000 - Train loss: 0.66641 - Test loss: 0.60688\n",
      "Epoch 8143 - lr: 0.01000 - Train loss: 0.66840 - Test loss: 0.60672\n",
      "Epoch 8144 - lr: 0.01000 - Train loss: 0.66934 - Test loss: 0.60704\n",
      "Epoch 8145 - lr: 0.01000 - Train loss: 0.67384 - Test loss: 0.60694\n",
      "Epoch 8146 - lr: 0.01000 - Train loss: 0.66927 - Test loss: 0.60640\n",
      "Epoch 8147 - lr: 0.01000 - Train loss: 0.66702 - Test loss: 0.60626\n",
      "Epoch 8148 - lr: 0.01000 - Train loss: 0.66486 - Test loss: 0.60595\n",
      "Epoch 8149 - lr: 0.01000 - Train loss: 0.66557 - Test loss: 0.60599\n",
      "Epoch 8150 - lr: 0.01000 - Train loss: 0.66644 - Test loss: 0.60586\n",
      "Epoch 8151 - lr: 0.01000 - Train loss: 0.66416 - Test loss: 0.60574\n",
      "Epoch 8152 - lr: 0.01000 - Train loss: 0.66538 - Test loss: 0.60579\n",
      "Epoch 8153 - lr: 0.01000 - Train loss: 0.66010 - Test loss: 0.60525\n",
      "Epoch 8154 - lr: 0.01000 - Train loss: 0.66930 - Test loss: 0.60625\n",
      "Epoch 8155 - lr: 0.01000 - Train loss: 0.67453 - Test loss: 0.60636\n",
      "Epoch 8156 - lr: 0.01000 - Train loss: 0.67490 - Test loss: 0.60622\n",
      "Epoch 8157 - lr: 0.01000 - Train loss: 0.67315 - Test loss: 0.60558\n",
      "Epoch 8158 - lr: 0.01000 - Train loss: 0.66602 - Test loss: 0.60522\n",
      "Epoch 8159 - lr: 0.01000 - Train loss: 0.66263 - Test loss: 0.60490\n",
      "Epoch 8160 - lr: 0.01000 - Train loss: 0.66462 - Test loss: 0.60505\n",
      "Epoch 8161 - lr: 0.01000 - Train loss: 0.66313 - Test loss: 0.60493\n",
      "Epoch 8162 - lr: 0.01000 - Train loss: 0.66470 - Test loss: 0.60506\n",
      "Epoch 8163 - lr: 0.01000 - Train loss: 0.66034 - Test loss: 0.60462\n",
      "Epoch 8164 - lr: 0.01000 - Train loss: 0.66828 - Test loss: 0.60541\n",
      "Epoch 8165 - lr: 0.01000 - Train loss: 0.66581 - Test loss: 0.60505\n",
      "Epoch 8166 - lr: 0.01000 - Train loss: 0.66252 - Test loss: 0.60474\n",
      "Epoch 8167 - lr: 0.01000 - Train loss: 0.66441 - Test loss: 0.60485\n",
      "Epoch 8168 - lr: 0.01000 - Train loss: 0.66208 - Test loss: 0.60462\n",
      "Epoch 8169 - lr: 0.01000 - Train loss: 0.66446 - Test loss: 0.60480\n",
      "Epoch 8170 - lr: 0.01000 - Train loss: 0.66551 - Test loss: 0.60497\n",
      "Epoch 8171 - lr: 0.01000 - Train loss: 0.66733 - Test loss: 0.60481\n",
      "Epoch 8172 - lr: 0.01000 - Train loss: 0.66869 - Test loss: 0.60518\n",
      "Epoch 8173 - lr: 0.01000 - Train loss: 0.67377 - Test loss: 0.60491\n",
      "Epoch 8174 - lr: 0.01000 - Train loss: 0.67212 - Test loss: 0.60474\n",
      "Epoch 8175 - lr: 0.01000 - Train loss: 0.66529 - Test loss: 0.60415\n",
      "Epoch 8176 - lr: 0.01000 - Train loss: 0.66119 - Test loss: 0.60369\n",
      "Epoch 8177 - lr: 0.01000 - Train loss: 0.66497 - Test loss: 0.60401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8178 - lr: 0.01000 - Train loss: 0.66831 - Test loss: 0.60443\n",
      "Epoch 8179 - lr: 0.01000 - Train loss: 0.67105 - Test loss: 0.60422\n",
      "Epoch 8180 - lr: 0.01000 - Train loss: 0.66444 - Test loss: 0.60374\n",
      "Epoch 8181 - lr: 0.01000 - Train loss: 0.65911 - Test loss: 0.60310\n",
      "Epoch 8182 - lr: 0.01000 - Train loss: 0.66860 - Test loss: 0.60412\n",
      "Epoch 8183 - lr: 0.01000 - Train loss: 0.67387 - Test loss: 0.60408\n",
      "Epoch 8184 - lr: 0.01000 - Train loss: 0.67408 - Test loss: 0.60408\n",
      "Epoch 8185 - lr: 0.01000 - Train loss: 0.67311 - Test loss: 0.60361\n",
      "Epoch 8186 - lr: 0.01000 - Train loss: 0.66860 - Test loss: 0.60328\n",
      "Epoch 8187 - lr: 0.01000 - Train loss: 0.66507 - Test loss: 0.60307\n",
      "Epoch 8188 - lr: 0.01000 - Train loss: 0.66616 - Test loss: 0.60309\n",
      "Epoch 8189 - lr: 0.01000 - Train loss: 0.66367 - Test loss: 0.60279\n",
      "Epoch 8190 - lr: 0.01000 - Train loss: 0.66108 - Test loss: 0.60248\n",
      "Epoch 8191 - lr: 0.01000 - Train loss: 0.66421 - Test loss: 0.60276\n",
      "Epoch 8192 - lr: 0.01000 - Train loss: 0.66794 - Test loss: 0.60331\n",
      "Epoch 8193 - lr: 0.01000 - Train loss: 0.67086 - Test loss: 0.60270\n",
      "Epoch 8194 - lr: 0.01000 - Train loss: 0.66378 - Test loss: 0.60262\n",
      "Epoch 8195 - lr: 0.01000 - Train loss: 0.65916 - Test loss: 0.60213\n",
      "Epoch 8196 - lr: 0.01000 - Train loss: 0.66796 - Test loss: 0.60303\n",
      "Epoch 8197 - lr: 0.01000 - Train loss: 0.66930 - Test loss: 0.60284\n",
      "Epoch 8198 - lr: 0.01000 - Train loss: 0.66343 - Test loss: 0.60244\n",
      "Epoch 8199 - lr: 0.01000 - Train loss: 0.66338 - Test loss: 0.60235\n",
      "Epoch 8200 - lr: 0.01000 - Train loss: 0.66433 - Test loss: 0.60232\n",
      "Epoch 8201 - lr: 0.01000 - Train loss: 0.65852 - Test loss: 0.60177\n",
      "Epoch 8202 - lr: 0.01000 - Train loss: 0.66813 - Test loss: 0.60281\n",
      "Epoch 8203 - lr: 0.01000 - Train loss: 0.67247 - Test loss: 0.60256\n",
      "Epoch 8204 - lr: 0.01000 - Train loss: 0.66660 - Test loss: 0.60222\n",
      "Epoch 8205 - lr: 0.01000 - Train loss: 0.66780 - Test loss: 0.60252\n",
      "Epoch 8206 - lr: 0.01000 - Train loss: 0.67359 - Test loss: 0.60250\n",
      "Epoch 8207 - lr: 0.01000 - Train loss: 0.67215 - Test loss: 0.60196\n",
      "Epoch 8208 - lr: 0.01000 - Train loss: 0.66546 - Test loss: 0.60158\n",
      "Epoch 8209 - lr: 0.01000 - Train loss: 0.66573 - Test loss: 0.60175\n",
      "Epoch 8210 - lr: 0.01000 - Train loss: 0.67197 - Test loss: 0.60184\n",
      "Epoch 8211 - lr: 0.01000 - Train loss: 0.66599 - Test loss: 0.60131\n",
      "Epoch 8212 - lr: 0.01000 - Train loss: 0.66751 - Test loss: 0.60164\n",
      "Epoch 8213 - lr: 0.01000 - Train loss: 0.67090 - Test loss: 0.60108\n",
      "Epoch 8214 - lr: 0.01000 - Train loss: 0.66376 - Test loss: 0.60098\n",
      "Epoch 8215 - lr: 0.01000 - Train loss: 0.65814 - Test loss: 0.60039\n",
      "Epoch 8216 - lr: 0.01000 - Train loss: 0.66783 - Test loss: 0.60145\n",
      "Epoch 8217 - lr: 0.01000 - Train loss: 0.67142 - Test loss: 0.60109\n",
      "Epoch 8218 - lr: 0.01000 - Train loss: 0.66409 - Test loss: 0.60087\n",
      "Epoch 8219 - lr: 0.01000 - Train loss: 0.65836 - Test loss: 0.60031\n",
      "Epoch 8220 - lr: 0.01000 - Train loss: 0.66768 - Test loss: 0.60126\n",
      "Epoch 8221 - lr: 0.01000 - Train loss: 0.67268 - Test loss: 0.60134\n",
      "Epoch 8222 - lr: 0.01000 - Train loss: 0.67217 - Test loss: 0.60113\n",
      "Epoch 8223 - lr: 0.01000 - Train loss: 0.66885 - Test loss: 0.60067\n",
      "Epoch 8224 - lr: 0.01000 - Train loss: 0.66276 - Test loss: 0.60023\n",
      "Epoch 8225 - lr: 0.01000 - Train loss: 0.66057 - Test loss: 0.59993\n",
      "Epoch 8226 - lr: 0.01000 - Train loss: 0.66298 - Test loss: 0.60014\n",
      "Epoch 8227 - lr: 0.01000 - Train loss: 0.66401 - Test loss: 0.60033\n",
      "Epoch 8228 - lr: 0.01000 - Train loss: 0.66518 - Test loss: 0.60019\n",
      "Epoch 8229 - lr: 0.01000 - Train loss: 0.66570 - Test loss: 0.60046\n",
      "Epoch 8230 - lr: 0.01000 - Train loss: 0.67301 - Test loss: 0.60063\n",
      "Epoch 8231 - lr: 0.01000 - Train loss: 0.67268 - Test loss: 0.60034\n",
      "Epoch 8232 - lr: 0.01000 - Train loss: 0.67248 - Test loss: 0.60028\n",
      "Epoch 8233 - lr: 0.01000 - Train loss: 0.67216 - Test loss: 0.60009\n",
      "Epoch 8234 - lr: 0.01000 - Train loss: 0.67021 - Test loss: 0.59974\n",
      "Epoch 8235 - lr: 0.01000 - Train loss: 0.66350 - Test loss: 0.59928\n",
      "Epoch 8236 - lr: 0.01000 - Train loss: 0.65799 - Test loss: 0.59866\n",
      "Epoch 8237 - lr: 0.01000 - Train loss: 0.66726 - Test loss: 0.59961\n",
      "Epoch 8238 - lr: 0.01000 - Train loss: 0.67161 - Test loss: 0.59966\n",
      "Epoch 8239 - lr: 0.01000 - Train loss: 0.66626 - Test loss: 0.59918\n",
      "Epoch 8240 - lr: 0.01000 - Train loss: 0.66641 - Test loss: 0.59931\n",
      "Epoch 8241 - lr: 0.01000 - Train loss: 0.66438 - Test loss: 0.59888\n",
      "Epoch 8242 - lr: 0.01000 - Train loss: 0.66315 - Test loss: 0.59885\n",
      "Epoch 8243 - lr: 0.01000 - Train loss: 0.66384 - Test loss: 0.59878\n",
      "Epoch 8244 - lr: 0.01000 - Train loss: 0.65911 - Test loss: 0.59841\n",
      "Epoch 8245 - lr: 0.01000 - Train loss: 0.66419 - Test loss: 0.59890\n",
      "Epoch 8246 - lr: 0.01000 - Train loss: 0.66355 - Test loss: 0.59890\n",
      "Epoch 8247 - lr: 0.01000 - Train loss: 0.66539 - Test loss: 0.59904\n",
      "Epoch 8248 - lr: 0.01000 - Train loss: 0.66283 - Test loss: 0.59877\n",
      "Epoch 8249 - lr: 0.01000 - Train loss: 0.65720 - Test loss: 0.59815\n",
      "Epoch 8250 - lr: 0.01000 - Train loss: 0.66697 - Test loss: 0.59919\n",
      "Epoch 8251 - lr: 0.01000 - Train loss: 0.67003 - Test loss: 0.59875\n",
      "Epoch 8252 - lr: 0.01000 - Train loss: 0.66281 - Test loss: 0.59866\n",
      "Epoch 8253 - lr: 0.01000 - Train loss: 0.65689 - Test loss: 0.59805\n",
      "Epoch 8254 - lr: 0.01000 - Train loss: 0.66657 - Test loss: 0.59909\n",
      "Epoch 8255 - lr: 0.01000 - Train loss: 0.66927 - Test loss: 0.59856\n",
      "Epoch 8256 - lr: 0.01000 - Train loss: 0.66217 - Test loss: 0.59853\n",
      "Epoch 8257 - lr: 0.01000 - Train loss: 0.65809 - Test loss: 0.59811\n",
      "Epoch 8258 - lr: 0.01000 - Train loss: 0.66567 - Test loss: 0.59880\n",
      "Epoch 8259 - lr: 0.01000 - Train loss: 0.66290 - Test loss: 0.59853\n",
      "Epoch 8260 - lr: 0.01000 - Train loss: 0.65658 - Test loss: 0.59787\n",
      "Epoch 8261 - lr: 0.01000 - Train loss: 0.66611 - Test loss: 0.59886\n",
      "Epoch 8262 - lr: 0.01000 - Train loss: 0.67035 - Test loss: 0.59852\n",
      "Epoch 8263 - lr: 0.01000 - Train loss: 0.66292 - Test loss: 0.59831\n",
      "Epoch 8264 - lr: 0.01000 - Train loss: 0.65651 - Test loss: 0.59767\n",
      "Epoch 8265 - lr: 0.01000 - Train loss: 0.66609 - Test loss: 0.59866\n",
      "Epoch 8266 - lr: 0.01000 - Train loss: 0.66993 - Test loss: 0.59826\n",
      "Epoch 8267 - lr: 0.01000 - Train loss: 0.66259 - Test loss: 0.59811\n",
      "Epoch 8268 - lr: 0.01000 - Train loss: 0.65617 - Test loss: 0.59744\n",
      "Epoch 8269 - lr: 0.01000 - Train loss: 0.66505 - Test loss: 0.59834\n",
      "Epoch 8270 - lr: 0.01000 - Train loss: 0.67240 - Test loss: 0.59858\n",
      "Epoch 8271 - lr: 0.01000 - Train loss: 0.67104 - Test loss: 0.59810\n",
      "Epoch 8272 - lr: 0.01000 - Train loss: 0.66479 - Test loss: 0.59770\n",
      "Epoch 8273 - lr: 0.01000 - Train loss: 0.66627 - Test loss: 0.59802\n",
      "Epoch 8274 - lr: 0.01000 - Train loss: 0.66979 - Test loss: 0.59746\n",
      "Epoch 8275 - lr: 0.01000 - Train loss: 0.66251 - Test loss: 0.59730\n",
      "Epoch 8276 - lr: 0.01000 - Train loss: 0.65626 - Test loss: 0.59666\n",
      "Epoch 8277 - lr: 0.01000 - Train loss: 0.66584 - Test loss: 0.59766\n",
      "Epoch 8278 - lr: 0.01000 - Train loss: 0.67000 - Test loss: 0.59732\n",
      "Epoch 8279 - lr: 0.01000 - Train loss: 0.66258 - Test loss: 0.59715\n",
      "Epoch 8280 - lr: 0.01000 - Train loss: 0.65614 - Test loss: 0.59651\n",
      "Epoch 8281 - lr: 0.01000 - Train loss: 0.66569 - Test loss: 0.59749\n",
      "Epoch 8282 - lr: 0.01000 - Train loss: 0.67020 - Test loss: 0.59720\n",
      "Epoch 8283 - lr: 0.01000 - Train loss: 0.66269 - Test loss: 0.59696\n",
      "Epoch 8284 - lr: 0.01000 - Train loss: 0.65642 - Test loss: 0.59636\n",
      "Epoch 8285 - lr: 0.01000 - Train loss: 0.66635 - Test loss: 0.59736\n",
      "Epoch 8286 - lr: 0.01000 - Train loss: 0.66938 - Test loss: 0.59687\n",
      "Epoch 8287 - lr: 0.01000 - Train loss: 0.66209 - Test loss: 0.59676\n",
      "Epoch 8288 - lr: 0.01000 - Train loss: 0.65577 - Test loss: 0.59609\n",
      "Epoch 8289 - lr: 0.01000 - Train loss: 0.66481 - Test loss: 0.59701\n",
      "Epoch 8290 - lr: 0.01000 - Train loss: 0.67216 - Test loss: 0.59722\n",
      "Epoch 8291 - lr: 0.01000 - Train loss: 0.66991 - Test loss: 0.59663\n",
      "Epoch 8292 - lr: 0.01000 - Train loss: 0.66246 - Test loss: 0.59636\n",
      "Epoch 8293 - lr: 0.01000 - Train loss: 0.65607 - Test loss: 0.59573\n",
      "Epoch 8294 - lr: 0.01000 - Train loss: 0.66597 - Test loss: 0.59674\n",
      "Epoch 8295 - lr: 0.01000 - Train loss: 0.66825 - Test loss: 0.59615\n",
      "Epoch 8296 - lr: 0.01000 - Train loss: 0.66125 - Test loss: 0.59612\n",
      "Epoch 8297 - lr: 0.01000 - Train loss: 0.65886 - Test loss: 0.59587\n",
      "Epoch 8298 - lr: 0.01000 - Train loss: 0.66139 - Test loss: 0.59606\n",
      "Epoch 8299 - lr: 0.01000 - Train loss: 0.66256 - Test loss: 0.59621\n",
      "Epoch 8300 - lr: 0.01000 - Train loss: 0.66380 - Test loss: 0.59604\n",
      "Epoch 8301 - lr: 0.01000 - Train loss: 0.66458 - Test loss: 0.59630\n",
      "Epoch 8302 - lr: 0.01000 - Train loss: 0.67176 - Test loss: 0.59632\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8303 - lr: 0.01000 - Train loss: 0.67111 - Test loss: 0.59602\n",
      "Epoch 8304 - lr: 0.01000 - Train loss: 0.66935 - Test loss: 0.59584\n",
      "Epoch 8305 - lr: 0.01000 - Train loss: 0.66219 - Test loss: 0.59532\n",
      "Epoch 8306 - lr: 0.01000 - Train loss: 0.65628 - Test loss: 0.59472\n",
      "Epoch 8307 - lr: 0.01000 - Train loss: 0.66584 - Test loss: 0.59564\n",
      "Epoch 8308 - lr: 0.01000 - Train loss: 0.67106 - Test loss: 0.59571\n",
      "Epoch 8309 - lr: 0.01000 - Train loss: 0.67055 - Test loss: 0.59552\n",
      "Epoch 8310 - lr: 0.01000 - Train loss: 0.66716 - Test loss: 0.59509\n",
      "Epoch 8311 - lr: 0.01000 - Train loss: 0.66080 - Test loss: 0.59468\n",
      "Epoch 8312 - lr: 0.01000 - Train loss: 0.65727 - Test loss: 0.59427\n",
      "Epoch 8313 - lr: 0.01000 - Train loss: 0.66342 - Test loss: 0.59483\n",
      "Epoch 8314 - lr: 0.01000 - Train loss: 0.66075 - Test loss: 0.59467\n",
      "Epoch 8315 - lr: 0.01000 - Train loss: 0.65817 - Test loss: 0.59440\n",
      "Epoch 8316 - lr: 0.01000 - Train loss: 0.66127 - Test loss: 0.59468\n",
      "Epoch 8317 - lr: 0.01000 - Train loss: 0.66492 - Test loss: 0.59516\n",
      "Epoch 8318 - lr: 0.01000 - Train loss: 0.66992 - Test loss: 0.59485\n",
      "Epoch 8319 - lr: 0.01000 - Train loss: 0.66256 - Test loss: 0.59454\n",
      "Epoch 8320 - lr: 0.01000 - Train loss: 0.65980 - Test loss: 0.59435\n",
      "Epoch 8321 - lr: 0.01000 - Train loss: 0.66122 - Test loss: 0.59445\n",
      "Epoch 8322 - lr: 0.01000 - Train loss: 0.65517 - Test loss: 0.59389\n",
      "Epoch 8323 - lr: 0.01000 - Train loss: 0.66459 - Test loss: 0.59486\n",
      "Epoch 8324 - lr: 0.01000 - Train loss: 0.67090 - Test loss: 0.59487\n",
      "Epoch 8325 - lr: 0.01000 - Train loss: 0.66984 - Test loss: 0.59479\n",
      "Epoch 8326 - lr: 0.01000 - Train loss: 0.66383 - Test loss: 0.59425\n",
      "Epoch 8327 - lr: 0.01000 - Train loss: 0.66520 - Test loss: 0.59450\n",
      "Epoch 8328 - lr: 0.01000 - Train loss: 0.67119 - Test loss: 0.59443\n",
      "Epoch 8329 - lr: 0.01000 - Train loss: 0.67081 - Test loss: 0.59418\n",
      "Epoch 8330 - lr: 0.01000 - Train loss: 0.67027 - Test loss: 0.59410\n",
      "Epoch 8331 - lr: 0.01000 - Train loss: 0.66720 - Test loss: 0.59372\n",
      "Epoch 8332 - lr: 0.01000 - Train loss: 0.66065 - Test loss: 0.59334\n",
      "Epoch 8333 - lr: 0.01000 - Train loss: 0.65561 - Test loss: 0.59279\n",
      "Epoch 8334 - lr: 0.01000 - Train loss: 0.66534 - Test loss: 0.59376\n",
      "Epoch 8335 - lr: 0.01000 - Train loss: 0.67117 - Test loss: 0.59387\n",
      "Epoch 8336 - lr: 0.01000 - Train loss: 0.67036 - Test loss: 0.59356\n",
      "Epoch 8337 - lr: 0.01000 - Train loss: 0.66696 - Test loss: 0.59333\n",
      "Epoch 8338 - lr: 0.01000 - Train loss: 0.66041 - Test loss: 0.59298\n",
      "Epoch 8339 - lr: 0.01000 - Train loss: 0.65577 - Test loss: 0.59247\n",
      "Epoch 8340 - lr: 0.01000 - Train loss: 0.66503 - Test loss: 0.59337\n",
      "Epoch 8341 - lr: 0.01000 - Train loss: 0.66698 - Test loss: 0.59324\n",
      "Epoch 8342 - lr: 0.01000 - Train loss: 0.66037 - Test loss: 0.59290\n",
      "Epoch 8343 - lr: 0.01000 - Train loss: 0.65542 - Test loss: 0.59236\n",
      "Epoch 8344 - lr: 0.01000 - Train loss: 0.66516 - Test loss: 0.59332\n",
      "Epoch 8345 - lr: 0.01000 - Train loss: 0.67098 - Test loss: 0.59342\n",
      "Epoch 8346 - lr: 0.01000 - Train loss: 0.67037 - Test loss: 0.59313\n",
      "Epoch 8347 - lr: 0.01000 - Train loss: 0.66842 - Test loss: 0.59295\n",
      "Epoch 8348 - lr: 0.01000 - Train loss: 0.66132 - Test loss: 0.59252\n",
      "Epoch 8349 - lr: 0.01000 - Train loss: 0.65528 - Test loss: 0.59190\n",
      "Epoch 8350 - lr: 0.01000 - Train loss: 0.66506 - Test loss: 0.59286\n",
      "Epoch 8351 - lr: 0.01000 - Train loss: 0.67091 - Test loss: 0.59296\n",
      "Epoch 8352 - lr: 0.01000 - Train loss: 0.67019 - Test loss: 0.59265\n",
      "Epoch 8353 - lr: 0.01000 - Train loss: 0.66740 - Test loss: 0.59245\n",
      "Epoch 8354 - lr: 0.01000 - Train loss: 0.66073 - Test loss: 0.59211\n",
      "Epoch 8355 - lr: 0.01000 - Train loss: 0.65530 - Test loss: 0.59150\n",
      "Epoch 8356 - lr: 0.01000 - Train loss: 0.66489 - Test loss: 0.59242\n",
      "Epoch 8357 - lr: 0.01000 - Train loss: 0.66942 - Test loss: 0.59245\n",
      "Epoch 8358 - lr: 0.01000 - Train loss: 0.66333 - Test loss: 0.59196\n",
      "Epoch 8359 - lr: 0.01000 - Train loss: 0.66463 - Test loss: 0.59224\n",
      "Epoch 8360 - lr: 0.01000 - Train loss: 0.67063 - Test loss: 0.59223\n",
      "Epoch 8361 - lr: 0.01000 - Train loss: 0.67049 - Test loss: 0.59202\n",
      "Epoch 8362 - lr: 0.01000 - Train loss: 0.67060 - Test loss: 0.59196\n",
      "Epoch 8363 - lr: 0.01000 - Train loss: 0.67048 - Test loss: 0.59174\n",
      "Epoch 8364 - lr: 0.01000 - Train loss: 0.67060 - Test loss: 0.59169\n",
      "Epoch 8365 - lr: 0.01000 - Train loss: 0.67044 - Test loss: 0.59147\n",
      "Epoch 8366 - lr: 0.01000 - Train loss: 0.67052 - Test loss: 0.59143\n",
      "Epoch 8367 - lr: 0.01000 - Train loss: 0.67060 - Test loss: 0.59127\n",
      "Epoch 8368 - lr: 0.01000 - Train loss: 0.67047 - Test loss: 0.59111\n",
      "Epoch 8369 - lr: 0.01000 - Train loss: 0.67061 - Test loss: 0.59108\n",
      "Epoch 8370 - lr: 0.01000 - Train loss: 0.67026 - Test loss: 0.59084\n",
      "Epoch 8371 - lr: 0.01000 - Train loss: 0.66957 - Test loss: 0.59079\n",
      "Epoch 8372 - lr: 0.01000 - Train loss: 0.66485 - Test loss: 0.59038\n",
      "Epoch 8373 - lr: 0.01000 - Train loss: 0.66090 - Test loss: 0.59021\n",
      "Epoch 8374 - lr: 0.01000 - Train loss: 0.66292 - Test loss: 0.59036\n",
      "Epoch 8375 - lr: 0.01000 - Train loss: 0.66074 - Test loss: 0.59025\n",
      "Epoch 8376 - lr: 0.01000 - Train loss: 0.65631 - Test loss: 0.58978\n",
      "Epoch 8377 - lr: 0.01000 - Train loss: 0.66154 - Test loss: 0.59027\n",
      "Epoch 8378 - lr: 0.01000 - Train loss: 0.66071 - Test loss: 0.59030\n",
      "Epoch 8379 - lr: 0.01000 - Train loss: 0.66340 - Test loss: 0.59055\n",
      "Epoch 8380 - lr: 0.01000 - Train loss: 0.66101 - Test loss: 0.59032\n",
      "Epoch 8381 - lr: 0.01000 - Train loss: 0.65537 - Test loss: 0.58978\n",
      "Epoch 8382 - lr: 0.01000 - Train loss: 0.66358 - Test loss: 0.59053\n",
      "Epoch 8383 - lr: 0.01000 - Train loss: 0.66097 - Test loss: 0.59035\n",
      "Epoch 8384 - lr: 0.01000 - Train loss: 0.65499 - Test loss: 0.58981\n",
      "Epoch 8385 - lr: 0.01000 - Train loss: 0.66415 - Test loss: 0.59066\n",
      "Epoch 8386 - lr: 0.01000 - Train loss: 0.66348 - Test loss: 0.59040\n",
      "Epoch 8387 - lr: 0.01000 - Train loss: 0.66364 - Test loss: 0.59058\n",
      "Epoch 8388 - lr: 0.01000 - Train loss: 0.66149 - Test loss: 0.59017\n",
      "Epoch 8389 - lr: 0.01000 - Train loss: 0.65932 - Test loss: 0.59005\n",
      "Epoch 8390 - lr: 0.01000 - Train loss: 0.66066 - Test loss: 0.59016\n",
      "Epoch 8391 - lr: 0.01000 - Train loss: 0.65495 - Test loss: 0.58964\n",
      "Epoch 8392 - lr: 0.01000 - Train loss: 0.66376 - Test loss: 0.59044\n",
      "Epoch 8393 - lr: 0.01000 - Train loss: 0.66149 - Test loss: 0.59013\n",
      "Epoch 8394 - lr: 0.01000 - Train loss: 0.65972 - Test loss: 0.59006\n",
      "Epoch 8395 - lr: 0.01000 - Train loss: 0.66066 - Test loss: 0.59008\n",
      "Epoch 8396 - lr: 0.01000 - Train loss: 0.65432 - Test loss: 0.58953\n",
      "Epoch 8397 - lr: 0.01000 - Train loss: 0.66425 - Test loss: 0.59047\n",
      "Epoch 8398 - lr: 0.01000 - Train loss: 0.67020 - Test loss: 0.59055\n",
      "Epoch 8399 - lr: 0.01000 - Train loss: 0.66987 - Test loss: 0.59031\n",
      "Epoch 8400 - lr: 0.01000 - Train loss: 0.66955 - Test loss: 0.59021\n",
      "Epoch 8401 - lr: 0.01000 - Train loss: 0.66821 - Test loss: 0.58994\n",
      "Epoch 8402 - lr: 0.01000 - Train loss: 0.66083 - Test loss: 0.58941\n",
      "Epoch 8403 - lr: 0.01000 - Train loss: 0.65650 - Test loss: 0.58904\n",
      "Epoch 8404 - lr: 0.01000 - Train loss: 0.65944 - Test loss: 0.58931\n",
      "Epoch 8405 - lr: 0.01000 - Train loss: 0.66254 - Test loss: 0.58970\n",
      "Epoch 8406 - lr: 0.01000 - Train loss: 0.66964 - Test loss: 0.58982\n",
      "Epoch 8407 - lr: 0.01000 - Train loss: 0.66894 - Test loss: 0.58966\n",
      "Epoch 8408 - lr: 0.01000 - Train loss: 0.66419 - Test loss: 0.58920\n",
      "Epoch 8409 - lr: 0.01000 - Train loss: 0.66006 - Test loss: 0.58899\n",
      "Epoch 8410 - lr: 0.01000 - Train loss: 0.66239 - Test loss: 0.58913\n",
      "Epoch 8411 - lr: 0.01000 - Train loss: 0.66012 - Test loss: 0.58894\n",
      "Epoch 8412 - lr: 0.01000 - Train loss: 0.65489 - Test loss: 0.58840\n",
      "Epoch 8413 - lr: 0.01000 - Train loss: 0.66242 - Test loss: 0.58906\n",
      "Epoch 8414 - lr: 0.01000 - Train loss: 0.65995 - Test loss: 0.58896\n",
      "Epoch 8415 - lr: 0.01000 - Train loss: 0.65449 - Test loss: 0.58842\n",
      "Epoch 8416 - lr: 0.01000 - Train loss: 0.66313 - Test loss: 0.58917\n",
      "Epoch 8417 - lr: 0.01000 - Train loss: 0.66052 - Test loss: 0.58889\n",
      "Epoch 8418 - lr: 0.01000 - Train loss: 0.65571 - Test loss: 0.58853\n",
      "Epoch 8419 - lr: 0.01000 - Train loss: 0.65992 - Test loss: 0.58889\n",
      "Epoch 8420 - lr: 0.01000 - Train loss: 0.66275 - Test loss: 0.58918\n",
      "Epoch 8421 - lr: 0.01000 - Train loss: 0.66018 - Test loss: 0.58883\n",
      "Epoch 8422 - lr: 0.01000 - Train loss: 0.65421 - Test loss: 0.58832\n",
      "Epoch 8423 - lr: 0.01000 - Train loss: 0.66321 - Test loss: 0.58911\n",
      "Epoch 8424 - lr: 0.01000 - Train loss: 0.66157 - Test loss: 0.58880\n",
      "Epoch 8425 - lr: 0.01000 - Train loss: 0.66253 - Test loss: 0.58905\n",
      "Epoch 8426 - lr: 0.01000 - Train loss: 0.66964 - Test loss: 0.58908\n",
      "Epoch 8427 - lr: 0.01000 - Train loss: 0.66954 - Test loss: 0.58892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8428 - lr: 0.01000 - Train loss: 0.66955 - Test loss: 0.58878\n",
      "Epoch 8429 - lr: 0.01000 - Train loss: 0.66951 - Test loss: 0.58860\n",
      "Epoch 8430 - lr: 0.01000 - Train loss: 0.66953 - Test loss: 0.58846\n",
      "Epoch 8431 - lr: 0.01000 - Train loss: 0.66950 - Test loss: 0.58830\n",
      "Epoch 8432 - lr: 0.01000 - Train loss: 0.66951 - Test loss: 0.58817\n",
      "Epoch 8433 - lr: 0.01000 - Train loss: 0.66948 - Test loss: 0.58802\n",
      "Epoch 8434 - lr: 0.01000 - Train loss: 0.66951 - Test loss: 0.58790\n",
      "Epoch 8435 - lr: 0.01000 - Train loss: 0.66947 - Test loss: 0.58775\n",
      "Epoch 8436 - lr: 0.01000 - Train loss: 0.66950 - Test loss: 0.58765\n",
      "Epoch 8437 - lr: 0.01000 - Train loss: 0.66944 - Test loss: 0.58750\n",
      "Epoch 8438 - lr: 0.01000 - Train loss: 0.66949 - Test loss: 0.58741\n",
      "Epoch 8439 - lr: 0.01000 - Train loss: 0.66941 - Test loss: 0.58726\n",
      "Epoch 8440 - lr: 0.01000 - Train loss: 0.66947 - Test loss: 0.58719\n",
      "Epoch 8441 - lr: 0.01000 - Train loss: 0.66940 - Test loss: 0.58704\n",
      "Epoch 8442 - lr: 0.01000 - Train loss: 0.66947 - Test loss: 0.58697\n",
      "Epoch 8443 - lr: 0.01000 - Train loss: 0.66935 - Test loss: 0.58681\n",
      "Epoch 8444 - lr: 0.01000 - Train loss: 0.66941 - Test loss: 0.58677\n",
      "Epoch 8445 - lr: 0.01000 - Train loss: 0.66942 - Test loss: 0.58663\n",
      "Epoch 8446 - lr: 0.01000 - Train loss: 0.66944 - Test loss: 0.58653\n",
      "Epoch 8447 - lr: 0.01000 - Train loss: 0.66940 - Test loss: 0.58642\n",
      "Epoch 8448 - lr: 0.01000 - Train loss: 0.66947 - Test loss: 0.58636\n",
      "Epoch 8449 - lr: 0.01000 - Train loss: 0.66922 - Test loss: 0.58619\n",
      "Epoch 8450 - lr: 0.01000 - Train loss: 0.66903 - Test loss: 0.58617\n",
      "Epoch 8451 - lr: 0.01000 - Train loss: 0.66839 - Test loss: 0.58603\n",
      "Epoch 8452 - lr: 0.01000 - Train loss: 0.66332 - Test loss: 0.58561\n",
      "Epoch 8453 - lr: 0.01000 - Train loss: 0.65995 - Test loss: 0.58551\n",
      "Epoch 8454 - lr: 0.01000 - Train loss: 0.65938 - Test loss: 0.58547\n",
      "Epoch 8455 - lr: 0.01000 - Train loss: 0.66183 - Test loss: 0.58569\n",
      "Epoch 8456 - lr: 0.01000 - Train loss: 0.65995 - Test loss: 0.58563\n",
      "Epoch 8457 - lr: 0.01000 - Train loss: 0.65672 - Test loss: 0.58525\n",
      "Epoch 8458 - lr: 0.01000 - Train loss: 0.65872 - Test loss: 0.58550\n",
      "Epoch 8459 - lr: 0.01000 - Train loss: 0.65434 - Test loss: 0.58516\n",
      "Epoch 8460 - lr: 0.01000 - Train loss: 0.66122 - Test loss: 0.58579\n",
      "Epoch 8461 - lr: 0.01000 - Train loss: 0.65878 - Test loss: 0.58575\n",
      "Epoch 8462 - lr: 0.01000 - Train loss: 0.65424 - Test loss: 0.58530\n",
      "Epoch 8463 - lr: 0.01000 - Train loss: 0.66109 - Test loss: 0.58590\n",
      "Epoch 8464 - lr: 0.01000 - Train loss: 0.65857 - Test loss: 0.58582\n",
      "Epoch 8465 - lr: 0.01000 - Train loss: 0.65382 - Test loss: 0.58535\n",
      "Epoch 8466 - lr: 0.01000 - Train loss: 0.66195 - Test loss: 0.58605\n",
      "Epoch 8467 - lr: 0.01000 - Train loss: 0.65962 - Test loss: 0.58593\n",
      "Epoch 8468 - lr: 0.01000 - Train loss: 0.65455 - Test loss: 0.58542\n",
      "Epoch 8469 - lr: 0.01000 - Train loss: 0.65958 - Test loss: 0.58583\n",
      "Epoch 8470 - lr: 0.01000 - Train loss: 0.65984 - Test loss: 0.58589\n",
      "Epoch 8471 - lr: 0.01000 - Train loss: 0.65826 - Test loss: 0.58575\n",
      "Epoch 8472 - lr: 0.01000 - Train loss: 0.66249 - Test loss: 0.58614\n",
      "Epoch 8473 - lr: 0.01000 - Train loss: 0.66868 - Test loss: 0.58601\n",
      "Epoch 8474 - lr: 0.01000 - Train loss: 0.66652 - Test loss: 0.58584\n",
      "Epoch 8475 - lr: 0.01000 - Train loss: 0.65938 - Test loss: 0.58549\n",
      "Epoch 8476 - lr: 0.01000 - Train loss: 0.65421 - Test loss: 0.58495\n",
      "Epoch 8477 - lr: 0.01000 - Train loss: 0.65977 - Test loss: 0.58540\n",
      "Epoch 8478 - lr: 0.01000 - Train loss: 0.65824 - Test loss: 0.58535\n",
      "Epoch 8479 - lr: 0.01000 - Train loss: 0.66254 - Test loss: 0.58575\n",
      "Epoch 8480 - lr: 0.01000 - Train loss: 0.66892 - Test loss: 0.58573\n",
      "Epoch 8481 - lr: 0.01000 - Train loss: 0.66887 - Test loss: 0.58559\n",
      "Epoch 8482 - lr: 0.01000 - Train loss: 0.66884 - Test loss: 0.58544\n",
      "Epoch 8483 - lr: 0.01000 - Train loss: 0.66882 - Test loss: 0.58529\n",
      "Epoch 8484 - lr: 0.01000 - Train loss: 0.66881 - Test loss: 0.58515\n",
      "Epoch 8485 - lr: 0.01000 - Train loss: 0.66880 - Test loss: 0.58501\n",
      "Epoch 8486 - lr: 0.01000 - Train loss: 0.66879 - Test loss: 0.58487\n",
      "Epoch 8487 - lr: 0.01000 - Train loss: 0.66878 - Test loss: 0.58474\n",
      "Epoch 8488 - lr: 0.01000 - Train loss: 0.66877 - Test loss: 0.58462\n",
      "Epoch 8489 - lr: 0.01000 - Train loss: 0.66876 - Test loss: 0.58450\n",
      "Epoch 8490 - lr: 0.01000 - Train loss: 0.66875 - Test loss: 0.58438\n",
      "Epoch 8491 - lr: 0.01000 - Train loss: 0.66874 - Test loss: 0.58426\n",
      "Epoch 8492 - lr: 0.01000 - Train loss: 0.66874 - Test loss: 0.58415\n",
      "Epoch 8493 - lr: 0.01000 - Train loss: 0.66873 - Test loss: 0.58404\n",
      "Epoch 8494 - lr: 0.01000 - Train loss: 0.66872 - Test loss: 0.58393\n",
      "Epoch 8495 - lr: 0.01000 - Train loss: 0.66871 - Test loss: 0.58382\n",
      "Epoch 8496 - lr: 0.01000 - Train loss: 0.66870 - Test loss: 0.58372\n",
      "Epoch 8497 - lr: 0.01000 - Train loss: 0.66869 - Test loss: 0.58362\n",
      "Epoch 8498 - lr: 0.01000 - Train loss: 0.66869 - Test loss: 0.58351\n",
      "Epoch 8499 - lr: 0.01000 - Train loss: 0.66868 - Test loss: 0.58342\n",
      "Epoch 8500 - lr: 0.01000 - Train loss: 0.66867 - Test loss: 0.58332\n",
      "Epoch 8501 - lr: 0.01000 - Train loss: 0.66866 - Test loss: 0.58322\n",
      "Epoch 8502 - lr: 0.01000 - Train loss: 0.66865 - Test loss: 0.58312\n",
      "Epoch 8503 - lr: 0.01000 - Train loss: 0.66864 - Test loss: 0.58303\n",
      "Epoch 8504 - lr: 0.01000 - Train loss: 0.66863 - Test loss: 0.58294\n",
      "Epoch 8505 - lr: 0.01000 - Train loss: 0.66862 - Test loss: 0.58284\n",
      "Epoch 8506 - lr: 0.01000 - Train loss: 0.66861 - Test loss: 0.58275\n",
      "Epoch 8507 - lr: 0.01000 - Train loss: 0.66860 - Test loss: 0.58266\n",
      "Epoch 8508 - lr: 0.01000 - Train loss: 0.66859 - Test loss: 0.58257\n",
      "Epoch 8509 - lr: 0.01000 - Train loss: 0.66858 - Test loss: 0.58248\n",
      "Epoch 8510 - lr: 0.01000 - Train loss: 0.66857 - Test loss: 0.58239\n",
      "Epoch 8511 - lr: 0.01000 - Train loss: 0.66855 - Test loss: 0.58230\n",
      "Epoch 8512 - lr: 0.01000 - Train loss: 0.66854 - Test loss: 0.58221\n",
      "Epoch 8513 - lr: 0.01000 - Train loss: 0.66853 - Test loss: 0.58212\n",
      "Epoch 8514 - lr: 0.01000 - Train loss: 0.66852 - Test loss: 0.58203\n",
      "Epoch 8515 - lr: 0.01000 - Train loss: 0.66851 - Test loss: 0.58194\n",
      "Epoch 8516 - lr: 0.01000 - Train loss: 0.66849 - Test loss: 0.58185\n",
      "Epoch 8517 - lr: 0.01000 - Train loss: 0.66850 - Test loss: 0.58177\n",
      "Epoch 8518 - lr: 0.01000 - Train loss: 0.66842 - Test loss: 0.58167\n",
      "Epoch 8519 - lr: 0.01000 - Train loss: 0.66854 - Test loss: 0.58162\n",
      "Epoch 8520 - lr: 0.01000 - Train loss: 0.66794 - Test loss: 0.58139\n",
      "Epoch 8521 - lr: 0.01000 - Train loss: 0.66523 - Test loss: 0.58123\n",
      "Epoch 8522 - lr: 0.01000 - Train loss: 0.65926 - Test loss: 0.58114\n",
      "Epoch 8523 - lr: 0.01000 - Train loss: 0.65886 - Test loss: 0.58095\n",
      "Epoch 8524 - lr: 0.01000 - Train loss: 0.65936 - Test loss: 0.58083\n",
      "Epoch 8525 - lr: 0.01000 - Train loss: 0.65632 - Test loss: 0.58071\n",
      "Epoch 8526 - lr: 0.01000 - Train loss: 0.65881 - Test loss: 0.58107\n",
      "Epoch 8527 - lr: 0.01000 - Train loss: 0.65724 - Test loss: 0.58087\n",
      "Epoch 8528 - lr: 0.01000 - Train loss: 0.65930 - Test loss: 0.58111\n",
      "Epoch 8529 - lr: 0.01000 - Train loss: 0.65762 - Test loss: 0.58090\n",
      "Epoch 8530 - lr: 0.01000 - Train loss: 0.65914 - Test loss: 0.58102\n",
      "Epoch 8531 - lr: 0.01000 - Train loss: 0.65596 - Test loss: 0.58072\n",
      "Epoch 8532 - lr: 0.01000 - Train loss: 0.65841 - Test loss: 0.58101\n",
      "Epoch 8533 - lr: 0.01000 - Train loss: 0.65626 - Test loss: 0.58076\n",
      "Epoch 8534 - lr: 0.01000 - Train loss: 0.65866 - Test loss: 0.58103\n",
      "Epoch 8535 - lr: 0.01000 - Train loss: 0.65694 - Test loss: 0.58080\n",
      "Epoch 8536 - lr: 0.01000 - Train loss: 0.65891 - Test loss: 0.58099\n",
      "Epoch 8537 - lr: 0.01000 - Train loss: 0.65671 - Test loss: 0.58073\n",
      "Epoch 8538 - lr: 0.01000 - Train loss: 0.65879 - Test loss: 0.58092\n",
      "Epoch 8539 - lr: 0.01000 - Train loss: 0.65675 - Test loss: 0.58067\n",
      "Epoch 8540 - lr: 0.01000 - Train loss: 0.65871 - Test loss: 0.58084\n",
      "Epoch 8541 - lr: 0.01000 - Train loss: 0.65640 - Test loss: 0.58057\n",
      "Epoch 8542 - lr: 0.01000 - Train loss: 0.65853 - Test loss: 0.58077\n",
      "Epoch 8543 - lr: 0.01000 - Train loss: 0.65647 - Test loss: 0.58051\n",
      "Epoch 8544 - lr: 0.01000 - Train loss: 0.65848 - Test loss: 0.58068\n",
      "Epoch 8545 - lr: 0.01000 - Train loss: 0.65617 - Test loss: 0.58040\n",
      "Epoch 8546 - lr: 0.01000 - Train loss: 0.65830 - Test loss: 0.58059\n",
      "Epoch 8547 - lr: 0.01000 - Train loss: 0.65607 - Test loss: 0.58032\n",
      "Epoch 8548 - lr: 0.01000 - Train loss: 0.65818 - Test loss: 0.58050\n",
      "Epoch 8549 - lr: 0.01000 - Train loss: 0.65585 - Test loss: 0.58022\n",
      "Epoch 8550 - lr: 0.01000 - Train loss: 0.65800 - Test loss: 0.58040\n",
      "Epoch 8551 - lr: 0.01000 - Train loss: 0.65556 - Test loss: 0.58011\n",
      "Epoch 8552 - lr: 0.01000 - Train loss: 0.65775 - Test loss: 0.58030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8553 - lr: 0.01000 - Train loss: 0.65508 - Test loss: 0.57999\n",
      "Epoch 8554 - lr: 0.01000 - Train loss: 0.65727 - Test loss: 0.58018\n",
      "Epoch 8555 - lr: 0.01000 - Train loss: 0.65396 - Test loss: 0.57984\n",
      "Epoch 8556 - lr: 0.01000 - Train loss: 0.65606 - Test loss: 0.58000\n",
      "Epoch 8557 - lr: 0.01000 - Train loss: 0.65219 - Test loss: 0.57968\n",
      "Epoch 8558 - lr: 0.01000 - Train loss: 0.65764 - Test loss: 0.58010\n",
      "Epoch 8559 - lr: 0.01000 - Train loss: 0.65637 - Test loss: 0.58002\n",
      "Epoch 8560 - lr: 0.01000 - Train loss: 0.66039 - Test loss: 0.58031\n",
      "Epoch 8561 - lr: 0.01000 - Train loss: 0.66052 - Test loss: 0.57991\n",
      "Epoch 8562 - lr: 0.01000 - Train loss: 0.65807 - Test loss: 0.57982\n",
      "Epoch 8563 - lr: 0.01000 - Train loss: 0.65566 - Test loss: 0.57958\n",
      "Epoch 8564 - lr: 0.01000 - Train loss: 0.65161 - Test loss: 0.57916\n",
      "Epoch 8565 - lr: 0.01000 - Train loss: 0.65784 - Test loss: 0.57959\n",
      "Epoch 8566 - lr: 0.01000 - Train loss: 0.65522 - Test loss: 0.57941\n",
      "Epoch 8567 - lr: 0.01000 - Train loss: 0.65331 - Test loss: 0.57919\n",
      "Epoch 8568 - lr: 0.01000 - Train loss: 0.65535 - Test loss: 0.57931\n",
      "Epoch 8569 - lr: 0.01000 - Train loss: 0.65149 - Test loss: 0.57899\n",
      "Epoch 8570 - lr: 0.01000 - Train loss: 0.65721 - Test loss: 0.57937\n",
      "Epoch 8571 - lr: 0.01000 - Train loss: 0.65528 - Test loss: 0.57920\n",
      "Epoch 8572 - lr: 0.01000 - Train loss: 0.65921 - Test loss: 0.57948\n",
      "Epoch 8573 - lr: 0.01000 - Train loss: 0.66640 - Test loss: 0.57941\n",
      "Epoch 8574 - lr: 0.01000 - Train loss: 0.66528 - Test loss: 0.57912\n",
      "Epoch 8575 - lr: 0.01000 - Train loss: 0.65807 - Test loss: 0.57850\n",
      "Epoch 8576 - lr: 0.01000 - Train loss: 0.65920 - Test loss: 0.57871\n",
      "Epoch 8577 - lr: 0.01000 - Train loss: 0.66630 - Test loss: 0.57861\n",
      "Epoch 8578 - lr: 0.01000 - Train loss: 0.66581 - Test loss: 0.57839\n",
      "Epoch 8579 - lr: 0.01000 - Train loss: 0.66320 - Test loss: 0.57799\n",
      "Epoch 8580 - lr: 0.01000 - Train loss: 0.65634 - Test loss: 0.57764\n",
      "Epoch 8581 - lr: 0.01000 - Train loss: 0.65308 - Test loss: 0.57723\n",
      "Epoch 8582 - lr: 0.01000 - Train loss: 0.65512 - Test loss: 0.57730\n",
      "Epoch 8583 - lr: 0.01000 - Train loss: 0.65051 - Test loss: 0.57691\n",
      "Epoch 8584 - lr: 0.01000 - Train loss: 0.65692 - Test loss: 0.57732\n",
      "Epoch 8585 - lr: 0.01000 - Train loss: 0.65423 - Test loss: 0.57710\n",
      "Epoch 8586 - lr: 0.01000 - Train loss: 0.65081 - Test loss: 0.57679\n",
      "Epoch 8587 - lr: 0.01000 - Train loss: 0.65556 - Test loss: 0.57706\n",
      "Epoch 8588 - lr: 0.01000 - Train loss: 0.65701 - Test loss: 0.57712\n",
      "Epoch 8589 - lr: 0.01000 - Train loss: 0.65461 - Test loss: 0.57683\n",
      "Epoch 8590 - lr: 0.01000 - Train loss: 0.64961 - Test loss: 0.57635\n",
      "Epoch 8591 - lr: 0.01000 - Train loss: 0.65797 - Test loss: 0.57688\n",
      "Epoch 8592 - lr: 0.01000 - Train loss: 0.65570 - Test loss: 0.57659\n",
      "Epoch 8593 - lr: 0.01000 - Train loss: 0.65056 - Test loss: 0.57613\n",
      "Epoch 8594 - lr: 0.01000 - Train loss: 0.65492 - Test loss: 0.57630\n",
      "Epoch 8595 - lr: 0.01000 - Train loss: 0.65802 - Test loss: 0.57650\n",
      "Epoch 8596 - lr: 0.01000 - Train loss: 0.65553 - Test loss: 0.57603\n",
      "Epoch 8597 - lr: 0.01000 - Train loss: 0.64907 - Test loss: 0.57555\n",
      "Epoch 8598 - lr: 0.01000 - Train loss: 0.65832 - Test loss: 0.57616\n",
      "Epoch 8599 - lr: 0.01000 - Train loss: 0.65570 - Test loss: 0.57567\n",
      "Epoch 8600 - lr: 0.01000 - Train loss: 0.65042 - Test loss: 0.57537\n",
      "Epoch 8601 - lr: 0.01000 - Train loss: 0.65460 - Test loss: 0.57553\n",
      "Epoch 8602 - lr: 0.01000 - Train loss: 0.65819 - Test loss: 0.57580\n",
      "Epoch 8603 - lr: 0.01000 - Train loss: 0.65615 - Test loss: 0.57522\n",
      "Epoch 8604 - lr: 0.01000 - Train loss: 0.65468 - Test loss: 0.57525\n",
      "Epoch 8605 - lr: 0.01000 - Train loss: 0.65533 - Test loss: 0.57495\n",
      "Epoch 8606 - lr: 0.01000 - Train loss: 0.64852 - Test loss: 0.57454\n",
      "Epoch 8607 - lr: 0.01000 - Train loss: 0.65865 - Test loss: 0.57527\n",
      "Epoch 8608 - lr: 0.01000 - Train loss: 0.66130 - Test loss: 0.57494\n",
      "Epoch 8609 - lr: 0.01000 - Train loss: 0.65424 - Test loss: 0.57454\n",
      "Epoch 8610 - lr: 0.01000 - Train loss: 0.64882 - Test loss: 0.57411\n",
      "Epoch 8611 - lr: 0.01000 - Train loss: 0.65790 - Test loss: 0.57470\n",
      "Epoch 8612 - lr: 0.01000 - Train loss: 0.65518 - Test loss: 0.57421\n",
      "Epoch 8613 - lr: 0.01000 - Train loss: 0.64860 - Test loss: 0.57386\n",
      "Epoch 8614 - lr: 0.01000 - Train loss: 0.65826 - Test loss: 0.57452\n",
      "Epoch 8615 - lr: 0.01000 - Train loss: 0.65693 - Test loss: 0.57402\n",
      "Epoch 8616 - lr: 0.01000 - Train loss: 0.65823 - Test loss: 0.57437\n",
      "Epoch 8617 - lr: 0.01000 - Train loss: 0.66503 - Test loss: 0.57420\n",
      "Epoch 8618 - lr: 0.01000 - Train loss: 0.66487 - Test loss: 0.57399\n",
      "Epoch 8619 - lr: 0.01000 - Train loss: 0.66449 - Test loss: 0.57376\n",
      "Epoch 8620 - lr: 0.01000 - Train loss: 0.66220 - Test loss: 0.57339\n",
      "Epoch 8621 - lr: 0.01000 - Train loss: 0.65495 - Test loss: 0.57295\n",
      "Epoch 8622 - lr: 0.01000 - Train loss: 0.64978 - Test loss: 0.57264\n",
      "Epoch 8623 - lr: 0.01000 - Train loss: 0.65477 - Test loss: 0.57289\n",
      "Epoch 8624 - lr: 0.01000 - Train loss: 0.65552 - Test loss: 0.57295\n",
      "Epoch 8625 - lr: 0.01000 - Train loss: 0.65306 - Test loss: 0.57268\n",
      "Epoch 8626 - lr: 0.01000 - Train loss: 0.65189 - Test loss: 0.57269\n",
      "Epoch 8627 - lr: 0.01000 - Train loss: 0.65371 - Test loss: 0.57265\n",
      "Epoch 8628 - lr: 0.01000 - Train loss: 0.64797 - Test loss: 0.57237\n",
      "Epoch 8629 - lr: 0.01000 - Train loss: 0.65842 - Test loss: 0.57319\n",
      "Epoch 8630 - lr: 0.01000 - Train loss: 0.66476 - Test loss: 0.57310\n",
      "Epoch 8631 - lr: 0.01000 - Train loss: 0.66436 - Test loss: 0.57289\n",
      "Epoch 8632 - lr: 0.01000 - Train loss: 0.66218 - Test loss: 0.57253\n",
      "Epoch 8633 - lr: 0.01000 - Train loss: 0.65479 - Test loss: 0.57207\n",
      "Epoch 8634 - lr: 0.01000 - Train loss: 0.64907 - Test loss: 0.57178\n",
      "Epoch 8635 - lr: 0.01000 - Train loss: 0.65619 - Test loss: 0.57220\n",
      "Epoch 8636 - lr: 0.01000 - Train loss: 0.65361 - Test loss: 0.57194\n",
      "Epoch 8637 - lr: 0.01000 - Train loss: 0.64801 - Test loss: 0.57166\n",
      "Epoch 8638 - lr: 0.01000 - Train loss: 0.65829 - Test loss: 0.57244\n",
      "Epoch 8639 - lr: 0.01000 - Train loss: 0.66302 - Test loss: 0.57226\n",
      "Epoch 8640 - lr: 0.01000 - Train loss: 0.65499 - Test loss: 0.57166\n",
      "Epoch 8641 - lr: 0.01000 - Train loss: 0.64969 - Test loss: 0.57152\n",
      "Epoch 8642 - lr: 0.01000 - Train loss: 0.65434 - Test loss: 0.57173\n",
      "Epoch 8643 - lr: 0.01000 - Train loss: 0.65638 - Test loss: 0.57191\n",
      "Epoch 8644 - lr: 0.01000 - Train loss: 0.65412 - Test loss: 0.57158\n",
      "Epoch 8645 - lr: 0.01000 - Train loss: 0.64860 - Test loss: 0.57130\n",
      "Epoch 8646 - lr: 0.01000 - Train loss: 0.65695 - Test loss: 0.57183\n",
      "Epoch 8647 - lr: 0.01000 - Train loss: 0.65451 - Test loss: 0.57146\n",
      "Epoch 8648 - lr: 0.01000 - Train loss: 0.64825 - Test loss: 0.57118\n",
      "Epoch 8649 - lr: 0.01000 - Train loss: 0.65760 - Test loss: 0.57182\n",
      "Epoch 8650 - lr: 0.01000 - Train loss: 0.65518 - Test loss: 0.57129\n",
      "Epoch 8651 - lr: 0.01000 - Train loss: 0.65179 - Test loss: 0.57134\n",
      "Epoch 8652 - lr: 0.01000 - Train loss: 0.65353 - Test loss: 0.57123\n",
      "Epoch 8653 - lr: 0.01000 - Train loss: 0.64726 - Test loss: 0.57097\n",
      "Epoch 8654 - lr: 0.01000 - Train loss: 0.65754 - Test loss: 0.57180\n",
      "Epoch 8655 - lr: 0.01000 - Train loss: 0.66462 - Test loss: 0.57171\n",
      "Epoch 8656 - lr: 0.01000 - Train loss: 0.66449 - Test loss: 0.57155\n",
      "Epoch 8657 - lr: 0.01000 - Train loss: 0.66436 - Test loss: 0.57135\n",
      "Epoch 8658 - lr: 0.01000 - Train loss: 0.66401 - Test loss: 0.57115\n",
      "Epoch 8659 - lr: 0.01000 - Train loss: 0.66190 - Test loss: 0.57082\n",
      "Epoch 8660 - lr: 0.01000 - Train loss: 0.65445 - Test loss: 0.57035\n",
      "Epoch 8661 - lr: 0.01000 - Train loss: 0.64838 - Test loss: 0.57013\n",
      "Epoch 8662 - lr: 0.01000 - Train loss: 0.65698 - Test loss: 0.57071\n",
      "Epoch 8663 - lr: 0.01000 - Train loss: 0.65444 - Test loss: 0.57030\n",
      "Epoch 8664 - lr: 0.01000 - Train loss: 0.64781 - Test loss: 0.57007\n",
      "Epoch 8665 - lr: 0.01000 - Train loss: 0.65793 - Test loss: 0.57084\n",
      "Epoch 8666 - lr: 0.01000 - Train loss: 0.66110 - Test loss: 0.57055\n",
      "Epoch 8667 - lr: 0.01000 - Train loss: 0.65402 - Test loss: 0.57016\n",
      "Epoch 8668 - lr: 0.01000 - Train loss: 0.64847 - Test loss: 0.56995\n",
      "Epoch 8669 - lr: 0.01000 - Train loss: 0.65648 - Test loss: 0.57045\n",
      "Epoch 8670 - lr: 0.01000 - Train loss: 0.65411 - Test loss: 0.57011\n",
      "Epoch 8671 - lr: 0.01000 - Train loss: 0.64818 - Test loss: 0.56989\n",
      "Epoch 8672 - lr: 0.01000 - Train loss: 0.65709 - Test loss: 0.57048\n",
      "Epoch 8673 - lr: 0.01000 - Train loss: 0.65434 - Test loss: 0.56998\n",
      "Epoch 8674 - lr: 0.01000 - Train loss: 0.64718 - Test loss: 0.56977\n",
      "Epoch 8675 - lr: 0.01000 - Train loss: 0.65758 - Test loss: 0.57062\n",
      "Epoch 8676 - lr: 0.01000 - Train loss: 0.66427 - Test loss: 0.57048\n",
      "Epoch 8677 - lr: 0.01000 - Train loss: 0.66375 - Test loss: 0.57033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8678 - lr: 0.01000 - Train loss: 0.66092 - Test loss: 0.56996\n",
      "Epoch 8679 - lr: 0.01000 - Train loss: 0.65387 - Test loss: 0.56956\n",
      "Epoch 8680 - lr: 0.01000 - Train loss: 0.64824 - Test loss: 0.56936\n",
      "Epoch 8681 - lr: 0.01000 - Train loss: 0.65672 - Test loss: 0.56991\n",
      "Epoch 8682 - lr: 0.01000 - Train loss: 0.65419 - Test loss: 0.56950\n",
      "Epoch 8683 - lr: 0.01000 - Train loss: 0.64742 - Test loss: 0.56929\n",
      "Epoch 8684 - lr: 0.01000 - Train loss: 0.65783 - Test loss: 0.57010\n",
      "Epoch 8685 - lr: 0.01000 - Train loss: 0.66408 - Test loss: 0.57000\n",
      "Epoch 8686 - lr: 0.01000 - Train loss: 0.66362 - Test loss: 0.56980\n",
      "Epoch 8687 - lr: 0.01000 - Train loss: 0.66064 - Test loss: 0.56943\n",
      "Epoch 8688 - lr: 0.01000 - Train loss: 0.65365 - Test loss: 0.56905\n",
      "Epoch 8689 - lr: 0.01000 - Train loss: 0.64806 - Test loss: 0.56888\n",
      "Epoch 8690 - lr: 0.01000 - Train loss: 0.65694 - Test loss: 0.56949\n",
      "Epoch 8691 - lr: 0.01000 - Train loss: 0.65421 - Test loss: 0.56900\n",
      "Epoch 8692 - lr: 0.01000 - Train loss: 0.64707 - Test loss: 0.56883\n",
      "Epoch 8693 - lr: 0.01000 - Train loss: 0.65747 - Test loss: 0.56970\n",
      "Epoch 8694 - lr: 0.01000 - Train loss: 0.66406 - Test loss: 0.56955\n",
      "Epoch 8695 - lr: 0.01000 - Train loss: 0.66329 - Test loss: 0.56939\n",
      "Epoch 8696 - lr: 0.01000 - Train loss: 0.65816 - Test loss: 0.56890\n",
      "Epoch 8697 - lr: 0.01000 - Train loss: 0.65351 - Test loss: 0.56878\n",
      "Epoch 8698 - lr: 0.01000 - Train loss: 0.65660 - Test loss: 0.56907\n",
      "Epoch 8699 - lr: 0.01000 - Train loss: 0.65411 - Test loss: 0.56858\n",
      "Epoch 8700 - lr: 0.01000 - Train loss: 0.64715 - Test loss: 0.56841\n",
      "Epoch 8701 - lr: 0.01000 - Train loss: 0.65762 - Test loss: 0.56928\n",
      "Epoch 8702 - lr: 0.01000 - Train loss: 0.66412 - Test loss: 0.56915\n",
      "Epoch 8703 - lr: 0.01000 - Train loss: 0.66395 - Test loss: 0.56904\n",
      "Epoch 8704 - lr: 0.01000 - Train loss: 0.66356 - Test loss: 0.56886\n",
      "Epoch 8705 - lr: 0.01000 - Train loss: 0.66104 - Test loss: 0.56853\n",
      "Epoch 8706 - lr: 0.01000 - Train loss: 0.65400 - Test loss: 0.56814\n",
      "Epoch 8707 - lr: 0.01000 - Train loss: 0.64863 - Test loss: 0.56804\n",
      "Epoch 8708 - lr: 0.01000 - Train loss: 0.65511 - Test loss: 0.56841\n",
      "Epoch 8709 - lr: 0.01000 - Train loss: 0.65235 - Test loss: 0.56815\n",
      "Epoch 8710 - lr: 0.01000 - Train loss: 0.64817 - Test loss: 0.56813\n",
      "Epoch 8711 - lr: 0.01000 - Train loss: 0.65648 - Test loss: 0.56868\n",
      "Epoch 8712 - lr: 0.01000 - Train loss: 0.65400 - Test loss: 0.56826\n",
      "Epoch 8713 - lr: 0.01000 - Train loss: 0.64732 - Test loss: 0.56810\n",
      "Epoch 8714 - lr: 0.01000 - Train loss: 0.65771 - Test loss: 0.56890\n",
      "Epoch 8715 - lr: 0.01000 - Train loss: 0.66367 - Test loss: 0.56877\n",
      "Epoch 8716 - lr: 0.01000 - Train loss: 0.66207 - Test loss: 0.56849\n",
      "Epoch 8717 - lr: 0.01000 - Train loss: 0.65411 - Test loss: 0.56790\n",
      "Epoch 8718 - lr: 0.01000 - Train loss: 0.64715 - Test loss: 0.56780\n",
      "Epoch 8719 - lr: 0.01000 - Train loss: 0.65765 - Test loss: 0.56866\n",
      "Epoch 8720 - lr: 0.01000 - Train loss: 0.66410 - Test loss: 0.56853\n",
      "Epoch 8721 - lr: 0.01000 - Train loss: 0.66410 - Test loss: 0.56841\n",
      "Epoch 8722 - lr: 0.01000 - Train loss: 0.66411 - Test loss: 0.56826\n",
      "Epoch 8723 - lr: 0.01000 - Train loss: 0.66411 - Test loss: 0.56813\n",
      "Epoch 8724 - lr: 0.01000 - Train loss: 0.66411 - Test loss: 0.56801\n",
      "Epoch 8725 - lr: 0.01000 - Train loss: 0.66411 - Test loss: 0.56789\n",
      "Epoch 8726 - lr: 0.01000 - Train loss: 0.66412 - Test loss: 0.56778\n",
      "Epoch 8727 - lr: 0.01000 - Train loss: 0.66413 - Test loss: 0.56767\n",
      "Epoch 8728 - lr: 0.01000 - Train loss: 0.66414 - Test loss: 0.56757\n",
      "Epoch 8729 - lr: 0.01000 - Train loss: 0.66415 - Test loss: 0.56747\n",
      "Epoch 8730 - lr: 0.01000 - Train loss: 0.66417 - Test loss: 0.56738\n",
      "Epoch 8731 - lr: 0.01000 - Train loss: 0.66417 - Test loss: 0.56729\n",
      "Epoch 8732 - lr: 0.01000 - Train loss: 0.66420 - Test loss: 0.56721\n",
      "Epoch 8733 - lr: 0.01000 - Train loss: 0.66419 - Test loss: 0.56712\n",
      "Epoch 8734 - lr: 0.01000 - Train loss: 0.66423 - Test loss: 0.56704\n",
      "Epoch 8735 - lr: 0.01000 - Train loss: 0.66419 - Test loss: 0.56695\n",
      "Epoch 8736 - lr: 0.01000 - Train loss: 0.66429 - Test loss: 0.56690\n",
      "Epoch 8737 - lr: 0.01000 - Train loss: 0.66405 - Test loss: 0.56677\n",
      "Epoch 8738 - lr: 0.01000 - Train loss: 0.66387 - Test loss: 0.56672\n",
      "Epoch 8739 - lr: 0.01000 - Train loss: 0.66313 - Test loss: 0.56657\n",
      "Epoch 8740 - lr: 0.01000 - Train loss: 0.65702 - Test loss: 0.56609\n",
      "Epoch 8741 - lr: 0.01000 - Train loss: 0.65666 - Test loss: 0.56644\n",
      "Epoch 8742 - lr: 0.01000 - Train loss: 0.65461 - Test loss: 0.56604\n",
      "Epoch 8743 - lr: 0.01000 - Train loss: 0.64968 - Test loss: 0.56608\n",
      "Epoch 8744 - lr: 0.01000 - Train loss: 0.65279 - Test loss: 0.56617\n",
      "Epoch 8745 - lr: 0.01000 - Train loss: 0.65607 - Test loss: 0.56677\n",
      "Epoch 8746 - lr: 0.01000 - Train loss: 0.66275 - Test loss: 0.56666\n",
      "Epoch 8747 - lr: 0.01000 - Train loss: 0.65493 - Test loss: 0.56608\n",
      "Epoch 8748 - lr: 0.01000 - Train loss: 0.65195 - Test loss: 0.56636\n",
      "Epoch 8749 - lr: 0.01000 - Train loss: 0.65418 - Test loss: 0.56624\n",
      "Epoch 8750 - lr: 0.01000 - Train loss: 0.65009 - Test loss: 0.56630\n",
      "Epoch 8751 - lr: 0.01000 - Train loss: 0.65249 - Test loss: 0.56626\n",
      "Epoch 8752 - lr: 0.01000 - Train loss: 0.65021 - Test loss: 0.56646\n",
      "Epoch 8753 - lr: 0.01000 - Train loss: 0.65250 - Test loss: 0.56640\n",
      "Epoch 8754 - lr: 0.01000 - Train loss: 0.64936 - Test loss: 0.56653\n",
      "Epoch 8755 - lr: 0.01000 - Train loss: 0.65330 - Test loss: 0.56663\n",
      "Epoch 8756 - lr: 0.01000 - Train loss: 0.65781 - Test loss: 0.56720\n",
      "Epoch 8757 - lr: 0.01000 - Train loss: 0.66413 - Test loss: 0.56706\n",
      "Epoch 8758 - lr: 0.01000 - Train loss: 0.66431 - Test loss: 0.56692\n",
      "Epoch 8759 - lr: 0.01000 - Train loss: 0.66408 - Test loss: 0.56673\n",
      "Epoch 8760 - lr: 0.01000 - Train loss: 0.66399 - Test loss: 0.56665\n",
      "Epoch 8761 - lr: 0.01000 - Train loss: 0.66379 - Test loss: 0.56652\n",
      "Epoch 8762 - lr: 0.01000 - Train loss: 0.66270 - Test loss: 0.56631\n",
      "Epoch 8763 - lr: 0.01000 - Train loss: 0.65497 - Test loss: 0.56573\n",
      "Epoch 8764 - lr: 0.01000 - Train loss: 0.65282 - Test loss: 0.56609\n",
      "Epoch 8765 - lr: 0.01000 - Train loss: 0.65459 - Test loss: 0.56587\n",
      "Epoch 8766 - lr: 0.01000 - Train loss: 0.65026 - Test loss: 0.56596\n",
      "Epoch 8767 - lr: 0.01000 - Train loss: 0.65247 - Test loss: 0.56589\n",
      "Epoch 8768 - lr: 0.01000 - Train loss: 0.64818 - Test loss: 0.56597\n",
      "Epoch 8769 - lr: 0.01000 - Train loss: 0.65623 - Test loss: 0.56648\n",
      "Epoch 8770 - lr: 0.01000 - Train loss: 0.65423 - Test loss: 0.56612\n",
      "Epoch 8771 - lr: 0.01000 - Train loss: 0.64979 - Test loss: 0.56615\n",
      "Epoch 8772 - lr: 0.01000 - Train loss: 0.65241 - Test loss: 0.56610\n",
      "Epoch 8773 - lr: 0.01000 - Train loss: 0.65196 - Test loss: 0.56642\n",
      "Epoch 8774 - lr: 0.01000 - Train loss: 0.65403 - Test loss: 0.56620\n",
      "Epoch 8775 - lr: 0.01000 - Train loss: 0.64890 - Test loss: 0.56621\n",
      "Epoch 8776 - lr: 0.01000 - Train loss: 0.65374 - Test loss: 0.56637\n",
      "Epoch 8777 - lr: 0.01000 - Train loss: 0.65573 - Test loss: 0.56659\n",
      "Epoch 8778 - lr: 0.01000 - Train loss: 0.65361 - Test loss: 0.56620\n",
      "Epoch 8779 - lr: 0.01000 - Train loss: 0.64829 - Test loss: 0.56617\n",
      "Epoch 8780 - lr: 0.01000 - Train loss: 0.65523 - Test loss: 0.56653\n",
      "Epoch 8781 - lr: 0.01000 - Train loss: 0.65251 - Test loss: 0.56620\n",
      "Epoch 8782 - lr: 0.01000 - Train loss: 0.64656 - Test loss: 0.56613\n",
      "Epoch 8783 - lr: 0.01000 - Train loss: 0.65702 - Test loss: 0.56701\n",
      "Epoch 8784 - lr: 0.01000 - Train loss: 0.66376 - Test loss: 0.56680\n",
      "Epoch 8785 - lr: 0.01000 - Train loss: 0.66156 - Test loss: 0.56654\n",
      "Epoch 8786 - lr: 0.01000 - Train loss: 0.65411 - Test loss: 0.56604\n",
      "Epoch 8787 - lr: 0.01000 - Train loss: 0.64773 - Test loss: 0.56600\n",
      "Epoch 8788 - lr: 0.01000 - Train loss: 0.65661 - Test loss: 0.56656\n",
      "Epoch 8789 - lr: 0.01000 - Train loss: 0.65405 - Test loss: 0.56603\n",
      "Epoch 8790 - lr: 0.01000 - Train loss: 0.64684 - Test loss: 0.56597\n",
      "Epoch 8791 - lr: 0.01000 - Train loss: 0.65763 - Test loss: 0.56683\n",
      "Epoch 8792 - lr: 0.01000 - Train loss: 0.66354 - Test loss: 0.56657\n",
      "Epoch 8793 - lr: 0.01000 - Train loss: 0.66013 - Test loss: 0.56624\n",
      "Epoch 8794 - lr: 0.01000 - Train loss: 0.65314 - Test loss: 0.56587\n",
      "Epoch 8795 - lr: 0.01000 - Train loss: 0.64729 - Test loss: 0.56583\n",
      "Epoch 8796 - lr: 0.01000 - Train loss: 0.65737 - Test loss: 0.56654\n",
      "Epoch 8797 - lr: 0.01000 - Train loss: 0.65829 - Test loss: 0.56607\n",
      "Epoch 8798 - lr: 0.01000 - Train loss: 0.65246 - Test loss: 0.56588\n",
      "Epoch 8799 - lr: 0.01000 - Train loss: 0.65669 - Test loss: 0.56649\n",
      "Epoch 8800 - lr: 0.01000 - Train loss: 0.66390 - Test loss: 0.56632\n",
      "Epoch 8801 - lr: 0.01000 - Train loss: 0.66382 - Test loss: 0.56623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8802 - lr: 0.01000 - Train loss: 0.66374 - Test loss: 0.56610\n",
      "Epoch 8803 - lr: 0.01000 - Train loss: 0.66350 - Test loss: 0.56595\n",
      "Epoch 8804 - lr: 0.01000 - Train loss: 0.66192 - Test loss: 0.56571\n",
      "Epoch 8805 - lr: 0.01000 - Train loss: 0.65400 - Test loss: 0.56517\n",
      "Epoch 8806 - lr: 0.01000 - Train loss: 0.64712 - Test loss: 0.56519\n",
      "Epoch 8807 - lr: 0.01000 - Train loss: 0.65750 - Test loss: 0.56599\n",
      "Epoch 8808 - lr: 0.01000 - Train loss: 0.66210 - Test loss: 0.56577\n",
      "Epoch 8809 - lr: 0.01000 - Train loss: 0.65405 - Test loss: 0.56518\n",
      "Epoch 8810 - lr: 0.01000 - Train loss: 0.64723 - Test loss: 0.56522\n",
      "Epoch 8811 - lr: 0.01000 - Train loss: 0.65738 - Test loss: 0.56597\n",
      "Epoch 8812 - lr: 0.01000 - Train loss: 0.65918 - Test loss: 0.56556\n",
      "Epoch 8813 - lr: 0.01000 - Train loss: 0.65213 - Test loss: 0.56524\n",
      "Epoch 8814 - lr: 0.01000 - Train loss: 0.64674 - Test loss: 0.56524\n",
      "Epoch 8815 - lr: 0.01000 - Train loss: 0.65756 - Test loss: 0.56612\n",
      "Epoch 8816 - lr: 0.01000 - Train loss: 0.66299 - Test loss: 0.56580\n",
      "Epoch 8817 - lr: 0.01000 - Train loss: 0.65613 - Test loss: 0.56532\n",
      "Epoch 8818 - lr: 0.01000 - Train loss: 0.65741 - Test loss: 0.56589\n",
      "Epoch 8819 - lr: 0.01000 - Train loss: 0.66383 - Test loss: 0.56575\n",
      "Epoch 8820 - lr: 0.01000 - Train loss: 0.66401 - Test loss: 0.56562\n",
      "Epoch 8821 - lr: 0.01000 - Train loss: 0.66381 - Test loss: 0.56545\n",
      "Epoch 8822 - lr: 0.01000 - Train loss: 0.66379 - Test loss: 0.56539\n",
      "Epoch 8823 - lr: 0.01000 - Train loss: 0.66390 - Test loss: 0.56529\n",
      "Epoch 8824 - lr: 0.01000 - Train loss: 0.66404 - Test loss: 0.56519\n",
      "Epoch 8825 - lr: 0.01000 - Train loss: 0.66372 - Test loss: 0.56504\n",
      "Epoch 8826 - lr: 0.01000 - Train loss: 0.66329 - Test loss: 0.56497\n",
      "Epoch 8827 - lr: 0.01000 - Train loss: 0.66025 - Test loss: 0.56465\n",
      "Epoch 8828 - lr: 0.01000 - Train loss: 0.65369 - Test loss: 0.56435\n",
      "Epoch 8829 - lr: 0.01000 - Train loss: 0.65011 - Test loss: 0.56450\n",
      "Epoch 8830 - lr: 0.01000 - Train loss: 0.65225 - Test loss: 0.56440\n",
      "Epoch 8831 - lr: 0.01000 - Train loss: 0.64698 - Test loss: 0.56447\n",
      "Epoch 8832 - lr: 0.01000 - Train loss: 0.65762 - Test loss: 0.56530\n",
      "Epoch 8833 - lr: 0.01000 - Train loss: 0.66391 - Test loss: 0.56520\n",
      "Epoch 8834 - lr: 0.01000 - Train loss: 0.66416 - Test loss: 0.56508\n",
      "Epoch 8835 - lr: 0.01000 - Train loss: 0.66302 - Test loss: 0.56481\n",
      "Epoch 8836 - lr: 0.01000 - Train loss: 0.65659 - Test loss: 0.56437\n",
      "Epoch 8837 - lr: 0.01000 - Train loss: 0.65669 - Test loss: 0.56480\n",
      "Epoch 8838 - lr: 0.01000 - Train loss: 0.65411 - Test loss: 0.56421\n",
      "Epoch 8839 - lr: 0.01000 - Train loss: 0.64782 - Test loss: 0.56431\n",
      "Epoch 8840 - lr: 0.01000 - Train loss: 0.65572 - Test loss: 0.56478\n",
      "Epoch 8841 - lr: 0.01000 - Train loss: 0.65375 - Test loss: 0.56444\n",
      "Epoch 8842 - lr: 0.01000 - Train loss: 0.64944 - Test loss: 0.56453\n",
      "Epoch 8843 - lr: 0.01000 - Train loss: 0.65198 - Test loss: 0.56446\n",
      "Epoch 8844 - lr: 0.01000 - Train loss: 0.65052 - Test loss: 0.56474\n",
      "Epoch 8845 - lr: 0.01000 - Train loss: 0.65261 - Test loss: 0.56456\n",
      "Epoch 8846 - lr: 0.01000 - Train loss: 0.64623 - Test loss: 0.56454\n",
      "Epoch 8847 - lr: 0.01000 - Train loss: 0.65690 - Test loss: 0.56542\n",
      "Epoch 8848 - lr: 0.01000 - Train loss: 0.66223 - Test loss: 0.56502\n",
      "Epoch 8849 - lr: 0.01000 - Train loss: 0.65397 - Test loss: 0.56453\n",
      "Epoch 8850 - lr: 0.01000 - Train loss: 0.64613 - Test loss: 0.56451\n",
      "Epoch 8851 - lr: 0.01000 - Train loss: 0.65660 - Test loss: 0.56537\n",
      "Epoch 8852 - lr: 0.01000 - Train loss: 0.66343 - Test loss: 0.56513\n",
      "Epoch 8853 - lr: 0.01000 - Train loss: 0.66050 - Test loss: 0.56482\n",
      "Epoch 8854 - lr: 0.01000 - Train loss: 0.65365 - Test loss: 0.56441\n",
      "Epoch 8855 - lr: 0.01000 - Train loss: 0.64878 - Test loss: 0.56447\n",
      "Epoch 8856 - lr: 0.01000 - Train loss: 0.65243 - Test loss: 0.56449\n",
      "Epoch 8857 - lr: 0.01000 - Train loss: 0.65708 - Test loss: 0.56514\n",
      "Epoch 8858 - lr: 0.01000 - Train loss: 0.66151 - Test loss: 0.56466\n",
      "Epoch 8859 - lr: 0.01000 - Train loss: 0.65395 - Test loss: 0.56431\n",
      "Epoch 8860 - lr: 0.01000 - Train loss: 0.64775 - Test loss: 0.56435\n",
      "Epoch 8861 - lr: 0.01000 - Train loss: 0.65499 - Test loss: 0.56470\n",
      "Epoch 8862 - lr: 0.01000 - Train loss: 0.65242 - Test loss: 0.56435\n",
      "Epoch 8863 - lr: 0.01000 - Train loss: 0.64605 - Test loss: 0.56431\n",
      "Epoch 8864 - lr: 0.01000 - Train loss: 0.65675 - Test loss: 0.56518\n",
      "Epoch 8865 - lr: 0.01000 - Train loss: 0.66225 - Test loss: 0.56480\n",
      "Epoch 8866 - lr: 0.01000 - Train loss: 0.65389 - Test loss: 0.56428\n",
      "Epoch 8867 - lr: 0.01000 - Train loss: 0.64667 - Test loss: 0.56433\n",
      "Epoch 8868 - lr: 0.01000 - Train loss: 0.65742 - Test loss: 0.56510\n",
      "Epoch 8869 - lr: 0.01000 - Train loss: 0.66377 - Test loss: 0.56495\n",
      "Epoch 8870 - lr: 0.01000 - Train loss: 0.66392 - Test loss: 0.56478\n",
      "Epoch 8871 - lr: 0.01000 - Train loss: 0.66323 - Test loss: 0.56454\n",
      "Epoch 8872 - lr: 0.01000 - Train loss: 0.66058 - Test loss: 0.56427\n",
      "Epoch 8873 - lr: 0.01000 - Train loss: 0.65368 - Test loss: 0.56387\n",
      "Epoch 8874 - lr: 0.01000 - Train loss: 0.64917 - Test loss: 0.56398\n",
      "Epoch 8875 - lr: 0.01000 - Train loss: 0.65166 - Test loss: 0.56389\n",
      "Epoch 8876 - lr: 0.01000 - Train loss: 0.64978 - Test loss: 0.56416\n",
      "Epoch 8877 - lr: 0.01000 - Train loss: 0.65184 - Test loss: 0.56399\n",
      "Epoch 8878 - lr: 0.01000 - Train loss: 0.64582 - Test loss: 0.56401\n",
      "Epoch 8879 - lr: 0.01000 - Train loss: 0.65614 - Test loss: 0.56485\n",
      "Epoch 8880 - lr: 0.01000 - Train loss: 0.66385 - Test loss: 0.56471\n",
      "Epoch 8881 - lr: 0.01000 - Train loss: 0.66372 - Test loss: 0.56455\n",
      "Epoch 8882 - lr: 0.01000 - Train loss: 0.66395 - Test loss: 0.56446\n",
      "Epoch 8883 - lr: 0.01000 - Train loss: 0.66231 - Test loss: 0.56411\n",
      "Epoch 8884 - lr: 0.01000 - Train loss: 0.65415 - Test loss: 0.56359\n",
      "Epoch 8885 - lr: 0.01000 - Train loss: 0.64997 - Test loss: 0.56387\n",
      "Epoch 8886 - lr: 0.01000 - Train loss: 0.65206 - Test loss: 0.56368\n",
      "Epoch 8887 - lr: 0.01000 - Train loss: 0.64559 - Test loss: 0.56367\n",
      "Epoch 8888 - lr: 0.01000 - Train loss: 0.65532 - Test loss: 0.56448\n",
      "Epoch 8889 - lr: 0.01000 - Train loss: 0.66242 - Test loss: 0.56430\n",
      "Epoch 8890 - lr: 0.01000 - Train loss: 0.65527 - Test loss: 0.56369\n",
      "Epoch 8891 - lr: 0.01000 - Train loss: 0.65683 - Test loss: 0.56433\n",
      "Epoch 8892 - lr: 0.01000 - Train loss: 0.66070 - Test loss: 0.56379\n",
      "Epoch 8893 - lr: 0.01000 - Train loss: 0.65354 - Test loss: 0.56353\n",
      "Epoch 8894 - lr: 0.01000 - Train loss: 0.64858 - Test loss: 0.56363\n",
      "Epoch 8895 - lr: 0.01000 - Train loss: 0.65187 - Test loss: 0.56361\n",
      "Epoch 8896 - lr: 0.01000 - Train loss: 0.65550 - Test loss: 0.56422\n",
      "Epoch 8897 - lr: 0.01000 - Train loss: 0.66362 - Test loss: 0.56412\n",
      "Epoch 8898 - lr: 0.01000 - Train loss: 0.66348 - Test loss: 0.56394\n",
      "Epoch 8899 - lr: 0.01000 - Train loss: 0.66376 - Test loss: 0.56387\n",
      "Epoch 8900 - lr: 0.01000 - Train loss: 0.66230 - Test loss: 0.56355\n",
      "Epoch 8901 - lr: 0.01000 - Train loss: 0.65447 - Test loss: 0.56306\n",
      "Epoch 8902 - lr: 0.01000 - Train loss: 0.65365 - Test loss: 0.56357\n",
      "Epoch 8903 - lr: 0.01000 - Train loss: 0.65432 - Test loss: 0.56305\n",
      "Epoch 8904 - lr: 0.01000 - Train loss: 0.65265 - Test loss: 0.56350\n",
      "Epoch 8905 - lr: 0.01000 - Train loss: 0.65355 - Test loss: 0.56306\n",
      "Epoch 8906 - lr: 0.01000 - Train loss: 0.64603 - Test loss: 0.56308\n",
      "Epoch 8907 - lr: 0.01000 - Train loss: 0.65713 - Test loss: 0.56394\n",
      "Epoch 8908 - lr: 0.01000 - Train loss: 0.66042 - Test loss: 0.56340\n",
      "Epoch 8909 - lr: 0.01000 - Train loss: 0.65329 - Test loss: 0.56315\n",
      "Epoch 8910 - lr: 0.01000 - Train loss: 0.64819 - Test loss: 0.56324\n",
      "Epoch 8911 - lr: 0.01000 - Train loss: 0.65221 - Test loss: 0.56328\n",
      "Epoch 8912 - lr: 0.01000 - Train loss: 0.65698 - Test loss: 0.56387\n",
      "Epoch 8913 - lr: 0.01000 - Train loss: 0.66357 - Test loss: 0.56368\n",
      "Epoch 8914 - lr: 0.01000 - Train loss: 0.66311 - Test loss: 0.56348\n",
      "Epoch 8915 - lr: 0.01000 - Train loss: 0.66215 - Test loss: 0.56333\n",
      "Epoch 8916 - lr: 0.01000 - Train loss: 0.65492 - Test loss: 0.56275\n",
      "Epoch 8917 - lr: 0.01000 - Train loss: 0.65642 - Test loss: 0.56342\n",
      "Epoch 8918 - lr: 0.01000 - Train loss: 0.66087 - Test loss: 0.56296\n",
      "Epoch 8919 - lr: 0.01000 - Train loss: 0.65363 - Test loss: 0.56266\n",
      "Epoch 8920 - lr: 0.01000 - Train loss: 0.64885 - Test loss: 0.56282\n",
      "Epoch 8921 - lr: 0.01000 - Train loss: 0.65128 - Test loss: 0.56271\n",
      "Epoch 8922 - lr: 0.01000 - Train loss: 0.64835 - Test loss: 0.56293\n",
      "Epoch 8923 - lr: 0.01000 - Train loss: 0.65173 - Test loss: 0.56292\n",
      "Epoch 8924 - lr: 0.01000 - Train loss: 0.65585 - Test loss: 0.56356\n",
      "Epoch 8925 - lr: 0.01000 - Train loss: 0.66284 - Test loss: 0.56331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8926 - lr: 0.01000 - Train loss: 0.66017 - Test loss: 0.56303\n",
      "Epoch 8927 - lr: 0.01000 - Train loss: 0.65331 - Test loss: 0.56262\n",
      "Epoch 8928 - lr: 0.01000 - Train loss: 0.64893 - Test loss: 0.56276\n",
      "Epoch 8929 - lr: 0.01000 - Train loss: 0.65120 - Test loss: 0.56261\n",
      "Epoch 8930 - lr: 0.01000 - Train loss: 0.64653 - Test loss: 0.56274\n",
      "Epoch 8931 - lr: 0.01000 - Train loss: 0.65625 - Test loss: 0.56333\n",
      "Epoch 8932 - lr: 0.01000 - Train loss: 0.65365 - Test loss: 0.56269\n",
      "Epoch 8933 - lr: 0.01000 - Train loss: 0.64925 - Test loss: 0.56294\n",
      "Epoch 8934 - lr: 0.01000 - Train loss: 0.65134 - Test loss: 0.56274\n",
      "Epoch 8935 - lr: 0.01000 - Train loss: 0.64498 - Test loss: 0.56275\n",
      "Epoch 8936 - lr: 0.01000 - Train loss: 0.65412 - Test loss: 0.56348\n",
      "Epoch 8937 - lr: 0.01000 - Train loss: 0.65743 - Test loss: 0.56304\n",
      "Epoch 8938 - lr: 0.01000 - Train loss: 0.65158 - Test loss: 0.56286\n",
      "Epoch 8939 - lr: 0.01000 - Train loss: 0.65635 - Test loss: 0.56347\n",
      "Epoch 8940 - lr: 0.01000 - Train loss: 0.65987 - Test loss: 0.56289\n",
      "Epoch 8941 - lr: 0.01000 - Train loss: 0.65255 - Test loss: 0.56263\n",
      "Epoch 8942 - lr: 0.01000 - Train loss: 0.64641 - Test loss: 0.56267\n",
      "Epoch 8943 - lr: 0.01000 - Train loss: 0.65588 - Test loss: 0.56320\n",
      "Epoch 8944 - lr: 0.01000 - Train loss: 0.65300 - Test loss: 0.56256\n",
      "Epoch 8945 - lr: 0.01000 - Train loss: 0.64501 - Test loss: 0.56258\n",
      "Epoch 8946 - lr: 0.01000 - Train loss: 0.65503 - Test loss: 0.56336\n",
      "Epoch 8947 - lr: 0.01000 - Train loss: 0.66309 - Test loss: 0.56325\n",
      "Epoch 8948 - lr: 0.01000 - Train loss: 0.66340 - Test loss: 0.56310\n",
      "Epoch 8949 - lr: 0.01000 - Train loss: 0.66112 - Test loss: 0.56268\n",
      "Epoch 8950 - lr: 0.01000 - Train loss: 0.65304 - Test loss: 0.56223\n",
      "Epoch 8951 - lr: 0.01000 - Train loss: 0.64507 - Test loss: 0.56224\n",
      "Epoch 8952 - lr: 0.01000 - Train loss: 0.65548 - Test loss: 0.56306\n",
      "Epoch 8953 - lr: 0.01000 - Train loss: 0.66315 - Test loss: 0.56290\n",
      "Epoch 8954 - lr: 0.01000 - Train loss: 0.66337 - Test loss: 0.56279\n",
      "Epoch 8955 - lr: 0.01000 - Train loss: 0.66150 - Test loss: 0.56244\n",
      "Epoch 8956 - lr: 0.01000 - Train loss: 0.65312 - Test loss: 0.56192\n",
      "Epoch 8957 - lr: 0.01000 - Train loss: 0.64575 - Test loss: 0.56202\n",
      "Epoch 8958 - lr: 0.01000 - Train loss: 0.65678 - Test loss: 0.56279\n",
      "Epoch 8959 - lr: 0.01000 - Train loss: 0.66303 - Test loss: 0.56259\n",
      "Epoch 8960 - lr: 0.01000 - Train loss: 0.66340 - Test loss: 0.56251\n",
      "Epoch 8961 - lr: 0.01000 - Train loss: 0.66027 - Test loss: 0.56203\n",
      "Epoch 8962 - lr: 0.01000 - Train loss: 0.65315 - Test loss: 0.56174\n",
      "Epoch 8963 - lr: 0.01000 - Train loss: 0.64846 - Test loss: 0.56190\n",
      "Epoch 8964 - lr: 0.01000 - Train loss: 0.65085 - Test loss: 0.56175\n",
      "Epoch 8965 - lr: 0.01000 - Train loss: 0.64703 - Test loss: 0.56193\n",
      "Epoch 8966 - lr: 0.01000 - Train loss: 0.65348 - Test loss: 0.56216\n",
      "Epoch 8967 - lr: 0.01000 - Train loss: 0.65072 - Test loss: 0.56188\n",
      "Epoch 8968 - lr: 0.01000 - Train loss: 0.64765 - Test loss: 0.56206\n",
      "Epoch 8969 - lr: 0.01000 - Train loss: 0.65163 - Test loss: 0.56207\n",
      "Epoch 8970 - lr: 0.01000 - Train loss: 0.65650 - Test loss: 0.56265\n",
      "Epoch 8971 - lr: 0.01000 - Train loss: 0.66240 - Test loss: 0.56235\n",
      "Epoch 8972 - lr: 0.01000 - Train loss: 0.66009 - Test loss: 0.56207\n",
      "Epoch 8973 - lr: 0.01000 - Train loss: 0.65289 - Test loss: 0.56161\n",
      "Epoch 8974 - lr: 0.01000 - Train loss: 0.64720 - Test loss: 0.56171\n",
      "Epoch 8975 - lr: 0.01000 - Train loss: 0.65222 - Test loss: 0.56182\n",
      "Epoch 8976 - lr: 0.01000 - Train loss: 0.65397 - Test loss: 0.56200\n",
      "Epoch 8977 - lr: 0.01000 - Train loss: 0.65166 - Test loss: 0.56162\n",
      "Epoch 8978 - lr: 0.01000 - Train loss: 0.64534 - Test loss: 0.56164\n",
      "Epoch 8979 - lr: 0.01000 - Train loss: 0.65645 - Test loss: 0.56242\n",
      "Epoch 8980 - lr: 0.01000 - Train loss: 0.66198 - Test loss: 0.56212\n",
      "Epoch 8981 - lr: 0.01000 - Train loss: 0.65680 - Test loss: 0.56170\n",
      "Epoch 8982 - lr: 0.01000 - Train loss: 0.65126 - Test loss: 0.56158\n",
      "Epoch 8983 - lr: 0.01000 - Train loss: 0.65620 - Test loss: 0.56217\n",
      "Epoch 8984 - lr: 0.01000 - Train loss: 0.66221 - Test loss: 0.56190\n",
      "Epoch 8985 - lr: 0.01000 - Train loss: 0.65978 - Test loss: 0.56164\n",
      "Epoch 8986 - lr: 0.01000 - Train loss: 0.65270 - Test loss: 0.56122\n",
      "Epoch 8987 - lr: 0.01000 - Train loss: 0.64752 - Test loss: 0.56135\n",
      "Epoch 8988 - lr: 0.01000 - Train loss: 0.65101 - Test loss: 0.56134\n",
      "Epoch 8989 - lr: 0.01000 - Train loss: 0.65546 - Test loss: 0.56199\n",
      "Epoch 8990 - lr: 0.01000 - Train loss: 0.66092 - Test loss: 0.56161\n",
      "Epoch 8991 - lr: 0.01000 - Train loss: 0.65259 - Test loss: 0.56113\n",
      "Epoch 8992 - lr: 0.01000 - Train loss: 0.64444 - Test loss: 0.56118\n",
      "Epoch 8993 - lr: 0.01000 - Train loss: 0.65396 - Test loss: 0.56193\n",
      "Epoch 8994 - lr: 0.01000 - Train loss: 0.65963 - Test loss: 0.56164\n",
      "Epoch 8995 - lr: 0.01000 - Train loss: 0.65254 - Test loss: 0.56119\n",
      "Epoch 8996 - lr: 0.01000 - Train loss: 0.64686 - Test loss: 0.56128\n",
      "Epoch 8997 - lr: 0.01000 - Train loss: 0.65224 - Test loss: 0.56141\n",
      "Epoch 8998 - lr: 0.01000 - Train loss: 0.65259 - Test loss: 0.56145\n",
      "Epoch 8999 - lr: 0.01000 - Train loss: 0.65073 - Test loss: 0.56126\n",
      "Epoch 9000 - lr: 0.01000 - Train loss: 0.65501 - Test loss: 0.56186\n",
      "Epoch 9001 - lr: 0.01000 - Train loss: 0.66206 - Test loss: 0.56162\n",
      "Epoch 9002 - lr: 0.01000 - Train loss: 0.65990 - Test loss: 0.56137\n",
      "Epoch 9003 - lr: 0.01000 - Train loss: 0.65244 - Test loss: 0.56088\n",
      "Epoch 9004 - lr: 0.01000 - Train loss: 0.64576 - Test loss: 0.56096\n",
      "Epoch 9005 - lr: 0.01000 - Train loss: 0.65509 - Test loss: 0.56147\n",
      "Epoch 9006 - lr: 0.01000 - Train loss: 0.65227 - Test loss: 0.56089\n",
      "Epoch 9007 - lr: 0.01000 - Train loss: 0.64400 - Test loss: 0.56091\n",
      "Epoch 9008 - lr: 0.01000 - Train loss: 0.65214 - Test loss: 0.56156\n",
      "Epoch 9009 - lr: 0.01000 - Train loss: 0.65243 - Test loss: 0.56101\n",
      "Epoch 9010 - lr: 0.01000 - Train loss: 0.64436 - Test loss: 0.56107\n",
      "Epoch 9011 - lr: 0.01000 - Train loss: 0.65467 - Test loss: 0.56184\n",
      "Epoch 9012 - lr: 0.01000 - Train loss: 0.66285 - Test loss: 0.56170\n",
      "Epoch 9013 - lr: 0.01000 - Train loss: 0.65996 - Test loss: 0.56124\n",
      "Epoch 9014 - lr: 0.01000 - Train loss: 0.65239 - Test loss: 0.56085\n",
      "Epoch 9015 - lr: 0.01000 - Train loss: 0.64524 - Test loss: 0.56089\n",
      "Epoch 9016 - lr: 0.01000 - Train loss: 0.65593 - Test loss: 0.56153\n",
      "Epoch 9017 - lr: 0.01000 - Train loss: 0.66047 - Test loss: 0.56123\n",
      "Epoch 9018 - lr: 0.01000 - Train loss: 0.65219 - Test loss: 0.56061\n",
      "Epoch 9019 - lr: 0.01000 - Train loss: 0.64408 - Test loss: 0.56065\n",
      "Epoch 9020 - lr: 0.01000 - Train loss: 0.65356 - Test loss: 0.56137\n",
      "Epoch 9021 - lr: 0.01000 - Train loss: 0.65898 - Test loss: 0.56105\n",
      "Epoch 9022 - lr: 0.01000 - Train loss: 0.65192 - Test loss: 0.56062\n",
      "Epoch 9023 - lr: 0.01000 - Train loss: 0.64569 - Test loss: 0.56068\n",
      "Epoch 9024 - lr: 0.01000 - Train loss: 0.65451 - Test loss: 0.56109\n",
      "Epoch 9025 - lr: 0.01000 - Train loss: 0.65206 - Test loss: 0.56057\n",
      "Epoch 9026 - lr: 0.01000 - Train loss: 0.64458 - Test loss: 0.56060\n",
      "Epoch 9027 - lr: 0.01000 - Train loss: 0.65581 - Test loss: 0.56137\n",
      "Epoch 9028 - lr: 0.01000 - Train loss: 0.65808 - Test loss: 0.56075\n",
      "Epoch 9029 - lr: 0.01000 - Train loss: 0.65027 - Test loss: 0.56054\n",
      "Epoch 9030 - lr: 0.01000 - Train loss: 0.64325 - Test loss: 0.56055\n",
      "Epoch 9031 - lr: 0.01000 - Train loss: 0.64695 - Test loss: 0.56093\n",
      "Epoch 9032 - lr: 0.01000 - Train loss: 0.65128 - Test loss: 0.56094\n",
      "Epoch 9033 - lr: 0.01000 - Train loss: 0.65535 - Test loss: 0.56134\n",
      "Epoch 9034 - lr: 0.01000 - Train loss: 0.65538 - Test loss: 0.56075\n",
      "Epoch 9035 - lr: 0.01000 - Train loss: 0.65282 - Test loss: 0.56083\n",
      "Epoch 9036 - lr: 0.01000 - Train loss: 0.64993 - Test loss: 0.56045\n",
      "Epoch 9037 - lr: 0.01000 - Train loss: 0.64319 - Test loss: 0.56045\n",
      "Epoch 9038 - lr: 0.01000 - Train loss: 0.64776 - Test loss: 0.56087\n",
      "Epoch 9039 - lr: 0.01000 - Train loss: 0.64991 - Test loss: 0.56067\n",
      "Epoch 9040 - lr: 0.01000 - Train loss: 0.64528 - Test loss: 0.56079\n",
      "Epoch 9041 - lr: 0.01000 - Train loss: 0.65503 - Test loss: 0.56127\n",
      "Epoch 9042 - lr: 0.01000 - Train loss: 0.65237 - Test loss: 0.56059\n",
      "Epoch 9043 - lr: 0.01000 - Train loss: 0.64795 - Test loss: 0.56083\n",
      "Epoch 9044 - lr: 0.01000 - Train loss: 0.64987 - Test loss: 0.56055\n",
      "Epoch 9045 - lr: 0.01000 - Train loss: 0.64281 - Test loss: 0.56054\n",
      "Epoch 9046 - lr: 0.01000 - Train loss: 0.64471 - Test loss: 0.56080\n",
      "Epoch 9047 - lr: 0.01000 - Train loss: 0.65585 - Test loss: 0.56151\n",
      "Epoch 9048 - lr: 0.01000 - Train loss: 0.65963 - Test loss: 0.56099\n",
      "Epoch 9049 - lr: 0.01000 - Train loss: 0.65156 - Test loss: 0.56048\n",
      "Epoch 9050 - lr: 0.01000 - Train loss: 0.64257 - Test loss: 0.56042\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9051 - lr: 0.01000 - Train loss: 0.64319 - Test loss: 0.56061\n",
      "Epoch 9052 - lr: 0.01000 - Train loss: 0.64815 - Test loss: 0.56103\n",
      "Epoch 9053 - lr: 0.01000 - Train loss: 0.64987 - Test loss: 0.56070\n",
      "Epoch 9054 - lr: 0.01000 - Train loss: 0.64233 - Test loss: 0.56063\n",
      "Epoch 9055 - lr: 0.01000 - Train loss: 0.64276 - Test loss: 0.56076\n",
      "Epoch 9056 - lr: 0.01000 - Train loss: 0.64404 - Test loss: 0.56093\n",
      "Epoch 9057 - lr: 0.01000 - Train loss: 0.64252 - Test loss: 0.56101\n",
      "Epoch 9058 - lr: 0.01000 - Train loss: 0.64247 - Test loss: 0.56110\n",
      "Epoch 9059 - lr: 0.01000 - Train loss: 0.64229 - Test loss: 0.56118\n",
      "Epoch 9060 - lr: 0.01000 - Train loss: 0.64423 - Test loss: 0.56132\n",
      "Epoch 9061 - lr: 0.01000 - Train loss: 0.64246 - Test loss: 0.56134\n",
      "Epoch 9062 - lr: 0.01000 - Train loss: 0.64239 - Test loss: 0.56138\n",
      "Epoch 9063 - lr: 0.01000 - Train loss: 0.64215 - Test loss: 0.56141\n",
      "Epoch 9064 - lr: 0.01000 - Train loss: 0.64498 - Test loss: 0.56155\n",
      "Epoch 9065 - lr: 0.01000 - Train loss: 0.64507 - Test loss: 0.56166\n",
      "Epoch 9066 - lr: 0.01000 - Train loss: 0.65527 - Test loss: 0.56201\n",
      "Epoch 9067 - lr: 0.01000 - Train loss: 0.65613 - Test loss: 0.56134\n",
      "Epoch 9068 - lr: 0.01000 - Train loss: 0.64973 - Test loss: 0.56094\n",
      "Epoch 9069 - lr: 0.01000 - Train loss: 0.65181 - Test loss: 0.56130\n",
      "Epoch 9070 - lr: 0.01000 - Train loss: 0.65448 - Test loss: 0.56071\n",
      "Epoch 9071 - lr: 0.01000 - Train loss: 0.65321 - Test loss: 0.56084\n",
      "Epoch 9072 - lr: 0.01000 - Train loss: 0.65025 - Test loss: 0.56025\n",
      "Epoch 9073 - lr: 0.01000 - Train loss: 0.64186 - Test loss: 0.56017\n",
      "Epoch 9074 - lr: 0.01000 - Train loss: 0.64319 - Test loss: 0.56035\n",
      "Epoch 9075 - lr: 0.01000 - Train loss: 0.64195 - Test loss: 0.56046\n",
      "Epoch 9076 - lr: 0.01000 - Train loss: 0.64446 - Test loss: 0.56069\n",
      "Epoch 9077 - lr: 0.01000 - Train loss: 0.64442 - Test loss: 0.56087\n",
      "Epoch 9078 - lr: 0.01000 - Train loss: 0.65516 - Test loss: 0.56140\n",
      "Epoch 9079 - lr: 0.01000 - Train loss: 0.66155 - Test loss: 0.56114\n",
      "Epoch 9080 - lr: 0.01000 - Train loss: 0.65742 - Test loss: 0.56050\n",
      "Epoch 9081 - lr: 0.01000 - Train loss: 0.64937 - Test loss: 0.56013\n",
      "Epoch 9082 - lr: 0.01000 - Train loss: 0.64194 - Test loss: 0.56008\n",
      "Epoch 9083 - lr: 0.01000 - Train loss: 0.64399 - Test loss: 0.56028\n",
      "Epoch 9084 - lr: 0.01000 - Train loss: 0.64306 - Test loss: 0.56042\n",
      "Epoch 9085 - lr: 0.01000 - Train loss: 0.65162 - Test loss: 0.56098\n",
      "Epoch 9086 - lr: 0.01000 - Train loss: 0.65426 - Test loss: 0.56042\n",
      "Epoch 9087 - lr: 0.01000 - Train loss: 0.65298 - Test loss: 0.56054\n",
      "Epoch 9088 - lr: 0.01000 - Train loss: 0.64980 - Test loss: 0.55993\n",
      "Epoch 9089 - lr: 0.01000 - Train loss: 0.64188 - Test loss: 0.55987\n",
      "Epoch 9090 - lr: 0.01000 - Train loss: 0.64368 - Test loss: 0.56006\n",
      "Epoch 9091 - lr: 0.01000 - Train loss: 0.64230 - Test loss: 0.56019\n",
      "Epoch 9092 - lr: 0.01000 - Train loss: 0.64602 - Test loss: 0.56051\n",
      "Epoch 9093 - lr: 0.01000 - Train loss: 0.65051 - Test loss: 0.56043\n",
      "Epoch 9094 - lr: 0.01000 - Train loss: 0.65373 - Test loss: 0.56066\n",
      "Epoch 9095 - lr: 0.01000 - Train loss: 0.65111 - Test loss: 0.55995\n",
      "Epoch 9096 - lr: 0.01000 - Train loss: 0.64667 - Test loss: 0.56014\n",
      "Epoch 9097 - lr: 0.01000 - Train loss: 0.64878 - Test loss: 0.55987\n",
      "Epoch 9098 - lr: 0.01000 - Train loss: 0.64452 - Test loss: 0.55999\n",
      "Epoch 9099 - lr: 0.01000 - Train loss: 0.65334 - Test loss: 0.56032\n",
      "Epoch 9100 - lr: 0.01000 - Train loss: 0.64996 - Test loss: 0.55966\n",
      "Epoch 9101 - lr: 0.01000 - Train loss: 0.64116 - Test loss: 0.55962\n",
      "Epoch 9102 - lr: 0.01000 - Train loss: 0.64386 - Test loss: 0.55986\n",
      "Epoch 9103 - lr: 0.01000 - Train loss: 0.64319 - Test loss: 0.56003\n",
      "Epoch 9104 - lr: 0.01000 - Train loss: 0.65356 - Test loss: 0.56068\n",
      "Epoch 9105 - lr: 0.01000 - Train loss: 0.65712 - Test loss: 0.56010\n",
      "Epoch 9106 - lr: 0.01000 - Train loss: 0.64895 - Test loss: 0.55965\n",
      "Epoch 9107 - lr: 0.01000 - Train loss: 0.64217 - Test loss: 0.55964\n",
      "Epoch 9108 - lr: 0.01000 - Train loss: 0.64219 - Test loss: 0.55976\n",
      "Epoch 9109 - lr: 0.01000 - Train loss: 0.64283 - Test loss: 0.55991\n",
      "Epoch 9110 - lr: 0.01000 - Train loss: 0.64112 - Test loss: 0.55997\n",
      "Epoch 9111 - lr: 0.01000 - Train loss: 0.64698 - Test loss: 0.56030\n",
      "Epoch 9112 - lr: 0.01000 - Train loss: 0.65418 - Test loss: 0.56079\n",
      "Epoch 9113 - lr: 0.01000 - Train loss: 0.65413 - Test loss: 0.55993\n",
      "Epoch 9114 - lr: 0.01000 - Train loss: 0.65393 - Test loss: 0.56036\n",
      "Epoch 9115 - lr: 0.01000 - Train loss: 0.65672 - Test loss: 0.55991\n",
      "Epoch 9116 - lr: 0.01000 - Train loss: 0.64882 - Test loss: 0.55932\n",
      "Epoch 9117 - lr: 0.01000 - Train loss: 0.64199 - Test loss: 0.55930\n",
      "Epoch 9118 - lr: 0.01000 - Train loss: 0.64190 - Test loss: 0.55943\n",
      "Epoch 9119 - lr: 0.01000 - Train loss: 0.64302 - Test loss: 0.55961\n",
      "Epoch 9120 - lr: 0.01000 - Train loss: 0.64076 - Test loss: 0.55967\n",
      "Epoch 9121 - lr: 0.01000 - Train loss: 0.64358 - Test loss: 0.55987\n",
      "Epoch 9122 - lr: 0.01000 - Train loss: 0.64176 - Test loss: 0.55995\n",
      "Epoch 9123 - lr: 0.01000 - Train loss: 0.64544 - Test loss: 0.56021\n",
      "Epoch 9124 - lr: 0.01000 - Train loss: 0.65037 - Test loss: 0.56010\n",
      "Epoch 9125 - lr: 0.01000 - Train loss: 0.65190 - Test loss: 0.56010\n",
      "Epoch 9126 - lr: 0.01000 - Train loss: 0.64824 - Test loss: 0.55948\n",
      "Epoch 9127 - lr: 0.01000 - Train loss: 0.64144 - Test loss: 0.55943\n",
      "Epoch 9128 - lr: 0.01000 - Train loss: 0.64360 - Test loss: 0.55961\n",
      "Epoch 9129 - lr: 0.01000 - Train loss: 0.64269 - Test loss: 0.55974\n",
      "Epoch 9130 - lr: 0.01000 - Train loss: 0.65276 - Test loss: 0.56033\n",
      "Epoch 9131 - lr: 0.01000 - Train loss: 0.65588 - Test loss: 0.55971\n",
      "Epoch 9132 - lr: 0.01000 - Train loss: 0.64805 - Test loss: 0.55930\n",
      "Epoch 9133 - lr: 0.01000 - Train loss: 0.64020 - Test loss: 0.55924\n",
      "Epoch 9134 - lr: 0.01000 - Train loss: 0.64583 - Test loss: 0.55956\n",
      "Epoch 9135 - lr: 0.01000 - Train loss: 0.65232 - Test loss: 0.56007\n",
      "Epoch 9136 - lr: 0.01000 - Train loss: 0.65722 - Test loss: 0.55958\n",
      "Epoch 9137 - lr: 0.01000 - Train loss: 0.64931 - Test loss: 0.55902\n",
      "Epoch 9138 - lr: 0.01000 - Train loss: 0.64089 - Test loss: 0.55902\n",
      "Epoch 9139 - lr: 0.01000 - Train loss: 0.64145 - Test loss: 0.55917\n",
      "Epoch 9140 - lr: 0.01000 - Train loss: 0.64527 - Test loss: 0.55948\n",
      "Epoch 9141 - lr: 0.01000 - Train loss: 0.64953 - Test loss: 0.55937\n",
      "Epoch 9142 - lr: 0.01000 - Train loss: 0.65277 - Test loss: 0.55961\n",
      "Epoch 9143 - lr: 0.01000 - Train loss: 0.65417 - Test loss: 0.55911\n",
      "Epoch 9144 - lr: 0.01000 - Train loss: 0.64789 - Test loss: 0.55875\n",
      "Epoch 9145 - lr: 0.01000 - Train loss: 0.64650 - Test loss: 0.55899\n",
      "Epoch 9146 - lr: 0.01000 - Train loss: 0.64766 - Test loss: 0.55861\n",
      "Epoch 9147 - lr: 0.01000 - Train loss: 0.63989 - Test loss: 0.55860\n",
      "Epoch 9148 - lr: 0.01000 - Train loss: 0.64451 - Test loss: 0.55891\n",
      "Epoch 9149 - lr: 0.01000 - Train loss: 0.64824 - Test loss: 0.55930\n",
      "Epoch 9150 - lr: 0.01000 - Train loss: 0.64886 - Test loss: 0.55876\n",
      "Epoch 9151 - lr: 0.01000 - Train loss: 0.64090 - Test loss: 0.55877\n",
      "Epoch 9152 - lr: 0.01000 - Train loss: 0.64358 - Test loss: 0.55902\n",
      "Epoch 9153 - lr: 0.01000 - Train loss: 0.65245 - Test loss: 0.55936\n",
      "Epoch 9154 - lr: 0.01000 - Train loss: 0.65268 - Test loss: 0.55885\n",
      "Epoch 9155 - lr: 0.01000 - Train loss: 0.64996 - Test loss: 0.55877\n",
      "Epoch 9156 - lr: 0.01000 - Train loss: 0.64842 - Test loss: 0.55853\n",
      "Epoch 9157 - lr: 0.01000 - Train loss: 0.65235 - Test loss: 0.55896\n",
      "Epoch 9158 - lr: 0.01000 - Train loss: 0.65553 - Test loss: 0.55847\n",
      "Epoch 9159 - lr: 0.01000 - Train loss: 0.64775 - Test loss: 0.55806\n",
      "Epoch 9160 - lr: 0.01000 - Train loss: 0.63959 - Test loss: 0.55807\n",
      "Epoch 9161 - lr: 0.01000 - Train loss: 0.64700 - Test loss: 0.55851\n",
      "Epoch 9162 - lr: 0.01000 - Train loss: 0.65224 - Test loss: 0.55885\n",
      "Epoch 9163 - lr: 0.01000 - Train loss: 0.65577 - Test loss: 0.55852\n",
      "Epoch 9164 - lr: 0.01000 - Train loss: 0.64897 - Test loss: 0.55798\n",
      "Epoch 9165 - lr: 0.01000 - Train loss: 0.64546 - Test loss: 0.55823\n",
      "Epoch 9166 - lr: 0.01000 - Train loss: 0.64753 - Test loss: 0.55799\n",
      "Epoch 9167 - lr: 0.01000 - Train loss: 0.64568 - Test loss: 0.55826\n",
      "Epoch 9168 - lr: 0.01000 - Train loss: 0.64739 - Test loss: 0.55797\n",
      "Epoch 9169 - lr: 0.01000 - Train loss: 0.64295 - Test loss: 0.55813\n",
      "Epoch 9170 - lr: 0.01000 - Train loss: 0.65204 - Test loss: 0.55853\n",
      "Epoch 9171 - lr: 0.01000 - Train loss: 0.65533 - Test loss: 0.55823\n",
      "Epoch 9172 - lr: 0.01000 - Train loss: 0.64860 - Test loss: 0.55771\n",
      "Epoch 9173 - lr: 0.01000 - Train loss: 0.64459 - Test loss: 0.55793\n",
      "Epoch 9174 - lr: 0.01000 - Train loss: 0.64837 - Test loss: 0.55785\n",
      "Epoch 9175 - lr: 0.01000 - Train loss: 0.65195 - Test loss: 0.55826\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9176 - lr: 0.01000 - Train loss: 0.65734 - Test loss: 0.55804\n",
      "Epoch 9177 - lr: 0.01000 - Train loss: 0.65726 - Test loss: 0.55786\n",
      "Epoch 9178 - lr: 0.01000 - Train loss: 0.65777 - Test loss: 0.55777\n",
      "Epoch 9179 - lr: 0.01000 - Train loss: 0.65463 - Test loss: 0.55734\n",
      "Epoch 9180 - lr: 0.01000 - Train loss: 0.64709 - Test loss: 0.55705\n",
      "Epoch 9181 - lr: 0.01000 - Train loss: 0.63929 - Test loss: 0.55709\n",
      "Epoch 9182 - lr: 0.01000 - Train loss: 0.64546 - Test loss: 0.55750\n",
      "Epoch 9183 - lr: 0.01000 - Train loss: 0.65183 - Test loss: 0.55805\n",
      "Epoch 9184 - lr: 0.01000 - Train loss: 0.65120 - Test loss: 0.55732\n",
      "Epoch 9185 - lr: 0.01000 - Train loss: 0.65003 - Test loss: 0.55790\n",
      "Epoch 9186 - lr: 0.01000 - Train loss: 0.65764 - Test loss: 0.55777\n",
      "Epoch 9187 - lr: 0.01000 - Train loss: 0.65679 - Test loss: 0.55748\n",
      "Epoch 9188 - lr: 0.01000 - Train loss: 0.65431 - Test loss: 0.55720\n",
      "Epoch 9189 - lr: 0.01000 - Train loss: 0.64698 - Test loss: 0.55677\n",
      "Epoch 9190 - lr: 0.01000 - Train loss: 0.63946 - Test loss: 0.55680\n",
      "Epoch 9191 - lr: 0.01000 - Train loss: 0.64695 - Test loss: 0.55726\n",
      "Epoch 9192 - lr: 0.01000 - Train loss: 0.65154 - Test loss: 0.55753\n",
      "Epoch 9193 - lr: 0.01000 - Train loss: 0.65078 - Test loss: 0.55705\n",
      "Epoch 9194 - lr: 0.01000 - Train loss: 0.65179 - Test loss: 0.55739\n",
      "Epoch 9195 - lr: 0.01000 - Train loss: 0.65645 - Test loss: 0.55717\n",
      "Epoch 9196 - lr: 0.01000 - Train loss: 0.65498 - Test loss: 0.55690\n",
      "Epoch 9197 - lr: 0.01000 - Train loss: 0.64772 - Test loss: 0.55643\n",
      "Epoch 9198 - lr: 0.01000 - Train loss: 0.63968 - Test loss: 0.55650\n",
      "Epoch 9199 - lr: 0.01000 - Train loss: 0.63914 - Test loss: 0.55667\n",
      "Epoch 9200 - lr: 0.01000 - Train loss: 0.64629 - Test loss: 0.55713\n",
      "Epoch 9201 - lr: 0.01000 - Train loss: 0.65203 - Test loss: 0.55754\n",
      "Epoch 9202 - lr: 0.01000 - Train loss: 0.65757 - Test loss: 0.55735\n",
      "Epoch 9203 - lr: 0.01000 - Train loss: 0.65338 - Test loss: 0.55682\n",
      "Epoch 9204 - lr: 0.01000 - Train loss: 0.64769 - Test loss: 0.55671\n",
      "Epoch 9205 - lr: 0.01000 - Train loss: 0.65103 - Test loss: 0.55720\n",
      "Epoch 9206 - lr: 0.01000 - Train loss: 0.65295 - Test loss: 0.55664\n",
      "Epoch 9207 - lr: 0.01000 - Train loss: 0.64872 - Test loss: 0.55667\n",
      "Epoch 9208 - lr: 0.01000 - Train loss: 0.65032 - Test loss: 0.55679\n",
      "Epoch 9209 - lr: 0.01000 - Train loss: 0.64679 - Test loss: 0.55634\n",
      "Epoch 9210 - lr: 0.01000 - Train loss: 0.63906 - Test loss: 0.55637\n",
      "Epoch 9211 - lr: 0.01000 - Train loss: 0.64734 - Test loss: 0.55684\n",
      "Epoch 9212 - lr: 0.01000 - Train loss: 0.65030 - Test loss: 0.55697\n",
      "Epoch 9213 - lr: 0.01000 - Train loss: 0.64680 - Test loss: 0.55650\n",
      "Epoch 9214 - lr: 0.01000 - Train loss: 0.63866 - Test loss: 0.55651\n",
      "Epoch 9215 - lr: 0.01000 - Train loss: 0.64689 - Test loss: 0.55696\n",
      "Epoch 9216 - lr: 0.01000 - Train loss: 0.65086 - Test loss: 0.55716\n",
      "Epoch 9217 - lr: 0.01000 - Train loss: 0.64896 - Test loss: 0.55666\n",
      "Epoch 9218 - lr: 0.01000 - Train loss: 0.64961 - Test loss: 0.55708\n",
      "Epoch 9219 - lr: 0.01000 - Train loss: 0.65702 - Test loss: 0.55694\n",
      "Epoch 9220 - lr: 0.01000 - Train loss: 0.65331 - Test loss: 0.55646\n",
      "Epoch 9221 - lr: 0.01000 - Train loss: 0.64687 - Test loss: 0.55627\n",
      "Epoch 9222 - lr: 0.01000 - Train loss: 0.64557 - Test loss: 0.55657\n",
      "Epoch 9223 - lr: 0.01000 - Train loss: 0.64660 - Test loss: 0.55624\n",
      "Epoch 9224 - lr: 0.01000 - Train loss: 0.63873 - Test loss: 0.55628\n",
      "Epoch 9225 - lr: 0.01000 - Train loss: 0.64203 - Test loss: 0.55655\n",
      "Epoch 9226 - lr: 0.01000 - Train loss: 0.63951 - Test loss: 0.55667\n",
      "Epoch 9227 - lr: 0.01000 - Train loss: 0.63900 - Test loss: 0.55681\n",
      "Epoch 9228 - lr: 0.01000 - Train loss: 0.63962 - Test loss: 0.55695\n",
      "Epoch 9229 - lr: 0.01000 - Train loss: 0.64720 - Test loss: 0.55736\n",
      "Epoch 9230 - lr: 0.01000 - Train loss: 0.65051 - Test loss: 0.55742\n",
      "Epoch 9231 - lr: 0.01000 - Train loss: 0.64818 - Test loss: 0.55686\n",
      "Epoch 9232 - lr: 0.01000 - Train loss: 0.64740 - Test loss: 0.55715\n",
      "Epoch 9233 - lr: 0.01000 - Train loss: 0.65122 - Test loss: 0.55675\n",
      "Epoch 9234 - lr: 0.01000 - Train loss: 0.64838 - Test loss: 0.55664\n",
      "Epoch 9235 - lr: 0.01000 - Train loss: 0.64853 - Test loss: 0.55656\n",
      "Epoch 9236 - lr: 0.01000 - Train loss: 0.64766 - Test loss: 0.55640\n",
      "Epoch 9237 - lr: 0.01000 - Train loss: 0.65030 - Test loss: 0.55661\n",
      "Epoch 9238 - lr: 0.01000 - Train loss: 0.65144 - Test loss: 0.55625\n",
      "Epoch 9239 - lr: 0.01000 - Train loss: 0.64693 - Test loss: 0.55605\n",
      "Epoch 9240 - lr: 0.01000 - Train loss: 0.65023 - Test loss: 0.55651\n",
      "Epoch 9241 - lr: 0.01000 - Train loss: 0.65009 - Test loss: 0.55589\n",
      "Epoch 9242 - lr: 0.01000 - Train loss: 0.65022 - Test loss: 0.55648\n",
      "Epoch 9243 - lr: 0.01000 - Train loss: 0.65008 - Test loss: 0.55588\n",
      "Epoch 9244 - lr: 0.01000 - Train loss: 0.64998 - Test loss: 0.55647\n",
      "Epoch 9245 - lr: 0.01000 - Train loss: 0.65154 - Test loss: 0.55595\n",
      "Epoch 9246 - lr: 0.01000 - Train loss: 0.64938 - Test loss: 0.55612\n",
      "Epoch 9247 - lr: 0.01000 - Train loss: 0.64627 - Test loss: 0.55578\n",
      "Epoch 9248 - lr: 0.01000 - Train loss: 0.64118 - Test loss: 0.55592\n",
      "Epoch 9249 - lr: 0.01000 - Train loss: 0.65087 - Test loss: 0.55646\n",
      "Epoch 9250 - lr: 0.01000 - Train loss: 0.64874 - Test loss: 0.55573\n",
      "Epoch 9251 - lr: 0.01000 - Train loss: 0.63799 - Test loss: 0.55587\n",
      "Epoch 9252 - lr: 0.01000 - Train loss: 0.64957 - Test loss: 0.55646\n",
      "Epoch 9253 - lr: 0.01000 - Train loss: 0.64655 - Test loss: 0.55611\n",
      "Epoch 9254 - lr: 0.01000 - Train loss: 0.64588 - Test loss: 0.55638\n",
      "Epoch 9255 - lr: 0.01000 - Train loss: 0.64752 - Test loss: 0.55598\n",
      "Epoch 9256 - lr: 0.01000 - Train loss: 0.64631 - Test loss: 0.55629\n",
      "Epoch 9257 - lr: 0.01000 - Train loss: 0.64866 - Test loss: 0.55591\n",
      "Epoch 9258 - lr: 0.01000 - Train loss: 0.65038 - Test loss: 0.55634\n",
      "Epoch 9259 - lr: 0.01000 - Train loss: 0.64832 - Test loss: 0.55558\n",
      "Epoch 9260 - lr: 0.01000 - Train loss: 0.63894 - Test loss: 0.55575\n",
      "Epoch 9261 - lr: 0.01000 - Train loss: 0.64655 - Test loss: 0.55619\n",
      "Epoch 9262 - lr: 0.01000 - Train loss: 0.64973 - Test loss: 0.55632\n",
      "Epoch 9263 - lr: 0.01000 - Train loss: 0.64797 - Test loss: 0.55587\n",
      "Epoch 9264 - lr: 0.01000 - Train loss: 0.64932 - Test loss: 0.55628\n",
      "Epoch 9265 - lr: 0.01000 - Train loss: 0.65047 - Test loss: 0.55570\n",
      "Epoch 9266 - lr: 0.01000 - Train loss: 0.65038 - Test loss: 0.55606\n",
      "Epoch 9267 - lr: 0.01000 - Train loss: 0.65474 - Test loss: 0.55589\n",
      "Epoch 9268 - lr: 0.01000 - Train loss: 0.65557 - Test loss: 0.55577\n",
      "Epoch 9269 - lr: 0.01000 - Train loss: 0.65084 - Test loss: 0.55523\n",
      "Epoch 9270 - lr: 0.01000 - Train loss: 0.64960 - Test loss: 0.55548\n",
      "Epoch 9271 - lr: 0.01000 - Train loss: 0.64705 - Test loss: 0.55509\n",
      "Epoch 9272 - lr: 0.01000 - Train loss: 0.64478 - Test loss: 0.55538\n",
      "Epoch 9273 - lr: 0.01000 - Train loss: 0.64612 - Test loss: 0.55510\n",
      "Epoch 9274 - lr: 0.01000 - Train loss: 0.64104 - Test loss: 0.55527\n",
      "Epoch 9275 - lr: 0.01000 - Train loss: 0.65055 - Test loss: 0.55578\n",
      "Epoch 9276 - lr: 0.01000 - Train loss: 0.64982 - Test loss: 0.55519\n",
      "Epoch 9277 - lr: 0.01000 - Train loss: 0.65041 - Test loss: 0.55575\n",
      "Epoch 9278 - lr: 0.01000 - Train loss: 0.64822 - Test loss: 0.55500\n",
      "Epoch 9279 - lr: 0.01000 - Train loss: 0.64682 - Test loss: 0.55547\n",
      "Epoch 9280 - lr: 0.01000 - Train loss: 0.64875 - Test loss: 0.55552\n",
      "Epoch 9281 - lr: 0.01000 - Train loss: 0.64598 - Test loss: 0.55519\n",
      "Epoch 9282 - lr: 0.01000 - Train loss: 0.64360 - Test loss: 0.55540\n",
      "Epoch 9283 - lr: 0.01000 - Train loss: 0.64644 - Test loss: 0.55524\n",
      "Epoch 9284 - lr: 0.01000 - Train loss: 0.64888 - Test loss: 0.55567\n",
      "Epoch 9285 - lr: 0.01000 - Train loss: 0.65176 - Test loss: 0.55522\n",
      "Epoch 9286 - lr: 0.01000 - Train loss: 0.64645 - Test loss: 0.55508\n",
      "Epoch 9287 - lr: 0.01000 - Train loss: 0.64914 - Test loss: 0.55553\n",
      "Epoch 9288 - lr: 0.01000 - Train loss: 0.64981 - Test loss: 0.55497\n",
      "Epoch 9289 - lr: 0.01000 - Train loss: 0.65044 - Test loss: 0.55547\n",
      "Epoch 9290 - lr: 0.01000 - Train loss: 0.64880 - Test loss: 0.55484\n",
      "Epoch 9291 - lr: 0.01000 - Train loss: 0.64474 - Test loss: 0.55522\n",
      "Epoch 9292 - lr: 0.01000 - Train loss: 0.64605 - Test loss: 0.55494\n",
      "Epoch 9293 - lr: 0.01000 - Train loss: 0.64089 - Test loss: 0.55508\n",
      "Epoch 9294 - lr: 0.01000 - Train loss: 0.65048 - Test loss: 0.55556\n",
      "Epoch 9295 - lr: 0.01000 - Train loss: 0.64886 - Test loss: 0.55492\n",
      "Epoch 9296 - lr: 0.01000 - Train loss: 0.64599 - Test loss: 0.55532\n",
      "Epoch 9297 - lr: 0.01000 - Train loss: 0.64808 - Test loss: 0.55498\n",
      "Epoch 9298 - lr: 0.01000 - Train loss: 0.64968 - Test loss: 0.55542\n",
      "Epoch 9299 - lr: 0.01000 - Train loss: 0.64794 - Test loss: 0.55467\n",
      "Epoch 9300 - lr: 0.01000 - Train loss: 0.64720 - Test loss: 0.55513\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9301 - lr: 0.01000 - Train loss: 0.64757 - Test loss: 0.55506\n",
      "Epoch 9302 - lr: 0.01000 - Train loss: 0.64795 - Test loss: 0.55502\n",
      "Epoch 9303 - lr: 0.01000 - Train loss: 0.64631 - Test loss: 0.55482\n",
      "Epoch 9304 - lr: 0.01000 - Train loss: 0.64971 - Test loss: 0.55523\n",
      "Epoch 9305 - lr: 0.01000 - Train loss: 0.64759 - Test loss: 0.55451\n",
      "Epoch 9306 - lr: 0.01000 - Train loss: 0.64389 - Test loss: 0.55487\n",
      "Epoch 9307 - lr: 0.01000 - Train loss: 0.64964 - Test loss: 0.55536\n",
      "Epoch 9308 - lr: 0.01000 - Train loss: 0.64796 - Test loss: 0.55462\n",
      "Epoch 9309 - lr: 0.01000 - Train loss: 0.64800 - Test loss: 0.55512\n",
      "Epoch 9310 - lr: 0.01000 - Train loss: 0.64635 - Test loss: 0.55491\n",
      "Epoch 9311 - lr: 0.01000 - Train loss: 0.64973 - Test loss: 0.55532\n",
      "Epoch 9312 - lr: 0.01000 - Train loss: 0.64778 - Test loss: 0.55457\n",
      "Epoch 9313 - lr: 0.01000 - Train loss: 0.64748 - Test loss: 0.55504\n",
      "Epoch 9314 - lr: 0.01000 - Train loss: 0.64679 - Test loss: 0.55490\n",
      "Epoch 9315 - lr: 0.01000 - Train loss: 0.64962 - Test loss: 0.55513\n",
      "Epoch 9316 - lr: 0.01000 - Train loss: 0.65373 - Test loss: 0.55494\n",
      "Epoch 9317 - lr: 0.01000 - Train loss: 0.65361 - Test loss: 0.55475\n",
      "Epoch 9318 - lr: 0.01000 - Train loss: 0.65268 - Test loss: 0.55453\n",
      "Epoch 9319 - lr: 0.01000 - Train loss: 0.64746 - Test loss: 0.55418\n",
      "Epoch 9320 - lr: 0.01000 - Train loss: 0.64896 - Test loss: 0.55461\n",
      "Epoch 9321 - lr: 0.01000 - Train loss: 0.64907 - Test loss: 0.55409\n",
      "Epoch 9322 - lr: 0.01000 - Train loss: 0.64932 - Test loss: 0.55464\n",
      "Epoch 9323 - lr: 0.01000 - Train loss: 0.64792 - Test loss: 0.55406\n",
      "Epoch 9324 - lr: 0.01000 - Train loss: 0.63750 - Test loss: 0.55424\n",
      "Epoch 9325 - lr: 0.01000 - Train loss: 0.64174 - Test loss: 0.55456\n",
      "Epoch 9326 - lr: 0.01000 - Train loss: 0.64212 - Test loss: 0.55481\n",
      "Epoch 9327 - lr: 0.01000 - Train loss: 0.64822 - Test loss: 0.55489\n",
      "Epoch 9328 - lr: 0.01000 - Train loss: 0.64578 - Test loss: 0.55462\n",
      "Epoch 9329 - lr: 0.01000 - Train loss: 0.64686 - Test loss: 0.55495\n",
      "Epoch 9330 - lr: 0.01000 - Train loss: 0.65391 - Test loss: 0.55480\n",
      "Epoch 9331 - lr: 0.01000 - Train loss: 0.65455 - Test loss: 0.55466\n",
      "Epoch 9332 - lr: 0.01000 - Train loss: 0.65307 - Test loss: 0.55434\n",
      "Epoch 9333 - lr: 0.01000 - Train loss: 0.64766 - Test loss: 0.55402\n",
      "Epoch 9334 - lr: 0.01000 - Train loss: 0.64928 - Test loss: 0.55446\n",
      "Epoch 9335 - lr: 0.01000 - Train loss: 0.64746 - Test loss: 0.55382\n",
      "Epoch 9336 - lr: 0.01000 - Train loss: 0.64082 - Test loss: 0.55408\n",
      "Epoch 9337 - lr: 0.01000 - Train loss: 0.63834 - Test loss: 0.55425\n",
      "Epoch 9338 - lr: 0.01000 - Train loss: 0.63882 - Test loss: 0.55445\n",
      "Epoch 9339 - lr: 0.01000 - Train loss: 0.64260 - Test loss: 0.55476\n",
      "Epoch 9340 - lr: 0.01000 - Train loss: 0.64705 - Test loss: 0.55471\n",
      "Epoch 9341 - lr: 0.01000 - Train loss: 0.64901 - Test loss: 0.55482\n",
      "Epoch 9342 - lr: 0.01000 - Train loss: 0.64789 - Test loss: 0.55441\n",
      "Epoch 9343 - lr: 0.01000 - Train loss: 0.64966 - Test loss: 0.55474\n",
      "Epoch 9344 - lr: 0.01000 - Train loss: 0.64849 - Test loss: 0.55414\n",
      "Epoch 9345 - lr: 0.01000 - Train loss: 0.64783 - Test loss: 0.55460\n",
      "Epoch 9346 - lr: 0.01000 - Train loss: 0.65420 - Test loss: 0.55443\n",
      "Epoch 9347 - lr: 0.01000 - Train loss: 0.65506 - Test loss: 0.55436\n",
      "Epoch 9348 - lr: 0.01000 - Train loss: 0.64735 - Test loss: 0.55371\n",
      "Epoch 9349 - lr: 0.01000 - Train loss: 0.64308 - Test loss: 0.55402\n",
      "Epoch 9350 - lr: 0.01000 - Train loss: 0.64867 - Test loss: 0.55449\n",
      "Epoch 9351 - lr: 0.01000 - Train loss: 0.64887 - Test loss: 0.55395\n",
      "Epoch 9352 - lr: 0.01000 - Train loss: 0.64908 - Test loss: 0.55447\n",
      "Epoch 9353 - lr: 0.01000 - Train loss: 0.64742 - Test loss: 0.55383\n",
      "Epoch 9354 - lr: 0.01000 - Train loss: 0.63998 - Test loss: 0.55404\n",
      "Epoch 9355 - lr: 0.01000 - Train loss: 0.63686 - Test loss: 0.55415\n",
      "Epoch 9356 - lr: 0.01000 - Train loss: 0.64989 - Test loss: 0.55470\n",
      "Epoch 9357 - lr: 0.01000 - Train loss: 0.64556 - Test loss: 0.55429\n",
      "Epoch 9358 - lr: 0.01000 - Train loss: 0.63977 - Test loss: 0.55436\n",
      "Epoch 9359 - lr: 0.01000 - Train loss: 0.64949 - Test loss: 0.55483\n",
      "Epoch 9360 - lr: 0.01000 - Train loss: 0.65075 - Test loss: 0.55413\n",
      "Epoch 9361 - lr: 0.01000 - Train loss: 0.63875 - Test loss: 0.55425\n",
      "Epoch 9362 - lr: 0.01000 - Train loss: 0.64145 - Test loss: 0.55446\n",
      "Epoch 9363 - lr: 0.01000 - Train loss: 0.64200 - Test loss: 0.55465\n",
      "Epoch 9364 - lr: 0.01000 - Train loss: 0.64740 - Test loss: 0.55463\n",
      "Epoch 9365 - lr: 0.01000 - Train loss: 0.64667 - Test loss: 0.55448\n",
      "Epoch 9366 - lr: 0.01000 - Train loss: 0.64831 - Test loss: 0.55452\n",
      "Epoch 9367 - lr: 0.01000 - Train loss: 0.64599 - Test loss: 0.55412\n",
      "Epoch 9368 - lr: 0.01000 - Train loss: 0.64518 - Test loss: 0.55437\n",
      "Epoch 9369 - lr: 0.01000 - Train loss: 0.64842 - Test loss: 0.55408\n",
      "Epoch 9370 - lr: 0.01000 - Train loss: 0.64837 - Test loss: 0.55420\n",
      "Epoch 9371 - lr: 0.01000 - Train loss: 0.64731 - Test loss: 0.55386\n",
      "Epoch 9372 - lr: 0.01000 - Train loss: 0.64905 - Test loss: 0.55420\n",
      "Epoch 9373 - lr: 0.01000 - Train loss: 0.64913 - Test loss: 0.55372\n",
      "Epoch 9374 - lr: 0.01000 - Train loss: 0.64894 - Test loss: 0.55403\n",
      "Epoch 9375 - lr: 0.01000 - Train loss: 0.65232 - Test loss: 0.55387\n",
      "Epoch 9376 - lr: 0.01000 - Train loss: 0.64921 - Test loss: 0.55360\n",
      "Epoch 9377 - lr: 0.01000 - Train loss: 0.64616 - Test loss: 0.55354\n",
      "Epoch 9378 - lr: 0.01000 - Train loss: 0.64789 - Test loss: 0.55367\n",
      "Epoch 9379 - lr: 0.01000 - Train loss: 0.64563 - Test loss: 0.55338\n",
      "Epoch 9380 - lr: 0.01000 - Train loss: 0.64470 - Test loss: 0.55368\n",
      "Epoch 9381 - lr: 0.01000 - Train loss: 0.64710 - Test loss: 0.55345\n",
      "Epoch 9382 - lr: 0.01000 - Train loss: 0.64885 - Test loss: 0.55385\n",
      "Epoch 9383 - lr: 0.01000 - Train loss: 0.64754 - Test loss: 0.55335\n",
      "Epoch 9384 - lr: 0.01000 - Train loss: 0.64423 - Test loss: 0.55372\n",
      "Epoch 9385 - lr: 0.01000 - Train loss: 0.64582 - Test loss: 0.55350\n",
      "Epoch 9386 - lr: 0.01000 - Train loss: 0.64519 - Test loss: 0.55383\n",
      "Epoch 9387 - lr: 0.01000 - Train loss: 0.64919 - Test loss: 0.55362\n",
      "Epoch 9388 - lr: 0.01000 - Train loss: 0.64596 - Test loss: 0.55355\n",
      "Epoch 9389 - lr: 0.01000 - Train loss: 0.64818 - Test loss: 0.55372\n",
      "Epoch 9390 - lr: 0.01000 - Train loss: 0.64845 - Test loss: 0.55345\n",
      "Epoch 9391 - lr: 0.01000 - Train loss: 0.64685 - Test loss: 0.55347\n",
      "Epoch 9392 - lr: 0.01000 - Train loss: 0.64508 - Test loss: 0.55332\n",
      "Epoch 9393 - lr: 0.01000 - Train loss: 0.64791 - Test loss: 0.55372\n",
      "Epoch 9394 - lr: 0.01000 - Train loss: 0.64652 - Test loss: 0.55319\n",
      "Epoch 9395 - lr: 0.01000 - Train loss: 0.63606 - Test loss: 0.55336\n",
      "Epoch 9396 - lr: 0.01000 - Train loss: 0.64863 - Test loss: 0.55393\n",
      "Epoch 9397 - lr: 0.01000 - Train loss: 0.64529 - Test loss: 0.55367\n",
      "Epoch 9398 - lr: 0.01000 - Train loss: 0.64560 - Test loss: 0.55399\n",
      "Epoch 9399 - lr: 0.01000 - Train loss: 0.65242 - Test loss: 0.55388\n",
      "Epoch 9400 - lr: 0.01000 - Train loss: 0.65309 - Test loss: 0.55377\n",
      "Epoch 9401 - lr: 0.01000 - Train loss: 0.65206 - Test loss: 0.55351\n",
      "Epoch 9402 - lr: 0.01000 - Train loss: 0.64891 - Test loss: 0.55329\n",
      "Epoch 9403 - lr: 0.01000 - Train loss: 0.64585 - Test loss: 0.55323\n",
      "Epoch 9404 - lr: 0.01000 - Train loss: 0.64770 - Test loss: 0.55336\n",
      "Epoch 9405 - lr: 0.01000 - Train loss: 0.64621 - Test loss: 0.55309\n",
      "Epoch 9406 - lr: 0.01000 - Train loss: 0.64795 - Test loss: 0.55349\n",
      "Epoch 9407 - lr: 0.01000 - Train loss: 0.64613 - Test loss: 0.55295\n",
      "Epoch 9408 - lr: 0.01000 - Train loss: 0.64100 - Test loss: 0.55324\n",
      "Epoch 9409 - lr: 0.01000 - Train loss: 0.64231 - Test loss: 0.55353\n",
      "Epoch 9410 - lr: 0.01000 - Train loss: 0.64528 - Test loss: 0.55345\n",
      "Epoch 9411 - lr: 0.01000 - Train loss: 0.64814 - Test loss: 0.55386\n",
      "Epoch 9412 - lr: 0.01000 - Train loss: 0.64744 - Test loss: 0.55326\n",
      "Epoch 9413 - lr: 0.01000 - Train loss: 0.64662 - Test loss: 0.55367\n",
      "Epoch 9414 - lr: 0.01000 - Train loss: 0.64553 - Test loss: 0.55354\n",
      "Epoch 9415 - lr: 0.01000 - Train loss: 0.64853 - Test loss: 0.55381\n",
      "Epoch 9416 - lr: 0.01000 - Train loss: 0.65239 - Test loss: 0.55359\n",
      "Epoch 9417 - lr: 0.01000 - Train loss: 0.65264 - Test loss: 0.55349\n",
      "Epoch 9418 - lr: 0.01000 - Train loss: 0.65359 - Test loss: 0.55341\n",
      "Epoch 9419 - lr: 0.01000 - Train loss: 0.64618 - Test loss: 0.55286\n",
      "Epoch 9420 - lr: 0.01000 - Train loss: 0.64045 - Test loss: 0.55310\n",
      "Epoch 9421 - lr: 0.01000 - Train loss: 0.63977 - Test loss: 0.55332\n",
      "Epoch 9422 - lr: 0.01000 - Train loss: 0.64898 - Test loss: 0.55375\n",
      "Epoch 9423 - lr: 0.01000 - Train loss: 0.64855 - Test loss: 0.55331\n",
      "Epoch 9424 - lr: 0.01000 - Train loss: 0.64880 - Test loss: 0.55362\n",
      "Epoch 9425 - lr: 0.01000 - Train loss: 0.65387 - Test loss: 0.55354\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9426 - lr: 0.01000 - Train loss: 0.64676 - Test loss: 0.55295\n",
      "Epoch 9427 - lr: 0.01000 - Train loss: 0.64701 - Test loss: 0.55335\n",
      "Epoch 9428 - lr: 0.01000 - Train loss: 0.64487 - Test loss: 0.55318\n",
      "Epoch 9429 - lr: 0.01000 - Train loss: 0.64737 - Test loss: 0.55355\n",
      "Epoch 9430 - lr: 0.01000 - Train loss: 0.64845 - Test loss: 0.55310\n",
      "Epoch 9431 - lr: 0.01000 - Train loss: 0.64892 - Test loss: 0.55345\n",
      "Epoch 9432 - lr: 0.01000 - Train loss: 0.65276 - Test loss: 0.55326\n",
      "Epoch 9433 - lr: 0.01000 - Train loss: 0.65258 - Test loss: 0.55314\n",
      "Epoch 9434 - lr: 0.01000 - Train loss: 0.65192 - Test loss: 0.55297\n",
      "Epoch 9435 - lr: 0.01000 - Train loss: 0.64791 - Test loss: 0.55269\n",
      "Epoch 9436 - lr: 0.01000 - Train loss: 0.64774 - Test loss: 0.55282\n",
      "Epoch 9437 - lr: 0.01000 - Train loss: 0.64551 - Test loss: 0.55254\n",
      "Epoch 9438 - lr: 0.01000 - Train loss: 0.64468 - Test loss: 0.55282\n",
      "Epoch 9439 - lr: 0.01000 - Train loss: 0.64731 - Test loss: 0.55262\n",
      "Epoch 9440 - lr: 0.01000 - Train loss: 0.64850 - Test loss: 0.55291\n",
      "Epoch 9441 - lr: 0.01000 - Train loss: 0.65374 - Test loss: 0.55287\n",
      "Epoch 9442 - lr: 0.01000 - Train loss: 0.64657 - Test loss: 0.55239\n",
      "Epoch 9443 - lr: 0.01000 - Train loss: 0.63609 - Test loss: 0.55256\n",
      "Epoch 9444 - lr: 0.01000 - Train loss: 0.64570 - Test loss: 0.55300\n",
      "Epoch 9445 - lr: 0.01000 - Train loss: 0.64617 - Test loss: 0.55302\n",
      "Epoch 9446 - lr: 0.01000 - Train loss: 0.64634 - Test loss: 0.55303\n",
      "Epoch 9447 - lr: 0.01000 - Train loss: 0.64534 - Test loss: 0.55294\n",
      "Epoch 9448 - lr: 0.01000 - Train loss: 0.64831 - Test loss: 0.55316\n",
      "Epoch 9449 - lr: 0.01000 - Train loss: 0.65295 - Test loss: 0.55305\n",
      "Epoch 9450 - lr: 0.01000 - Train loss: 0.65321 - Test loss: 0.55291\n",
      "Epoch 9451 - lr: 0.01000 - Train loss: 0.65312 - Test loss: 0.55278\n",
      "Epoch 9452 - lr: 0.01000 - Train loss: 0.65394 - Test loss: 0.55275\n",
      "Epoch 9453 - lr: 0.01000 - Train loss: 0.64650 - Test loss: 0.55227\n",
      "Epoch 9454 - lr: 0.01000 - Train loss: 0.63687 - Test loss: 0.55245\n",
      "Epoch 9455 - lr: 0.01000 - Train loss: 0.64564 - Test loss: 0.55287\n",
      "Epoch 9456 - lr: 0.01000 - Train loss: 0.64605 - Test loss: 0.55289\n",
      "Epoch 9457 - lr: 0.01000 - Train loss: 0.64659 - Test loss: 0.55292\n",
      "Epoch 9458 - lr: 0.01000 - Train loss: 0.64450 - Test loss: 0.55277\n",
      "Epoch 9459 - lr: 0.01000 - Train loss: 0.64679 - Test loss: 0.55310\n",
      "Epoch 9460 - lr: 0.01000 - Train loss: 0.65255 - Test loss: 0.55292\n",
      "Epoch 9461 - lr: 0.01000 - Train loss: 0.65116 - Test loss: 0.55276\n",
      "Epoch 9462 - lr: 0.01000 - Train loss: 0.64495 - Test loss: 0.55249\n",
      "Epoch 9463 - lr: 0.01000 - Train loss: 0.64125 - Test loss: 0.55265\n",
      "Epoch 9464 - lr: 0.01000 - Train loss: 0.64547 - Test loss: 0.55267\n",
      "Epoch 9465 - lr: 0.01000 - Train loss: 0.64798 - Test loss: 0.55287\n",
      "Epoch 9466 - lr: 0.01000 - Train loss: 0.64867 - Test loss: 0.55264\n",
      "Epoch 9467 - lr: 0.01000 - Train loss: 0.64516 - Test loss: 0.55258\n",
      "Epoch 9468 - lr: 0.01000 - Train loss: 0.64804 - Test loss: 0.55279\n",
      "Epoch 9469 - lr: 0.01000 - Train loss: 0.65129 - Test loss: 0.55264\n",
      "Epoch 9470 - lr: 0.01000 - Train loss: 0.64654 - Test loss: 0.55239\n",
      "Epoch 9471 - lr: 0.01000 - Train loss: 0.64831 - Test loss: 0.55273\n",
      "Epoch 9472 - lr: 0.01000 - Train loss: 0.64756 - Test loss: 0.55233\n",
      "Epoch 9473 - lr: 0.01000 - Train loss: 0.64795 - Test loss: 0.55279\n",
      "Epoch 9474 - lr: 0.01000 - Train loss: 0.64613 - Test loss: 0.55234\n",
      "Epoch 9475 - lr: 0.01000 - Train loss: 0.64304 - Test loss: 0.55266\n",
      "Epoch 9476 - lr: 0.01000 - Train loss: 0.64871 - Test loss: 0.55305\n",
      "Epoch 9477 - lr: 0.01000 - Train loss: 0.64749 - Test loss: 0.55263\n",
      "Epoch 9478 - lr: 0.01000 - Train loss: 0.64746 - Test loss: 0.55306\n",
      "Epoch 9479 - lr: 0.01000 - Train loss: 0.64805 - Test loss: 0.55267\n",
      "Epoch 9480 - lr: 0.01000 - Train loss: 0.64891 - Test loss: 0.55309\n",
      "Epoch 9481 - lr: 0.01000 - Train loss: 0.64683 - Test loss: 0.55258\n",
      "Epoch 9482 - lr: 0.01000 - Train loss: 0.64599 - Test loss: 0.55292\n",
      "Epoch 9483 - lr: 0.01000 - Train loss: 0.64508 - Test loss: 0.55284\n",
      "Epoch 9484 - lr: 0.01000 - Train loss: 0.64868 - Test loss: 0.55312\n",
      "Epoch 9485 - lr: 0.01000 - Train loss: 0.65103 - Test loss: 0.55280\n",
      "Epoch 9486 - lr: 0.01000 - Train loss: 0.64414 - Test loss: 0.55257\n",
      "Epoch 9487 - lr: 0.01000 - Train loss: 0.63642 - Test loss: 0.55264\n",
      "Epoch 9488 - lr: 0.01000 - Train loss: 0.63588 - Test loss: 0.55280\n",
      "Epoch 9489 - lr: 0.01000 - Train loss: 0.64807 - Test loss: 0.55326\n",
      "Epoch 9490 - lr: 0.01000 - Train loss: 0.64407 - Test loss: 0.55302\n",
      "Epoch 9491 - lr: 0.01000 - Train loss: 0.63852 - Test loss: 0.55310\n",
      "Epoch 9492 - lr: 0.01000 - Train loss: 0.64807 - Test loss: 0.55352\n",
      "Epoch 9493 - lr: 0.01000 - Train loss: 0.64885 - Test loss: 0.55299\n",
      "Epoch 9494 - lr: 0.01000 - Train loss: 0.64003 - Test loss: 0.55315\n",
      "Epoch 9495 - lr: 0.01000 - Train loss: 0.63976 - Test loss: 0.55332\n",
      "Epoch 9496 - lr: 0.01000 - Train loss: 0.64831 - Test loss: 0.55356\n",
      "Epoch 9497 - lr: 0.01000 - Train loss: 0.65173 - Test loss: 0.55337\n",
      "Epoch 9498 - lr: 0.01000 - Train loss: 0.64909 - Test loss: 0.55308\n",
      "Epoch 9499 - lr: 0.01000 - Train loss: 0.64416 - Test loss: 0.55288\n",
      "Epoch 9500 - lr: 0.01000 - Train loss: 0.64620 - Test loss: 0.55315\n",
      "Epoch 9501 - lr: 0.01000 - Train loss: 0.65303 - Test loss: 0.55303\n",
      "Epoch 9502 - lr: 0.01000 - Train loss: 0.65203 - Test loss: 0.55282\n",
      "Epoch 9503 - lr: 0.01000 - Train loss: 0.64897 - Test loss: 0.55261\n",
      "Epoch 9504 - lr: 0.01000 - Train loss: 0.64424 - Test loss: 0.55250\n",
      "Epoch 9505 - lr: 0.01000 - Train loss: 0.64729 - Test loss: 0.55283\n",
      "Epoch 9506 - lr: 0.01000 - Train loss: 0.64688 - Test loss: 0.55243\n",
      "Epoch 9507 - lr: 0.01000 - Train loss: 0.64423 - Test loss: 0.55274\n",
      "Epoch 9508 - lr: 0.01000 - Train loss: 0.64616 - Test loss: 0.55256\n",
      "Epoch 9509 - lr: 0.01000 - Train loss: 0.64782 - Test loss: 0.55293\n",
      "Epoch 9510 - lr: 0.01000 - Train loss: 0.64630 - Test loss: 0.55246\n",
      "Epoch 9511 - lr: 0.01000 - Train loss: 0.64526 - Test loss: 0.55278\n",
      "Epoch 9512 - lr: 0.01000 - Train loss: 0.64546 - Test loss: 0.55278\n",
      "Epoch 9513 - lr: 0.01000 - Train loss: 0.64654 - Test loss: 0.55283\n",
      "Epoch 9514 - lr: 0.01000 - Train loss: 0.64363 - Test loss: 0.55263\n",
      "Epoch 9515 - lr: 0.01000 - Train loss: 0.63936 - Test loss: 0.55274\n",
      "Epoch 9516 - lr: 0.01000 - Train loss: 0.64802 - Test loss: 0.55302\n",
      "Epoch 9517 - lr: 0.01000 - Train loss: 0.65222 - Test loss: 0.55292\n",
      "Epoch 9518 - lr: 0.01000 - Train loss: 0.65294 - Test loss: 0.55283\n",
      "Epoch 9519 - lr: 0.01000 - Train loss: 0.65085 - Test loss: 0.55254\n",
      "Epoch 9520 - lr: 0.01000 - Train loss: 0.64411 - Test loss: 0.55234\n",
      "Epoch 9521 - lr: 0.01000 - Train loss: 0.63784 - Test loss: 0.55245\n",
      "Epoch 9522 - lr: 0.01000 - Train loss: 0.64597 - Test loss: 0.55284\n",
      "Epoch 9523 - lr: 0.01000 - Train loss: 0.65357 - Test loss: 0.55286\n",
      "Epoch 9524 - lr: 0.01000 - Train loss: 0.64604 - Test loss: 0.55240\n",
      "Epoch 9525 - lr: 0.01000 - Train loss: 0.64438 - Test loss: 0.55269\n",
      "Epoch 9526 - lr: 0.01000 - Train loss: 0.64669 - Test loss: 0.55279\n",
      "Epoch 9527 - lr: 0.01000 - Train loss: 0.64355 - Test loss: 0.55259\n",
      "Epoch 9528 - lr: 0.01000 - Train loss: 0.63745 - Test loss: 0.55267\n",
      "Epoch 9529 - lr: 0.01000 - Train loss: 0.64415 - Test loss: 0.55299\n",
      "Epoch 9530 - lr: 0.01000 - Train loss: 0.64682 - Test loss: 0.55280\n",
      "Epoch 9531 - lr: 0.01000 - Train loss: 0.64805 - Test loss: 0.55303\n",
      "Epoch 9532 - lr: 0.01000 - Train loss: 0.65329 - Test loss: 0.55294\n",
      "Epoch 9533 - lr: 0.01000 - Train loss: 0.64585 - Test loss: 0.55247\n",
      "Epoch 9534 - lr: 0.01000 - Train loss: 0.63610 - Test loss: 0.55261\n",
      "Epoch 9535 - lr: 0.01000 - Train loss: 0.64584 - Test loss: 0.55297\n",
      "Epoch 9536 - lr: 0.01000 - Train loss: 0.64443 - Test loss: 0.55290\n",
      "Epoch 9537 - lr: 0.01000 - Train loss: 0.64803 - Test loss: 0.55321\n",
      "Epoch 9538 - lr: 0.01000 - Train loss: 0.64669 - Test loss: 0.55272\n",
      "Epoch 9539 - lr: 0.01000 - Train loss: 0.64521 - Test loss: 0.55299\n",
      "Epoch 9540 - lr: 0.01000 - Train loss: 0.64510 - Test loss: 0.55295\n",
      "Epoch 9541 - lr: 0.01000 - Train loss: 0.64691 - Test loss: 0.55302\n",
      "Epoch 9542 - lr: 0.01000 - Train loss: 0.64441 - Test loss: 0.55276\n",
      "Epoch 9543 - lr: 0.01000 - Train loss: 0.64221 - Test loss: 0.55290\n",
      "Epoch 9544 - lr: 0.01000 - Train loss: 0.64368 - Test loss: 0.55275\n",
      "Epoch 9545 - lr: 0.01000 - Train loss: 0.63968 - Test loss: 0.55288\n",
      "Epoch 9546 - lr: 0.01000 - Train loss: 0.64679 - Test loss: 0.55302\n",
      "Epoch 9547 - lr: 0.01000 - Train loss: 0.64429 - Test loss: 0.55281\n",
      "Epoch 9548 - lr: 0.01000 - Train loss: 0.64289 - Test loss: 0.55299\n",
      "Epoch 9549 - lr: 0.01000 - Train loss: 0.64466 - Test loss: 0.55281\n",
      "Epoch 9550 - lr: 0.01000 - Train loss: 0.64474 - Test loss: 0.55306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9551 - lr: 0.01000 - Train loss: 0.65160 - Test loss: 0.55298\n",
      "Epoch 9552 - lr: 0.01000 - Train loss: 0.65229 - Test loss: 0.55289\n",
      "Epoch 9553 - lr: 0.01000 - Train loss: 0.65070 - Test loss: 0.55263\n",
      "Epoch 9554 - lr: 0.01000 - Train loss: 0.64508 - Test loss: 0.55243\n",
      "Epoch 9555 - lr: 0.01000 - Train loss: 0.64590 - Test loss: 0.55272\n",
      "Epoch 9556 - lr: 0.01000 - Train loss: 0.65200 - Test loss: 0.55261\n",
      "Epoch 9557 - lr: 0.01000 - Train loss: 0.65255 - Test loss: 0.55257\n",
      "Epoch 9558 - lr: 0.01000 - Train loss: 0.65115 - Test loss: 0.55236\n",
      "Epoch 9559 - lr: 0.01000 - Train loss: 0.64586 - Test loss: 0.55218\n",
      "Epoch 9560 - lr: 0.01000 - Train loss: 0.64759 - Test loss: 0.55251\n",
      "Epoch 9561 - lr: 0.01000 - Train loss: 0.64560 - Test loss: 0.55211\n",
      "Epoch 9562 - lr: 0.01000 - Train loss: 0.64233 - Test loss: 0.55237\n",
      "Epoch 9563 - lr: 0.01000 - Train loss: 0.64809 - Test loss: 0.55274\n",
      "Epoch 9564 - lr: 0.01000 - Train loss: 0.64669 - Test loss: 0.55239\n",
      "Epoch 9565 - lr: 0.01000 - Train loss: 0.64545 - Test loss: 0.55272\n",
      "Epoch 9566 - lr: 0.01000 - Train loss: 0.65279 - Test loss: 0.55271\n",
      "Epoch 9567 - lr: 0.01000 - Train loss: 0.65234 - Test loss: 0.55254\n",
      "Epoch 9568 - lr: 0.01000 - Train loss: 0.65242 - Test loss: 0.55245\n",
      "Epoch 9569 - lr: 0.01000 - Train loss: 0.65296 - Test loss: 0.55236\n",
      "Epoch 9570 - lr: 0.01000 - Train loss: 0.65218 - Test loss: 0.55217\n",
      "Epoch 9571 - lr: 0.01000 - Train loss: 0.64969 - Test loss: 0.55200\n",
      "Epoch 9572 - lr: 0.01000 - Train loss: 0.64303 - Test loss: 0.55186\n",
      "Epoch 9573 - lr: 0.01000 - Train loss: 0.63563 - Test loss: 0.55194\n",
      "Epoch 9574 - lr: 0.01000 - Train loss: 0.63903 - Test loss: 0.55218\n",
      "Epoch 9575 - lr: 0.01000 - Train loss: 0.63731 - Test loss: 0.55239\n",
      "Epoch 9576 - lr: 0.01000 - Train loss: 0.64380 - Test loss: 0.55274\n",
      "Epoch 9577 - lr: 0.01000 - Train loss: 0.64528 - Test loss: 0.55258\n",
      "Epoch 9578 - lr: 0.01000 - Train loss: 0.64486 - Test loss: 0.55283\n",
      "Epoch 9579 - lr: 0.01000 - Train loss: 0.65064 - Test loss: 0.55268\n",
      "Epoch 9580 - lr: 0.01000 - Train loss: 0.64408 - Test loss: 0.55242\n",
      "Epoch 9581 - lr: 0.01000 - Train loss: 0.63816 - Test loss: 0.55248\n",
      "Epoch 9582 - lr: 0.01000 - Train loss: 0.64796 - Test loss: 0.55287\n",
      "Epoch 9583 - lr: 0.01000 - Train loss: 0.64898 - Test loss: 0.55245\n",
      "Epoch 9584 - lr: 0.01000 - Train loss: 0.63635 - Test loss: 0.55257\n",
      "Epoch 9585 - lr: 0.01000 - Train loss: 0.64320 - Test loss: 0.55284\n",
      "Epoch 9586 - lr: 0.01000 - Train loss: 0.64770 - Test loss: 0.55305\n",
      "Epoch 9587 - lr: 0.01000 - Train loss: 0.65016 - Test loss: 0.55285\n",
      "Epoch 9588 - lr: 0.01000 - Train loss: 0.64361 - Test loss: 0.55259\n",
      "Epoch 9589 - lr: 0.01000 - Train loss: 0.63666 - Test loss: 0.55263\n",
      "Epoch 9590 - lr: 0.01000 - Train loss: 0.64044 - Test loss: 0.55283\n",
      "Epoch 9591 - lr: 0.01000 - Train loss: 0.64492 - Test loss: 0.55286\n",
      "Epoch 9592 - lr: 0.01000 - Train loss: 0.64673 - Test loss: 0.55296\n",
      "Epoch 9593 - lr: 0.01000 - Train loss: 0.64423 - Test loss: 0.55271\n",
      "Epoch 9594 - lr: 0.01000 - Train loss: 0.64203 - Test loss: 0.55284\n",
      "Epoch 9595 - lr: 0.01000 - Train loss: 0.64335 - Test loss: 0.55269\n",
      "Epoch 9596 - lr: 0.01000 - Train loss: 0.63816 - Test loss: 0.55278\n",
      "Epoch 9597 - lr: 0.01000 - Train loss: 0.64774 - Test loss: 0.55314\n",
      "Epoch 9598 - lr: 0.01000 - Train loss: 0.64556 - Test loss: 0.55273\n",
      "Epoch 9599 - lr: 0.01000 - Train loss: 0.64570 - Test loss: 0.55301\n",
      "Epoch 9600 - lr: 0.01000 - Train loss: 0.64368 - Test loss: 0.55292\n",
      "Epoch 9601 - lr: 0.01000 - Train loss: 0.64646 - Test loss: 0.55320\n",
      "Epoch 9602 - lr: 0.01000 - Train loss: 0.64568 - Test loss: 0.55280\n",
      "Epoch 9603 - lr: 0.01000 - Train loss: 0.63802 - Test loss: 0.55294\n",
      "Epoch 9604 - lr: 0.01000 - Train loss: 0.64753 - Test loss: 0.55334\n",
      "Epoch 9605 - lr: 0.01000 - Train loss: 0.64927 - Test loss: 0.55292\n",
      "Epoch 9606 - lr: 0.01000 - Train loss: 0.63495 - Test loss: 0.55302\n",
      "Epoch 9607 - lr: 0.01000 - Train loss: 0.64756 - Test loss: 0.55338\n",
      "Epoch 9608 - lr: 0.01000 - Train loss: 0.64358 - Test loss: 0.55317\n",
      "Epoch 9609 - lr: 0.01000 - Train loss: 0.64011 - Test loss: 0.55326\n",
      "Epoch 9610 - lr: 0.01000 - Train loss: 0.64479 - Test loss: 0.55323\n",
      "Epoch 9611 - lr: 0.01000 - Train loss: 0.64608 - Test loss: 0.55326\n",
      "Epoch 9612 - lr: 0.01000 - Train loss: 0.64350 - Test loss: 0.55303\n",
      "Epoch 9613 - lr: 0.01000 - Train loss: 0.64154 - Test loss: 0.55313\n",
      "Epoch 9614 - lr: 0.01000 - Train loss: 0.64342 - Test loss: 0.55299\n",
      "Epoch 9615 - lr: 0.01000 - Train loss: 0.64238 - Test loss: 0.55315\n",
      "Epoch 9616 - lr: 0.01000 - Train loss: 0.64445 - Test loss: 0.55299\n",
      "Epoch 9617 - lr: 0.01000 - Train loss: 0.64565 - Test loss: 0.55325\n",
      "Epoch 9618 - lr: 0.01000 - Train loss: 0.64835 - Test loss: 0.55295\n",
      "Epoch 9619 - lr: 0.01000 - Train loss: 0.64406 - Test loss: 0.55291\n",
      "Epoch 9620 - lr: 0.01000 - Train loss: 0.64707 - Test loss: 0.55312\n",
      "Epoch 9621 - lr: 0.01000 - Train loss: 0.65135 - Test loss: 0.55297\n",
      "Epoch 9622 - lr: 0.01000 - Train loss: 0.65224 - Test loss: 0.55291\n",
      "Epoch 9623 - lr: 0.01000 - Train loss: 0.64484 - Test loss: 0.55252\n",
      "Epoch 9624 - lr: 0.01000 - Train loss: 0.63731 - Test loss: 0.55268\n",
      "Epoch 9625 - lr: 0.01000 - Train loss: 0.63546 - Test loss: 0.55288\n",
      "Epoch 9626 - lr: 0.01000 - Train loss: 0.64504 - Test loss: 0.55322\n",
      "Epoch 9627 - lr: 0.01000 - Train loss: 0.64414 - Test loss: 0.55319\n",
      "Epoch 9628 - lr: 0.01000 - Train loss: 0.64708 - Test loss: 0.55337\n",
      "Epoch 9629 - lr: 0.01000 - Train loss: 0.65216 - Test loss: 0.55327\n",
      "Epoch 9630 - lr: 0.01000 - Train loss: 0.64464 - Test loss: 0.55284\n",
      "Epoch 9631 - lr: 0.01000 - Train loss: 0.64182 - Test loss: 0.55301\n",
      "Epoch 9632 - lr: 0.01000 - Train loss: 0.64734 - Test loss: 0.55332\n",
      "Epoch 9633 - lr: 0.01000 - Train loss: 0.64461 - Test loss: 0.55293\n",
      "Epoch 9634 - lr: 0.01000 - Train loss: 0.64074 - Test loss: 0.55310\n",
      "Epoch 9635 - lr: 0.01000 - Train loss: 0.64589 - Test loss: 0.55343\n",
      "Epoch 9636 - lr: 0.01000 - Train loss: 0.64706 - Test loss: 0.55311\n",
      "Epoch 9637 - lr: 0.01000 - Train loss: 0.64742 - Test loss: 0.55333\n",
      "Epoch 9638 - lr: 0.01000 - Train loss: 0.65208 - Test loss: 0.55321\n",
      "Epoch 9639 - lr: 0.01000 - Train loss: 0.64923 - Test loss: 0.55291\n",
      "Epoch 9640 - lr: 0.01000 - Train loss: 0.64303 - Test loss: 0.55277\n",
      "Epoch 9641 - lr: 0.01000 - Train loss: 0.64003 - Test loss: 0.55287\n",
      "Epoch 9642 - lr: 0.01000 - Train loss: 0.64428 - Test loss: 0.55288\n",
      "Epoch 9643 - lr: 0.01000 - Train loss: 0.64660 - Test loss: 0.55300\n",
      "Epoch 9644 - lr: 0.01000 - Train loss: 0.64718 - Test loss: 0.55280\n",
      "Epoch 9645 - lr: 0.01000 - Train loss: 0.64425 - Test loss: 0.55273\n",
      "Epoch 9646 - lr: 0.01000 - Train loss: 0.64575 - Test loss: 0.55278\n",
      "Epoch 9647 - lr: 0.01000 - Train loss: 0.64322 - Test loss: 0.55261\n",
      "Epoch 9648 - lr: 0.01000 - Train loss: 0.64094 - Test loss: 0.55273\n",
      "Epoch 9649 - lr: 0.01000 - Train loss: 0.64307 - Test loss: 0.55268\n",
      "Epoch 9650 - lr: 0.01000 - Train loss: 0.64332 - Test loss: 0.55290\n",
      "Epoch 9651 - lr: 0.01000 - Train loss: 0.64790 - Test loss: 0.55278\n",
      "Epoch 9652 - lr: 0.01000 - Train loss: 0.64304 - Test loss: 0.55268\n",
      "Epoch 9653 - lr: 0.01000 - Train loss: 0.64454 - Test loss: 0.55291\n",
      "Epoch 9654 - lr: 0.01000 - Train loss: 0.65185 - Test loss: 0.55289\n",
      "Epoch 9655 - lr: 0.01000 - Train loss: 0.64450 - Test loss: 0.55254\n",
      "Epoch 9656 - lr: 0.01000 - Train loss: 0.63781 - Test loss: 0.55272\n",
      "Epoch 9657 - lr: 0.01000 - Train loss: 0.63423 - Test loss: 0.55292\n",
      "Epoch 9658 - lr: 0.01000 - Train loss: 0.64422 - Test loss: 0.55324\n",
      "Epoch 9659 - lr: 0.01000 - Train loss: 0.64475 - Test loss: 0.55327\n",
      "Epoch 9660 - lr: 0.01000 - Train loss: 0.64440 - Test loss: 0.55323\n",
      "Epoch 9661 - lr: 0.01000 - Train loss: 0.64506 - Test loss: 0.55321\n",
      "Epoch 9662 - lr: 0.01000 - Train loss: 0.64320 - Test loss: 0.55306\n",
      "Epoch 9663 - lr: 0.01000 - Train loss: 0.64568 - Test loss: 0.55329\n",
      "Epoch 9664 - lr: 0.01000 - Train loss: 0.64422 - Test loss: 0.55292\n",
      "Epoch 9665 - lr: 0.01000 - Train loss: 0.63581 - Test loss: 0.55306\n",
      "Epoch 9666 - lr: 0.01000 - Train loss: 0.64147 - Test loss: 0.55331\n",
      "Epoch 9667 - lr: 0.01000 - Train loss: 0.64680 - Test loss: 0.55363\n",
      "Epoch 9668 - lr: 0.01000 - Train loss: 0.64590 - Test loss: 0.55324\n",
      "Epoch 9669 - lr: 0.01000 - Train loss: 0.64428 - Test loss: 0.55345\n",
      "Epoch 9670 - lr: 0.01000 - Train loss: 0.64438 - Test loss: 0.55341\n",
      "Epoch 9671 - lr: 0.01000 - Train loss: 0.64496 - Test loss: 0.55339\n",
      "Epoch 9672 - lr: 0.01000 - Train loss: 0.64330 - Test loss: 0.55324\n",
      "Epoch 9673 - lr: 0.01000 - Train loss: 0.64608 - Test loss: 0.55346\n",
      "Epoch 9674 - lr: 0.01000 - Train loss: 0.64479 - Test loss: 0.55307\n",
      "Epoch 9675 - lr: 0.01000 - Train loss: 0.64576 - Test loss: 0.55331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9676 - lr: 0.01000 - Train loss: 0.64330 - Test loss: 0.55321\n",
      "Epoch 9677 - lr: 0.01000 - Train loss: 0.64556 - Test loss: 0.55346\n",
      "Epoch 9678 - lr: 0.01000 - Train loss: 0.64378 - Test loss: 0.55311\n",
      "Epoch 9679 - lr: 0.01000 - Train loss: 0.64220 - Test loss: 0.55330\n",
      "Epoch 9680 - lr: 0.01000 - Train loss: 0.64646 - Test loss: 0.55351\n",
      "Epoch 9681 - lr: 0.01000 - Train loss: 0.65129 - Test loss: 0.55345\n",
      "Epoch 9682 - lr: 0.01000 - Train loss: 0.64391 - Test loss: 0.55307\n",
      "Epoch 9683 - lr: 0.01000 - Train loss: 0.64339 - Test loss: 0.55327\n",
      "Epoch 9684 - lr: 0.01000 - Train loss: 0.64519 - Test loss: 0.55333\n",
      "Epoch 9685 - lr: 0.01000 - Train loss: 0.64322 - Test loss: 0.55319\n",
      "Epoch 9686 - lr: 0.01000 - Train loss: 0.64498 - Test loss: 0.55341\n",
      "Epoch 9687 - lr: 0.01000 - Train loss: 0.64657 - Test loss: 0.55311\n",
      "Epoch 9688 - lr: 0.01000 - Train loss: 0.64615 - Test loss: 0.55325\n",
      "Epoch 9689 - lr: 0.01000 - Train loss: 0.64846 - Test loss: 0.55311\n",
      "Epoch 9690 - lr: 0.01000 - Train loss: 0.64403 - Test loss: 0.55293\n",
      "Epoch 9691 - lr: 0.01000 - Train loss: 0.64595 - Test loss: 0.55316\n",
      "Epoch 9692 - lr: 0.01000 - Train loss: 0.64419 - Test loss: 0.55282\n",
      "Epoch 9693 - lr: 0.01000 - Train loss: 0.64523 - Test loss: 0.55307\n",
      "Epoch 9694 - lr: 0.01000 - Train loss: 0.64312 - Test loss: 0.55302\n",
      "Epoch 9695 - lr: 0.01000 - Train loss: 0.64579 - Test loss: 0.55329\n",
      "Epoch 9696 - lr: 0.01000 - Train loss: 0.64431 - Test loss: 0.55296\n",
      "Epoch 9697 - lr: 0.01000 - Train loss: 0.64579 - Test loss: 0.55322\n",
      "Epoch 9698 - lr: 0.01000 - Train loss: 0.64311 - Test loss: 0.55313\n",
      "Epoch 9699 - lr: 0.01000 - Train loss: 0.64491 - Test loss: 0.55338\n",
      "Epoch 9700 - lr: 0.01000 - Train loss: 0.64651 - Test loss: 0.55310\n",
      "Epoch 9701 - lr: 0.01000 - Train loss: 0.64611 - Test loss: 0.55324\n",
      "Epoch 9702 - lr: 0.01000 - Train loss: 0.64848 - Test loss: 0.55311\n",
      "Epoch 9703 - lr: 0.01000 - Train loss: 0.64409 - Test loss: 0.55293\n",
      "Epoch 9704 - lr: 0.01000 - Train loss: 0.64604 - Test loss: 0.55315\n",
      "Epoch 9705 - lr: 0.01000 - Train loss: 0.64420 - Test loss: 0.55282\n",
      "Epoch 9706 - lr: 0.01000 - Train loss: 0.64521 - Test loss: 0.55306\n",
      "Epoch 9707 - lr: 0.01000 - Train loss: 0.64300 - Test loss: 0.55301\n",
      "Epoch 9708 - lr: 0.01000 - Train loss: 0.64552 - Test loss: 0.55328\n",
      "Epoch 9709 - lr: 0.01000 - Train loss: 0.64377 - Test loss: 0.55297\n",
      "Epoch 9710 - lr: 0.01000 - Train loss: 0.64381 - Test loss: 0.55319\n",
      "Epoch 9711 - lr: 0.01000 - Train loss: 0.64423 - Test loss: 0.55322\n",
      "Epoch 9712 - lr: 0.01000 - Train loss: 0.64412 - Test loss: 0.55321\n",
      "Epoch 9713 - lr: 0.01000 - Train loss: 0.64411 - Test loss: 0.55317\n",
      "Epoch 9714 - lr: 0.01000 - Train loss: 0.64391 - Test loss: 0.55313\n",
      "Epoch 9715 - lr: 0.01000 - Train loss: 0.64433 - Test loss: 0.55311\n",
      "Epoch 9716 - lr: 0.01000 - Train loss: 0.64317 - Test loss: 0.55303\n",
      "Epoch 9717 - lr: 0.01000 - Train loss: 0.64595 - Test loss: 0.55321\n",
      "Epoch 9718 - lr: 0.01000 - Train loss: 0.64825 - Test loss: 0.55299\n",
      "Epoch 9719 - lr: 0.01000 - Train loss: 0.64346 - Test loss: 0.55289\n",
      "Epoch 9720 - lr: 0.01000 - Train loss: 0.64536 - Test loss: 0.55315\n",
      "Epoch 9721 - lr: 0.01000 - Train loss: 0.64354 - Test loss: 0.55286\n",
      "Epoch 9722 - lr: 0.01000 - Train loss: 0.64422 - Test loss: 0.55311\n",
      "Epoch 9723 - lr: 0.01000 - Train loss: 0.64353 - Test loss: 0.55312\n",
      "Epoch 9724 - lr: 0.01000 - Train loss: 0.64557 - Test loss: 0.55325\n",
      "Epoch 9725 - lr: 0.01000 - Train loss: 0.64799 - Test loss: 0.55312\n",
      "Epoch 9726 - lr: 0.01000 - Train loss: 0.64429 - Test loss: 0.55296\n",
      "Epoch 9727 - lr: 0.01000 - Train loss: 0.64586 - Test loss: 0.55312\n",
      "Epoch 9728 - lr: 0.01000 - Train loss: 0.64950 - Test loss: 0.55297\n",
      "Epoch 9729 - lr: 0.01000 - Train loss: 0.64959 - Test loss: 0.55290\n",
      "Epoch 9730 - lr: 0.01000 - Train loss: 0.65049 - Test loss: 0.55285\n",
      "Epoch 9731 - lr: 0.01000 - Train loss: 0.64598 - Test loss: 0.55257\n",
      "Epoch 9732 - lr: 0.01000 - Train loss: 0.64618 - Test loss: 0.55280\n",
      "Epoch 9733 - lr: 0.01000 - Train loss: 0.65092 - Test loss: 0.55278\n",
      "Epoch 9734 - lr: 0.01000 - Train loss: 0.64432 - Test loss: 0.55251\n",
      "Epoch 9735 - lr: 0.01000 - Train loss: 0.63545 - Test loss: 0.55268\n",
      "Epoch 9736 - lr: 0.01000 - Train loss: 0.63652 - Test loss: 0.55292\n",
      "Epoch 9737 - lr: 0.01000 - Train loss: 0.64431 - Test loss: 0.55330\n",
      "Epoch 9738 - lr: 0.01000 - Train loss: 0.65066 - Test loss: 0.55325\n",
      "Epoch 9739 - lr: 0.01000 - Train loss: 0.65116 - Test loss: 0.55318\n",
      "Epoch 9740 - lr: 0.01000 - Train loss: 0.64623 - Test loss: 0.55288\n",
      "Epoch 9741 - lr: 0.01000 - Train loss: 0.64670 - Test loss: 0.55308\n",
      "Epoch 9742 - lr: 0.01000 - Train loss: 0.65023 - Test loss: 0.55292\n",
      "Epoch 9743 - lr: 0.01000 - Train loss: 0.64946 - Test loss: 0.55280\n",
      "Epoch 9744 - lr: 0.01000 - Train loss: 0.64552 - Test loss: 0.55261\n",
      "Epoch 9745 - lr: 0.01000 - Train loss: 0.64588 - Test loss: 0.55269\n",
      "Epoch 9746 - lr: 0.01000 - Train loss: 0.64624 - Test loss: 0.55252\n",
      "Epoch 9747 - lr: 0.01000 - Train loss: 0.64410 - Test loss: 0.55251\n",
      "Epoch 9748 - lr: 0.01000 - Train loss: 0.64340 - Test loss: 0.55249\n",
      "Epoch 9749 - lr: 0.01000 - Train loss: 0.64563 - Test loss: 0.55260\n",
      "Epoch 9750 - lr: 0.01000 - Train loss: 0.64543 - Test loss: 0.55247\n",
      "Epoch 9751 - lr: 0.01000 - Train loss: 0.64536 - Test loss: 0.55256\n",
      "Epoch 9752 - lr: 0.01000 - Train loss: 0.64384 - Test loss: 0.55243\n",
      "Epoch 9753 - lr: 0.01000 - Train loss: 0.64550 - Test loss: 0.55269\n",
      "Epoch 9754 - lr: 0.01000 - Train loss: 0.64380 - Test loss: 0.55244\n",
      "Epoch 9755 - lr: 0.01000 - Train loss: 0.64094 - Test loss: 0.55265\n",
      "Epoch 9756 - lr: 0.01000 - Train loss: 0.64643 - Test loss: 0.55300\n",
      "Epoch 9757 - lr: 0.01000 - Train loss: 0.64436 - Test loss: 0.55275\n",
      "Epoch 9758 - lr: 0.01000 - Train loss: 0.64509 - Test loss: 0.55300\n",
      "Epoch 9759 - lr: 0.01000 - Train loss: 0.64253 - Test loss: 0.55296\n",
      "Epoch 9760 - lr: 0.01000 - Train loss: 0.64373 - Test loss: 0.55318\n",
      "Epoch 9761 - lr: 0.01000 - Train loss: 0.65132 - Test loss: 0.55317\n",
      "Epoch 9762 - lr: 0.01000 - Train loss: 0.64426 - Test loss: 0.55285\n",
      "Epoch 9763 - lr: 0.01000 - Train loss: 0.64479 - Test loss: 0.55305\n",
      "Epoch 9764 - lr: 0.01000 - Train loss: 0.64261 - Test loss: 0.55301\n",
      "Epoch 9765 - lr: 0.01000 - Train loss: 0.64489 - Test loss: 0.55326\n",
      "Epoch 9766 - lr: 0.01000 - Train loss: 0.64644 - Test loss: 0.55300\n",
      "Epoch 9767 - lr: 0.01000 - Train loss: 0.64604 - Test loss: 0.55314\n",
      "Epoch 9768 - lr: 0.01000 - Train loss: 0.64744 - Test loss: 0.55299\n",
      "Epoch 9769 - lr: 0.01000 - Train loss: 0.64227 - Test loss: 0.55288\n",
      "Epoch 9770 - lr: 0.01000 - Train loss: 0.64213 - Test loss: 0.55302\n",
      "Epoch 9771 - lr: 0.01000 - Train loss: 0.64507 - Test loss: 0.55290\n",
      "Epoch 9772 - lr: 0.01000 - Train loss: 0.64597 - Test loss: 0.55304\n",
      "Epoch 9773 - lr: 0.01000 - Train loss: 0.65020 - Test loss: 0.55296\n",
      "Epoch 9774 - lr: 0.01000 - Train loss: 0.65122 - Test loss: 0.55289\n",
      "Epoch 9775 - lr: 0.01000 - Train loss: 0.64391 - Test loss: 0.55260\n",
      "Epoch 9776 - lr: 0.01000 - Train loss: 0.64043 - Test loss: 0.55277\n",
      "Epoch 9777 - lr: 0.01000 - Train loss: 0.64614 - Test loss: 0.55312\n",
      "Epoch 9778 - lr: 0.01000 - Train loss: 0.64715 - Test loss: 0.55286\n",
      "Epoch 9779 - lr: 0.01000 - Train loss: 0.63647 - Test loss: 0.55302\n",
      "Epoch 9780 - lr: 0.01000 - Train loss: 0.63477 - Test loss: 0.55324\n",
      "Epoch 9781 - lr: 0.01000 - Train loss: 0.64398 - Test loss: 0.55352\n",
      "Epoch 9782 - lr: 0.01000 - Train loss: 0.64352 - Test loss: 0.55352\n",
      "Epoch 9783 - lr: 0.01000 - Train loss: 0.64586 - Test loss: 0.55362\n",
      "Epoch 9784 - lr: 0.01000 - Train loss: 0.64774 - Test loss: 0.55343\n",
      "Epoch 9785 - lr: 0.01000 - Train loss: 0.64236 - Test loss: 0.55325\n",
      "Epoch 9786 - lr: 0.01000 - Train loss: 0.64098 - Test loss: 0.55334\n",
      "Epoch 9787 - lr: 0.01000 - Train loss: 0.64283 - Test loss: 0.55322\n",
      "Epoch 9788 - lr: 0.01000 - Train loss: 0.64252 - Test loss: 0.55339\n",
      "Epoch 9789 - lr: 0.01000 - Train loss: 0.64756 - Test loss: 0.55327\n",
      "Epoch 9790 - lr: 0.01000 - Train loss: 0.64237 - Test loss: 0.55315\n",
      "Epoch 9791 - lr: 0.01000 - Train loss: 0.64201 - Test loss: 0.55329\n",
      "Epoch 9792 - lr: 0.01000 - Train loss: 0.64568 - Test loss: 0.55317\n",
      "Epoch 9793 - lr: 0.01000 - Train loss: 0.64415 - Test loss: 0.55318\n",
      "Epoch 9794 - lr: 0.01000 - Train loss: 0.64248 - Test loss: 0.55311\n",
      "Epoch 9795 - lr: 0.01000 - Train loss: 0.64540 - Test loss: 0.55335\n",
      "Epoch 9796 - lr: 0.01000 - Train loss: 0.64414 - Test loss: 0.55307\n",
      "Epoch 9797 - lr: 0.01000 - Train loss: 0.64460 - Test loss: 0.55329\n",
      "Epoch 9798 - lr: 0.01000 - Train loss: 0.64251 - Test loss: 0.55327\n",
      "Epoch 9799 - lr: 0.01000 - Train loss: 0.64513 - Test loss: 0.55354\n",
      "Epoch 9800 - lr: 0.01000 - Train loss: 0.64367 - Test loss: 0.55326\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9801 - lr: 0.01000 - Train loss: 0.64507 - Test loss: 0.55349\n",
      "Epoch 9802 - lr: 0.01000 - Train loss: 0.64247 - Test loss: 0.55343\n",
      "Epoch 9803 - lr: 0.01000 - Train loss: 0.64417 - Test loss: 0.55366\n",
      "Epoch 9804 - lr: 0.01000 - Train loss: 0.64767 - Test loss: 0.55344\n",
      "Epoch 9805 - lr: 0.01000 - Train loss: 0.64248 - Test loss: 0.55337\n",
      "Epoch 9806 - lr: 0.01000 - Train loss: 0.64393 - Test loss: 0.55358\n",
      "Epoch 9807 - lr: 0.01000 - Train loss: 0.64941 - Test loss: 0.55344\n",
      "Epoch 9808 - lr: 0.01000 - Train loss: 0.64838 - Test loss: 0.55332\n",
      "Epoch 9809 - lr: 0.01000 - Train loss: 0.64414 - Test loss: 0.55316\n",
      "Epoch 9810 - lr: 0.01000 - Train loss: 0.64586 - Test loss: 0.55334\n",
      "Epoch 9811 - lr: 0.01000 - Train loss: 0.64586 - Test loss: 0.55308\n",
      "Epoch 9812 - lr: 0.01000 - Train loss: 0.64577 - Test loss: 0.55325\n",
      "Epoch 9813 - lr: 0.01000 - Train loss: 0.64970 - Test loss: 0.55319\n",
      "Epoch 9814 - lr: 0.01000 - Train loss: 0.65059 - Test loss: 0.55313\n",
      "Epoch 9815 - lr: 0.01000 - Train loss: 0.64601 - Test loss: 0.55287\n",
      "Epoch 9816 - lr: 0.01000 - Train loss: 0.64584 - Test loss: 0.55304\n",
      "Epoch 9817 - lr: 0.01000 - Train loss: 0.64898 - Test loss: 0.55296\n",
      "Epoch 9818 - lr: 0.01000 - Train loss: 0.64539 - Test loss: 0.55282\n",
      "Epoch 9819 - lr: 0.01000 - Train loss: 0.64471 - Test loss: 0.55288\n",
      "Epoch 9820 - lr: 0.01000 - Train loss: 0.64210 - Test loss: 0.55279\n",
      "Epoch 9821 - lr: 0.01000 - Train loss: 0.63922 - Test loss: 0.55292\n",
      "Epoch 9822 - lr: 0.01000 - Train loss: 0.64275 - Test loss: 0.55297\n",
      "Epoch 9823 - lr: 0.01000 - Train loss: 0.64594 - Test loss: 0.55322\n",
      "Epoch 9824 - lr: 0.01000 - Train loss: 0.64627 - Test loss: 0.55300\n",
      "Epoch 9825 - lr: 0.01000 - Train loss: 0.64493 - Test loss: 0.55311\n",
      "Epoch 9826 - lr: 0.01000 - Train loss: 0.64249 - Test loss: 0.55303\n",
      "Epoch 9827 - lr: 0.01000 - Train loss: 0.64138 - Test loss: 0.55319\n",
      "Epoch 9828 - lr: 0.01000 - Train loss: 0.64360 - Test loss: 0.55311\n",
      "Epoch 9829 - lr: 0.01000 - Train loss: 0.64532 - Test loss: 0.55337\n",
      "Epoch 9830 - lr: 0.01000 - Train loss: 0.64460 - Test loss: 0.55312\n",
      "Epoch 9831 - lr: 0.01000 - Train loss: 0.64353 - Test loss: 0.55332\n",
      "Epoch 9832 - lr: 0.01000 - Train loss: 0.64303 - Test loss: 0.55334\n",
      "Epoch 9833 - lr: 0.01000 - Train loss: 0.64544 - Test loss: 0.55348\n",
      "Epoch 9834 - lr: 0.01000 - Train loss: 0.64807 - Test loss: 0.55335\n",
      "Epoch 9835 - lr: 0.01000 - Train loss: 0.64336 - Test loss: 0.55320\n",
      "Epoch 9836 - lr: 0.01000 - Train loss: 0.64503 - Test loss: 0.55341\n",
      "Epoch 9837 - lr: 0.01000 - Train loss: 0.64341 - Test loss: 0.55316\n",
      "Epoch 9838 - lr: 0.01000 - Train loss: 0.64322 - Test loss: 0.55336\n",
      "Epoch 9839 - lr: 0.01000 - Train loss: 0.64332 - Test loss: 0.55340\n",
      "Epoch 9840 - lr: 0.01000 - Train loss: 0.64431 - Test loss: 0.55347\n",
      "Epoch 9841 - lr: 0.01000 - Train loss: 0.64208 - Test loss: 0.55338\n",
      "Epoch 9842 - lr: 0.01000 - Train loss: 0.64241 - Test loss: 0.55354\n",
      "Epoch 9843 - lr: 0.01000 - Train loss: 0.64849 - Test loss: 0.55346\n",
      "Epoch 9844 - lr: 0.01000 - Train loss: 0.64567 - Test loss: 0.55331\n",
      "Epoch 9845 - lr: 0.01000 - Train loss: 0.64312 - Test loss: 0.55328\n",
      "Epoch 9846 - lr: 0.01000 - Train loss: 0.64407 - Test loss: 0.55332\n",
      "Epoch 9847 - lr: 0.01000 - Train loss: 0.64201 - Test loss: 0.55324\n",
      "Epoch 9848 - lr: 0.01000 - Train loss: 0.64316 - Test loss: 0.55344\n",
      "Epoch 9849 - lr: 0.01000 - Train loss: 0.65034 - Test loss: 0.55343\n",
      "Epoch 9850 - lr: 0.01000 - Train loss: 0.64311 - Test loss: 0.55319\n",
      "Epoch 9851 - lr: 0.01000 - Train loss: 0.64154 - Test loss: 0.55338\n",
      "Epoch 9852 - lr: 0.01000 - Train loss: 0.64531 - Test loss: 0.55358\n",
      "Epoch 9853 - lr: 0.01000 - Train loss: 0.64808 - Test loss: 0.55350\n",
      "Epoch 9854 - lr: 0.01000 - Train loss: 0.64426 - Test loss: 0.55335\n",
      "Epoch 9855 - lr: 0.01000 - Train loss: 0.64536 - Test loss: 0.55348\n",
      "Epoch 9856 - lr: 0.01000 - Train loss: 0.65015 - Test loss: 0.55343\n",
      "Epoch 9857 - lr: 0.01000 - Train loss: 0.64450 - Test loss: 0.55316\n",
      "Epoch 9858 - lr: 0.01000 - Train loss: 0.64365 - Test loss: 0.55341\n",
      "Epoch 9859 - lr: 0.01000 - Train loss: 0.65050 - Test loss: 0.55340\n",
      "Epoch 9860 - lr: 0.01000 - Train loss: 0.64536 - Test loss: 0.55318\n",
      "Epoch 9861 - lr: 0.01000 - Train loss: 0.64608 - Test loss: 0.55346\n",
      "Epoch 9862 - lr: 0.01000 - Train loss: 0.64335 - Test loss: 0.55324\n",
      "Epoch 9863 - lr: 0.01000 - Train loss: 0.63943 - Test loss: 0.55342\n",
      "Epoch 9864 - lr: 0.01000 - Train loss: 0.64458 - Test loss: 0.55374\n",
      "Epoch 9865 - lr: 0.01000 - Train loss: 0.64506 - Test loss: 0.55352\n",
      "Epoch 9866 - lr: 0.01000 - Train loss: 0.64550 - Test loss: 0.55382\n",
      "Epoch 9867 - lr: 0.01000 - Train loss: 0.64682 - Test loss: 0.55354\n",
      "Epoch 9868 - lr: 0.01000 - Train loss: 0.63608 - Test loss: 0.55369\n",
      "Epoch 9869 - lr: 0.01000 - Train loss: 0.63434 - Test loss: 0.55389\n",
      "Epoch 9870 - lr: 0.01000 - Train loss: 0.64386 - Test loss: 0.55413\n",
      "Epoch 9871 - lr: 0.01000 - Train loss: 0.64276 - Test loss: 0.55409\n",
      "Epoch 9872 - lr: 0.01000 - Train loss: 0.64605 - Test loss: 0.55426\n",
      "Epoch 9873 - lr: 0.01000 - Train loss: 0.64430 - Test loss: 0.55394\n",
      "Epoch 9874 - lr: 0.01000 - Train loss: 0.64075 - Test loss: 0.55406\n",
      "Epoch 9875 - lr: 0.01000 - Train loss: 0.64243 - Test loss: 0.55395\n",
      "Epoch 9876 - lr: 0.01000 - Train loss: 0.64087 - Test loss: 0.55407\n",
      "Epoch 9877 - lr: 0.01000 - Train loss: 0.64285 - Test loss: 0.55393\n",
      "Epoch 9878 - lr: 0.01000 - Train loss: 0.64321 - Test loss: 0.55410\n",
      "Epoch 9879 - lr: 0.01000 - Train loss: 0.65084 - Test loss: 0.55403\n",
      "Epoch 9880 - lr: 0.01000 - Train loss: 0.64511 - Test loss: 0.55372\n",
      "Epoch 9881 - lr: 0.01000 - Train loss: 0.64181 - Test loss: 0.55385\n",
      "Epoch 9882 - lr: 0.01000 - Train loss: 0.64504 - Test loss: 0.55395\n",
      "Epoch 9883 - lr: 0.01000 - Train loss: 0.64388 - Test loss: 0.55380\n",
      "Epoch 9884 - lr: 0.01000 - Train loss: 0.64572 - Test loss: 0.55399\n",
      "Epoch 9885 - lr: 0.01000 - Train loss: 0.64318 - Test loss: 0.55370\n",
      "Epoch 9886 - lr: 0.01000 - Train loss: 0.64092 - Test loss: 0.55385\n",
      "Epoch 9887 - lr: 0.01000 - Train loss: 0.64584 - Test loss: 0.55407\n",
      "Epoch 9888 - lr: 0.01000 - Train loss: 0.64981 - Test loss: 0.55395\n",
      "Epoch 9889 - lr: 0.01000 - Train loss: 0.65057 - Test loss: 0.55388\n",
      "Epoch 9890 - lr: 0.01000 - Train loss: 0.64614 - Test loss: 0.55360\n",
      "Epoch 9891 - lr: 0.01000 - Train loss: 0.64492 - Test loss: 0.55368\n",
      "Epoch 9892 - lr: 0.01000 - Train loss: 0.64237 - Test loss: 0.55356\n",
      "Epoch 9893 - lr: 0.01000 - Train loss: 0.64025 - Test loss: 0.55369\n",
      "Epoch 9894 - lr: 0.01000 - Train loss: 0.64176 - Test loss: 0.55363\n",
      "Epoch 9895 - lr: 0.01000 - Train loss: 0.63817 - Test loss: 0.55376\n",
      "Epoch 9896 - lr: 0.01000 - Train loss: 0.64382 - Test loss: 0.55383\n",
      "Epoch 9897 - lr: 0.01000 - Train loss: 0.64215 - Test loss: 0.55381\n",
      "Epoch 9898 - lr: 0.01000 - Train loss: 0.64542 - Test loss: 0.55404\n",
      "Epoch 9899 - lr: 0.01000 - Train loss: 0.64495 - Test loss: 0.55376\n",
      "Epoch 9900 - lr: 0.01000 - Train loss: 0.64148 - Test loss: 0.55392\n",
      "Epoch 9901 - lr: 0.01000 - Train loss: 0.64513 - Test loss: 0.55406\n",
      "Epoch 9902 - lr: 0.01000 - Train loss: 0.64579 - Test loss: 0.55392\n",
      "Epoch 9903 - lr: 0.01000 - Train loss: 0.64266 - Test loss: 0.55387\n",
      "Epoch 9904 - lr: 0.01000 - Train loss: 0.64491 - Test loss: 0.55395\n",
      "Epoch 9905 - lr: 0.01000 - Train loss: 0.64574 - Test loss: 0.55379\n",
      "Epoch 9906 - lr: 0.01000 - Train loss: 0.64239 - Test loss: 0.55374\n",
      "Epoch 9907 - lr: 0.01000 - Train loss: 0.64516 - Test loss: 0.55387\n",
      "Epoch 9908 - lr: 0.01000 - Train loss: 0.64988 - Test loss: 0.55381\n",
      "Epoch 9909 - lr: 0.01000 - Train loss: 0.64736 - Test loss: 0.55359\n",
      "Epoch 9910 - lr: 0.01000 - Train loss: 0.64158 - Test loss: 0.55355\n",
      "Epoch 9911 - lr: 0.01000 - Train loss: 0.64031 - Test loss: 0.55372\n",
      "Epoch 9912 - lr: 0.01000 - Train loss: 0.64199 - Test loss: 0.55368\n",
      "Epoch 9913 - lr: 0.01000 - Train loss: 0.64076 - Test loss: 0.55387\n",
      "Epoch 9914 - lr: 0.01000 - Train loss: 0.64288 - Test loss: 0.55380\n",
      "Epoch 9915 - lr: 0.01000 - Train loss: 0.64426 - Test loss: 0.55406\n",
      "Epoch 9916 - lr: 0.01000 - Train loss: 0.64348 - Test loss: 0.55383\n",
      "Epoch 9917 - lr: 0.01000 - Train loss: 0.63480 - Test loss: 0.55401\n",
      "Epoch 9918 - lr: 0.01000 - Train loss: 0.63781 - Test loss: 0.55423\n",
      "Epoch 9919 - lr: 0.01000 - Train loss: 0.64453 - Test loss: 0.55437\n",
      "Epoch 9920 - lr: 0.01000 - Train loss: 0.64243 - Test loss: 0.55426\n",
      "Epoch 9921 - lr: 0.01000 - Train loss: 0.64336 - Test loss: 0.55445\n",
      "Epoch 9922 - lr: 0.01000 - Train loss: 0.64888 - Test loss: 0.55428\n",
      "Epoch 9923 - lr: 0.01000 - Train loss: 0.64770 - Test loss: 0.55413\n",
      "Epoch 9924 - lr: 0.01000 - Train loss: 0.64296 - Test loss: 0.55396\n",
      "Epoch 9925 - lr: 0.01000 - Train loss: 0.64465 - Test loss: 0.55416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9926 - lr: 0.01000 - Train loss: 0.64354 - Test loss: 0.55390\n",
      "Epoch 9927 - lr: 0.01000 - Train loss: 0.64397 - Test loss: 0.55409\n",
      "Epoch 9928 - lr: 0.01000 - Train loss: 0.64178 - Test loss: 0.55406\n",
      "Epoch 9929 - lr: 0.01000 - Train loss: 0.64399 - Test loss: 0.55430\n",
      "Epoch 9930 - lr: 0.01000 - Train loss: 0.64430 - Test loss: 0.55406\n",
      "Epoch 9931 - lr: 0.01000 - Train loss: 0.64395 - Test loss: 0.55432\n",
      "Epoch 9932 - lr: 0.01000 - Train loss: 0.64632 - Test loss: 0.55411\n",
      "Epoch 9933 - lr: 0.01000 - Train loss: 0.64312 - Test loss: 0.55413\n",
      "Epoch 9934 - lr: 0.01000 - Train loss: 0.64352 - Test loss: 0.55415\n",
      "Epoch 9935 - lr: 0.01000 - Train loss: 0.64184 - Test loss: 0.55408\n",
      "Epoch 9936 - lr: 0.01000 - Train loss: 0.64487 - Test loss: 0.55430\n",
      "Epoch 9937 - lr: 0.01000 - Train loss: 0.64487 - Test loss: 0.55403\n",
      "Epoch 9938 - lr: 0.01000 - Train loss: 0.64062 - Test loss: 0.55418\n",
      "Epoch 9939 - lr: 0.01000 - Train loss: 0.64539 - Test loss: 0.55439\n",
      "Epoch 9940 - lr: 0.01000 - Train loss: 0.64970 - Test loss: 0.55430\n",
      "Epoch 9941 - lr: 0.01000 - Train loss: 0.64980 - Test loss: 0.55418\n",
      "Epoch 9942 - lr: 0.01000 - Train loss: 0.65021 - Test loss: 0.55409\n",
      "Epoch 9943 - lr: 0.01000 - Train loss: 0.64721 - Test loss: 0.55387\n",
      "Epoch 9944 - lr: 0.01000 - Train loss: 0.64146 - Test loss: 0.55383\n",
      "Epoch 9945 - lr: 0.01000 - Train loss: 0.64099 - Test loss: 0.55400\n",
      "Epoch 9946 - lr: 0.01000 - Train loss: 0.64287 - Test loss: 0.55391\n",
      "Epoch 9947 - lr: 0.01000 - Train loss: 0.64371 - Test loss: 0.55414\n",
      "Epoch 9948 - lr: 0.01000 - Train loss: 0.64856 - Test loss: 0.55399\n",
      "Epoch 9949 - lr: 0.01000 - Train loss: 0.64356 - Test loss: 0.55387\n",
      "Epoch 9950 - lr: 0.01000 - Train loss: 0.64532 - Test loss: 0.55411\n",
      "Epoch 9951 - lr: 0.01000 - Train loss: 0.64507 - Test loss: 0.55386\n",
      "Epoch 9952 - lr: 0.01000 - Train loss: 0.63993 - Test loss: 0.55402\n",
      "Epoch 9953 - lr: 0.01000 - Train loss: 0.64569 - Test loss: 0.55432\n",
      "Epoch 9954 - lr: 0.01000 - Train loss: 0.64427 - Test loss: 0.55409\n",
      "Epoch 9955 - lr: 0.01000 - Train loss: 0.64282 - Test loss: 0.55427\n",
      "Epoch 9956 - lr: 0.01000 - Train loss: 0.64265 - Test loss: 0.55429\n",
      "Epoch 9957 - lr: 0.01000 - Train loss: 0.64477 - Test loss: 0.55438\n",
      "Epoch 9958 - lr: 0.01000 - Train loss: 0.64385 - Test loss: 0.55421\n",
      "Epoch 9959 - lr: 0.01000 - Train loss: 0.64547 - Test loss: 0.55437\n",
      "Epoch 9960 - lr: 0.01000 - Train loss: 0.64680 - Test loss: 0.55412\n",
      "Epoch 9961 - lr: 0.01000 - Train loss: 0.64173 - Test loss: 0.55409\n",
      "Epoch 9962 - lr: 0.01000 - Train loss: 0.64447 - Test loss: 0.55433\n",
      "Epoch 9963 - lr: 0.01000 - Train loss: 0.64318 - Test loss: 0.55410\n",
      "Epoch 9964 - lr: 0.01000 - Train loss: 0.63362 - Test loss: 0.55430\n",
      "Epoch 9965 - lr: 0.01000 - Train loss: 0.64378 - Test loss: 0.55455\n",
      "Epoch 9966 - lr: 0.01000 - Train loss: 0.64166 - Test loss: 0.55454\n",
      "Epoch 9967 - lr: 0.01000 - Train loss: 0.64384 - Test loss: 0.55477\n",
      "Epoch 9968 - lr: 0.01000 - Train loss: 0.64655 - Test loss: 0.55453\n",
      "Epoch 9969 - lr: 0.01000 - Train loss: 0.64235 - Test loss: 0.55449\n",
      "Epoch 9970 - lr: 0.01000 - Train loss: 0.64537 - Test loss: 0.55464\n",
      "Epoch 9971 - lr: 0.01000 - Train loss: 0.65034 - Test loss: 0.55454\n",
      "Epoch 9972 - lr: 0.01000 - Train loss: 0.64383 - Test loss: 0.55426\n",
      "Epoch 9973 - lr: 0.01000 - Train loss: 0.63864 - Test loss: 0.55441\n",
      "Epoch 9974 - lr: 0.01000 - Train loss: 0.64222 - Test loss: 0.55443\n",
      "Epoch 9975 - lr: 0.01000 - Train loss: 0.64555 - Test loss: 0.55465\n",
      "Epoch 9976 - lr: 0.01000 - Train loss: 0.64519 - Test loss: 0.55439\n",
      "Epoch 9977 - lr: 0.01000 - Train loss: 0.64566 - Test loss: 0.55458\n",
      "Epoch 9978 - lr: 0.01000 - Train loss: 0.64973 - Test loss: 0.55445\n",
      "Epoch 9979 - lr: 0.01000 - Train loss: 0.65050 - Test loss: 0.55438\n",
      "Epoch 9980 - lr: 0.01000 - Train loss: 0.64562 - Test loss: 0.55412\n",
      "Epoch 9981 - lr: 0.01000 - Train loss: 0.64528 - Test loss: 0.55425\n",
      "Epoch 9982 - lr: 0.01000 - Train loss: 0.64637 - Test loss: 0.55411\n",
      "Epoch 9983 - lr: 0.01000 - Train loss: 0.64129 - Test loss: 0.55405\n",
      "Epoch 9984 - lr: 0.01000 - Train loss: 0.64266 - Test loss: 0.55425\n",
      "Epoch 9985 - lr: 0.01000 - Train loss: 0.64931 - Test loss: 0.55417\n",
      "Epoch 9986 - lr: 0.01000 - Train loss: 0.64811 - Test loss: 0.55404\n",
      "Epoch 9987 - lr: 0.01000 - Train loss: 0.64196 - Test loss: 0.55391\n",
      "Epoch 9988 - lr: 0.01000 - Train loss: 0.63809 - Test loss: 0.55404\n",
      "Epoch 9989 - lr: 0.01000 - Train loss: 0.64265 - Test loss: 0.55410\n",
      "Epoch 9990 - lr: 0.01000 - Train loss: 0.64412 - Test loss: 0.55420\n",
      "Epoch 9991 - lr: 0.01000 - Train loss: 0.64112 - Test loss: 0.55412\n",
      "Epoch 9992 - lr: 0.01000 - Train loss: 0.63478 - Test loss: 0.55426\n",
      "Epoch 9993 - lr: 0.01000 - Train loss: 0.64065 - Test loss: 0.55449\n",
      "Epoch 9994 - lr: 0.01000 - Train loss: 0.64202 - Test loss: 0.55444\n",
      "Epoch 9995 - lr: 0.01000 - Train loss: 0.63952 - Test loss: 0.55461\n",
      "Epoch 9996 - lr: 0.01000 - Train loss: 0.64119 - Test loss: 0.55456\n",
      "Epoch 9997 - lr: 0.01000 - Train loss: 0.63781 - Test loss: 0.55470\n",
      "Epoch 9998 - lr: 0.01000 - Train loss: 0.64305 - Test loss: 0.55474\n",
      "Epoch 9999 - lr: 0.01000 - Train loss: 0.64250 - Test loss: 0.55472\n",
      "Epoch 10000 - lr: 0.01000 - Train loss: 0.64381 - Test loss: 0.55475\n",
      "Epoch 10001 - lr: 0.01000 - Train loss: 0.64131 - Test loss: 0.55462\n",
      "Epoch 10002 - lr: 0.01000 - Train loss: 0.63933 - Test loss: 0.55474\n",
      "Epoch 10003 - lr: 0.01000 - Train loss: 0.64129 - Test loss: 0.55467\n",
      "Epoch 10004 - lr: 0.01000 - Train loss: 0.64055 - Test loss: 0.55484\n",
      "Epoch 10005 - lr: 0.01000 - Train loss: 0.64319 - Test loss: 0.55472\n",
      "Epoch 10006 - lr: 0.01000 - Train loss: 0.64495 - Test loss: 0.55493\n",
      "Epoch 10007 - lr: 0.01000 - Train loss: 0.64255 - Test loss: 0.55468\n",
      "Epoch 10008 - lr: 0.01000 - Train loss: 0.63358 - Test loss: 0.55487\n",
      "Epoch 10009 - lr: 0.01000 - Train loss: 0.64219 - Test loss: 0.55510\n",
      "Epoch 10010 - lr: 0.01000 - Train loss: 0.64287 - Test loss: 0.55514\n",
      "Epoch 10011 - lr: 0.01000 - Train loss: 0.64242 - Test loss: 0.55511\n",
      "Epoch 10012 - lr: 0.01000 - Train loss: 0.64343 - Test loss: 0.55511\n",
      "Epoch 10013 - lr: 0.01000 - Train loss: 0.64142 - Test loss: 0.55497\n",
      "Epoch 10014 - lr: 0.01000 - Train loss: 0.64261 - Test loss: 0.55515\n",
      "Epoch 10015 - lr: 0.01000 - Train loss: 0.64875 - Test loss: 0.55502\n",
      "Epoch 10016 - lr: 0.01000 - Train loss: 0.64970 - Test loss: 0.55495\n",
      "Epoch 10017 - lr: 0.01000 - Train loss: 0.64284 - Test loss: 0.55470\n",
      "Epoch 10018 - lr: 0.01000 - Train loss: 0.64337 - Test loss: 0.55488\n",
      "Epoch 10019 - lr: 0.01000 - Train loss: 0.64135 - Test loss: 0.55485\n",
      "Epoch 10020 - lr: 0.01000 - Train loss: 0.64387 - Test loss: 0.55511\n",
      "Epoch 10021 - lr: 0.01000 - Train loss: 0.64228 - Test loss: 0.55488\n",
      "Epoch 10022 - lr: 0.01000 - Train loss: 0.64126 - Test loss: 0.55505\n",
      "Epoch 10023 - lr: 0.01000 - Train loss: 0.64383 - Test loss: 0.55515\n",
      "Epoch 10024 - lr: 0.01000 - Train loss: 0.64198 - Test loss: 0.55502\n",
      "Epoch 10025 - lr: 0.01000 - Train loss: 0.64323 - Test loss: 0.55523\n",
      "Epoch 10026 - lr: 0.01000 - Train loss: 0.64510 - Test loss: 0.55499\n",
      "Epoch 10027 - lr: 0.01000 - Train loss: 0.64401 - Test loss: 0.55506\n",
      "Epoch 10028 - lr: 0.01000 - Train loss: 0.64259 - Test loss: 0.55492\n",
      "Epoch 10029 - lr: 0.01000 - Train loss: 0.64447 - Test loss: 0.55515\n",
      "Epoch 10030 - lr: 0.01000 - Train loss: 0.64403 - Test loss: 0.55488\n",
      "Epoch 10031 - lr: 0.01000 - Train loss: 0.64104 - Test loss: 0.55504\n",
      "Epoch 10032 - lr: 0.01000 - Train loss: 0.64386 - Test loss: 0.55514\n",
      "Epoch 10033 - lr: 0.01000 - Train loss: 0.64241 - Test loss: 0.55502\n",
      "Epoch 10034 - lr: 0.01000 - Train loss: 0.64427 - Test loss: 0.55525\n",
      "Epoch 10035 - lr: 0.01000 - Train loss: 0.64412 - Test loss: 0.55498\n",
      "Epoch 10036 - lr: 0.01000 - Train loss: 0.64070 - Test loss: 0.55513\n",
      "Epoch 10037 - lr: 0.01000 - Train loss: 0.64422 - Test loss: 0.55527\n",
      "Epoch 10038 - lr: 0.01000 - Train loss: 0.64544 - Test loss: 0.55513\n",
      "Epoch 10039 - lr: 0.01000 - Train loss: 0.64121 - Test loss: 0.55504\n",
      "Epoch 10040 - lr: 0.01000 - Train loss: 0.64390 - Test loss: 0.55527\n",
      "Epoch 10041 - lr: 0.01000 - Train loss: 0.64261 - Test loss: 0.55502\n",
      "Epoch 10042 - lr: 0.01000 - Train loss: 0.64351 - Test loss: 0.55521\n",
      "Epoch 10043 - lr: 0.01000 - Train loss: 0.64120 - Test loss: 0.55517\n",
      "Epoch 10044 - lr: 0.01000 - Train loss: 0.64321 - Test loss: 0.55541\n",
      "Epoch 10045 - lr: 0.01000 - Train loss: 0.64380 - Test loss: 0.55517\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10046 - lr: 0.01000 - Train loss: 0.64412 - Test loss: 0.55544\n",
      "Epoch 10047 - lr: 0.01000 - Train loss: 0.64379 - Test loss: 0.55520\n",
      "Epoch 10048 - lr: 0.01000 - Train loss: 0.64208 - Test loss: 0.55538\n",
      "Epoch 10049 - lr: 0.01000 - Train loss: 0.64237 - Test loss: 0.55539\n",
      "Epoch 10050 - lr: 0.01000 - Train loss: 0.64289 - Test loss: 0.55539\n",
      "Epoch 10051 - lr: 0.01000 - Train loss: 0.64125 - Test loss: 0.55530\n",
      "Epoch 10052 - lr: 0.01000 - Train loss: 0.64411 - Test loss: 0.55552\n",
      "Epoch 10053 - lr: 0.01000 - Train loss: 0.64391 - Test loss: 0.55524\n",
      "Epoch 10054 - lr: 0.01000 - Train loss: 0.64090 - Test loss: 0.55539\n",
      "Epoch 10055 - lr: 0.01000 - Train loss: 0.64370 - Test loss: 0.55549\n",
      "Epoch 10056 - lr: 0.01000 - Train loss: 0.64260 - Test loss: 0.55534\n",
      "Epoch 10057 - lr: 0.01000 - Train loss: 0.64445 - Test loss: 0.55554\n",
      "Epoch 10058 - lr: 0.01000 - Train loss: 0.64207 - Test loss: 0.55528\n",
      "Epoch 10059 - lr: 0.01000 - Train loss: 0.63317 - Test loss: 0.55548\n",
      "Epoch 10060 - lr: 0.01000 - Train loss: 0.64207 - Test loss: 0.55570\n",
      "Epoch 10061 - lr: 0.01000 - Train loss: 0.64234 - Test loss: 0.55571\n",
      "Epoch 10062 - lr: 0.01000 - Train loss: 0.64270 - Test loss: 0.55571\n",
      "Epoch 10063 - lr: 0.01000 - Train loss: 0.64145 - Test loss: 0.55561\n",
      "Epoch 10064 - lr: 0.01000 - Train loss: 0.64429 - Test loss: 0.55577\n",
      "Epoch 10065 - lr: 0.01000 - Train loss: 0.64385 - Test loss: 0.55549\n",
      "Epoch 10066 - lr: 0.01000 - Train loss: 0.64448 - Test loss: 0.55570\n",
      "Epoch 10067 - lr: 0.01000 - Train loss: 0.64174 - Test loss: 0.55546\n",
      "Epoch 10068 - lr: 0.01000 - Train loss: 0.64163 - Test loss: 0.55563\n",
      "Epoch 10069 - lr: 0.01000 - Train loss: 0.64262 - Test loss: 0.55566\n",
      "Epoch 10070 - lr: 0.01000 - Train loss: 0.64154 - Test loss: 0.55561\n",
      "Epoch 10071 - lr: 0.01000 - Train loss: 0.64416 - Test loss: 0.55574\n",
      "Epoch 10072 - lr: 0.01000 - Train loss: 0.64860 - Test loss: 0.55562\n",
      "Epoch 10073 - lr: 0.01000 - Train loss: 0.64590 - Test loss: 0.55538\n",
      "Epoch 10074 - lr: 0.01000 - Train loss: 0.64116 - Test loss: 0.55531\n",
      "Epoch 10075 - lr: 0.01000 - Train loss: 0.64370 - Test loss: 0.55557\n",
      "Epoch 10076 - lr: 0.01000 - Train loss: 0.64292 - Test loss: 0.55533\n",
      "Epoch 10077 - lr: 0.01000 - Train loss: 0.64284 - Test loss: 0.55552\n",
      "Epoch 10078 - lr: 0.01000 - Train loss: 0.64123 - Test loss: 0.55550\n",
      "Epoch 10079 - lr: 0.01000 - Train loss: 0.64417 - Test loss: 0.55574\n",
      "Epoch 10080 - lr: 0.01000 - Train loss: 0.64335 - Test loss: 0.55548\n",
      "Epoch 10081 - lr: 0.01000 - Train loss: 0.64190 - Test loss: 0.55566\n",
      "Epoch 10082 - lr: 0.01000 - Train loss: 0.64204 - Test loss: 0.55566\n",
      "Epoch 10083 - lr: 0.01000 - Train loss: 0.64279 - Test loss: 0.55569\n",
      "Epoch 10084 - lr: 0.01000 - Train loss: 0.64106 - Test loss: 0.55558\n",
      "Epoch 10085 - lr: 0.01000 - Train loss: 0.64332 - Test loss: 0.55581\n",
      "Epoch 10086 - lr: 0.01000 - Train loss: 0.64187 - Test loss: 0.55556\n",
      "Epoch 10087 - lr: 0.01000 - Train loss: 0.64350 - Test loss: 0.55576\n",
      "Epoch 10088 - lr: 0.01000 - Train loss: 0.64117 - Test loss: 0.55569\n",
      "Epoch 10089 - lr: 0.01000 - Train loss: 0.64338 - Test loss: 0.55594\n",
      "Epoch 10090 - lr: 0.01000 - Train loss: 0.64241 - Test loss: 0.55569\n",
      "Epoch 10091 - lr: 0.01000 - Train loss: 0.64378 - Test loss: 0.55589\n",
      "Epoch 10092 - lr: 0.01000 - Train loss: 0.64129 - Test loss: 0.55580\n",
      "Epoch 10093 - lr: 0.01000 - Train loss: 0.64334 - Test loss: 0.55604\n",
      "Epoch 10094 - lr: 0.01000 - Train loss: 0.64234 - Test loss: 0.55578\n",
      "Epoch 10095 - lr: 0.01000 - Train loss: 0.64390 - Test loss: 0.55598\n",
      "Epoch 10096 - lr: 0.01000 - Train loss: 0.64136 - Test loss: 0.55588\n",
      "Epoch 10097 - lr: 0.01000 - Train loss: 0.64340 - Test loss: 0.55612\n",
      "Epoch 10098 - lr: 0.01000 - Train loss: 0.64285 - Test loss: 0.55584\n",
      "Epoch 10099 - lr: 0.01000 - Train loss: 0.64295 - Test loss: 0.55602\n",
      "Epoch 10100 - lr: 0.01000 - Train loss: 0.64120 - Test loss: 0.55595\n",
      "Epoch 10101 - lr: 0.01000 - Train loss: 0.64396 - Test loss: 0.55618\n",
      "Epoch 10102 - lr: 0.01000 - Train loss: 0.64332 - Test loss: 0.55589\n",
      "Epoch 10103 - lr: 0.01000 - Train loss: 0.64151 - Test loss: 0.55605\n",
      "Epoch 10104 - lr: 0.01000 - Train loss: 0.64226 - Test loss: 0.55606\n",
      "Epoch 10105 - lr: 0.01000 - Train loss: 0.64163 - Test loss: 0.55601\n",
      "Epoch 10106 - lr: 0.01000 - Train loss: 0.64316 - Test loss: 0.55604\n",
      "Epoch 10107 - lr: 0.01000 - Train loss: 0.64308 - Test loss: 0.55587\n",
      "Epoch 10108 - lr: 0.01000 - Train loss: 0.64293 - Test loss: 0.55589\n",
      "Epoch 10109 - lr: 0.01000 - Train loss: 0.64223 - Test loss: 0.55574\n",
      "Epoch 10110 - lr: 0.01000 - Train loss: 0.64363 - Test loss: 0.55588\n",
      "Epoch 10111 - lr: 0.01000 - Train loss: 0.64816 - Test loss: 0.55579\n",
      "Epoch 10112 - lr: 0.01000 - Train loss: 0.64335 - Test loss: 0.55556\n",
      "Epoch 10113 - lr: 0.01000 - Train loss: 0.64389 - Test loss: 0.55583\n",
      "Epoch 10114 - lr: 0.01000 - Train loss: 0.64283 - Test loss: 0.55562\n",
      "Epoch 10115 - lr: 0.01000 - Train loss: 0.64216 - Test loss: 0.55584\n",
      "Epoch 10116 - lr: 0.01000 - Train loss: 0.64133 - Test loss: 0.55584\n",
      "Epoch 10117 - lr: 0.01000 - Train loss: 0.64376 - Test loss: 0.55600\n",
      "Epoch 10118 - lr: 0.01000 - Train loss: 0.64857 - Test loss: 0.55595\n",
      "Epoch 10119 - lr: 0.01000 - Train loss: 0.64176 - Test loss: 0.55570\n",
      "Epoch 10120 - lr: 0.01000 - Train loss: 0.64311 - Test loss: 0.55590\n",
      "Epoch 10121 - lr: 0.01000 - Train loss: 0.64089 - Test loss: 0.55584\n",
      "Epoch 10122 - lr: 0.01000 - Train loss: 0.64317 - Test loss: 0.55611\n",
      "Epoch 10123 - lr: 0.01000 - Train loss: 0.64212 - Test loss: 0.55587\n",
      "Epoch 10124 - lr: 0.01000 - Train loss: 0.64359 - Test loss: 0.55608\n",
      "Epoch 10125 - lr: 0.01000 - Train loss: 0.64105 - Test loss: 0.55599\n",
      "Epoch 10126 - lr: 0.01000 - Train loss: 0.64296 - Test loss: 0.55625\n",
      "Epoch 10127 - lr: 0.01000 - Train loss: 0.64139 - Test loss: 0.55601\n",
      "Epoch 10128 - lr: 0.01000 - Train loss: 0.64238 - Test loss: 0.55619\n",
      "Epoch 10129 - lr: 0.01000 - Train loss: 0.64120 - Test loss: 0.55615\n",
      "Epoch 10130 - lr: 0.01000 - Train loss: 0.64396 - Test loss: 0.55633\n",
      "Epoch 10131 - lr: 0.01000 - Train loss: 0.64498 - Test loss: 0.55608\n",
      "Epoch 10132 - lr: 0.01000 - Train loss: 0.64157 - Test loss: 0.55603\n",
      "Epoch 10133 - lr: 0.01000 - Train loss: 0.64313 - Test loss: 0.55610\n",
      "Epoch 10134 - lr: 0.01000 - Train loss: 0.64299 - Test loss: 0.55593\n",
      "Epoch 10135 - lr: 0.01000 - Train loss: 0.64292 - Test loss: 0.55597\n",
      "Epoch 10136 - lr: 0.01000 - Train loss: 0.64229 - Test loss: 0.55582\n",
      "Epoch 10137 - lr: 0.01000 - Train loss: 0.64354 - Test loss: 0.55596\n",
      "Epoch 10138 - lr: 0.01000 - Train loss: 0.64834 - Test loss: 0.55590\n",
      "Epoch 10139 - lr: 0.01000 - Train loss: 0.64140 - Test loss: 0.55568\n",
      "Epoch 10140 - lr: 0.01000 - Train loss: 0.64182 - Test loss: 0.55588\n",
      "Epoch 10141 - lr: 0.01000 - Train loss: 0.64125 - Test loss: 0.55590\n",
      "Epoch 10142 - lr: 0.01000 - Train loss: 0.64337 - Test loss: 0.55604\n",
      "Epoch 10143 - lr: 0.01000 - Train loss: 0.64639 - Test loss: 0.55593\n",
      "Epoch 10144 - lr: 0.01000 - Train loss: 0.64446 - Test loss: 0.55578\n",
      "Epoch 10145 - lr: 0.01000 - Train loss: 0.64064 - Test loss: 0.55571\n",
      "Epoch 10146 - lr: 0.01000 - Train loss: 0.64342 - Test loss: 0.55596\n",
      "Epoch 10147 - lr: 0.01000 - Train loss: 0.64217 - Test loss: 0.55574\n",
      "Epoch 10148 - lr: 0.01000 - Train loss: 0.64248 - Test loss: 0.55596\n",
      "Epoch 10149 - lr: 0.01000 - Train loss: 0.64078 - Test loss: 0.55594\n",
      "Epoch 10150 - lr: 0.01000 - Train loss: 0.64354 - Test loss: 0.55622\n",
      "Epoch 10151 - lr: 0.01000 - Train loss: 0.64331 - Test loss: 0.55597\n",
      "Epoch 10152 - lr: 0.01000 - Train loss: 0.63988 - Test loss: 0.55616\n",
      "Epoch 10153 - lr: 0.01000 - Train loss: 0.64343 - Test loss: 0.55631\n",
      "Epoch 10154 - lr: 0.01000 - Train loss: 0.64640 - Test loss: 0.55620\n",
      "Epoch 10155 - lr: 0.01000 - Train loss: 0.64468 - Test loss: 0.55604\n",
      "Epoch 10156 - lr: 0.01000 - Train loss: 0.64057 - Test loss: 0.55593\n",
      "Epoch 10157 - lr: 0.01000 - Train loss: 0.64294 - Test loss: 0.55618\n",
      "Epoch 10158 - lr: 0.01000 - Train loss: 0.64197 - Test loss: 0.55595\n",
      "Epoch 10159 - lr: 0.01000 - Train loss: 0.64291 - Test loss: 0.55616\n",
      "Epoch 10160 - lr: 0.01000 - Train loss: 0.64071 - Test loss: 0.55611\n",
      "Epoch 10161 - lr: 0.01000 - Train loss: 0.64302 - Test loss: 0.55639\n",
      "Epoch 10162 - lr: 0.01000 - Train loss: 0.64311 - Test loss: 0.55614\n",
      "Epoch 10163 - lr: 0.01000 - Train loss: 0.64045 - Test loss: 0.55632\n",
      "Epoch 10164 - lr: 0.01000 - Train loss: 0.64268 - Test loss: 0.55640\n",
      "Epoch 10165 - lr: 0.01000 - Train loss: 0.64128 - Test loss: 0.55628\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10166 - lr: 0.01000 - Train loss: 0.64338 - Test loss: 0.55652\n",
      "Epoch 10167 - lr: 0.01000 - Train loss: 0.64411 - Test loss: 0.55622\n",
      "Epoch 10168 - lr: 0.01000 - Train loss: 0.63632 - Test loss: 0.55639\n",
      "Epoch 10169 - lr: 0.01000 - Train loss: 0.63731 - Test loss: 0.55661\n",
      "Epoch 10170 - lr: 0.01000 - Train loss: 0.64131 - Test loss: 0.55659\n",
      "Epoch 10171 - lr: 0.01000 - Train loss: 0.64334 - Test loss: 0.55671\n",
      "Epoch 10172 - lr: 0.01000 - Train loss: 0.64748 - Test loss: 0.55660\n",
      "Epoch 10173 - lr: 0.01000 - Train loss: 0.64636 - Test loss: 0.55638\n",
      "Epoch 10174 - lr: 0.01000 - Train loss: 0.64323 - Test loss: 0.55621\n",
      "Epoch 10175 - lr: 0.01000 - Train loss: 0.64207 - Test loss: 0.55621\n",
      "Epoch 10176 - lr: 0.01000 - Train loss: 0.64052 - Test loss: 0.55612\n",
      "Epoch 10177 - lr: 0.01000 - Train loss: 0.64309 - Test loss: 0.55637\n",
      "Epoch 10178 - lr: 0.01000 - Train loss: 0.64322 - Test loss: 0.55611\n",
      "Epoch 10179 - lr: 0.01000 - Train loss: 0.63906 - Test loss: 0.55630\n",
      "Epoch 10180 - lr: 0.01000 - Train loss: 0.64362 - Test loss: 0.55654\n",
      "Epoch 10181 - lr: 0.01000 - Train loss: 0.64578 - Test loss: 0.55637\n",
      "Epoch 10182 - lr: 0.01000 - Train loss: 0.64122 - Test loss: 0.55625\n",
      "Epoch 10183 - lr: 0.01000 - Train loss: 0.64318 - Test loss: 0.55652\n",
      "Epoch 10184 - lr: 0.01000 - Train loss: 0.64461 - Test loss: 0.55624\n",
      "Epoch 10185 - lr: 0.01000 - Train loss: 0.63371 - Test loss: 0.55644\n",
      "Epoch 10186 - lr: 0.01000 - Train loss: 0.63394 - Test loss: 0.55669\n",
      "Epoch 10187 - lr: 0.01000 - Train loss: 0.63297 - Test loss: 0.55695\n",
      "Epoch 10188 - lr: 0.01000 - Train loss: 0.63880 - Test loss: 0.55717\n",
      "Epoch 10189 - lr: 0.01000 - Train loss: 0.64380 - Test loss: 0.55742\n",
      "Epoch 10190 - lr: 0.01000 - Train loss: 0.64533 - Test loss: 0.55706\n",
      "Epoch 10191 - lr: 0.01000 - Train loss: 0.63187 - Test loss: 0.55723\n",
      "Epoch 10192 - lr: 0.01000 - Train loss: 0.64381 - Test loss: 0.55744\n",
      "Epoch 10193 - lr: 0.01000 - Train loss: 0.64187 - Test loss: 0.55725\n",
      "Epoch 10194 - lr: 0.01000 - Train loss: 0.64341 - Test loss: 0.55734\n",
      "Epoch 10195 - lr: 0.01000 - Train loss: 0.64723 - Test loss: 0.55714\n",
      "Epoch 10196 - lr: 0.01000 - Train loss: 0.64773 - Test loss: 0.55697\n",
      "Epoch 10197 - lr: 0.01000 - Train loss: 0.64404 - Test loss: 0.55668\n",
      "Epoch 10198 - lr: 0.01000 - Train loss: 0.64235 - Test loss: 0.55669\n",
      "Epoch 10199 - lr: 0.01000 - Train loss: 0.64064 - Test loss: 0.55656\n",
      "Epoch 10200 - lr: 0.01000 - Train loss: 0.64282 - Test loss: 0.55680\n",
      "Epoch 10201 - lr: 0.01000 - Train loss: 0.64356 - Test loss: 0.55651\n",
      "Epoch 10202 - lr: 0.01000 - Train loss: 0.63757 - Test loss: 0.55668\n",
      "Epoch 10203 - lr: 0.01000 - Train loss: 0.64247 - Test loss: 0.55699\n",
      "Epoch 10204 - lr: 0.01000 - Train loss: 0.64111 - Test loss: 0.55677\n",
      "Epoch 10205 - lr: 0.01000 - Train loss: 0.64387 - Test loss: 0.55699\n",
      "Epoch 10206 - lr: 0.01000 - Train loss: 0.64147 - Test loss: 0.55685\n",
      "Epoch 10207 - lr: 0.01000 - Train loss: 0.64347 - Test loss: 0.55707\n",
      "Epoch 10208 - lr: 0.01000 - Train loss: 0.64111 - Test loss: 0.55680\n",
      "Epoch 10209 - lr: 0.01000 - Train loss: 0.64356 - Test loss: 0.55699\n",
      "Epoch 10210 - lr: 0.01000 - Train loss: 0.64111 - Test loss: 0.55686\n",
      "Epoch 10211 - lr: 0.01000 - Train loss: 0.64322 - Test loss: 0.55710\n",
      "Epoch 10212 - lr: 0.01000 - Train loss: 0.64476 - Test loss: 0.55677\n",
      "Epoch 10213 - lr: 0.01000 - Train loss: 0.63261 - Test loss: 0.55697\n",
      "Epoch 10214 - lr: 0.01000 - Train loss: 0.63915 - Test loss: 0.55717\n",
      "Epoch 10215 - lr: 0.01000 - Train loss: 0.64352 - Test loss: 0.55737\n",
      "Epoch 10216 - lr: 0.01000 - Train loss: 0.64680 - Test loss: 0.55719\n",
      "Epoch 10217 - lr: 0.01000 - Train loss: 0.64687 - Test loss: 0.55704\n",
      "Epoch 10218 - lr: 0.01000 - Train loss: 0.64783 - Test loss: 0.55693\n",
      "Epoch 10219 - lr: 0.01000 - Train loss: 0.64236 - Test loss: 0.55664\n",
      "Epoch 10220 - lr: 0.01000 - Train loss: 0.64157 - Test loss: 0.55688\n",
      "Epoch 10221 - lr: 0.01000 - Train loss: 0.64741 - Test loss: 0.55677\n",
      "Epoch 10222 - lr: 0.01000 - Train loss: 0.64819 - Test loss: 0.55670\n",
      "Epoch 10223 - lr: 0.01000 - Train loss: 0.64288 - Test loss: 0.55645\n",
      "Epoch 10224 - lr: 0.01000 - Train loss: 0.64335 - Test loss: 0.55674\n",
      "Epoch 10225 - lr: 0.01000 - Train loss: 0.64508 - Test loss: 0.55647\n",
      "Epoch 10226 - lr: 0.01000 - Train loss: 0.63220 - Test loss: 0.55670\n",
      "Epoch 10227 - lr: 0.01000 - Train loss: 0.64169 - Test loss: 0.55694\n",
      "Epoch 10228 - lr: 0.01000 - Train loss: 0.64068 - Test loss: 0.55691\n",
      "Epoch 10229 - lr: 0.01000 - Train loss: 0.64378 - Test loss: 0.55712\n",
      "Epoch 10230 - lr: 0.01000 - Train loss: 0.64260 - Test loss: 0.55685\n",
      "Epoch 10231 - lr: 0.01000 - Train loss: 0.64227 - Test loss: 0.55710\n",
      "Epoch 10232 - lr: 0.01000 - Train loss: 0.64389 - Test loss: 0.55686\n",
      "Epoch 10233 - lr: 0.01000 - Train loss: 0.64346 - Test loss: 0.55696\n",
      "Epoch 10234 - lr: 0.01000 - Train loss: 0.64510 - Test loss: 0.55678\n",
      "Epoch 10235 - lr: 0.01000 - Train loss: 0.63985 - Test loss: 0.55664\n",
      "Epoch 10236 - lr: 0.01000 - Train loss: 0.63872 - Test loss: 0.55681\n",
      "Epoch 10237 - lr: 0.01000 - Train loss: 0.64071 - Test loss: 0.55667\n",
      "Epoch 10238 - lr: 0.01000 - Train loss: 0.64113 - Test loss: 0.55691\n",
      "Epoch 10239 - lr: 0.01000 - Train loss: 0.64859 - Test loss: 0.55686\n",
      "Epoch 10240 - lr: 0.01000 - Train loss: 0.64281 - Test loss: 0.55659\n",
      "Epoch 10241 - lr: 0.01000 - Train loss: 0.64018 - Test loss: 0.55678\n",
      "Epoch 10242 - lr: 0.01000 - Train loss: 0.64225 - Test loss: 0.55685\n",
      "Epoch 10243 - lr: 0.01000 - Train loss: 0.63985 - Test loss: 0.55675\n",
      "Epoch 10244 - lr: 0.01000 - Train loss: 0.63918 - Test loss: 0.55695\n",
      "Epoch 10245 - lr: 0.01000 - Train loss: 0.64194 - Test loss: 0.55679\n",
      "Epoch 10246 - lr: 0.01000 - Train loss: 0.64358 - Test loss: 0.55701\n",
      "Epoch 10247 - lr: 0.01000 - Train loss: 0.64234 - Test loss: 0.55676\n",
      "Epoch 10248 - lr: 0.01000 - Train loss: 0.64118 - Test loss: 0.55702\n",
      "Epoch 10249 - lr: 0.01000 - Train loss: 0.64868 - Test loss: 0.55699\n",
      "Epoch 10250 - lr: 0.01000 - Train loss: 0.64346 - Test loss: 0.55671\n",
      "Epoch 10251 - lr: 0.01000 - Train loss: 0.63798 - Test loss: 0.55689\n",
      "Epoch 10252 - lr: 0.01000 - Train loss: 0.64368 - Test loss: 0.55721\n",
      "Epoch 10253 - lr: 0.01000 - Train loss: 0.64703 - Test loss: 0.55689\n",
      "Epoch 10254 - lr: 0.01000 - Train loss: 0.63470 - Test loss: 0.55708\n",
      "Epoch 10255 - lr: 0.01000 - Train loss: 0.64404 - Test loss: 0.55736\n",
      "Epoch 10256 - lr: 0.01000 - Train loss: 0.64364 - Test loss: 0.55708\n",
      "Epoch 10257 - lr: 0.01000 - Train loss: 0.63804 - Test loss: 0.55724\n",
      "Epoch 10258 - lr: 0.01000 - Train loss: 0.64382 - Test loss: 0.55753\n",
      "Epoch 10259 - lr: 0.01000 - Train loss: 0.64840 - Test loss: 0.55716\n",
      "Epoch 10260 - lr: 0.01000 - Train loss: 0.63997 - Test loss: 0.55736\n",
      "Epoch 10261 - lr: 0.01000 - Train loss: 0.64366 - Test loss: 0.55716\n",
      "Epoch 10262 - lr: 0.01000 - Train loss: 0.64173 - Test loss: 0.55713\n",
      "Epoch 10263 - lr: 0.01000 - Train loss: 0.64011 - Test loss: 0.55704\n",
      "Epoch 10264 - lr: 0.01000 - Train loss: 0.64335 - Test loss: 0.55728\n",
      "Epoch 10265 - lr: 0.01000 - Train loss: 0.64385 - Test loss: 0.55697\n",
      "Epoch 10266 - lr: 0.01000 - Train loss: 0.63601 - Test loss: 0.55714\n",
      "Epoch 10267 - lr: 0.01000 - Train loss: 0.63803 - Test loss: 0.55738\n",
      "Epoch 10268 - lr: 0.01000 - Train loss: 0.63988 - Test loss: 0.55729\n",
      "Epoch 10269 - lr: 0.01000 - Train loss: 0.63790 - Test loss: 0.55748\n",
      "Epoch 10270 - lr: 0.01000 - Train loss: 0.63990 - Test loss: 0.55736\n",
      "Epoch 10271 - lr: 0.01000 - Train loss: 0.63875 - Test loss: 0.55755\n",
      "Epoch 10272 - lr: 0.01000 - Train loss: 0.64095 - Test loss: 0.55737\n",
      "Epoch 10273 - lr: 0.01000 - Train loss: 0.64214 - Test loss: 0.55761\n",
      "Epoch 10274 - lr: 0.01000 - Train loss: 0.64214 - Test loss: 0.55734\n",
      "Epoch 10275 - lr: 0.01000 - Train loss: 0.63970 - Test loss: 0.55755\n",
      "Epoch 10276 - lr: 0.01000 - Train loss: 0.64348 - Test loss: 0.55738\n",
      "Epoch 10277 - lr: 0.01000 - Train loss: 0.64134 - Test loss: 0.55735\n",
      "Epoch 10278 - lr: 0.01000 - Train loss: 0.64061 - Test loss: 0.55728\n",
      "Epoch 10279 - lr: 0.01000 - Train loss: 0.64270 - Test loss: 0.55736\n",
      "Epoch 10280 - lr: 0.01000 - Train loss: 0.64314 - Test loss: 0.55716\n",
      "Epoch 10281 - lr: 0.01000 - Train loss: 0.64111 - Test loss: 0.55713\n",
      "Epoch 10282 - lr: 0.01000 - Train loss: 0.64056 - Test loss: 0.55709\n",
      "Epoch 10283 - lr: 0.01000 - Train loss: 0.64220 - Test loss: 0.55717\n",
      "Epoch 10284 - lr: 0.01000 - Train loss: 0.64089 - Test loss: 0.55702\n",
      "Epoch 10285 - lr: 0.01000 - Train loss: 0.64272 - Test loss: 0.55729\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10286 - lr: 0.01000 - Train loss: 0.64163 - Test loss: 0.55706\n",
      "Epoch 10287 - lr: 0.01000 - Train loss: 0.64126 - Test loss: 0.55729\n",
      "Epoch 10288 - lr: 0.01000 - Train loss: 0.64012 - Test loss: 0.55728\n",
      "Epoch 10289 - lr: 0.01000 - Train loss: 0.64314 - Test loss: 0.55754\n",
      "Epoch 10290 - lr: 0.01000 - Train loss: 0.64157 - Test loss: 0.55731\n",
      "Epoch 10291 - lr: 0.01000 - Train loss: 0.63710 - Test loss: 0.55752\n",
      "Epoch 10292 - lr: 0.01000 - Train loss: 0.64007 - Test loss: 0.55748\n",
      "Epoch 10293 - lr: 0.01000 - Train loss: 0.64283 - Test loss: 0.55778\n",
      "Epoch 10294 - lr: 0.01000 - Train loss: 0.64484 - Test loss: 0.55746\n",
      "Epoch 10295 - lr: 0.01000 - Train loss: 0.63107 - Test loss: 0.55770\n",
      "Epoch 10296 - lr: 0.01000 - Train loss: 0.64363 - Test loss: 0.55796\n",
      "Epoch 10297 - lr: 0.01000 - Train loss: 0.64092 - Test loss: 0.55780\n",
      "Epoch 10298 - lr: 0.01000 - Train loss: 0.64289 - Test loss: 0.55806\n",
      "Epoch 10299 - lr: 0.01000 - Train loss: 0.64545 - Test loss: 0.55769\n",
      "Epoch 10300 - lr: 0.01000 - Train loss: 0.63053 - Test loss: 0.55791\n",
      "Epoch 10301 - lr: 0.01000 - Train loss: 0.64210 - Test loss: 0.55813\n",
      "Epoch 10302 - lr: 0.01000 - Train loss: 0.64006 - Test loss: 0.55802\n",
      "Epoch 10303 - lr: 0.01000 - Train loss: 0.64256 - Test loss: 0.55827\n",
      "Epoch 10304 - lr: 0.01000 - Train loss: 0.64501 - Test loss: 0.55790\n",
      "Epoch 10305 - lr: 0.01000 - Train loss: 0.63060 - Test loss: 0.55811\n",
      "Epoch 10306 - lr: 0.01000 - Train loss: 0.64467 - Test loss: 0.55837\n",
      "Epoch 10307 - lr: 0.01000 - Train loss: 0.64330 - Test loss: 0.55817\n",
      "Epoch 10308 - lr: 0.01000 - Train loss: 0.64048 - Test loss: 0.55806\n",
      "Epoch 10309 - lr: 0.01000 - Train loss: 0.64234 - Test loss: 0.55810\n",
      "Epoch 10310 - lr: 0.01000 - Train loss: 0.64383 - Test loss: 0.55788\n",
      "Epoch 10311 - lr: 0.01000 - Train loss: 0.63981 - Test loss: 0.55773\n",
      "Epoch 10312 - lr: 0.01000 - Train loss: 0.64222 - Test loss: 0.55797\n",
      "Epoch 10313 - lr: 0.01000 - Train loss: 0.64175 - Test loss: 0.55769\n",
      "Epoch 10314 - lr: 0.01000 - Train loss: 0.64076 - Test loss: 0.55789\n",
      "Epoch 10315 - lr: 0.01000 - Train loss: 0.64041 - Test loss: 0.55787\n",
      "Epoch 10316 - lr: 0.01000 - Train loss: 0.64210 - Test loss: 0.55796\n",
      "Epoch 10317 - lr: 0.01000 - Train loss: 0.64288 - Test loss: 0.55778\n",
      "Epoch 10318 - lr: 0.01000 - Train loss: 0.64028 - Test loss: 0.55772\n",
      "Epoch 10319 - lr: 0.01000 - Train loss: 0.64184 - Test loss: 0.55779\n",
      "Epoch 10320 - lr: 0.01000 - Train loss: 0.64196 - Test loss: 0.55762\n",
      "Epoch 10321 - lr: 0.01000 - Train loss: 0.64130 - Test loss: 0.55766\n",
      "Epoch 10322 - lr: 0.01000 - Train loss: 0.63998 - Test loss: 0.55755\n",
      "Epoch 10323 - lr: 0.01000 - Train loss: 0.64221 - Test loss: 0.55782\n",
      "Epoch 10324 - lr: 0.01000 - Train loss: 0.64068 - Test loss: 0.55760\n",
      "Epoch 10325 - lr: 0.01000 - Train loss: 0.64202 - Test loss: 0.55785\n",
      "Epoch 10326 - lr: 0.01000 - Train loss: 0.64005 - Test loss: 0.55779\n",
      "Epoch 10327 - lr: 0.01000 - Train loss: 0.64239 - Test loss: 0.55807\n",
      "Epoch 10328 - lr: 0.01000 - Train loss: 0.64103 - Test loss: 0.55783\n",
      "Epoch 10329 - lr: 0.01000 - Train loss: 0.64192 - Test loss: 0.55807\n",
      "Epoch 10330 - lr: 0.01000 - Train loss: 0.64006 - Test loss: 0.55798\n",
      "Epoch 10331 - lr: 0.01000 - Train loss: 0.64245 - Test loss: 0.55824\n",
      "Epoch 10332 - lr: 0.01000 - Train loss: 0.64068 - Test loss: 0.55798\n",
      "Epoch 10333 - lr: 0.01000 - Train loss: 0.64262 - Test loss: 0.55821\n",
      "Epoch 10334 - lr: 0.01000 - Train loss: 0.64056 - Test loss: 0.55809\n",
      "Epoch 10335 - lr: 0.01000 - Train loss: 0.64252 - Test loss: 0.55830\n",
      "Epoch 10336 - lr: 0.01000 - Train loss: 0.64064 - Test loss: 0.55804\n",
      "Epoch 10337 - lr: 0.01000 - Train loss: 0.63327 - Test loss: 0.55825\n",
      "Epoch 10338 - lr: 0.01000 - Train loss: 0.64124 - Test loss: 0.55856\n",
      "Epoch 10339 - lr: 0.01000 - Train loss: 0.64043 - Test loss: 0.55832\n",
      "Epoch 10340 - lr: 0.01000 - Train loss: 0.63019 - Test loss: 0.55857\n",
      "Epoch 10341 - lr: 0.01000 - Train loss: 0.64063 - Test loss: 0.55879\n",
      "Epoch 10342 - lr: 0.01000 - Train loss: 0.64084 - Test loss: 0.55872\n",
      "Epoch 10343 - lr: 0.01000 - Train loss: 0.64100 - Test loss: 0.55865\n",
      "Epoch 10344 - lr: 0.01000 - Train loss: 0.64029 - Test loss: 0.55850\n",
      "Epoch 10345 - lr: 0.01000 - Train loss: 0.64185 - Test loss: 0.55852\n",
      "Epoch 10346 - lr: 0.01000 - Train loss: 0.64420 - Test loss: 0.55832\n",
      "Epoch 10347 - lr: 0.01000 - Train loss: 0.64206 - Test loss: 0.55811\n",
      "Epoch 10348 - lr: 0.01000 - Train loss: 0.64046 - Test loss: 0.55805\n",
      "Epoch 10349 - lr: 0.01000 - Train loss: 0.64037 - Test loss: 0.55800\n",
      "Epoch 10350 - lr: 0.01000 - Train loss: 0.64048 - Test loss: 0.55797\n",
      "Epoch 10351 - lr: 0.01000 - Train loss: 0.64017 - Test loss: 0.55793\n",
      "Epoch 10352 - lr: 0.01000 - Train loss: 0.64086 - Test loss: 0.55796\n",
      "Epoch 10353 - lr: 0.01000 - Train loss: 0.64021 - Test loss: 0.55787\n",
      "Epoch 10354 - lr: 0.01000 - Train loss: 0.64163 - Test loss: 0.55801\n",
      "Epoch 10355 - lr: 0.01000 - Train loss: 0.64590 - Test loss: 0.55798\n",
      "Epoch 10356 - lr: 0.01000 - Train loss: 0.63927 - Test loss: 0.55779\n",
      "Epoch 10357 - lr: 0.01000 - Train loss: 0.63595 - Test loss: 0.55802\n",
      "Epoch 10358 - lr: 0.01000 - Train loss: 0.63934 - Test loss: 0.55835\n",
      "Epoch 10359 - lr: 0.01000 - Train loss: 0.64651 - Test loss: 0.55836\n",
      "Epoch 10360 - lr: 0.01000 - Train loss: 0.64373 - Test loss: 0.55807\n",
      "Epoch 10361 - lr: 0.01000 - Train loss: 0.63114 - Test loss: 0.55833\n",
      "Epoch 10362 - lr: 0.01000 - Train loss: 0.64060 - Test loss: 0.55859\n",
      "Epoch 10363 - lr: 0.01000 - Train loss: 0.64043 - Test loss: 0.55855\n",
      "Epoch 10364 - lr: 0.01000 - Train loss: 0.64110 - Test loss: 0.55855\n",
      "Epoch 10365 - lr: 0.01000 - Train loss: 0.64065 - Test loss: 0.55839\n",
      "Epoch 10366 - lr: 0.01000 - Train loss: 0.64148 - Test loss: 0.55844\n",
      "Epoch 10367 - lr: 0.01000 - Train loss: 0.64408 - Test loss: 0.55829\n",
      "Epoch 10368 - lr: 0.01000 - Train loss: 0.64382 - Test loss: 0.55814\n",
      "Epoch 10369 - lr: 0.01000 - Train loss: 0.64266 - Test loss: 0.55798\n",
      "Epoch 10370 - lr: 0.01000 - Train loss: 0.63999 - Test loss: 0.55788\n",
      "Epoch 10371 - lr: 0.01000 - Train loss: 0.64157 - Test loss: 0.55803\n",
      "Epoch 10372 - lr: 0.01000 - Train loss: 0.64598 - Test loss: 0.55800\n",
      "Epoch 10373 - lr: 0.01000 - Train loss: 0.64080 - Test loss: 0.55778\n",
      "Epoch 10374 - lr: 0.01000 - Train loss: 0.64053 - Test loss: 0.55804\n",
      "Epoch 10375 - lr: 0.01000 - Train loss: 0.63999 - Test loss: 0.55804\n",
      "Epoch 10376 - lr: 0.01000 - Train loss: 0.64130 - Test loss: 0.55815\n",
      "Epoch 10377 - lr: 0.01000 - Train loss: 0.64298 - Test loss: 0.55804\n",
      "Epoch 10378 - lr: 0.01000 - Train loss: 0.64037 - Test loss: 0.55793\n",
      "Epoch 10379 - lr: 0.01000 - Train loss: 0.64139 - Test loss: 0.55806\n",
      "Epoch 10380 - lr: 0.01000 - Train loss: 0.64488 - Test loss: 0.55799\n",
      "Epoch 10381 - lr: 0.01000 - Train loss: 0.64588 - Test loss: 0.55795\n",
      "Epoch 10382 - lr: 0.01000 - Train loss: 0.63926 - Test loss: 0.55776\n",
      "Epoch 10383 - lr: 0.01000 - Train loss: 0.63356 - Test loss: 0.55803\n",
      "Epoch 10384 - lr: 0.01000 - Train loss: 0.62948 - Test loss: 0.55839\n",
      "Epoch 10385 - lr: 0.01000 - Train loss: 0.64280 - Test loss: 0.55872\n",
      "Epoch 10386 - lr: 0.01000 - Train loss: 0.64210 - Test loss: 0.55861\n",
      "Epoch 10387 - lr: 0.01000 - Train loss: 0.64018 - Test loss: 0.55853\n",
      "Epoch 10388 - lr: 0.01000 - Train loss: 0.64094 - Test loss: 0.55853\n",
      "Epoch 10389 - lr: 0.01000 - Train loss: 0.64090 - Test loss: 0.55837\n",
      "Epoch 10390 - lr: 0.01000 - Train loss: 0.64075 - Test loss: 0.55836\n",
      "Epoch 10391 - lr: 0.01000 - Train loss: 0.64058 - Test loss: 0.55822\n",
      "Epoch 10392 - lr: 0.01000 - Train loss: 0.64080 - Test loss: 0.55826\n",
      "Epoch 10393 - lr: 0.01000 - Train loss: 0.64130 - Test loss: 0.55813\n",
      "Epoch 10394 - lr: 0.01000 - Train loss: 0.64008 - Test loss: 0.55810\n",
      "Epoch 10395 - lr: 0.01000 - Train loss: 0.64012 - Test loss: 0.55809\n",
      "Epoch 10396 - lr: 0.01000 - Train loss: 0.64004 - Test loss: 0.55808\n",
      "Epoch 10397 - lr: 0.01000 - Train loss: 0.64016 - Test loss: 0.55808\n",
      "Epoch 10398 - lr: 0.01000 - Train loss: 0.64001 - Test loss: 0.55806\n",
      "Epoch 10399 - lr: 0.01000 - Train loss: 0.64026 - Test loss: 0.55809\n",
      "Epoch 10400 - lr: 0.01000 - Train loss: 0.64019 - Test loss: 0.55805\n",
      "Epoch 10401 - lr: 0.01000 - Train loss: 0.64028 - Test loss: 0.55809\n",
      "Epoch 10402 - lr: 0.01000 - Train loss: 0.64040 - Test loss: 0.55804\n",
      "Epoch 10403 - lr: 0.01000 - Train loss: 0.64016 - Test loss: 0.55807\n",
      "Epoch 10404 - lr: 0.01000 - Train loss: 0.64020 - Test loss: 0.55804\n",
      "Epoch 10405 - lr: 0.01000 - Train loss: 0.64014 - Test loss: 0.55807\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10406 - lr: 0.01000 - Train loss: 0.64022 - Test loss: 0.55805\n",
      "Epoch 10407 - lr: 0.01000 - Train loss: 0.64013 - Test loss: 0.55807\n",
      "Epoch 10408 - lr: 0.01000 - Train loss: 0.64022 - Test loss: 0.55806\n",
      "Epoch 10409 - lr: 0.01000 - Train loss: 0.64017 - Test loss: 0.55807\n",
      "Epoch 10410 - lr: 0.01000 - Train loss: 0.64024 - Test loss: 0.55806\n",
      "Epoch 10411 - lr: 0.01000 - Train loss: 0.64025 - Test loss: 0.55807\n",
      "Epoch 10412 - lr: 0.01000 - Train loss: 0.64031 - Test loss: 0.55806\n",
      "Epoch 10413 - lr: 0.01000 - Train loss: 0.64035 - Test loss: 0.55806\n",
      "Epoch 10414 - lr: 0.01000 - Train loss: 0.64041 - Test loss: 0.55806\n",
      "Epoch 10415 - lr: 0.01000 - Train loss: 0.64048 - Test loss: 0.55805\n",
      "Epoch 10416 - lr: 0.01000 - Train loss: 0.64057 - Test loss: 0.55805\n",
      "Epoch 10417 - lr: 0.01000 - Train loss: 0.64068 - Test loss: 0.55804\n",
      "Epoch 10418 - lr: 0.01000 - Train loss: 0.64083 - Test loss: 0.55803\n",
      "Epoch 10419 - lr: 0.01000 - Train loss: 0.64106 - Test loss: 0.55802\n",
      "Epoch 10420 - lr: 0.01000 - Train loss: 0.64144 - Test loss: 0.55801\n",
      "Epoch 10421 - lr: 0.01000 - Train loss: 0.64225 - Test loss: 0.55800\n",
      "Epoch 10422 - lr: 0.01000 - Train loss: 0.64399 - Test loss: 0.55805\n",
      "Epoch 10423 - lr: 0.01000 - Train loss: 0.64354 - Test loss: 0.55781\n",
      "Epoch 10424 - lr: 0.01000 - Train loss: 0.62883 - Test loss: 0.55818\n",
      "Epoch 10425 - lr: 0.01000 - Train loss: 0.64052 - Test loss: 0.55852\n",
      "Epoch 10426 - lr: 0.01000 - Train loss: 0.64097 - Test loss: 0.55855\n",
      "Epoch 10427 - lr: 0.01000 - Train loss: 0.64138 - Test loss: 0.55852\n",
      "Epoch 10428 - lr: 0.01000 - Train loss: 0.64207 - Test loss: 0.55846\n",
      "Epoch 10429 - lr: 0.01000 - Train loss: 0.64369 - Test loss: 0.55845\n",
      "Epoch 10430 - lr: 0.01000 - Train loss: 0.63747 - Test loss: 0.55827\n",
      "Epoch 10431 - lr: 0.01000 - Train loss: 0.64192 - Test loss: 0.55857\n",
      "Epoch 10432 - lr: 0.01000 - Train loss: 0.64299 - Test loss: 0.55856\n",
      "Epoch 10433 - lr: 0.01000 - Train loss: 0.64425 - Test loss: 0.55853\n",
      "Epoch 10434 - lr: 0.01000 - Train loss: 0.63816 - Test loss: 0.55835\n",
      "Epoch 10435 - lr: 0.01000 - Train loss: 0.64380 - Test loss: 0.55869\n",
      "Epoch 10436 - lr: 0.01000 - Train loss: 0.64453 - Test loss: 0.55868\n",
      "Epoch 10437 - lr: 0.01000 - Train loss: 0.63848 - Test loss: 0.55849\n",
      "Epoch 10438 - lr: 0.01000 - Train loss: 0.64404 - Test loss: 0.55883\n",
      "Epoch 10439 - lr: 0.01000 - Train loss: 0.64480 - Test loss: 0.55880\n",
      "Epoch 10440 - lr: 0.01000 - Train loss: 0.63907 - Test loss: 0.55858\n",
      "Epoch 10441 - lr: 0.01000 - Train loss: 0.64364 - Test loss: 0.55888\n",
      "Epoch 10442 - lr: 0.01000 - Train loss: 0.64522 - Test loss: 0.55888\n",
      "Epoch 10443 - lr: 0.01000 - Train loss: 0.64957 - Test loss: 0.55849\n",
      "Epoch 10444 - lr: 0.01000 - Train loss: 0.64134 - Test loss: 0.55864\n",
      "Epoch 10445 - lr: 0.01000 - Train loss: 0.64392 - Test loss: 0.55849\n",
      "Epoch 10446 - lr: 0.01000 - Train loss: 0.64401 - Test loss: 0.55840\n",
      "Epoch 10447 - lr: 0.01000 - Train loss: 0.64530 - Test loss: 0.55837\n",
      "Epoch 10448 - lr: 0.01000 - Train loss: 0.63985 - Test loss: 0.55815\n",
      "Epoch 10449 - lr: 0.01000 - Train loss: 0.64165 - Test loss: 0.55842\n",
      "Epoch 10450 - lr: 0.01000 - Train loss: 0.64045 - Test loss: 0.55834\n",
      "Epoch 10451 - lr: 0.01000 - Train loss: 0.64078 - Test loss: 0.55844\n",
      "Epoch 10452 - lr: 0.01000 - Train loss: 0.64179 - Test loss: 0.55832\n",
      "Epoch 10453 - lr: 0.01000 - Train loss: 0.63943 - Test loss: 0.55825\n",
      "Epoch 10454 - lr: 0.01000 - Train loss: 0.64098 - Test loss: 0.55839\n",
      "Epoch 10455 - lr: 0.01000 - Train loss: 0.64472 - Test loss: 0.55835\n",
      "Epoch 10456 - lr: 0.01000 - Train loss: 0.64192 - Test loss: 0.55817\n",
      "Epoch 10457 - lr: 0.01000 - Train loss: 0.64019 - Test loss: 0.55822\n",
      "Epoch 10458 - lr: 0.01000 - Train loss: 0.63937 - Test loss: 0.55819\n",
      "Epoch 10459 - lr: 0.01000 - Train loss: 0.64107 - Test loss: 0.55837\n",
      "Epoch 10460 - lr: 0.01000 - Train loss: 0.64536 - Test loss: 0.55838\n",
      "Epoch 10461 - lr: 0.01000 - Train loss: 0.64305 - Test loss: 0.55812\n",
      "Epoch 10462 - lr: 0.01000 - Train loss: 0.62974 - Test loss: 0.55845\n",
      "Epoch 10463 - lr: 0.01000 - Train loss: 0.64315 - Test loss: 0.55882\n",
      "Epoch 10464 - lr: 0.01000 - Train loss: 0.64391 - Test loss: 0.55877\n",
      "Epoch 10465 - lr: 0.01000 - Train loss: 0.64476 - Test loss: 0.55870\n",
      "Epoch 10466 - lr: 0.01000 - Train loss: 0.64439 - Test loss: 0.55857\n",
      "Epoch 10467 - lr: 0.01000 - Train loss: 0.64475 - Test loss: 0.55849\n",
      "Epoch 10468 - lr: 0.01000 - Train loss: 0.64587 - Test loss: 0.55845\n",
      "Epoch 10469 - lr: 0.01000 - Train loss: 0.64257 - Test loss: 0.55819\n",
      "Epoch 10470 - lr: 0.01000 - Train loss: 0.63212 - Test loss: 0.55847\n",
      "Epoch 10471 - lr: 0.01000 - Train loss: 0.63213 - Test loss: 0.55879\n",
      "Epoch 10472 - lr: 0.01000 - Train loss: 0.63230 - Test loss: 0.55910\n",
      "Epoch 10473 - lr: 0.01000 - Train loss: 0.63154 - Test loss: 0.55941\n",
      "Epoch 10474 - lr: 0.01000 - Train loss: 0.63638 - Test loss: 0.55965\n",
      "Epoch 10475 - lr: 0.01000 - Train loss: 0.64115 - Test loss: 0.55997\n",
      "Epoch 10476 - lr: 0.01000 - Train loss: 0.65569 - Test loss: 0.55952\n",
      "Epoch 10477 - lr: 0.01000 - Train loss: 0.64099 - Test loss: 0.55927\n",
      "Epoch 10478 - lr: 0.01000 - Train loss: 0.64043 - Test loss: 0.55921\n",
      "Epoch 10479 - lr: 0.01000 - Train loss: 0.63953 - Test loss: 0.55903\n",
      "Epoch 10480 - lr: 0.01000 - Train loss: 0.64142 - Test loss: 0.55918\n",
      "Epoch 10481 - lr: 0.01000 - Train loss: 0.64309 - Test loss: 0.55896\n",
      "Epoch 10482 - lr: 0.01000 - Train loss: 0.63952 - Test loss: 0.55883\n",
      "Epoch 10483 - lr: 0.01000 - Train loss: 0.64145 - Test loss: 0.55904\n",
      "Epoch 10484 - lr: 0.01000 - Train loss: 0.64071 - Test loss: 0.55883\n",
      "Epoch 10485 - lr: 0.01000 - Train loss: 0.64148 - Test loss: 0.55913\n",
      "Epoch 10486 - lr: 0.01000 - Train loss: 0.64953 - Test loss: 0.55879\n",
      "Epoch 10487 - lr: 0.01000 - Train loss: 0.64149 - Test loss: 0.55897\n",
      "Epoch 10488 - lr: 0.01000 - Train loss: 0.64581 - Test loss: 0.55893\n",
      "Epoch 10489 - lr: 0.01000 - Train loss: 0.63894 - Test loss: 0.55873\n",
      "Epoch 10490 - lr: 0.01000 - Train loss: 0.64068 - Test loss: 0.55898\n",
      "Epoch 10491 - lr: 0.01000 - Train loss: 0.63899 - Test loss: 0.55892\n",
      "Epoch 10492 - lr: 0.01000 - Train loss: 0.64156 - Test loss: 0.55923\n",
      "Epoch 10493 - lr: 0.01000 - Train loss: 0.64707 - Test loss: 0.55888\n",
      "Epoch 10494 - lr: 0.01000 - Train loss: 0.63988 - Test loss: 0.55919\n",
      "Epoch 10495 - lr: 0.01000 - Train loss: 0.64241 - Test loss: 0.55900\n",
      "Epoch 10496 - lr: 0.01000 - Train loss: 0.64010 - Test loss: 0.55901\n",
      "Epoch 10497 - lr: 0.01000 - Train loss: 0.63924 - Test loss: 0.55897\n",
      "Epoch 10498 - lr: 0.01000 - Train loss: 0.64126 - Test loss: 0.55910\n",
      "Epoch 10499 - lr: 0.01000 - Train loss: 0.64448 - Test loss: 0.55899\n",
      "Epoch 10500 - lr: 0.01000 - Train loss: 0.64471 - Test loss: 0.55887\n",
      "Epoch 10501 - lr: 0.01000 - Train loss: 0.64550 - Test loss: 0.55880\n",
      "Epoch 10502 - lr: 0.01000 - Train loss: 0.64424 - Test loss: 0.55865\n",
      "Epoch 10503 - lr: 0.01000 - Train loss: 0.64033 - Test loss: 0.55852\n",
      "Epoch 10504 - lr: 0.01000 - Train loss: 0.64167 - Test loss: 0.55876\n",
      "Epoch 10505 - lr: 0.01000 - Train loss: 0.64525 - Test loss: 0.55869\n",
      "Epoch 10506 - lr: 0.01000 - Train loss: 0.64594 - Test loss: 0.55866\n",
      "Epoch 10507 - lr: 0.01000 - Train loss: 0.64372 - Test loss: 0.55852\n",
      "Epoch 10508 - lr: 0.01000 - Train loss: 0.63839 - Test loss: 0.55844\n",
      "Epoch 10509 - lr: 0.01000 - Train loss: 0.63775 - Test loss: 0.55876\n",
      "Epoch 10510 - lr: 0.01000 - Train loss: 0.64106 - Test loss: 0.55865\n",
      "Epoch 10511 - lr: 0.01000 - Train loss: 0.64114 - Test loss: 0.55879\n",
      "Epoch 10512 - lr: 0.01000 - Train loss: 0.64094 - Test loss: 0.55865\n",
      "Epoch 10513 - lr: 0.01000 - Train loss: 0.64101 - Test loss: 0.55878\n",
      "Epoch 10514 - lr: 0.01000 - Train loss: 0.64055 - Test loss: 0.55864\n",
      "Epoch 10515 - lr: 0.01000 - Train loss: 0.64141 - Test loss: 0.55882\n",
      "Epoch 10516 - lr: 0.01000 - Train loss: 0.64533 - Test loss: 0.55877\n",
      "Epoch 10517 - lr: 0.01000 - Train loss: 0.64642 - Test loss: 0.55875\n",
      "Epoch 10518 - lr: 0.01000 - Train loss: 0.64138 - Test loss: 0.55854\n",
      "Epoch 10519 - lr: 0.01000 - Train loss: 0.63681 - Test loss: 0.55880\n",
      "Epoch 10520 - lr: 0.01000 - Train loss: 0.64207 - Test loss: 0.55914\n",
      "Epoch 10521 - lr: 0.01000 - Train loss: 0.63946 - Test loss: 0.55898\n",
      "Epoch 10522 - lr: 0.01000 - Train loss: 0.64052 - Test loss: 0.55924\n",
      "Epoch 10523 - lr: 0.01000 - Train loss: 0.63845 - Test loss: 0.55918\n",
      "Epoch 10524 - lr: 0.01000 - Train loss: 0.64033 - Test loss: 0.55951\n",
      "Epoch 10525 - lr: 0.01000 - Train loss: 0.64072 - Test loss: 0.55930\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10526 - lr: 0.01000 - Train loss: 0.64004 - Test loss: 0.55961\n",
      "Epoch 10527 - lr: 0.01000 - Train loss: 0.64521 - Test loss: 0.55947\n",
      "Epoch 10528 - lr: 0.01000 - Train loss: 0.64270 - Test loss: 0.55927\n",
      "Epoch 10529 - lr: 0.01000 - Train loss: 0.63860 - Test loss: 0.55918\n",
      "Epoch 10530 - lr: 0.01000 - Train loss: 0.64169 - Test loss: 0.55948\n",
      "Epoch 10531 - lr: 0.01000 - Train loss: 0.64804 - Test loss: 0.55912\n",
      "Epoch 10532 - lr: 0.01000 - Train loss: 0.64185 - Test loss: 0.55944\n",
      "Epoch 10533 - lr: 0.01000 - Train loss: 0.64760 - Test loss: 0.55910\n",
      "Epoch 10534 - lr: 0.01000 - Train loss: 0.64132 - Test loss: 0.55943\n",
      "Epoch 10535 - lr: 0.01000 - Train loss: 0.64160 - Test loss: 0.55919\n",
      "Epoch 10536 - lr: 0.01000 - Train loss: 0.63723 - Test loss: 0.55942\n",
      "Epoch 10537 - lr: 0.01000 - Train loss: 0.64230 - Test loss: 0.55967\n",
      "Epoch 10538 - lr: 0.01000 - Train loss: 0.64573 - Test loss: 0.55954\n",
      "Epoch 10539 - lr: 0.01000 - Train loss: 0.64432 - Test loss: 0.55936\n",
      "Epoch 10540 - lr: 0.01000 - Train loss: 0.63847 - Test loss: 0.55917\n",
      "Epoch 10541 - lr: 0.01000 - Train loss: 0.63429 - Test loss: 0.55941\n",
      "Epoch 10542 - lr: 0.01000 - Train loss: 0.64097 - Test loss: 0.55949\n",
      "Epoch 10543 - lr: 0.01000 - Train loss: 0.63823 - Test loss: 0.55934\n",
      "Epoch 10544 - lr: 0.01000 - Train loss: 0.63449 - Test loss: 0.55959\n",
      "Epoch 10545 - lr: 0.01000 - Train loss: 0.64039 - Test loss: 0.55963\n",
      "Epoch 10546 - lr: 0.01000 - Train loss: 0.63825 - Test loss: 0.55954\n",
      "Epoch 10547 - lr: 0.01000 - Train loss: 0.63987 - Test loss: 0.55984\n",
      "Epoch 10548 - lr: 0.01000 - Train loss: 0.64572 - Test loss: 0.55972\n",
      "Epoch 10549 - lr: 0.01000 - Train loss: 0.64597 - Test loss: 0.55960\n",
      "Epoch 10550 - lr: 0.01000 - Train loss: 0.64703 - Test loss: 0.55954\n",
      "Epoch 10551 - lr: 0.01000 - Train loss: 0.64214 - Test loss: 0.55926\n",
      "Epoch 10552 - lr: 0.01000 - Train loss: 0.63443 - Test loss: 0.55949\n",
      "Epoch 10553 - lr: 0.01000 - Train loss: 0.63648 - Test loss: 0.55980\n",
      "Epoch 10554 - lr: 0.01000 - Train loss: 0.63815 - Test loss: 0.55969\n",
      "Epoch 10555 - lr: 0.01000 - Train loss: 0.63456 - Test loss: 0.55995\n",
      "Epoch 10556 - lr: 0.01000 - Train loss: 0.64027 - Test loss: 0.55996\n",
      "Epoch 10557 - lr: 0.01000 - Train loss: 0.63852 - Test loss: 0.55987\n",
      "Epoch 10558 - lr: 0.01000 - Train loss: 0.64162 - Test loss: 0.56017\n",
      "Epoch 10559 - lr: 0.01000 - Train loss: 0.64697 - Test loss: 0.55979\n",
      "Epoch 10560 - lr: 0.01000 - Train loss: 0.63991 - Test loss: 0.56007\n",
      "Epoch 10561 - lr: 0.01000 - Train loss: 0.64659 - Test loss: 0.55998\n",
      "Epoch 10562 - lr: 0.01000 - Train loss: 0.64509 - Test loss: 0.55980\n",
      "Epoch 10563 - lr: 0.01000 - Train loss: 0.63989 - Test loss: 0.55958\n",
      "Epoch 10564 - lr: 0.01000 - Train loss: 0.64124 - Test loss: 0.55991\n",
      "Epoch 10565 - lr: 0.01000 - Train loss: 0.64045 - Test loss: 0.55967\n",
      "Epoch 10566 - lr: 0.01000 - Train loss: 0.63968 - Test loss: 0.55989\n",
      "Epoch 10567 - lr: 0.01000 - Train loss: 0.63868 - Test loss: 0.55985\n",
      "Epoch 10568 - lr: 0.01000 - Train loss: 0.64216 - Test loss: 0.56015\n",
      "Epoch 10569 - lr: 0.01000 - Train loss: 0.64247 - Test loss: 0.55985\n",
      "Epoch 10570 - lr: 0.01000 - Train loss: 0.63330 - Test loss: 0.56007\n",
      "Epoch 10571 - lr: 0.01000 - Train loss: 0.63071 - Test loss: 0.56036\n",
      "Epoch 10572 - lr: 0.01000 - Train loss: 0.62972 - Test loss: 0.56067\n",
      "Epoch 10573 - lr: 0.01000 - Train loss: 0.63720 - Test loss: 0.56088\n",
      "Epoch 10574 - lr: 0.01000 - Train loss: 0.64246 - Test loss: 0.56109\n",
      "Epoch 10575 - lr: 0.01000 - Train loss: 0.64146 - Test loss: 0.56077\n",
      "Epoch 10576 - lr: 0.01000 - Train loss: 0.64240 - Test loss: 0.56100\n",
      "Epoch 10577 - lr: 0.01000 - Train loss: 0.65000 - Test loss: 0.56053\n",
      "Epoch 10578 - lr: 0.01000 - Train loss: 0.64195 - Test loss: 0.56060\n",
      "Epoch 10579 - lr: 0.01000 - Train loss: 0.64584 - Test loss: 0.56040\n",
      "Epoch 10580 - lr: 0.01000 - Train loss: 0.64591 - Test loss: 0.56021\n",
      "Epoch 10581 - lr: 0.01000 - Train loss: 0.64605 - Test loss: 0.56004\n",
      "Epoch 10582 - lr: 0.01000 - Train loss: 0.64657 - Test loss: 0.55991\n",
      "Epoch 10583 - lr: 0.01000 - Train loss: 0.64688 - Test loss: 0.55980\n",
      "Epoch 10584 - lr: 0.01000 - Train loss: 0.64597 - Test loss: 0.55967\n",
      "Epoch 10585 - lr: 0.01000 - Train loss: 0.64272 - Test loss: 0.55948\n",
      "Epoch 10586 - lr: 0.01000 - Train loss: 0.63861 - Test loss: 0.55946\n",
      "Epoch 10587 - lr: 0.01000 - Train loss: 0.64227 - Test loss: 0.55978\n",
      "Epoch 10588 - lr: 0.01000 - Train loss: 0.64086 - Test loss: 0.55959\n",
      "Epoch 10589 - lr: 0.01000 - Train loss: 0.63800 - Test loss: 0.55990\n",
      "Epoch 10590 - lr: 0.01000 - Train loss: 0.63913 - Test loss: 0.55974\n",
      "Epoch 10591 - lr: 0.01000 - Train loss: 0.63586 - Test loss: 0.56003\n",
      "Epoch 10592 - lr: 0.01000 - Train loss: 0.63785 - Test loss: 0.55995\n",
      "Epoch 10593 - lr: 0.01000 - Train loss: 0.63528 - Test loss: 0.56023\n",
      "Epoch 10594 - lr: 0.01000 - Train loss: 0.63851 - Test loss: 0.56017\n",
      "Epoch 10595 - lr: 0.01000 - Train loss: 0.64183 - Test loss: 0.56051\n",
      "Epoch 10596 - lr: 0.01000 - Train loss: 0.64519 - Test loss: 0.56017\n",
      "Epoch 10597 - lr: 0.01000 - Train loss: 0.63341 - Test loss: 0.56040\n",
      "Epoch 10598 - lr: 0.01000 - Train loss: 0.64246 - Test loss: 0.56064\n",
      "Epoch 10599 - lr: 0.01000 - Train loss: 0.64713 - Test loss: 0.56054\n",
      "Epoch 10600 - lr: 0.01000 - Train loss: 0.64467 - Test loss: 0.56032\n",
      "Epoch 10601 - lr: 0.01000 - Train loss: 0.63781 - Test loss: 0.56011\n",
      "Epoch 10602 - lr: 0.01000 - Train loss: 0.62968 - Test loss: 0.56038\n",
      "Epoch 10603 - lr: 0.01000 - Train loss: 0.63819 - Test loss: 0.56060\n",
      "Epoch 10604 - lr: 0.01000 - Train loss: 0.64088 - Test loss: 0.56067\n",
      "Epoch 10605 - lr: 0.01000 - Train loss: 0.63773 - Test loss: 0.56046\n",
      "Epoch 10606 - lr: 0.01000 - Train loss: 0.63041 - Test loss: 0.56070\n",
      "Epoch 10607 - lr: 0.01000 - Train loss: 0.62946 - Test loss: 0.56098\n",
      "Epoch 10608 - lr: 0.01000 - Train loss: 0.64004 - Test loss: 0.56121\n",
      "Epoch 10609 - lr: 0.01000 - Train loss: 0.63834 - Test loss: 0.56107\n",
      "Epoch 10610 - lr: 0.01000 - Train loss: 0.64104 - Test loss: 0.56135\n",
      "Epoch 10611 - lr: 0.01000 - Train loss: 0.64006 - Test loss: 0.56103\n",
      "Epoch 10612 - lr: 0.01000 - Train loss: 0.64048 - Test loss: 0.56119\n",
      "Epoch 10613 - lr: 0.01000 - Train loss: 0.63791 - Test loss: 0.56101\n",
      "Epoch 10614 - lr: 0.01000 - Train loss: 0.63735 - Test loss: 0.56124\n",
      "Epoch 10615 - lr: 0.01000 - Train loss: 0.63940 - Test loss: 0.56096\n",
      "Epoch 10616 - lr: 0.01000 - Train loss: 0.64022 - Test loss: 0.56124\n",
      "Epoch 10617 - lr: 0.01000 - Train loss: 0.64359 - Test loss: 0.56099\n",
      "Epoch 10618 - lr: 0.01000 - Train loss: 0.63788 - Test loss: 0.56081\n",
      "Epoch 10619 - lr: 0.01000 - Train loss: 0.63708 - Test loss: 0.56105\n",
      "Epoch 10620 - lr: 0.01000 - Train loss: 0.63880 - Test loss: 0.56082\n",
      "Epoch 10621 - lr: 0.01000 - Train loss: 0.63786 - Test loss: 0.56108\n",
      "Epoch 10622 - lr: 0.01000 - Train loss: 0.64136 - Test loss: 0.56085\n",
      "Epoch 10623 - lr: 0.01000 - Train loss: 0.64031 - Test loss: 0.56087\n",
      "Epoch 10624 - lr: 0.01000 - Train loss: 0.63770 - Test loss: 0.56068\n",
      "Epoch 10625 - lr: 0.01000 - Train loss: 0.63480 - Test loss: 0.56092\n",
      "Epoch 10626 - lr: 0.01000 - Train loss: 0.63860 - Test loss: 0.56086\n",
      "Epoch 10627 - lr: 0.01000 - Train loss: 0.64157 - Test loss: 0.56108\n",
      "Epoch 10628 - lr: 0.01000 - Train loss: 0.64551 - Test loss: 0.56095\n",
      "Epoch 10629 - lr: 0.01000 - Train loss: 0.64605 - Test loss: 0.56084\n",
      "Epoch 10630 - lr: 0.01000 - Train loss: 0.64556 - Test loss: 0.56070\n",
      "Epoch 10631 - lr: 0.01000 - Train loss: 0.64523 - Test loss: 0.56055\n",
      "Epoch 10632 - lr: 0.01000 - Train loss: 0.64348 - Test loss: 0.56037\n",
      "Epoch 10633 - lr: 0.01000 - Train loss: 0.63748 - Test loss: 0.56022\n",
      "Epoch 10634 - lr: 0.01000 - Train loss: 0.63182 - Test loss: 0.56051\n",
      "Epoch 10635 - lr: 0.01000 - Train loss: 0.64017 - Test loss: 0.56088\n",
      "Epoch 10636 - lr: 0.01000 - Train loss: 0.64376 - Test loss: 0.56074\n",
      "Epoch 10637 - lr: 0.01000 - Train loss: 0.63760 - Test loss: 0.56060\n",
      "Epoch 10638 - lr: 0.01000 - Train loss: 0.63302 - Test loss: 0.56088\n",
      "Epoch 10639 - lr: 0.01000 - Train loss: 0.64187 - Test loss: 0.56112\n",
      "Epoch 10640 - lr: 0.01000 - Train loss: 0.64640 - Test loss: 0.56103\n",
      "Epoch 10641 - lr: 0.01000 - Train loss: 0.64457 - Test loss: 0.56084\n",
      "Epoch 10642 - lr: 0.01000 - Train loss: 0.63844 - Test loss: 0.56061\n",
      "Epoch 10643 - lr: 0.01000 - Train loss: 0.63494 - Test loss: 0.56087\n",
      "Epoch 10644 - lr: 0.01000 - Train loss: 0.63828 - Test loss: 0.56081\n",
      "Epoch 10645 - lr: 0.01000 - Train loss: 0.64166 - Test loss: 0.56113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10646 - lr: 0.01000 - Train loss: 0.64358 - Test loss: 0.56081\n",
      "Epoch 10647 - lr: 0.01000 - Train loss: 0.62926 - Test loss: 0.56109\n",
      "Epoch 10648 - lr: 0.01000 - Train loss: 0.63922 - Test loss: 0.56134\n",
      "Epoch 10649 - lr: 0.01000 - Train loss: 0.63857 - Test loss: 0.56127\n",
      "Epoch 10650 - lr: 0.01000 - Train loss: 0.64180 - Test loss: 0.56148\n",
      "Epoch 10651 - lr: 0.01000 - Train loss: 0.64448 - Test loss: 0.56126\n",
      "Epoch 10652 - lr: 0.01000 - Train loss: 0.63878 - Test loss: 0.56100\n",
      "Epoch 10653 - lr: 0.01000 - Train loss: 0.63789 - Test loss: 0.56126\n",
      "Epoch 10654 - lr: 0.01000 - Train loss: 0.64160 - Test loss: 0.56103\n",
      "Epoch 10655 - lr: 0.01000 - Train loss: 0.63940 - Test loss: 0.56100\n",
      "Epoch 10656 - lr: 0.01000 - Train loss: 0.63846 - Test loss: 0.56094\n",
      "Epoch 10657 - lr: 0.01000 - Train loss: 0.64128 - Test loss: 0.56111\n",
      "Epoch 10658 - lr: 0.01000 - Train loss: 0.64572 - Test loss: 0.56100\n",
      "Epoch 10659 - lr: 0.01000 - Train loss: 0.64644 - Test loss: 0.56092\n",
      "Epoch 10660 - lr: 0.01000 - Train loss: 0.64075 - Test loss: 0.56070\n",
      "Epoch 10661 - lr: 0.01000 - Train loss: 0.64082 - Test loss: 0.56105\n",
      "Epoch 10662 - lr: 0.01000 - Train loss: 0.63947 - Test loss: 0.56086\n",
      "Epoch 10663 - lr: 0.01000 - Train loss: 0.63126 - Test loss: 0.56114\n",
      "Epoch 10664 - lr: 0.01000 - Train loss: 0.63300 - Test loss: 0.56142\n",
      "Epoch 10665 - lr: 0.01000 - Train loss: 0.63077 - Test loss: 0.56172\n",
      "Epoch 10666 - lr: 0.01000 - Train loss: 0.63291 - Test loss: 0.56198\n",
      "Epoch 10667 - lr: 0.01000 - Train loss: 0.64227 - Test loss: 0.56221\n",
      "Epoch 10668 - lr: 0.01000 - Train loss: 0.63938 - Test loss: 0.56190\n",
      "Epoch 10669 - lr: 0.01000 - Train loss: 0.63013 - Test loss: 0.56209\n",
      "Epoch 10670 - lr: 0.01000 - Train loss: 0.63981 - Test loss: 0.56226\n",
      "Epoch 10671 - lr: 0.01000 - Train loss: 0.63814 - Test loss: 0.56206\n",
      "Epoch 10672 - lr: 0.01000 - Train loss: 0.64096 - Test loss: 0.56231\n",
      "Epoch 10673 - lr: 0.01000 - Train loss: 0.64384 - Test loss: 0.56186\n",
      "Epoch 10674 - lr: 0.01000 - Train loss: 0.62905 - Test loss: 0.56207\n",
      "Epoch 10675 - lr: 0.01000 - Train loss: 0.63845 - Test loss: 0.56223\n",
      "Epoch 10676 - lr: 0.01000 - Train loss: 0.63967 - Test loss: 0.56216\n",
      "Epoch 10677 - lr: 0.01000 - Train loss: 0.63832 - Test loss: 0.56198\n",
      "Epoch 10678 - lr: 0.01000 - Train loss: 0.64159 - Test loss: 0.56216\n",
      "Epoch 10679 - lr: 0.01000 - Train loss: 0.63926 - Test loss: 0.56184\n",
      "Epoch 10680 - lr: 0.01000 - Train loss: 0.62886 - Test loss: 0.56208\n",
      "Epoch 10681 - lr: 0.01000 - Train loss: 0.64032 - Test loss: 0.56229\n",
      "Epoch 10682 - lr: 0.01000 - Train loss: 0.63780 - Test loss: 0.56209\n",
      "Epoch 10683 - lr: 0.01000 - Train loss: 0.63844 - Test loss: 0.56234\n",
      "Epoch 10684 - lr: 0.01000 - Train loss: 0.64590 - Test loss: 0.56218\n",
      "Epoch 10685 - lr: 0.01000 - Train loss: 0.64299 - Test loss: 0.56189\n",
      "Epoch 10686 - lr: 0.01000 - Train loss: 0.63771 - Test loss: 0.56170\n",
      "Epoch 10687 - lr: 0.01000 - Train loss: 0.63895 - Test loss: 0.56197\n",
      "Epoch 10688 - lr: 0.01000 - Train loss: 0.64647 - Test loss: 0.56188\n",
      "Epoch 10689 - lr: 0.01000 - Train loss: 0.64071 - Test loss: 0.56157\n",
      "Epoch 10690 - lr: 0.01000 - Train loss: 0.63718 - Test loss: 0.56176\n",
      "Epoch 10691 - lr: 0.01000 - Train loss: 0.64084 - Test loss: 0.56187\n",
      "Epoch 10692 - lr: 0.01000 - Train loss: 0.64056 - Test loss: 0.56163\n",
      "Epoch 10693 - lr: 0.01000 - Train loss: 0.64064 - Test loss: 0.56171\n",
      "Epoch 10694 - lr: 0.01000 - Train loss: 0.64007 - Test loss: 0.56146\n",
      "Epoch 10695 - lr: 0.01000 - Train loss: 0.64108 - Test loss: 0.56163\n",
      "Epoch 10696 - lr: 0.01000 - Train loss: 0.64595 - Test loss: 0.56155\n",
      "Epoch 10697 - lr: 0.01000 - Train loss: 0.64145 - Test loss: 0.56132\n",
      "Epoch 10698 - lr: 0.01000 - Train loss: 0.64097 - Test loss: 0.56146\n",
      "Epoch 10699 - lr: 0.01000 - Train loss: 0.64176 - Test loss: 0.56127\n",
      "Epoch 10700 - lr: 0.01000 - Train loss: 0.63797 - Test loss: 0.56122\n",
      "Epoch 10701 - lr: 0.01000 - Train loss: 0.64130 - Test loss: 0.56150\n",
      "Epoch 10702 - lr: 0.01000 - Train loss: 0.64220 - Test loss: 0.56132\n",
      "Epoch 10703 - lr: 0.01000 - Train loss: 0.63873 - Test loss: 0.56131\n",
      "Epoch 10704 - lr: 0.01000 - Train loss: 0.63990 - Test loss: 0.56140\n",
      "Epoch 10705 - lr: 0.01000 - Train loss: 0.63720 - Test loss: 0.56126\n",
      "Epoch 10706 - lr: 0.01000 - Train loss: 0.63327 - Test loss: 0.56155\n",
      "Epoch 10707 - lr: 0.01000 - Train loss: 0.64037 - Test loss: 0.56167\n",
      "Epoch 10708 - lr: 0.01000 - Train loss: 0.63840 - Test loss: 0.56149\n",
      "Epoch 10709 - lr: 0.01000 - Train loss: 0.63883 - Test loss: 0.56182\n",
      "Epoch 10710 - lr: 0.01000 - Train loss: 0.64623 - Test loss: 0.56178\n",
      "Epoch 10711 - lr: 0.01000 - Train loss: 0.63963 - Test loss: 0.56154\n",
      "Epoch 10712 - lr: 0.01000 - Train loss: 0.63878 - Test loss: 0.56176\n",
      "Epoch 10713 - lr: 0.01000 - Train loss: 0.63811 - Test loss: 0.56172\n",
      "Epoch 10714 - lr: 0.01000 - Train loss: 0.64128 - Test loss: 0.56197\n",
      "Epoch 10715 - lr: 0.01000 - Train loss: 0.64426 - Test loss: 0.56180\n",
      "Epoch 10716 - lr: 0.01000 - Train loss: 0.63943 - Test loss: 0.56156\n",
      "Epoch 10717 - lr: 0.01000 - Train loss: 0.64115 - Test loss: 0.56187\n",
      "Epoch 10718 - lr: 0.01000 - Train loss: 0.64102 - Test loss: 0.56160\n",
      "Epoch 10719 - lr: 0.01000 - Train loss: 0.63497 - Test loss: 0.56182\n",
      "Epoch 10720 - lr: 0.01000 - Train loss: 0.64063 - Test loss: 0.56220\n",
      "Epoch 10721 - lr: 0.01000 - Train loss: 0.64045 - Test loss: 0.56194\n",
      "Epoch 10722 - lr: 0.01000 - Train loss: 0.63773 - Test loss: 0.56214\n",
      "Epoch 10723 - lr: 0.01000 - Train loss: 0.63978 - Test loss: 0.56217\n",
      "Epoch 10724 - lr: 0.01000 - Train loss: 0.63722 - Test loss: 0.56199\n",
      "Epoch 10725 - lr: 0.01000 - Train loss: 0.63557 - Test loss: 0.56224\n",
      "Epoch 10726 - lr: 0.01000 - Train loss: 0.63737 - Test loss: 0.56205\n",
      "Epoch 10727 - lr: 0.01000 - Train loss: 0.63479 - Test loss: 0.56230\n",
      "Epoch 10728 - lr: 0.01000 - Train loss: 0.63764 - Test loss: 0.56217\n",
      "Epoch 10729 - lr: 0.01000 - Train loss: 0.64003 - Test loss: 0.56248\n",
      "Epoch 10730 - lr: 0.01000 - Train loss: 0.63863 - Test loss: 0.56223\n",
      "Epoch 10731 - lr: 0.01000 - Train loss: 0.63468 - Test loss: 0.56242\n",
      "Epoch 10732 - lr: 0.01000 - Train loss: 0.63975 - Test loss: 0.56275\n",
      "Epoch 10733 - lr: 0.01000 - Train loss: 0.64043 - Test loss: 0.56249\n",
      "Epoch 10734 - lr: 0.01000 - Train loss: 0.64100 - Test loss: 0.56277\n",
      "Epoch 10735 - lr: 0.01000 - Train loss: 0.64624 - Test loss: 0.56235\n",
      "Epoch 10736 - lr: 0.01000 - Train loss: 0.63883 - Test loss: 0.56261\n",
      "Epoch 10737 - lr: 0.01000 - Train loss: 0.64654 - Test loss: 0.56251\n",
      "Epoch 10738 - lr: 0.01000 - Train loss: 0.64200 - Test loss: 0.56216\n",
      "Epoch 10739 - lr: 0.01000 - Train loss: 0.63154 - Test loss: 0.56236\n",
      "Epoch 10740 - lr: 0.01000 - Train loss: 0.62927 - Test loss: 0.56266\n",
      "Epoch 10741 - lr: 0.01000 - Train loss: 0.64063 - Test loss: 0.56290\n",
      "Epoch 10742 - lr: 0.01000 - Train loss: 0.63739 - Test loss: 0.56267\n",
      "Epoch 10743 - lr: 0.01000 - Train loss: 0.63433 - Test loss: 0.56288\n",
      "Epoch 10744 - lr: 0.01000 - Train loss: 0.63821 - Test loss: 0.56273\n",
      "Epoch 10745 - lr: 0.01000 - Train loss: 0.64127 - Test loss: 0.56289\n",
      "Epoch 10746 - lr: 0.01000 - Train loss: 0.64360 - Test loss: 0.56264\n",
      "Epoch 10747 - lr: 0.01000 - Train loss: 0.63778 - Test loss: 0.56236\n",
      "Epoch 10748 - lr: 0.01000 - Train loss: 0.63559 - Test loss: 0.56259\n",
      "Epoch 10749 - lr: 0.01000 - Train loss: 0.63737 - Test loss: 0.56238\n",
      "Epoch 10750 - lr: 0.01000 - Train loss: 0.63514 - Test loss: 0.56263\n",
      "Epoch 10751 - lr: 0.01000 - Train loss: 0.63735 - Test loss: 0.56246\n",
      "Epoch 10752 - lr: 0.01000 - Train loss: 0.63726 - Test loss: 0.56273\n",
      "Epoch 10753 - lr: 0.01000 - Train loss: 0.64193 - Test loss: 0.56250\n",
      "Epoch 10754 - lr: 0.01000 - Train loss: 0.63731 - Test loss: 0.56234\n",
      "Epoch 10755 - lr: 0.01000 - Train loss: 0.63915 - Test loss: 0.56264\n",
      "Epoch 10756 - lr: 0.01000 - Train loss: 0.64292 - Test loss: 0.56244\n",
      "Epoch 10757 - lr: 0.01000 - Train loss: 0.63729 - Test loss: 0.56225\n",
      "Epoch 10758 - lr: 0.01000 - Train loss: 0.63604 - Test loss: 0.56253\n",
      "Epoch 10759 - lr: 0.01000 - Train loss: 0.63810 - Test loss: 0.56233\n",
      "Epoch 10760 - lr: 0.01000 - Train loss: 0.63883 - Test loss: 0.56265\n",
      "Epoch 10761 - lr: 0.01000 - Train loss: 0.64465 - Test loss: 0.56252\n",
      "Epoch 10762 - lr: 0.01000 - Train loss: 0.64489 - Test loss: 0.56239\n",
      "Epoch 10763 - lr: 0.01000 - Train loss: 0.64592 - Test loss: 0.56231\n",
      "Epoch 10764 - lr: 0.01000 - Train loss: 0.63948 - Test loss: 0.56206\n",
      "Epoch 10765 - lr: 0.01000 - Train loss: 0.63825 - Test loss: 0.56228\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10766 - lr: 0.01000 - Train loss: 0.63816 - Test loss: 0.56226\n",
      "Epoch 10767 - lr: 0.01000 - Train loss: 0.64037 - Test loss: 0.56240\n",
      "Epoch 10768 - lr: 0.01000 - Train loss: 0.64067 - Test loss: 0.56218\n",
      "Epoch 10769 - lr: 0.01000 - Train loss: 0.63895 - Test loss: 0.56218\n",
      "Epoch 10770 - lr: 0.01000 - Train loss: 0.63729 - Test loss: 0.56210\n",
      "Epoch 10771 - lr: 0.01000 - Train loss: 0.64029 - Test loss: 0.56244\n",
      "Epoch 10772 - lr: 0.01000 - Train loss: 0.64006 - Test loss: 0.56221\n",
      "Epoch 10773 - lr: 0.01000 - Train loss: 0.63670 - Test loss: 0.56244\n",
      "Epoch 10774 - lr: 0.01000 - Train loss: 0.64028 - Test loss: 0.56259\n",
      "Epoch 10775 - lr: 0.01000 - Train loss: 0.64014 - Test loss: 0.56238\n",
      "Epoch 10776 - lr: 0.01000 - Train loss: 0.63986 - Test loss: 0.56246\n",
      "Epoch 10777 - lr: 0.01000 - Train loss: 0.63814 - Test loss: 0.56225\n",
      "Epoch 10778 - lr: 0.01000 - Train loss: 0.63922 - Test loss: 0.56258\n",
      "Epoch 10779 - lr: 0.01000 - Train loss: 0.64162 - Test loss: 0.56239\n",
      "Epoch 10780 - lr: 0.01000 - Train loss: 0.63855 - Test loss: 0.56237\n",
      "Epoch 10781 - lr: 0.01000 - Train loss: 0.63870 - Test loss: 0.56239\n",
      "Epoch 10782 - lr: 0.01000 - Train loss: 0.63772 - Test loss: 0.56233\n",
      "Epoch 10783 - lr: 0.01000 - Train loss: 0.64052 - Test loss: 0.56254\n",
      "Epoch 10784 - lr: 0.01000 - Train loss: 0.64552 - Test loss: 0.56250\n",
      "Epoch 10785 - lr: 0.01000 - Train loss: 0.63879 - Test loss: 0.56228\n",
      "Epoch 10786 - lr: 0.01000 - Train loss: 0.63847 - Test loss: 0.56252\n",
      "Epoch 10787 - lr: 0.01000 - Train loss: 0.63762 - Test loss: 0.56248\n",
      "Epoch 10788 - lr: 0.01000 - Train loss: 0.64080 - Test loss: 0.56276\n",
      "Epoch 10789 - lr: 0.01000 - Train loss: 0.64009 - Test loss: 0.56255\n",
      "Epoch 10790 - lr: 0.01000 - Train loss: 0.64078 - Test loss: 0.56287\n",
      "Epoch 10791 - lr: 0.01000 - Train loss: 0.64373 - Test loss: 0.56255\n",
      "Epoch 10792 - lr: 0.01000 - Train loss: 0.63065 - Test loss: 0.56282\n",
      "Epoch 10793 - lr: 0.01000 - Train loss: 0.63678 - Test loss: 0.56314\n",
      "Epoch 10794 - lr: 0.01000 - Train loss: 0.63968 - Test loss: 0.56290\n",
      "Epoch 10795 - lr: 0.01000 - Train loss: 0.64089 - Test loss: 0.56308\n",
      "Epoch 10796 - lr: 0.01000 - Train loss: 0.64537 - Test loss: 0.56296\n",
      "Epoch 10797 - lr: 0.01000 - Train loss: 0.64396 - Test loss: 0.56275\n",
      "Epoch 10798 - lr: 0.01000 - Train loss: 0.63908 - Test loss: 0.56249\n",
      "Epoch 10799 - lr: 0.01000 - Train loss: 0.64080 - Test loss: 0.56279\n",
      "Epoch 10800 - lr: 0.01000 - Train loss: 0.64227 - Test loss: 0.56249\n",
      "Epoch 10801 - lr: 0.01000 - Train loss: 0.62885 - Test loss: 0.56278\n",
      "Epoch 10802 - lr: 0.01000 - Train loss: 0.64004 - Test loss: 0.56306\n",
      "Epoch 10803 - lr: 0.01000 - Train loss: 0.63691 - Test loss: 0.56289\n",
      "Epoch 10804 - lr: 0.01000 - Train loss: 0.63372 - Test loss: 0.56315\n",
      "Epoch 10805 - lr: 0.01000 - Train loss: 0.63834 - Test loss: 0.56308\n",
      "Epoch 10806 - lr: 0.01000 - Train loss: 0.63968 - Test loss: 0.56311\n",
      "Epoch 10807 - lr: 0.01000 - Train loss: 0.63733 - Test loss: 0.56287\n",
      "Epoch 10808 - lr: 0.01000 - Train loss: 0.63588 - Test loss: 0.56312\n",
      "Epoch 10809 - lr: 0.01000 - Train loss: 0.63789 - Test loss: 0.56290\n",
      "Epoch 10810 - lr: 0.01000 - Train loss: 0.63837 - Test loss: 0.56320\n",
      "Epoch 10811 - lr: 0.01000 - Train loss: 0.64553 - Test loss: 0.56312\n",
      "Epoch 10812 - lr: 0.01000 - Train loss: 0.63836 - Test loss: 0.56289\n",
      "Epoch 10813 - lr: 0.01000 - Train loss: 0.63071 - Test loss: 0.56313\n",
      "Epoch 10814 - lr: 0.01000 - Train loss: 0.63079 - Test loss: 0.56342\n",
      "Epoch 10815 - lr: 0.01000 - Train loss: 0.63035 - Test loss: 0.56370\n",
      "Epoch 10816 - lr: 0.01000 - Train loss: 0.63311 - Test loss: 0.56392\n",
      "Epoch 10817 - lr: 0.01000 - Train loss: 0.63401 - Test loss: 0.56415\n",
      "Epoch 10818 - lr: 0.01000 - Train loss: 0.63820 - Test loss: 0.56398\n",
      "Epoch 10819 - lr: 0.01000 - Train loss: 0.64047 - Test loss: 0.56402\n",
      "Epoch 10820 - lr: 0.01000 - Train loss: 0.64472 - Test loss: 0.56380\n",
      "Epoch 10821 - lr: 0.01000 - Train loss: 0.64388 - Test loss: 0.56354\n",
      "Epoch 10822 - lr: 0.01000 - Train loss: 0.64195 - Test loss: 0.56325\n",
      "Epoch 10823 - lr: 0.01000 - Train loss: 0.63691 - Test loss: 0.56302\n",
      "Epoch 10824 - lr: 0.01000 - Train loss: 0.63679 - Test loss: 0.56329\n",
      "Epoch 10825 - lr: 0.01000 - Train loss: 0.64142 - Test loss: 0.56307\n",
      "Epoch 10826 - lr: 0.01000 - Train loss: 0.63697 - Test loss: 0.56292\n",
      "Epoch 10827 - lr: 0.01000 - Train loss: 0.63910 - Test loss: 0.56325\n",
      "Epoch 10828 - lr: 0.01000 - Train loss: 0.63888 - Test loss: 0.56304\n",
      "Epoch 10829 - lr: 0.01000 - Train loss: 0.63442 - Test loss: 0.56331\n",
      "Epoch 10830 - lr: 0.01000 - Train loss: 0.63725 - Test loss: 0.56320\n",
      "Epoch 10831 - lr: 0.01000 - Train loss: 0.63982 - Test loss: 0.56354\n",
      "Epoch 10832 - lr: 0.01000 - Train loss: 0.64184 - Test loss: 0.56322\n",
      "Epoch 10833 - lr: 0.01000 - Train loss: 0.62896 - Test loss: 0.56350\n",
      "Epoch 10834 - lr: 0.01000 - Train loss: 0.63899 - Test loss: 0.56374\n",
      "Epoch 10835 - lr: 0.01000 - Train loss: 0.63726 - Test loss: 0.56359\n",
      "Epoch 10836 - lr: 0.01000 - Train loss: 0.63993 - Test loss: 0.56389\n",
      "Epoch 10837 - lr: 0.01000 - Train loss: 0.64445 - Test loss: 0.56349\n",
      "Epoch 10838 - lr: 0.01000 - Train loss: 0.63418 - Test loss: 0.56371\n",
      "Epoch 10839 - lr: 0.01000 - Train loss: 0.63741 - Test loss: 0.56356\n",
      "Epoch 10840 - lr: 0.01000 - Train loss: 0.64039 - Test loss: 0.56384\n",
      "Epoch 10841 - lr: 0.01000 - Train loss: 0.64396 - Test loss: 0.56345\n",
      "Epoch 10842 - lr: 0.01000 - Train loss: 0.63231 - Test loss: 0.56368\n",
      "Epoch 10843 - lr: 0.01000 - Train loss: 0.64039 - Test loss: 0.56381\n",
      "Epoch 10844 - lr: 0.01000 - Train loss: 0.64459 - Test loss: 0.56368\n",
      "Epoch 10845 - lr: 0.01000 - Train loss: 0.64418 - Test loss: 0.56350\n",
      "Epoch 10846 - lr: 0.01000 - Train loss: 0.64435 - Test loss: 0.56333\n",
      "Epoch 10847 - lr: 0.01000 - Train loss: 0.64530 - Test loss: 0.56323\n",
      "Epoch 10848 - lr: 0.01000 - Train loss: 0.63842 - Test loss: 0.56300\n",
      "Epoch 10849 - lr: 0.01000 - Train loss: 0.62823 - Test loss: 0.56331\n",
      "Epoch 10850 - lr: 0.01000 - Train loss: 0.64001 - Test loss: 0.56359\n",
      "Epoch 10851 - lr: 0.01000 - Train loss: 0.63686 - Test loss: 0.56341\n",
      "Epoch 10852 - lr: 0.01000 - Train loss: 0.63462 - Test loss: 0.56369\n",
      "Epoch 10853 - lr: 0.01000 - Train loss: 0.63690 - Test loss: 0.56351\n",
      "Epoch 10854 - lr: 0.01000 - Train loss: 0.63703 - Test loss: 0.56380\n",
      "Epoch 10855 - lr: 0.01000 - Train loss: 0.64270 - Test loss: 0.56359\n",
      "Epoch 10856 - lr: 0.01000 - Train loss: 0.63781 - Test loss: 0.56333\n",
      "Epoch 10857 - lr: 0.01000 - Train loss: 0.63886 - Test loss: 0.56363\n",
      "Epoch 10858 - lr: 0.01000 - Train loss: 0.64071 - Test loss: 0.56341\n",
      "Epoch 10859 - lr: 0.01000 - Train loss: 0.63976 - Test loss: 0.56347\n",
      "Epoch 10860 - lr: 0.01000 - Train loss: 0.63832 - Test loss: 0.56324\n",
      "Epoch 10861 - lr: 0.01000 - Train loss: 0.64010 - Test loss: 0.56356\n",
      "Epoch 10862 - lr: 0.01000 - Train loss: 0.64275 - Test loss: 0.56325\n",
      "Epoch 10863 - lr: 0.01000 - Train loss: 0.62856 - Test loss: 0.56355\n",
      "Epoch 10864 - lr: 0.01000 - Train loss: 0.63125 - Test loss: 0.56384\n",
      "Epoch 10865 - lr: 0.01000 - Train loss: 0.62781 - Test loss: 0.56419\n",
      "Epoch 10866 - lr: 0.01000 - Train loss: 0.64205 - Test loss: 0.56448\n",
      "Epoch 10867 - lr: 0.01000 - Train loss: 0.64070 - Test loss: 0.56420\n",
      "Epoch 10868 - lr: 0.01000 - Train loss: 0.63777 - Test loss: 0.56404\n",
      "Epoch 10869 - lr: 0.01000 - Train loss: 0.63976 - Test loss: 0.56407\n",
      "Epoch 10870 - lr: 0.01000 - Train loss: 0.64080 - Test loss: 0.56379\n",
      "Epoch 10871 - lr: 0.01000 - Train loss: 0.63710 - Test loss: 0.56363\n",
      "Epoch 10872 - lr: 0.01000 - Train loss: 0.64016 - Test loss: 0.56388\n",
      "Epoch 10873 - lr: 0.01000 - Train loss: 0.63866 - Test loss: 0.56363\n",
      "Epoch 10874 - lr: 0.01000 - Train loss: 0.63867 - Test loss: 0.56385\n",
      "Epoch 10875 - lr: 0.01000 - Train loss: 0.63696 - Test loss: 0.56373\n",
      "Epoch 10876 - lr: 0.01000 - Train loss: 0.63961 - Test loss: 0.56406\n",
      "Epoch 10877 - lr: 0.01000 - Train loss: 0.64200 - Test loss: 0.56374\n",
      "Epoch 10878 - lr: 0.01000 - Train loss: 0.62779 - Test loss: 0.56405\n",
      "Epoch 10879 - lr: 0.01000 - Train loss: 0.64120 - Test loss: 0.56434\n",
      "Epoch 10880 - lr: 0.01000 - Train loss: 0.63924 - Test loss: 0.56410\n",
      "Epoch 10881 - lr: 0.01000 - Train loss: 0.63964 - Test loss: 0.56417\n",
      "Epoch 10882 - lr: 0.01000 - Train loss: 0.64118 - Test loss: 0.56393\n",
      "Epoch 10883 - lr: 0.01000 - Train loss: 0.63682 - Test loss: 0.56373\n",
      "Epoch 10884 - lr: 0.01000 - Train loss: 0.63863 - Test loss: 0.56405\n",
      "Epoch 10885 - lr: 0.01000 - Train loss: 0.63849 - Test loss: 0.56383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10886 - lr: 0.01000 - Train loss: 0.63438 - Test loss: 0.56409\n",
      "Epoch 10887 - lr: 0.01000 - Train loss: 0.63704 - Test loss: 0.56395\n",
      "Epoch 10888 - lr: 0.01000 - Train loss: 0.63919 - Test loss: 0.56428\n",
      "Epoch 10889 - lr: 0.01000 - Train loss: 0.63941 - Test loss: 0.56401\n",
      "Epoch 10890 - lr: 0.01000 - Train loss: 0.63688 - Test loss: 0.56421\n",
      "Epoch 10891 - lr: 0.01000 - Train loss: 0.63876 - Test loss: 0.56421\n",
      "Epoch 10892 - lr: 0.01000 - Train loss: 0.63694 - Test loss: 0.56403\n",
      "Epoch 10893 - lr: 0.01000 - Train loss: 0.63894 - Test loss: 0.56434\n",
      "Epoch 10894 - lr: 0.01000 - Train loss: 0.63777 - Test loss: 0.56410\n",
      "Epoch 10895 - lr: 0.01000 - Train loss: 0.63978 - Test loss: 0.56432\n",
      "Epoch 10896 - lr: 0.01000 - Train loss: 0.63730 - Test loss: 0.56412\n",
      "Epoch 10897 - lr: 0.01000 - Train loss: 0.63910 - Test loss: 0.56444\n",
      "Epoch 10898 - lr: 0.01000 - Train loss: 0.63925 - Test loss: 0.56416\n",
      "Epoch 10899 - lr: 0.01000 - Train loss: 0.63711 - Test loss: 0.56435\n",
      "Epoch 10900 - lr: 0.01000 - Train loss: 0.63833 - Test loss: 0.56433\n",
      "Epoch 10901 - lr: 0.01000 - Train loss: 0.63717 - Test loss: 0.56420\n",
      "Epoch 10902 - lr: 0.01000 - Train loss: 0.63993 - Test loss: 0.56439\n",
      "Epoch 10903 - lr: 0.01000 - Train loss: 0.64090 - Test loss: 0.56417\n",
      "Epoch 10904 - lr: 0.01000 - Train loss: 0.63757 - Test loss: 0.56407\n",
      "Epoch 10905 - lr: 0.01000 - Train loss: 0.63916 - Test loss: 0.56414\n",
      "Epoch 10906 - lr: 0.01000 - Train loss: 0.63919 - Test loss: 0.56393\n",
      "Epoch 10907 - lr: 0.01000 - Train loss: 0.63863 - Test loss: 0.56396\n",
      "Epoch 10908 - lr: 0.01000 - Train loss: 0.63711 - Test loss: 0.56380\n",
      "Epoch 10909 - lr: 0.01000 - Train loss: 0.63909 - Test loss: 0.56413\n",
      "Epoch 10910 - lr: 0.01000 - Train loss: 0.63975 - Test loss: 0.56390\n",
      "Epoch 10911 - lr: 0.01000 - Train loss: 0.63387 - Test loss: 0.56415\n",
      "Epoch 10912 - lr: 0.01000 - Train loss: 0.63886 - Test loss: 0.56452\n",
      "Epoch 10913 - lr: 0.01000 - Train loss: 0.63820 - Test loss: 0.56431\n",
      "Epoch 10914 - lr: 0.01000 - Train loss: 0.63951 - Test loss: 0.56455\n",
      "Epoch 10915 - lr: 0.01000 - Train loss: 0.63723 - Test loss: 0.56436\n",
      "Epoch 10916 - lr: 0.01000 - Train loss: 0.63932 - Test loss: 0.56468\n",
      "Epoch 10917 - lr: 0.01000 - Train loss: 0.64357 - Test loss: 0.56431\n",
      "Epoch 10918 - lr: 0.01000 - Train loss: 0.63215 - Test loss: 0.56455\n",
      "Epoch 10919 - lr: 0.01000 - Train loss: 0.63949 - Test loss: 0.56463\n",
      "Epoch 10920 - lr: 0.01000 - Train loss: 0.64092 - Test loss: 0.56441\n",
      "Epoch 10921 - lr: 0.01000 - Train loss: 0.63683 - Test loss: 0.56422\n",
      "Epoch 10922 - lr: 0.01000 - Train loss: 0.63906 - Test loss: 0.56453\n",
      "Epoch 10923 - lr: 0.01000 - Train loss: 0.64053 - Test loss: 0.56424\n",
      "Epoch 10924 - lr: 0.01000 - Train loss: 0.63087 - Test loss: 0.56449\n",
      "Epoch 10925 - lr: 0.01000 - Train loss: 0.62719 - Test loss: 0.56486\n",
      "Epoch 10926 - lr: 0.01000 - Train loss: 0.64149 - Test loss: 0.56515\n",
      "Epoch 10927 - lr: 0.01000 - Train loss: 0.64122 - Test loss: 0.56493\n",
      "Epoch 10928 - lr: 0.01000 - Train loss: 0.63741 - Test loss: 0.56468\n",
      "Epoch 10929 - lr: 0.01000 - Train loss: 0.63963 - Test loss: 0.56492\n",
      "Epoch 10930 - lr: 0.01000 - Train loss: 0.64343 - Test loss: 0.56453\n",
      "Epoch 10931 - lr: 0.01000 - Train loss: 0.63185 - Test loss: 0.56475\n",
      "Epoch 10932 - lr: 0.01000 - Train loss: 0.63967 - Test loss: 0.56487\n",
      "Epoch 10933 - lr: 0.01000 - Train loss: 0.64399 - Test loss: 0.56476\n",
      "Epoch 10934 - lr: 0.01000 - Train loss: 0.63799 - Test loss: 0.56451\n",
      "Epoch 10935 - lr: 0.01000 - Train loss: 0.63211 - Test loss: 0.56475\n",
      "Epoch 10936 - lr: 0.01000 - Train loss: 0.63951 - Test loss: 0.56483\n",
      "Epoch 10937 - lr: 0.01000 - Train loss: 0.64151 - Test loss: 0.56462\n",
      "Epoch 10938 - lr: 0.01000 - Train loss: 0.63751 - Test loss: 0.56438\n",
      "Epoch 10939 - lr: 0.01000 - Train loss: 0.63954 - Test loss: 0.56466\n",
      "Epoch 10940 - lr: 0.01000 - Train loss: 0.64297 - Test loss: 0.56431\n",
      "Epoch 10941 - lr: 0.01000 - Train loss: 0.63037 - Test loss: 0.56458\n",
      "Epoch 10942 - lr: 0.01000 - Train loss: 0.63851 - Test loss: 0.56493\n",
      "Epoch 10943 - lr: 0.01000 - Train loss: 0.63720 - Test loss: 0.56473\n",
      "Epoch 10944 - lr: 0.01000 - Train loss: 0.63104 - Test loss: 0.56495\n",
      "Epoch 10945 - lr: 0.01000 - Train loss: 0.62712 - Test loss: 0.56530\n",
      "Epoch 10946 - lr: 0.01000 - Train loss: 0.64109 - Test loss: 0.56555\n",
      "Epoch 10947 - lr: 0.01000 - Train loss: 0.64009 - Test loss: 0.56526\n",
      "Epoch 10948 - lr: 0.01000 - Train loss: 0.63738 - Test loss: 0.56508\n",
      "Epoch 10949 - lr: 0.01000 - Train loss: 0.63923 - Test loss: 0.56509\n",
      "Epoch 10950 - lr: 0.01000 - Train loss: 0.64173 - Test loss: 0.56485\n",
      "Epoch 10951 - lr: 0.01000 - Train loss: 0.63906 - Test loss: 0.56458\n",
      "Epoch 10952 - lr: 0.01000 - Train loss: 0.63817 - Test loss: 0.56455\n",
      "Epoch 10953 - lr: 0.01000 - Train loss: 0.63675 - Test loss: 0.56439\n",
      "Epoch 10954 - lr: 0.01000 - Train loss: 0.63923 - Test loss: 0.56469\n",
      "Epoch 10955 - lr: 0.01000 - Train loss: 0.64164 - Test loss: 0.56439\n",
      "Epoch 10956 - lr: 0.01000 - Train loss: 0.62710 - Test loss: 0.56475\n",
      "Epoch 10957 - lr: 0.01000 - Train loss: 0.63956 - Test loss: 0.56504\n",
      "Epoch 10958 - lr: 0.01000 - Train loss: 0.63774 - Test loss: 0.56486\n",
      "Epoch 10959 - lr: 0.01000 - Train loss: 0.63952 - Test loss: 0.56506\n",
      "Epoch 10960 - lr: 0.01000 - Train loss: 0.63938 - Test loss: 0.56484\n",
      "Epoch 10961 - lr: 0.01000 - Train loss: 0.63948 - Test loss: 0.56495\n",
      "Epoch 10962 - lr: 0.01000 - Train loss: 0.64410 - Test loss: 0.56489\n",
      "Epoch 10963 - lr: 0.01000 - Train loss: 0.64194 - Test loss: 0.56455\n",
      "Epoch 10964 - lr: 0.01000 - Train loss: 0.62736 - Test loss: 0.56487\n",
      "Epoch 10965 - lr: 0.01000 - Train loss: 0.63640 - Test loss: 0.56511\n",
      "Epoch 10966 - lr: 0.01000 - Train loss: 0.63864 - Test loss: 0.56514\n",
      "Epoch 10967 - lr: 0.01000 - Train loss: 0.63752 - Test loss: 0.56492\n",
      "Epoch 10968 - lr: 0.01000 - Train loss: 0.63956 - Test loss: 0.56515\n",
      "Epoch 10969 - lr: 0.01000 - Train loss: 0.63720 - Test loss: 0.56491\n",
      "Epoch 10970 - lr: 0.01000 - Train loss: 0.63996 - Test loss: 0.56514\n",
      "Epoch 10971 - lr: 0.01000 - Train loss: 0.63819 - Test loss: 0.56492\n",
      "Epoch 10972 - lr: 0.01000 - Train loss: 0.63925 - Test loss: 0.56504\n",
      "Epoch 10973 - lr: 0.01000 - Train loss: 0.64381 - Test loss: 0.56496\n",
      "Epoch 10974 - lr: 0.01000 - Train loss: 0.63728 - Test loss: 0.56473\n",
      "Epoch 10975 - lr: 0.01000 - Train loss: 0.63935 - Test loss: 0.56496\n",
      "Epoch 10976 - lr: 0.01000 - Train loss: 0.63714 - Test loss: 0.56478\n",
      "Epoch 10977 - lr: 0.01000 - Train loss: 0.63928 - Test loss: 0.56509\n",
      "Epoch 10978 - lr: 0.01000 - Train loss: 0.64421 - Test loss: 0.56474\n",
      "Epoch 10979 - lr: 0.01000 - Train loss: 0.63591 - Test loss: 0.56502\n",
      "Epoch 10980 - lr: 0.01000 - Train loss: 0.64116 - Test loss: 0.56484\n",
      "Epoch 10981 - lr: 0.01000 - Train loss: 0.63693 - Test loss: 0.56464\n",
      "Epoch 10982 - lr: 0.01000 - Train loss: 0.63884 - Test loss: 0.56497\n",
      "Epoch 10983 - lr: 0.01000 - Train loss: 0.64107 - Test loss: 0.56469\n",
      "Epoch 10984 - lr: 0.01000 - Train loss: 0.62765 - Test loss: 0.56503\n",
      "Epoch 10985 - lr: 0.01000 - Train loss: 0.64017 - Test loss: 0.56532\n",
      "Epoch 10986 - lr: 0.01000 - Train loss: 0.63833 - Test loss: 0.56511\n",
      "Epoch 10987 - lr: 0.01000 - Train loss: 0.63927 - Test loss: 0.56523\n",
      "Epoch 10988 - lr: 0.01000 - Train loss: 0.64359 - Test loss: 0.56511\n",
      "Epoch 10989 - lr: 0.01000 - Train loss: 0.63951 - Test loss: 0.56488\n",
      "Epoch 10990 - lr: 0.01000 - Train loss: 0.63940 - Test loss: 0.56498\n",
      "Epoch 10991 - lr: 0.01000 - Train loss: 0.64328 - Test loss: 0.56487\n",
      "Epoch 10992 - lr: 0.01000 - Train loss: 0.64415 - Test loss: 0.56480\n",
      "Epoch 10993 - lr: 0.01000 - Train loss: 0.63716 - Test loss: 0.56460\n",
      "Epoch 10994 - lr: 0.01000 - Train loss: 0.63092 - Test loss: 0.56487\n",
      "Epoch 10995 - lr: 0.01000 - Train loss: 0.62734 - Test loss: 0.56525\n",
      "Epoch 10996 - lr: 0.01000 - Train loss: 0.63688 - Test loss: 0.56550\n",
      "Epoch 10997 - lr: 0.01000 - Train loss: 0.63797 - Test loss: 0.56545\n",
      "Epoch 10998 - lr: 0.01000 - Train loss: 0.63700 - Test loss: 0.56531\n",
      "Epoch 10999 - lr: 0.01000 - Train loss: 0.63944 - Test loss: 0.56543\n",
      "Epoch 11000 - lr: 0.01000 - Train loss: 0.64426 - Test loss: 0.56533\n",
      "Epoch 11001 - lr: 0.01000 - Train loss: 0.64272 - Test loss: 0.56494\n",
      "Epoch 11002 - lr: 0.01000 - Train loss: 0.63000 - Test loss: 0.56520\n",
      "Epoch 11003 - lr: 0.01000 - Train loss: 0.63769 - Test loss: 0.56553\n",
      "Epoch 11004 - lr: 0.01000 - Train loss: 0.64265 - Test loss: 0.56537\n",
      "Epoch 11005 - lr: 0.01000 - Train loss: 0.63973 - Test loss: 0.56510\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11006 - lr: 0.01000 - Train loss: 0.63748 - Test loss: 0.56502\n",
      "Epoch 11007 - lr: 0.01000 - Train loss: 0.63759 - Test loss: 0.56497\n",
      "Epoch 11008 - lr: 0.01000 - Train loss: 0.63701 - Test loss: 0.56489\n",
      "Epoch 11009 - lr: 0.01000 - Train loss: 0.63868 - Test loss: 0.56497\n",
      "Epoch 11010 - lr: 0.01000 - Train loss: 0.63823 - Test loss: 0.56476\n",
      "Epoch 11011 - lr: 0.01000 - Train loss: 0.63909 - Test loss: 0.56492\n",
      "Epoch 11012 - lr: 0.01000 - Train loss: 0.64303 - Test loss: 0.56483\n",
      "Epoch 11013 - lr: 0.01000 - Train loss: 0.64406 - Test loss: 0.56479\n",
      "Epoch 11014 - lr: 0.01000 - Train loss: 0.63743 - Test loss: 0.56461\n",
      "Epoch 11015 - lr: 0.01000 - Train loss: 0.63800 - Test loss: 0.56487\n",
      "Epoch 11016 - lr: 0.01000 - Train loss: 0.63621 - Test loss: 0.56479\n",
      "Epoch 11017 - lr: 0.01000 - Train loss: 0.63862 - Test loss: 0.56517\n",
      "Epoch 11018 - lr: 0.01000 - Train loss: 0.63849 - Test loss: 0.56498\n",
      "Epoch 11019 - lr: 0.01000 - Train loss: 0.63703 - Test loss: 0.56521\n",
      "Epoch 11020 - lr: 0.01000 - Train loss: 0.63714 - Test loss: 0.56517\n",
      "Epoch 11021 - lr: 0.01000 - Train loss: 0.63856 - Test loss: 0.56523\n",
      "Epoch 11022 - lr: 0.01000 - Train loss: 0.63713 - Test loss: 0.56502\n",
      "Epoch 11023 - lr: 0.01000 - Train loss: 0.63902 - Test loss: 0.56535\n",
      "Epoch 11024 - lr: 0.01000 - Train loss: 0.64311 - Test loss: 0.56502\n",
      "Epoch 11025 - lr: 0.01000 - Train loss: 0.63234 - Test loss: 0.56530\n",
      "Epoch 11026 - lr: 0.01000 - Train loss: 0.63799 - Test loss: 0.56531\n",
      "Epoch 11027 - lr: 0.01000 - Train loss: 0.63634 - Test loss: 0.56519\n",
      "Epoch 11028 - lr: 0.01000 - Train loss: 0.63918 - Test loss: 0.56551\n",
      "Epoch 11029 - lr: 0.01000 - Train loss: 0.64442 - Test loss: 0.56516\n",
      "Epoch 11030 - lr: 0.01000 - Train loss: 0.63705 - Test loss: 0.56546\n",
      "Epoch 11031 - lr: 0.01000 - Train loss: 0.64441 - Test loss: 0.56542\n",
      "Epoch 11032 - lr: 0.01000 - Train loss: 0.64117 - Test loss: 0.56512\n",
      "Epoch 11033 - lr: 0.01000 - Train loss: 0.62743 - Test loss: 0.56545\n",
      "Epoch 11034 - lr: 0.01000 - Train loss: 0.64021 - Test loss: 0.56573\n",
      "Epoch 11035 - lr: 0.01000 - Train loss: 0.63768 - Test loss: 0.56550\n",
      "Epoch 11036 - lr: 0.01000 - Train loss: 0.63962 - Test loss: 0.56578\n",
      "Epoch 11037 - lr: 0.01000 - Train loss: 0.64313 - Test loss: 0.56542\n",
      "Epoch 11038 - lr: 0.01000 - Train loss: 0.63199 - Test loss: 0.56567\n",
      "Epoch 11039 - lr: 0.01000 - Train loss: 0.63875 - Test loss: 0.56570\n",
      "Epoch 11040 - lr: 0.01000 - Train loss: 0.63688 - Test loss: 0.56546\n",
      "Epoch 11041 - lr: 0.01000 - Train loss: 0.63812 - Test loss: 0.56578\n",
      "Epoch 11042 - lr: 0.01000 - Train loss: 0.63771 - Test loss: 0.56556\n",
      "Epoch 11043 - lr: 0.01000 - Train loss: 0.63066 - Test loss: 0.56582\n",
      "Epoch 11044 - lr: 0.01000 - Train loss: 0.63980 - Test loss: 0.56612\n",
      "Epoch 11045 - lr: 0.01000 - Train loss: 0.64874 - Test loss: 0.56569\n",
      "Epoch 11046 - lr: 0.01000 - Train loss: 0.63826 - Test loss: 0.56564\n",
      "Epoch 11047 - lr: 0.01000 - Train loss: 0.63596 - Test loss: 0.56543\n",
      "Epoch 11048 - lr: 0.01000 - Train loss: 0.63585 - Test loss: 0.56573\n",
      "Epoch 11049 - lr: 0.01000 - Train loss: 0.64064 - Test loss: 0.56550\n",
      "Epoch 11050 - lr: 0.01000 - Train loss: 0.63595 - Test loss: 0.56533\n",
      "Epoch 11051 - lr: 0.01000 - Train loss: 0.63707 - Test loss: 0.56565\n",
      "Epoch 11052 - lr: 0.01000 - Train loss: 0.64433 - Test loss: 0.56560\n",
      "Epoch 11053 - lr: 0.01000 - Train loss: 0.63821 - Test loss: 0.56537\n",
      "Epoch 11054 - lr: 0.01000 - Train loss: 0.63739 - Test loss: 0.56559\n",
      "Epoch 11055 - lr: 0.01000 - Train loss: 0.63646 - Test loss: 0.56551\n",
      "Epoch 11056 - lr: 0.01000 - Train loss: 0.63965 - Test loss: 0.56579\n",
      "Epoch 11057 - lr: 0.01000 - Train loss: 0.63701 - Test loss: 0.56559\n",
      "Epoch 11058 - lr: 0.01000 - Train loss: 0.63673 - Test loss: 0.56579\n",
      "Epoch 11059 - lr: 0.01000 - Train loss: 0.63732 - Test loss: 0.56575\n",
      "Epoch 11060 - lr: 0.01000 - Train loss: 0.63765 - Test loss: 0.56573\n",
      "Epoch 11061 - lr: 0.01000 - Train loss: 0.63638 - Test loss: 0.56561\n",
      "Epoch 11062 - lr: 0.01000 - Train loss: 0.63949 - Test loss: 0.56586\n",
      "Epoch 11063 - lr: 0.01000 - Train loss: 0.63690 - Test loss: 0.56567\n",
      "Epoch 11064 - lr: 0.01000 - Train loss: 0.63045 - Test loss: 0.56592\n",
      "Epoch 11065 - lr: 0.01000 - Train loss: 0.62685 - Test loss: 0.56630\n",
      "Epoch 11066 - lr: 0.01000 - Train loss: 0.64018 - Test loss: 0.56656\n",
      "Epoch 11067 - lr: 0.01000 - Train loss: 0.63805 - Test loss: 0.56629\n",
      "Epoch 11068 - lr: 0.01000 - Train loss: 0.63945 - Test loss: 0.56642\n",
      "Epoch 11069 - lr: 0.01000 - Train loss: 0.64300 - Test loss: 0.56624\n",
      "Epoch 11070 - lr: 0.01000 - Train loss: 0.64375 - Test loss: 0.56607\n",
      "Epoch 11071 - lr: 0.01000 - Train loss: 0.64076 - Test loss: 0.56583\n",
      "Epoch 11072 - lr: 0.01000 - Train loss: 0.63628 - Test loss: 0.56566\n",
      "Epoch 11073 - lr: 0.01000 - Train loss: 0.63930 - Test loss: 0.56597\n",
      "Epoch 11074 - lr: 0.01000 - Train loss: 0.64330 - Test loss: 0.56563\n",
      "Epoch 11075 - lr: 0.01000 - Train loss: 0.63344 - Test loss: 0.56590\n",
      "Epoch 11076 - lr: 0.01000 - Train loss: 0.63605 - Test loss: 0.56577\n",
      "Epoch 11077 - lr: 0.01000 - Train loss: 0.63801 - Test loss: 0.56612\n",
      "Epoch 11078 - lr: 0.01000 - Train loss: 0.63836 - Test loss: 0.56591\n",
      "Epoch 11079 - lr: 0.01000 - Train loss: 0.63777 - Test loss: 0.56621\n",
      "Epoch 11080 - lr: 0.01000 - Train loss: 0.64226 - Test loss: 0.56604\n",
      "Epoch 11081 - lr: 0.01000 - Train loss: 0.63708 - Test loss: 0.56576\n",
      "Epoch 11082 - lr: 0.01000 - Train loss: 0.63801 - Test loss: 0.56609\n",
      "Epoch 11083 - lr: 0.01000 - Train loss: 0.63963 - Test loss: 0.56588\n",
      "Epoch 11084 - lr: 0.01000 - Train loss: 0.63941 - Test loss: 0.56597\n",
      "Epoch 11085 - lr: 0.01000 - Train loss: 0.64173 - Test loss: 0.56576\n",
      "Epoch 11086 - lr: 0.01000 - Train loss: 0.63634 - Test loss: 0.56551\n",
      "Epoch 11087 - lr: 0.01000 - Train loss: 0.63494 - Test loss: 0.56581\n",
      "Epoch 11088 - lr: 0.01000 - Train loss: 0.63691 - Test loss: 0.56559\n",
      "Epoch 11089 - lr: 0.01000 - Train loss: 0.63765 - Test loss: 0.56594\n",
      "Epoch 11090 - lr: 0.01000 - Train loss: 0.64244 - Test loss: 0.56581\n",
      "Epoch 11091 - lr: 0.01000 - Train loss: 0.63771 - Test loss: 0.56555\n",
      "Epoch 11092 - lr: 0.01000 - Train loss: 0.63948 - Test loss: 0.56588\n",
      "Epoch 11093 - lr: 0.01000 - Train loss: 0.64260 - Test loss: 0.56557\n",
      "Epoch 11094 - lr: 0.01000 - Train loss: 0.63081 - Test loss: 0.56586\n",
      "Epoch 11095 - lr: 0.01000 - Train loss: 0.63997 - Test loss: 0.56615\n",
      "Epoch 11096 - lr: 0.01000 - Train loss: 0.63719 - Test loss: 0.56597\n",
      "Epoch 11097 - lr: 0.01000 - Train loss: 0.63686 - Test loss: 0.56616\n",
      "Epoch 11098 - lr: 0.01000 - Train loss: 0.63692 - Test loss: 0.56609\n",
      "Epoch 11099 - lr: 0.01000 - Train loss: 0.63890 - Test loss: 0.56616\n",
      "Epoch 11100 - lr: 0.01000 - Train loss: 0.63798 - Test loss: 0.56589\n",
      "Epoch 11101 - lr: 0.01000 - Train loss: 0.63965 - Test loss: 0.56612\n",
      "Epoch 11102 - lr: 0.01000 - Train loss: 0.63796 - Test loss: 0.56592\n",
      "Epoch 11103 - lr: 0.01000 - Train loss: 0.63389 - Test loss: 0.56618\n",
      "Epoch 11104 - lr: 0.01000 - Train loss: 0.63571 - Test loss: 0.56600\n",
      "Epoch 11105 - lr: 0.01000 - Train loss: 0.63361 - Test loss: 0.56630\n",
      "Epoch 11106 - lr: 0.01000 - Train loss: 0.63577 - Test loss: 0.56612\n",
      "Epoch 11107 - lr: 0.01000 - Train loss: 0.63527 - Test loss: 0.56642\n",
      "Epoch 11108 - lr: 0.01000 - Train loss: 0.63835 - Test loss: 0.56615\n",
      "Epoch 11109 - lr: 0.01000 - Train loss: 0.63931 - Test loss: 0.56630\n",
      "Epoch 11110 - lr: 0.01000 - Train loss: 0.64417 - Test loss: 0.56620\n",
      "Epoch 11111 - lr: 0.01000 - Train loss: 0.63701 - Test loss: 0.56598\n",
      "Epoch 11112 - lr: 0.01000 - Train loss: 0.63609 - Test loss: 0.56618\n",
      "Epoch 11113 - lr: 0.01000 - Train loss: 0.63775 - Test loss: 0.56618\n",
      "Epoch 11114 - lr: 0.01000 - Train loss: 0.63578 - Test loss: 0.56605\n",
      "Epoch 11115 - lr: 0.01000 - Train loss: 0.63799 - Test loss: 0.56639\n",
      "Epoch 11116 - lr: 0.01000 - Train loss: 0.63805 - Test loss: 0.56619\n",
      "Epoch 11117 - lr: 0.01000 - Train loss: 0.63594 - Test loss: 0.56648\n",
      "Epoch 11118 - lr: 0.01000 - Train loss: 0.64054 - Test loss: 0.56624\n",
      "Epoch 11119 - lr: 0.01000 - Train loss: 0.63564 - Test loss: 0.56607\n",
      "Epoch 11120 - lr: 0.01000 - Train loss: 0.63639 - Test loss: 0.56639\n",
      "Epoch 11121 - lr: 0.01000 - Train loss: 0.64333 - Test loss: 0.56625\n",
      "Epoch 11122 - lr: 0.01000 - Train loss: 0.64444 - Test loss: 0.56617\n",
      "Epoch 11123 - lr: 0.01000 - Train loss: 0.64006 - Test loss: 0.56589\n",
      "Epoch 11124 - lr: 0.01000 - Train loss: 0.62980 - Test loss: 0.56616\n",
      "Epoch 11125 - lr: 0.01000 - Train loss: 0.62771 - Test loss: 0.56652\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11126 - lr: 0.01000 - Train loss: 0.63850 - Test loss: 0.56678\n",
      "Epoch 11127 - lr: 0.01000 - Train loss: 0.63570 - Test loss: 0.56657\n",
      "Epoch 11128 - lr: 0.01000 - Train loss: 0.63435 - Test loss: 0.56685\n",
      "Epoch 11129 - lr: 0.01000 - Train loss: 0.63631 - Test loss: 0.56658\n",
      "Epoch 11130 - lr: 0.01000 - Train loss: 0.63578 - Test loss: 0.56685\n",
      "Epoch 11131 - lr: 0.01000 - Train loss: 0.64109 - Test loss: 0.56660\n",
      "Epoch 11132 - lr: 0.01000 - Train loss: 0.63586 - Test loss: 0.56634\n",
      "Epoch 11133 - lr: 0.01000 - Train loss: 0.63471 - Test loss: 0.56662\n",
      "Epoch 11134 - lr: 0.01000 - Train loss: 0.63718 - Test loss: 0.56636\n",
      "Epoch 11135 - lr: 0.01000 - Train loss: 0.63901 - Test loss: 0.56668\n",
      "Epoch 11136 - lr: 0.01000 - Train loss: 0.64431 - Test loss: 0.56631\n",
      "Epoch 11137 - lr: 0.01000 - Train loss: 0.63742 - Test loss: 0.56661\n",
      "Epoch 11138 - lr: 0.01000 - Train loss: 0.64258 - Test loss: 0.56648\n",
      "Epoch 11139 - lr: 0.01000 - Train loss: 0.63960 - Test loss: 0.56622\n",
      "Epoch 11140 - lr: 0.01000 - Train loss: 0.63658 - Test loss: 0.56615\n",
      "Epoch 11141 - lr: 0.01000 - Train loss: 0.63850 - Test loss: 0.56625\n",
      "Epoch 11142 - lr: 0.01000 - Train loss: 0.63748 - Test loss: 0.56603\n",
      "Epoch 11143 - lr: 0.01000 - Train loss: 0.63927 - Test loss: 0.56632\n",
      "Epoch 11144 - lr: 0.01000 - Train loss: 0.63683 - Test loss: 0.56617\n",
      "Epoch 11145 - lr: 0.01000 - Train loss: 0.63599 - Test loss: 0.56640\n",
      "Epoch 11146 - lr: 0.01000 - Train loss: 0.63743 - Test loss: 0.56642\n",
      "Epoch 11147 - lr: 0.01000 - Train loss: 0.63587 - Test loss: 0.56633\n",
      "Epoch 11148 - lr: 0.01000 - Train loss: 0.63904 - Test loss: 0.56666\n",
      "Epoch 11149 - lr: 0.01000 - Train loss: 0.64272 - Test loss: 0.56635\n",
      "Epoch 11150 - lr: 0.01000 - Train loss: 0.63206 - Test loss: 0.56663\n",
      "Epoch 11151 - lr: 0.01000 - Train loss: 0.63731 - Test loss: 0.56661\n",
      "Epoch 11152 - lr: 0.01000 - Train loss: 0.63640 - Test loss: 0.56653\n",
      "Epoch 11153 - lr: 0.01000 - Train loss: 0.63886 - Test loss: 0.56668\n",
      "Epoch 11154 - lr: 0.01000 - Train loss: 0.64234 - Test loss: 0.56652\n",
      "Epoch 11155 - lr: 0.01000 - Train loss: 0.64146 - Test loss: 0.56632\n",
      "Epoch 11156 - lr: 0.01000 - Train loss: 0.63683 - Test loss: 0.56609\n",
      "Epoch 11157 - lr: 0.01000 - Train loss: 0.63843 - Test loss: 0.56645\n",
      "Epoch 11158 - lr: 0.01000 - Train loss: 0.63830 - Test loss: 0.56627\n",
      "Epoch 11159 - lr: 0.01000 - Train loss: 0.63520 - Test loss: 0.56650\n",
      "Epoch 11160 - lr: 0.01000 - Train loss: 0.63840 - Test loss: 0.56662\n",
      "Epoch 11161 - lr: 0.01000 - Train loss: 0.63703 - Test loss: 0.56641\n",
      "Epoch 11162 - lr: 0.01000 - Train loss: 0.63886 - Test loss: 0.56675\n",
      "Epoch 11163 - lr: 0.01000 - Train loss: 0.64244 - Test loss: 0.56645\n",
      "Epoch 11164 - lr: 0.01000 - Train loss: 0.63110 - Test loss: 0.56674\n",
      "Epoch 11165 - lr: 0.01000 - Train loss: 0.63902 - Test loss: 0.56688\n",
      "Epoch 11166 - lr: 0.01000 - Train loss: 0.64193 - Test loss: 0.56671\n",
      "Epoch 11167 - lr: 0.01000 - Train loss: 0.63836 - Test loss: 0.56645\n",
      "Epoch 11168 - lr: 0.01000 - Train loss: 0.63844 - Test loss: 0.56654\n",
      "Epoch 11169 - lr: 0.01000 - Train loss: 0.63787 - Test loss: 0.56632\n",
      "Epoch 11170 - lr: 0.01000 - Train loss: 0.63893 - Test loss: 0.56651\n",
      "Epoch 11171 - lr: 0.01000 - Train loss: 0.64380 - Test loss: 0.56647\n",
      "Epoch 11172 - lr: 0.01000 - Train loss: 0.63675 - Test loss: 0.56631\n",
      "Epoch 11173 - lr: 0.01000 - Train loss: 0.63055 - Test loss: 0.56659\n",
      "Epoch 11174 - lr: 0.01000 - Train loss: 0.62785 - Test loss: 0.56697\n",
      "Epoch 11175 - lr: 0.01000 - Train loss: 0.62662 - Test loss: 0.56737\n",
      "Epoch 11176 - lr: 0.01000 - Train loss: 0.63952 - Test loss: 0.56762\n",
      "Epoch 11177 - lr: 0.01000 - Train loss: 0.63667 - Test loss: 0.56734\n",
      "Epoch 11178 - lr: 0.01000 - Train loss: 0.63814 - Test loss: 0.56763\n",
      "Epoch 11179 - lr: 0.01000 - Train loss: 0.63843 - Test loss: 0.56732\n",
      "Epoch 11180 - lr: 0.01000 - Train loss: 0.63606 - Test loss: 0.56746\n",
      "Epoch 11181 - lr: 0.01000 - Train loss: 0.63734 - Test loss: 0.56738\n",
      "Epoch 11182 - lr: 0.01000 - Train loss: 0.63607 - Test loss: 0.56721\n",
      "Epoch 11183 - lr: 0.01000 - Train loss: 0.63917 - Test loss: 0.56741\n",
      "Epoch 11184 - lr: 0.01000 - Train loss: 0.63685 - Test loss: 0.56717\n",
      "Epoch 11185 - lr: 0.01000 - Train loss: 0.62685 - Test loss: 0.56747\n",
      "Epoch 11186 - lr: 0.01000 - Train loss: 0.63436 - Test loss: 0.56767\n",
      "Epoch 11187 - lr: 0.01000 - Train loss: 0.63934 - Test loss: 0.56785\n",
      "Epoch 11188 - lr: 0.01000 - Train loss: 0.63901 - Test loss: 0.56759\n",
      "Epoch 11189 - lr: 0.01000 - Train loss: 0.63908 - Test loss: 0.56763\n",
      "Epoch 11190 - lr: 0.01000 - Train loss: 0.64357 - Test loss: 0.56747\n",
      "Epoch 11191 - lr: 0.01000 - Train loss: 0.63789 - Test loss: 0.56720\n",
      "Epoch 11192 - lr: 0.01000 - Train loss: 0.63704 - Test loss: 0.56746\n",
      "Epoch 11193 - lr: 0.01000 - Train loss: 0.64372 - Test loss: 0.56734\n",
      "Epoch 11194 - lr: 0.01000 - Train loss: 0.64127 - Test loss: 0.56712\n",
      "Epoch 11195 - lr: 0.01000 - Train loss: 0.63536 - Test loss: 0.56686\n",
      "Epoch 11196 - lr: 0.01000 - Train loss: 0.63175 - Test loss: 0.56715\n",
      "Epoch 11197 - lr: 0.01000 - Train loss: 0.63743 - Test loss: 0.56712\n",
      "Epoch 11198 - lr: 0.01000 - Train loss: 0.63567 - Test loss: 0.56698\n",
      "Epoch 11199 - lr: 0.01000 - Train loss: 0.63870 - Test loss: 0.56730\n",
      "Epoch 11200 - lr: 0.01000 - Train loss: 0.64372 - Test loss: 0.56694\n",
      "Epoch 11201 - lr: 0.01000 - Train loss: 0.63624 - Test loss: 0.56723\n",
      "Epoch 11202 - lr: 0.01000 - Train loss: 0.64312 - Test loss: 0.56709\n",
      "Epoch 11203 - lr: 0.01000 - Train loss: 0.64418 - Test loss: 0.56700\n",
      "Epoch 11204 - lr: 0.01000 - Train loss: 0.63781 - Test loss: 0.56677\n",
      "Epoch 11205 - lr: 0.01000 - Train loss: 0.63636 - Test loss: 0.56697\n",
      "Epoch 11206 - lr: 0.01000 - Train loss: 0.63642 - Test loss: 0.56693\n",
      "Epoch 11207 - lr: 0.01000 - Train loss: 0.63859 - Test loss: 0.56704\n",
      "Epoch 11208 - lr: 0.01000 - Train loss: 0.63807 - Test loss: 0.56679\n",
      "Epoch 11209 - lr: 0.01000 - Train loss: 0.63895 - Test loss: 0.56696\n",
      "Epoch 11210 - lr: 0.01000 - Train loss: 0.64315 - Test loss: 0.56684\n",
      "Epoch 11211 - lr: 0.01000 - Train loss: 0.64421 - Test loss: 0.56678\n",
      "Epoch 11212 - lr: 0.01000 - Train loss: 0.63749 - Test loss: 0.56659\n",
      "Epoch 11213 - lr: 0.01000 - Train loss: 0.63606 - Test loss: 0.56682\n",
      "Epoch 11214 - lr: 0.01000 - Train loss: 0.63672 - Test loss: 0.56683\n",
      "Epoch 11215 - lr: 0.01000 - Train loss: 0.63734 - Test loss: 0.56686\n",
      "Epoch 11216 - lr: 0.01000 - Train loss: 0.63523 - Test loss: 0.56676\n",
      "Epoch 11217 - lr: 0.01000 - Train loss: 0.63669 - Test loss: 0.56712\n",
      "Epoch 11218 - lr: 0.01000 - Train loss: 0.64427 - Test loss: 0.56708\n",
      "Epoch 11219 - lr: 0.01000 - Train loss: 0.63808 - Test loss: 0.56687\n",
      "Epoch 11220 - lr: 0.01000 - Train loss: 0.63553 - Test loss: 0.56708\n",
      "Epoch 11221 - lr: 0.01000 - Train loss: 0.63763 - Test loss: 0.56712\n",
      "Epoch 11222 - lr: 0.01000 - Train loss: 0.63501 - Test loss: 0.56697\n",
      "Epoch 11223 - lr: 0.01000 - Train loss: 0.63268 - Test loss: 0.56729\n",
      "Epoch 11224 - lr: 0.01000 - Train loss: 0.63550 - Test loss: 0.56716\n",
      "Epoch 11225 - lr: 0.01000 - Train loss: 0.63790 - Test loss: 0.56752\n",
      "Epoch 11226 - lr: 0.01000 - Train loss: 0.63715 - Test loss: 0.56733\n",
      "Epoch 11227 - lr: 0.01000 - Train loss: 0.62766 - Test loss: 0.56764\n",
      "Epoch 11228 - lr: 0.01000 - Train loss: 0.62679 - Test loss: 0.56799\n",
      "Epoch 11229 - lr: 0.01000 - Train loss: 0.64027 - Test loss: 0.56823\n",
      "Epoch 11230 - lr: 0.01000 - Train loss: 0.63768 - Test loss: 0.56790\n",
      "Epoch 11231 - lr: 0.01000 - Train loss: 0.63938 - Test loss: 0.56808\n",
      "Epoch 11232 - lr: 0.01000 - Train loss: 0.63665 - Test loss: 0.56782\n",
      "Epoch 11233 - lr: 0.01000 - Train loss: 0.62854 - Test loss: 0.56803\n",
      "Epoch 11234 - lr: 0.01000 - Train loss: 0.63247 - Test loss: 0.56822\n",
      "Epoch 11235 - lr: 0.01000 - Train loss: 0.63733 - Test loss: 0.56851\n",
      "Epoch 11236 - lr: 0.01000 - Train loss: 0.64090 - Test loss: 0.56824\n",
      "Epoch 11237 - lr: 0.01000 - Train loss: 0.63544 - Test loss: 0.56794\n",
      "Epoch 11238 - lr: 0.01000 - Train loss: 0.63506 - Test loss: 0.56818\n",
      "Epoch 11239 - lr: 0.01000 - Train loss: 0.63874 - Test loss: 0.56785\n",
      "Epoch 11240 - lr: 0.01000 - Train loss: 0.63758 - Test loss: 0.56778\n",
      "Epoch 11241 - lr: 0.01000 - Train loss: 0.63518 - Test loss: 0.56754\n",
      "Epoch 11242 - lr: 0.01000 - Train loss: 0.63432 - Test loss: 0.56782\n",
      "Epoch 11243 - lr: 0.01000 - Train loss: 0.63670 - Test loss: 0.56755\n",
      "Epoch 11244 - lr: 0.01000 - Train loss: 0.63842 - Test loss: 0.56787\n",
      "Epoch 11245 - lr: 0.01000 - Train loss: 0.64242 - Test loss: 0.56752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11246 - lr: 0.01000 - Train loss: 0.63165 - Test loss: 0.56779\n",
      "Epoch 11247 - lr: 0.01000 - Train loss: 0.63712 - Test loss: 0.56775\n",
      "Epoch 11248 - lr: 0.01000 - Train loss: 0.63573 - Test loss: 0.56763\n",
      "Epoch 11249 - lr: 0.01000 - Train loss: 0.63894 - Test loss: 0.56788\n",
      "Epoch 11250 - lr: 0.01000 - Train loss: 0.63629 - Test loss: 0.56769\n",
      "Epoch 11251 - lr: 0.01000 - Train loss: 0.63626 - Test loss: 0.56788\n",
      "Epoch 11252 - lr: 0.01000 - Train loss: 0.63623 - Test loss: 0.56779\n",
      "Epoch 11253 - lr: 0.01000 - Train loss: 0.63813 - Test loss: 0.56786\n",
      "Epoch 11254 - lr: 0.01000 - Train loss: 0.63801 - Test loss: 0.56761\n",
      "Epoch 11255 - lr: 0.01000 - Train loss: 0.63781 - Test loss: 0.56765\n",
      "Epoch 11256 - lr: 0.01000 - Train loss: 0.63646 - Test loss: 0.56742\n",
      "Epoch 11257 - lr: 0.01000 - Train loss: 0.63832 - Test loss: 0.56775\n",
      "Epoch 11258 - lr: 0.01000 - Train loss: 0.64083 - Test loss: 0.56747\n",
      "Epoch 11259 - lr: 0.01000 - Train loss: 0.62651 - Test loss: 0.56783\n",
      "Epoch 11260 - lr: 0.01000 - Train loss: 0.63436 - Test loss: 0.56808\n",
      "Epoch 11261 - lr: 0.01000 - Train loss: 0.63868 - Test loss: 0.56824\n",
      "Epoch 11262 - lr: 0.01000 - Train loss: 0.64350 - Test loss: 0.56815\n",
      "Epoch 11263 - lr: 0.01000 - Train loss: 0.63809 - Test loss: 0.56787\n",
      "Epoch 11264 - lr: 0.01000 - Train loss: 0.63496 - Test loss: 0.56804\n",
      "Epoch 11265 - lr: 0.01000 - Train loss: 0.63794 - Test loss: 0.56809\n",
      "Epoch 11266 - lr: 0.01000 - Train loss: 0.63653 - Test loss: 0.56783\n",
      "Epoch 11267 - lr: 0.01000 - Train loss: 0.63841 - Test loss: 0.56813\n",
      "Epoch 11268 - lr: 0.01000 - Train loss: 0.64312 - Test loss: 0.56777\n",
      "Epoch 11269 - lr: 0.01000 - Train loss: 0.63479 - Test loss: 0.56804\n",
      "Epoch 11270 - lr: 0.01000 - Train loss: 0.63829 - Test loss: 0.56779\n",
      "Epoch 11271 - lr: 0.01000 - Train loss: 0.63750 - Test loss: 0.56781\n",
      "Epoch 11272 - lr: 0.01000 - Train loss: 0.63529 - Test loss: 0.56760\n",
      "Epoch 11273 - lr: 0.01000 - Train loss: 0.63493 - Test loss: 0.56793\n",
      "Epoch 11274 - lr: 0.01000 - Train loss: 0.63958 - Test loss: 0.56772\n",
      "Epoch 11275 - lr: 0.01000 - Train loss: 0.63514 - Test loss: 0.56757\n",
      "Epoch 11276 - lr: 0.01000 - Train loss: 0.63710 - Test loss: 0.56793\n",
      "Epoch 11277 - lr: 0.01000 - Train loss: 0.63781 - Test loss: 0.56777\n",
      "Epoch 11278 - lr: 0.01000 - Train loss: 0.63837 - Test loss: 0.56808\n",
      "Epoch 11279 - lr: 0.01000 - Train loss: 0.64314 - Test loss: 0.56775\n",
      "Epoch 11280 - lr: 0.01000 - Train loss: 0.63498 - Test loss: 0.56805\n",
      "Epoch 11281 - lr: 0.01000 - Train loss: 0.63901 - Test loss: 0.56781\n",
      "Epoch 11282 - lr: 0.01000 - Train loss: 0.63601 - Test loss: 0.56773\n",
      "Epoch 11283 - lr: 0.01000 - Train loss: 0.63803 - Test loss: 0.56783\n",
      "Epoch 11284 - lr: 0.01000 - Train loss: 0.63822 - Test loss: 0.56760\n",
      "Epoch 11285 - lr: 0.01000 - Train loss: 0.63702 - Test loss: 0.56761\n",
      "Epoch 11286 - lr: 0.01000 - Train loss: 0.63496 - Test loss: 0.56748\n",
      "Epoch 11287 - lr: 0.01000 - Train loss: 0.63612 - Test loss: 0.56785\n",
      "Epoch 11288 - lr: 0.01000 - Train loss: 0.64339 - Test loss: 0.56782\n",
      "Epoch 11289 - lr: 0.01000 - Train loss: 0.63776 - Test loss: 0.56763\n",
      "Epoch 11290 - lr: 0.01000 - Train loss: 0.63496 - Test loss: 0.56785\n",
      "Epoch 11291 - lr: 0.01000 - Train loss: 0.63753 - Test loss: 0.56792\n",
      "Epoch 11292 - lr: 0.01000 - Train loss: 0.63532 - Test loss: 0.56773\n",
      "Epoch 11293 - lr: 0.01000 - Train loss: 0.63490 - Test loss: 0.56807\n",
      "Epoch 11294 - lr: 0.01000 - Train loss: 0.63966 - Test loss: 0.56786\n",
      "Epoch 11295 - lr: 0.01000 - Train loss: 0.63500 - Test loss: 0.56770\n",
      "Epoch 11296 - lr: 0.01000 - Train loss: 0.63619 - Test loss: 0.56805\n",
      "Epoch 11297 - lr: 0.01000 - Train loss: 0.64326 - Test loss: 0.56800\n",
      "Epoch 11298 - lr: 0.01000 - Train loss: 0.63607 - Test loss: 0.56783\n",
      "Epoch 11299 - lr: 0.01000 - Train loss: 0.63391 - Test loss: 0.56804\n",
      "Epoch 11300 - lr: 0.01000 - Train loss: 0.63879 - Test loss: 0.56826\n",
      "Epoch 11301 - lr: 0.01000 - Train loss: 0.64257 - Test loss: 0.56815\n",
      "Epoch 11302 - lr: 0.01000 - Train loss: 0.64360 - Test loss: 0.56804\n",
      "Epoch 11303 - lr: 0.01000 - Train loss: 0.63657 - Test loss: 0.56784\n",
      "Epoch 11304 - lr: 0.01000 - Train loss: 0.63679 - Test loss: 0.56803\n",
      "Epoch 11305 - lr: 0.01000 - Train loss: 0.63516 - Test loss: 0.56791\n",
      "Epoch 11306 - lr: 0.01000 - Train loss: 0.63771 - Test loss: 0.56826\n",
      "Epoch 11307 - lr: 0.01000 - Train loss: 0.63647 - Test loss: 0.56808\n",
      "Epoch 11308 - lr: 0.01000 - Train loss: 0.63706 - Test loss: 0.56827\n",
      "Epoch 11309 - lr: 0.01000 - Train loss: 0.63505 - Test loss: 0.56811\n",
      "Epoch 11310 - lr: 0.01000 - Train loss: 0.63652 - Test loss: 0.56844\n",
      "Epoch 11311 - lr: 0.01000 - Train loss: 0.64354 - Test loss: 0.56835\n",
      "Epoch 11312 - lr: 0.01000 - Train loss: 0.63793 - Test loss: 0.56811\n",
      "Epoch 11313 - lr: 0.01000 - Train loss: 0.63826 - Test loss: 0.56841\n",
      "Epoch 11314 - lr: 0.01000 - Train loss: 0.63970 - Test loss: 0.56812\n",
      "Epoch 11315 - lr: 0.01000 - Train loss: 0.62865 - Test loss: 0.56837\n",
      "Epoch 11316 - lr: 0.01000 - Train loss: 0.62966 - Test loss: 0.56864\n",
      "Epoch 11317 - lr: 0.01000 - Train loss: 0.62618 - Test loss: 0.56900\n",
      "Epoch 11318 - lr: 0.01000 - Train loss: 0.63927 - Test loss: 0.56919\n",
      "Epoch 11319 - lr: 0.01000 - Train loss: 0.63608 - Test loss: 0.56884\n",
      "Epoch 11320 - lr: 0.01000 - Train loss: 0.63635 - Test loss: 0.56910\n",
      "Epoch 11321 - lr: 0.01000 - Train loss: 0.64403 - Test loss: 0.56894\n",
      "Epoch 11322 - lr: 0.01000 - Train loss: 0.64291 - Test loss: 0.56848\n",
      "Epoch 11323 - lr: 0.01000 - Train loss: 0.63388 - Test loss: 0.56870\n",
      "Epoch 11324 - lr: 0.01000 - Train loss: 0.63536 - Test loss: 0.56839\n",
      "Epoch 11325 - lr: 0.01000 - Train loss: 0.63169 - Test loss: 0.56866\n",
      "Epoch 11326 - lr: 0.01000 - Train loss: 0.63628 - Test loss: 0.56854\n",
      "Epoch 11327 - lr: 0.01000 - Train loss: 0.63771 - Test loss: 0.56854\n",
      "Epoch 11328 - lr: 0.01000 - Train loss: 0.63536 - Test loss: 0.56827\n",
      "Epoch 11329 - lr: 0.01000 - Train loss: 0.63379 - Test loss: 0.56855\n",
      "Epoch 11330 - lr: 0.01000 - Train loss: 0.63568 - Test loss: 0.56829\n",
      "Epoch 11331 - lr: 0.01000 - Train loss: 0.63550 - Test loss: 0.56861\n",
      "Epoch 11332 - lr: 0.01000 - Train loss: 0.64220 - Test loss: 0.56844\n",
      "Epoch 11333 - lr: 0.01000 - Train loss: 0.64257 - Test loss: 0.56827\n",
      "Epoch 11334 - lr: 0.01000 - Train loss: 0.64368 - Test loss: 0.56819\n",
      "Epoch 11335 - lr: 0.01000 - Train loss: 0.63875 - Test loss: 0.56793\n",
      "Epoch 11336 - lr: 0.01000 - Train loss: 0.63107 - Test loss: 0.56817\n",
      "Epoch 11337 - lr: 0.01000 - Train loss: 0.63324 - Test loss: 0.56852\n",
      "Epoch 11338 - lr: 0.01000 - Train loss: 0.63486 - Test loss: 0.56832\n",
      "Epoch 11339 - lr: 0.01000 - Train loss: 0.63110 - Test loss: 0.56863\n",
      "Epoch 11340 - lr: 0.01000 - Train loss: 0.63726 - Test loss: 0.56861\n",
      "Epoch 11341 - lr: 0.01000 - Train loss: 0.63481 - Test loss: 0.56841\n",
      "Epoch 11342 - lr: 0.01000 - Train loss: 0.63429 - Test loss: 0.56871\n",
      "Epoch 11343 - lr: 0.01000 - Train loss: 0.63712 - Test loss: 0.56843\n",
      "Epoch 11344 - lr: 0.01000 - Train loss: 0.63870 - Test loss: 0.56864\n",
      "Epoch 11345 - lr: 0.01000 - Train loss: 0.63805 - Test loss: 0.56844\n",
      "Epoch 11346 - lr: 0.01000 - Train loss: 0.63892 - Test loss: 0.56867\n",
      "Epoch 11347 - lr: 0.01000 - Train loss: 0.63754 - Test loss: 0.56845\n",
      "Epoch 11348 - lr: 0.01000 - Train loss: 0.63572 - Test loss: 0.56863\n",
      "Epoch 11349 - lr: 0.01000 - Train loss: 0.63615 - Test loss: 0.56856\n",
      "Epoch 11350 - lr: 0.01000 - Train loss: 0.63720 - Test loss: 0.56857\n",
      "Epoch 11351 - lr: 0.01000 - Train loss: 0.63474 - Test loss: 0.56835\n",
      "Epoch 11352 - lr: 0.01000 - Train loss: 0.63312 - Test loss: 0.56866\n",
      "Epoch 11353 - lr: 0.01000 - Train loss: 0.63503 - Test loss: 0.56844\n",
      "Epoch 11354 - lr: 0.01000 - Train loss: 0.63400 - Test loss: 0.56876\n",
      "Epoch 11355 - lr: 0.01000 - Train loss: 0.63671 - Test loss: 0.56851\n",
      "Epoch 11356 - lr: 0.01000 - Train loss: 0.63852 - Test loss: 0.56877\n",
      "Epoch 11357 - lr: 0.01000 - Train loss: 0.63702 - Test loss: 0.56857\n",
      "Epoch 11358 - lr: 0.01000 - Train loss: 0.63651 - Test loss: 0.56876\n",
      "Epoch 11359 - lr: 0.01000 - Train loss: 0.63515 - Test loss: 0.56863\n",
      "Epoch 11360 - lr: 0.01000 - Train loss: 0.63818 - Test loss: 0.56895\n",
      "Epoch 11361 - lr: 0.01000 - Train loss: 0.64436 - Test loss: 0.56857\n",
      "Epoch 11362 - lr: 0.01000 - Train loss: 0.63818 - Test loss: 0.56886\n",
      "Epoch 11363 - lr: 0.01000 - Train loss: 0.64305 - Test loss: 0.56851\n",
      "Epoch 11364 - lr: 0.01000 - Train loss: 0.63539 - Test loss: 0.56881\n",
      "Epoch 11365 - lr: 0.01000 - Train loss: 0.64152 - Test loss: 0.56862\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11366 - lr: 0.01000 - Train loss: 0.63828 - Test loss: 0.56836\n",
      "Epoch 11367 - lr: 0.01000 - Train loss: 0.63667 - Test loss: 0.56834\n",
      "Epoch 11368 - lr: 0.01000 - Train loss: 0.63476 - Test loss: 0.56823\n",
      "Epoch 11369 - lr: 0.01000 - Train loss: 0.63703 - Test loss: 0.56860\n",
      "Epoch 11370 - lr: 0.01000 - Train loss: 0.63776 - Test loss: 0.56845\n",
      "Epoch 11371 - lr: 0.01000 - Train loss: 0.63852 - Test loss: 0.56876\n",
      "Epoch 11372 - lr: 0.01000 - Train loss: 0.64379 - Test loss: 0.56843\n",
      "Epoch 11373 - lr: 0.01000 - Train loss: 0.63745 - Test loss: 0.56876\n",
      "Epoch 11374 - lr: 0.01000 - Train loss: 0.63664 - Test loss: 0.56860\n",
      "Epoch 11375 - lr: 0.01000 - Train loss: 0.62717 - Test loss: 0.56892\n",
      "Epoch 11376 - lr: 0.01000 - Train loss: 0.62623 - Test loss: 0.56928\n",
      "Epoch 11377 - lr: 0.01000 - Train loss: 0.63967 - Test loss: 0.56952\n",
      "Epoch 11378 - lr: 0.01000 - Train loss: 0.63695 - Test loss: 0.56919\n",
      "Epoch 11379 - lr: 0.01000 - Train loss: 0.63885 - Test loss: 0.56942\n",
      "Epoch 11380 - lr: 0.01000 - Train loss: 0.64218 - Test loss: 0.56901\n",
      "Epoch 11381 - lr: 0.01000 - Train loss: 0.63164 - Test loss: 0.56923\n",
      "Epoch 11382 - lr: 0.01000 - Train loss: 0.63596 - Test loss: 0.56909\n",
      "Epoch 11383 - lr: 0.01000 - Train loss: 0.63807 - Test loss: 0.56914\n",
      "Epoch 11384 - lr: 0.01000 - Train loss: 0.63835 - Test loss: 0.56884\n",
      "Epoch 11385 - lr: 0.01000 - Train loss: 0.63652 - Test loss: 0.56877\n",
      "Epoch 11386 - lr: 0.01000 - Train loss: 0.63507 - Test loss: 0.56864\n",
      "Epoch 11387 - lr: 0.01000 - Train loss: 0.63839 - Test loss: 0.56894\n",
      "Epoch 11388 - lr: 0.01000 - Train loss: 0.63962 - Test loss: 0.56867\n",
      "Epoch 11389 - lr: 0.01000 - Train loss: 0.62679 - Test loss: 0.56900\n",
      "Epoch 11390 - lr: 0.01000 - Train loss: 0.63686 - Test loss: 0.56925\n",
      "Epoch 11391 - lr: 0.01000 - Train loss: 0.63483 - Test loss: 0.56908\n",
      "Epoch 11392 - lr: 0.01000 - Train loss: 0.63640 - Test loss: 0.56941\n",
      "Epoch 11393 - lr: 0.01000 - Train loss: 0.64183 - Test loss: 0.56925\n",
      "Epoch 11394 - lr: 0.01000 - Train loss: 0.63984 - Test loss: 0.56895\n",
      "Epoch 11395 - lr: 0.01000 - Train loss: 0.63453 - Test loss: 0.56872\n",
      "Epoch 11396 - lr: 0.01000 - Train loss: 0.63294 - Test loss: 0.56903\n",
      "Epoch 11397 - lr: 0.01000 - Train loss: 0.63477 - Test loss: 0.56882\n",
      "Epoch 11398 - lr: 0.01000 - Train loss: 0.63318 - Test loss: 0.56914\n",
      "Epoch 11399 - lr: 0.01000 - Train loss: 0.63511 - Test loss: 0.56891\n",
      "Epoch 11400 - lr: 0.01000 - Train loss: 0.63467 - Test loss: 0.56923\n",
      "Epoch 11401 - lr: 0.01000 - Train loss: 0.63981 - Test loss: 0.56900\n",
      "Epoch 11402 - lr: 0.01000 - Train loss: 0.63475 - Test loss: 0.56878\n",
      "Epoch 11403 - lr: 0.01000 - Train loss: 0.63422 - Test loss: 0.56910\n",
      "Epoch 11404 - lr: 0.01000 - Train loss: 0.63808 - Test loss: 0.56886\n",
      "Epoch 11405 - lr: 0.01000 - Train loss: 0.63615 - Test loss: 0.56882\n",
      "Epoch 11406 - lr: 0.01000 - Train loss: 0.63543 - Test loss: 0.56875\n",
      "Epoch 11407 - lr: 0.01000 - Train loss: 0.63753 - Test loss: 0.56887\n",
      "Epoch 11408 - lr: 0.01000 - Train loss: 0.63908 - Test loss: 0.56869\n",
      "Epoch 11409 - lr: 0.01000 - Train loss: 0.63460 - Test loss: 0.56854\n",
      "Epoch 11410 - lr: 0.01000 - Train loss: 0.63602 - Test loss: 0.56891\n",
      "Epoch 11411 - lr: 0.01000 - Train loss: 0.64110 - Test loss: 0.56883\n",
      "Epoch 11412 - lr: 0.01000 - Train loss: 0.63871 - Test loss: 0.56862\n",
      "Epoch 11413 - lr: 0.01000 - Train loss: 0.63484 - Test loss: 0.56852\n",
      "Epoch 11414 - lr: 0.01000 - Train loss: 0.63790 - Test loss: 0.56887\n",
      "Epoch 11415 - lr: 0.01000 - Train loss: 0.64025 - Test loss: 0.56864\n",
      "Epoch 11416 - lr: 0.01000 - Train loss: 0.62610 - Test loss: 0.56902\n",
      "Epoch 11417 - lr: 0.01000 - Train loss: 0.63172 - Test loss: 0.56931\n",
      "Epoch 11418 - lr: 0.01000 - Train loss: 0.63647 - Test loss: 0.56967\n",
      "Epoch 11419 - lr: 0.01000 - Train loss: 0.63768 - Test loss: 0.56947\n",
      "Epoch 11420 - lr: 0.01000 - Train loss: 0.63858 - Test loss: 0.56967\n",
      "Epoch 11421 - lr: 0.01000 - Train loss: 0.63910 - Test loss: 0.56936\n",
      "Epoch 11422 - lr: 0.01000 - Train loss: 0.62826 - Test loss: 0.56960\n",
      "Epoch 11423 - lr: 0.01000 - Train loss: 0.62858 - Test loss: 0.56987\n",
      "Epoch 11424 - lr: 0.01000 - Train loss: 0.62697 - Test loss: 0.57017\n",
      "Epoch 11425 - lr: 0.01000 - Train loss: 0.63627 - Test loss: 0.57033\n",
      "Epoch 11426 - lr: 0.01000 - Train loss: 0.63557 - Test loss: 0.57009\n",
      "Epoch 11427 - lr: 0.01000 - Train loss: 0.63823 - Test loss: 0.57017\n",
      "Epoch 11428 - lr: 0.01000 - Train loss: 0.64044 - Test loss: 0.56991\n",
      "Epoch 11429 - lr: 0.01000 - Train loss: 0.63626 - Test loss: 0.56955\n",
      "Epoch 11430 - lr: 0.01000 - Train loss: 0.63818 - Test loss: 0.56975\n",
      "Epoch 11431 - lr: 0.01000 - Train loss: 0.63939 - Test loss: 0.56942\n",
      "Epoch 11432 - lr: 0.01000 - Train loss: 0.62642 - Test loss: 0.56971\n",
      "Epoch 11433 - lr: 0.01000 - Train loss: 0.63755 - Test loss: 0.56993\n",
      "Epoch 11434 - lr: 0.01000 - Train loss: 0.63526 - Test loss: 0.56968\n",
      "Epoch 11435 - lr: 0.01000 - Train loss: 0.63719 - Test loss: 0.56997\n",
      "Epoch 11436 - lr: 0.01000 - Train loss: 0.64135 - Test loss: 0.56959\n",
      "Epoch 11437 - lr: 0.01000 - Train loss: 0.62913 - Test loss: 0.56983\n",
      "Epoch 11438 - lr: 0.01000 - Train loss: 0.63821 - Test loss: 0.57010\n",
      "Epoch 11439 - lr: 0.01000 - Train loss: 0.64833 - Test loss: 0.56963\n",
      "Epoch 11440 - lr: 0.01000 - Train loss: 0.63554 - Test loss: 0.56945\n",
      "Epoch 11441 - lr: 0.01000 - Train loss: 0.63732 - Test loss: 0.56949\n",
      "Epoch 11442 - lr: 0.01000 - Train loss: 0.63727 - Test loss: 0.56923\n",
      "Epoch 11443 - lr: 0.01000 - Train loss: 0.63690 - Test loss: 0.56926\n",
      "Epoch 11444 - lr: 0.01000 - Train loss: 0.63536 - Test loss: 0.56904\n",
      "Epoch 11445 - lr: 0.01000 - Train loss: 0.63716 - Test loss: 0.56939\n",
      "Epoch 11446 - lr: 0.01000 - Train loss: 0.63866 - Test loss: 0.56916\n",
      "Epoch 11447 - lr: 0.01000 - Train loss: 0.62824 - Test loss: 0.56947\n",
      "Epoch 11448 - lr: 0.01000 - Train loss: 0.62723 - Test loss: 0.56984\n",
      "Epoch 11449 - lr: 0.01000 - Train loss: 0.63321 - Test loss: 0.57007\n",
      "Epoch 11450 - lr: 0.01000 - Train loss: 0.63822 - Test loss: 0.57027\n",
      "Epoch 11451 - lr: 0.01000 - Train loss: 0.63515 - Test loss: 0.57006\n",
      "Epoch 11452 - lr: 0.01000 - Train loss: 0.63011 - Test loss: 0.57023\n",
      "Epoch 11453 - lr: 0.01000 - Train loss: 0.62891 - Test loss: 0.57049\n",
      "Epoch 11454 - lr: 0.01000 - Train loss: 0.63797 - Test loss: 0.57073\n",
      "Epoch 11455 - lr: 0.01000 - Train loss: 0.65246 - Test loss: 0.57021\n",
      "Epoch 11456 - lr: 0.01000 - Train loss: 0.63674 - Test loss: 0.56983\n",
      "Epoch 11457 - lr: 0.01000 - Train loss: 0.63776 - Test loss: 0.56990\n",
      "Epoch 11458 - lr: 0.01000 - Train loss: 0.64249 - Test loss: 0.56977\n",
      "Epoch 11459 - lr: 0.01000 - Train loss: 0.63630 - Test loss: 0.56953\n",
      "Epoch 11460 - lr: 0.01000 - Train loss: 0.63682 - Test loss: 0.56971\n",
      "Epoch 11461 - lr: 0.01000 - Train loss: 0.63465 - Test loss: 0.56952\n",
      "Epoch 11462 - lr: 0.01000 - Train loss: 0.63625 - Test loss: 0.56986\n",
      "Epoch 11463 - lr: 0.01000 - Train loss: 0.63792 - Test loss: 0.56967\n",
      "Epoch 11464 - lr: 0.01000 - Train loss: 0.63792 - Test loss: 0.56975\n",
      "Epoch 11465 - lr: 0.01000 - Train loss: 0.64221 - Test loss: 0.56963\n",
      "Epoch 11466 - lr: 0.01000 - Train loss: 0.63995 - Test loss: 0.56944\n",
      "Epoch 11467 - lr: 0.01000 - Train loss: 0.63449 - Test loss: 0.56920\n",
      "Epoch 11468 - lr: 0.01000 - Train loss: 0.63342 - Test loss: 0.56953\n",
      "Epoch 11469 - lr: 0.01000 - Train loss: 0.63578 - Test loss: 0.56929\n",
      "Epoch 11470 - lr: 0.01000 - Train loss: 0.63753 - Test loss: 0.56963\n",
      "Epoch 11471 - lr: 0.01000 - Train loss: 0.64239 - Test loss: 0.56931\n",
      "Epoch 11472 - lr: 0.01000 - Train loss: 0.63435 - Test loss: 0.56961\n",
      "Epoch 11473 - lr: 0.01000 - Train loss: 0.63861 - Test loss: 0.56937\n",
      "Epoch 11474 - lr: 0.01000 - Train loss: 0.63483 - Test loss: 0.56925\n",
      "Epoch 11475 - lr: 0.01000 - Train loss: 0.63808 - Test loss: 0.56954\n",
      "Epoch 11476 - lr: 0.01000 - Train loss: 0.63699 - Test loss: 0.56936\n",
      "Epoch 11477 - lr: 0.01000 - Train loss: 0.63486 - Test loss: 0.56956\n",
      "Epoch 11478 - lr: 0.01000 - Train loss: 0.63611 - Test loss: 0.56954\n",
      "Epoch 11479 - lr: 0.01000 - Train loss: 0.63503 - Test loss: 0.56944\n",
      "Epoch 11480 - lr: 0.01000 - Train loss: 0.63798 - Test loss: 0.56965\n",
      "Epoch 11481 - lr: 0.01000 - Train loss: 0.64136 - Test loss: 0.56954\n",
      "Epoch 11482 - lr: 0.01000 - Train loss: 0.64032 - Test loss: 0.56932\n",
      "Epoch 11483 - lr: 0.01000 - Train loss: 0.63556 - Test loss: 0.56907\n",
      "Epoch 11484 - lr: 0.01000 - Train loss: 0.63698 - Test loss: 0.56944\n",
      "Epoch 11485 - lr: 0.01000 - Train loss: 0.63570 - Test loss: 0.56932\n",
      "Epoch 11486 - lr: 0.01000 - Train loss: 0.63614 - Test loss: 0.56954\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11487 - lr: 0.01000 - Train loss: 0.63453 - Test loss: 0.56942\n",
      "Epoch 11488 - lr: 0.01000 - Train loss: 0.63715 - Test loss: 0.56979\n",
      "Epoch 11489 - lr: 0.01000 - Train loss: 0.63781 - Test loss: 0.56957\n",
      "Epoch 11490 - lr: 0.01000 - Train loss: 0.63225 - Test loss: 0.56977\n",
      "Epoch 11491 - lr: 0.01000 - Train loss: 0.63803 - Test loss: 0.57011\n",
      "Epoch 11492 - lr: 0.01000 - Train loss: 0.64714 - Test loss: 0.56969\n",
      "Epoch 11493 - lr: 0.01000 - Train loss: 0.63644 - Test loss: 0.56963\n",
      "Epoch 11494 - lr: 0.01000 - Train loss: 0.63429 - Test loss: 0.56946\n",
      "Epoch 11495 - lr: 0.01000 - Train loss: 0.63556 - Test loss: 0.56980\n",
      "Epoch 11496 - lr: 0.01000 - Train loss: 0.64312 - Test loss: 0.56974\n",
      "Epoch 11497 - lr: 0.01000 - Train loss: 0.64032 - Test loss: 0.56943\n",
      "Epoch 11498 - lr: 0.01000 - Train loss: 0.62629 - Test loss: 0.56978\n",
      "Epoch 11499 - lr: 0.01000 - Train loss: 0.62846 - Test loss: 0.57008\n",
      "Epoch 11500 - lr: 0.01000 - Train loss: 0.62653 - Test loss: 0.57044\n",
      "Epoch 11501 - lr: 0.01000 - Train loss: 0.63697 - Test loss: 0.57064\n",
      "Epoch 11502 - lr: 0.01000 - Train loss: 0.63458 - Test loss: 0.57037\n",
      "Epoch 11503 - lr: 0.01000 - Train loss: 0.63514 - Test loss: 0.57063\n",
      "Epoch 11504 - lr: 0.01000 - Train loss: 0.64284 - Test loss: 0.57045\n",
      "Epoch 11505 - lr: 0.01000 - Train loss: 0.63551 - Test loss: 0.57019\n",
      "Epoch 11506 - lr: 0.01000 - Train loss: 0.63635 - Test loss: 0.57031\n",
      "Epoch 11507 - lr: 0.01000 - Train loss: 0.63455 - Test loss: 0.57010\n",
      "Epoch 11508 - lr: 0.01000 - Train loss: 0.63683 - Test loss: 0.57040\n",
      "Epoch 11509 - lr: 0.01000 - Train loss: 0.63542 - Test loss: 0.57019\n",
      "Epoch 11510 - lr: 0.01000 - Train loss: 0.63536 - Test loss: 0.57033\n",
      "Epoch 11511 - lr: 0.01000 - Train loss: 0.63550 - Test loss: 0.57020\n",
      "Epoch 11512 - lr: 0.01000 - Train loss: 0.63716 - Test loss: 0.57021\n",
      "Epoch 11513 - lr: 0.01000 - Train loss: 0.63574 - Test loss: 0.56991\n",
      "Epoch 11514 - lr: 0.01000 - Train loss: 0.63758 - Test loss: 0.57021\n",
      "Epoch 11515 - lr: 0.01000 - Train loss: 0.64297 - Test loss: 0.56984\n",
      "Epoch 11516 - lr: 0.01000 - Train loss: 0.63617 - Test loss: 0.57013\n",
      "Epoch 11517 - lr: 0.01000 - Train loss: 0.64100 - Test loss: 0.56999\n",
      "Epoch 11518 - lr: 0.01000 - Train loss: 0.63656 - Test loss: 0.56968\n",
      "Epoch 11519 - lr: 0.01000 - Train loss: 0.63818 - Test loss: 0.56993\n",
      "Epoch 11520 - lr: 0.01000 - Train loss: 0.63613 - Test loss: 0.56977\n",
      "Epoch 11521 - lr: 0.01000 - Train loss: 0.62824 - Test loss: 0.57007\n",
      "Epoch 11522 - lr: 0.01000 - Train loss: 0.63591 - Test loss: 0.57040\n",
      "Epoch 11523 - lr: 0.01000 - Train loss: 0.64320 - Test loss: 0.57031\n",
      "Epoch 11524 - lr: 0.01000 - Train loss: 0.63634 - Test loss: 0.57006\n",
      "Epoch 11525 - lr: 0.01000 - Train loss: 0.63680 - Test loss: 0.57022\n",
      "Epoch 11526 - lr: 0.01000 - Train loss: 0.63411 - Test loss: 0.56999\n",
      "Epoch 11527 - lr: 0.01000 - Train loss: 0.63197 - Test loss: 0.57029\n",
      "Epoch 11528 - lr: 0.01000 - Train loss: 0.63428 - Test loss: 0.57007\n",
      "Epoch 11529 - lr: 0.01000 - Train loss: 0.63421 - Test loss: 0.57037\n",
      "Epoch 11530 - lr: 0.01000 - Train loss: 0.63814 - Test loss: 0.57007\n",
      "Epoch 11531 - lr: 0.01000 - Train loss: 0.63566 - Test loss: 0.56997\n",
      "Epoch 11532 - lr: 0.01000 - Train loss: 0.63593 - Test loss: 0.56991\n",
      "Epoch 11533 - lr: 0.01000 - Train loss: 0.63478 - Test loss: 0.56980\n",
      "Epoch 11534 - lr: 0.01000 - Train loss: 0.63796 - Test loss: 0.57003\n",
      "Epoch 11535 - lr: 0.01000 - Train loss: 0.63860 - Test loss: 0.56987\n",
      "Epoch 11536 - lr: 0.01000 - Train loss: 0.63577 - Test loss: 0.56979\n",
      "Epoch 11537 - lr: 0.01000 - Train loss: 0.63547 - Test loss: 0.56975\n",
      "Epoch 11538 - lr: 0.01000 - Train loss: 0.63600 - Test loss: 0.56976\n",
      "Epoch 11539 - lr: 0.01000 - Train loss: 0.63431 - Test loss: 0.56965\n",
      "Epoch 11540 - lr: 0.01000 - Train loss: 0.63718 - Test loss: 0.57002\n",
      "Epoch 11541 - lr: 0.01000 - Train loss: 0.63922 - Test loss: 0.56978\n",
      "Epoch 11542 - lr: 0.01000 - Train loss: 0.62570 - Test loss: 0.57016\n",
      "Epoch 11543 - lr: 0.01000 - Train loss: 0.63802 - Test loss: 0.57043\n",
      "Epoch 11544 - lr: 0.01000 - Train loss: 0.63521 - Test loss: 0.57019\n",
      "Epoch 11545 - lr: 0.01000 - Train loss: 0.63655 - Test loss: 0.57052\n",
      "Epoch 11546 - lr: 0.01000 - Train loss: 0.63513 - Test loss: 0.57035\n",
      "Epoch 11547 - lr: 0.01000 - Train loss: 0.63364 - Test loss: 0.57052\n",
      "Epoch 11548 - lr: 0.01000 - Train loss: 0.63753 - Test loss: 0.57060\n",
      "Epoch 11549 - lr: 0.01000 - Train loss: 0.63994 - Test loss: 0.57035\n",
      "Epoch 11550 - lr: 0.01000 - Train loss: 0.63533 - Test loss: 0.57005\n",
      "Epoch 11551 - lr: 0.01000 - Train loss: 0.63691 - Test loss: 0.57037\n",
      "Epoch 11552 - lr: 0.01000 - Train loss: 0.63754 - Test loss: 0.57014\n",
      "Epoch 11553 - lr: 0.01000 - Train loss: 0.63212 - Test loss: 0.57034\n",
      "Epoch 11554 - lr: 0.01000 - Train loss: 0.63785 - Test loss: 0.57066\n",
      "Epoch 11555 - lr: 0.01000 - Train loss: 0.64580 - Test loss: 0.57024\n",
      "Epoch 11556 - lr: 0.01000 - Train loss: 0.63750 - Test loss: 0.57030\n",
      "Epoch 11557 - lr: 0.01000 - Train loss: 0.63934 - Test loss: 0.57006\n",
      "Epoch 11558 - lr: 0.01000 - Train loss: 0.63401 - Test loss: 0.56984\n",
      "Epoch 11559 - lr: 0.01000 - Train loss: 0.63182 - Test loss: 0.57017\n",
      "Epoch 11560 - lr: 0.01000 - Train loss: 0.63415 - Test loss: 0.57001\n",
      "Epoch 11561 - lr: 0.01000 - Train loss: 0.63493 - Test loss: 0.57037\n",
      "Epoch 11562 - lr: 0.01000 - Train loss: 0.64235 - Test loss: 0.57028\n",
      "Epoch 11563 - lr: 0.01000 - Train loss: 0.63612 - Test loss: 0.57010\n",
      "Epoch 11564 - lr: 0.01000 - Train loss: 0.63097 - Test loss: 0.57038\n",
      "Epoch 11565 - lr: 0.01000 - Train loss: 0.63536 - Test loss: 0.57029\n",
      "Epoch 11566 - lr: 0.01000 - Train loss: 0.63689 - Test loss: 0.57033\n",
      "Epoch 11567 - lr: 0.01000 - Train loss: 0.63510 - Test loss: 0.57007\n",
      "Epoch 11568 - lr: 0.01000 - Train loss: 0.63621 - Test loss: 0.57041\n",
      "Epoch 11569 - lr: 0.01000 - Train loss: 0.63695 - Test loss: 0.57024\n",
      "Epoch 11570 - lr: 0.01000 - Train loss: 0.63768 - Test loss: 0.57052\n",
      "Epoch 11571 - lr: 0.01000 - Train loss: 0.64544 - Test loss: 0.57013\n",
      "Epoch 11572 - lr: 0.01000 - Train loss: 0.63773 - Test loss: 0.57024\n",
      "Epoch 11573 - lr: 0.01000 - Train loss: 0.64179 - Test loss: 0.57010\n",
      "Epoch 11574 - lr: 0.01000 - Train loss: 0.64290 - Test loss: 0.57001\n",
      "Epoch 11575 - lr: 0.01000 - Train loss: 0.63659 - Test loss: 0.56981\n",
      "Epoch 11576 - lr: 0.01000 - Train loss: 0.63474 - Test loss: 0.57002\n",
      "Epoch 11577 - lr: 0.01000 - Train loss: 0.63555 - Test loss: 0.57000\n",
      "Epoch 11578 - lr: 0.01000 - Train loss: 0.63580 - Test loss: 0.57000\n",
      "Epoch 11579 - lr: 0.01000 - Train loss: 0.63467 - Test loss: 0.56992\n",
      "Epoch 11580 - lr: 0.01000 - Train loss: 0.63796 - Test loss: 0.57017\n",
      "Epoch 11581 - lr: 0.01000 - Train loss: 0.64015 - Test loss: 0.57005\n",
      "Epoch 11582 - lr: 0.01000 - Train loss: 0.63402 - Test loss: 0.56980\n",
      "Epoch 11583 - lr: 0.01000 - Train loss: 0.62870 - Test loss: 0.57017\n",
      "Epoch 11584 - lr: 0.01000 - Train loss: 0.63805 - Test loss: 0.57050\n",
      "Epoch 11585 - lr: 0.01000 - Train loss: 0.64460 - Test loss: 0.57014\n",
      "Epoch 11586 - lr: 0.01000 - Train loss: 0.63823 - Test loss: 0.57037\n",
      "Epoch 11587 - lr: 0.01000 - Train loss: 0.63592 - Test loss: 0.57021\n",
      "Epoch 11588 - lr: 0.01000 - Train loss: 0.62619 - Test loss: 0.57054\n",
      "Epoch 11589 - lr: 0.01000 - Train loss: 0.62764 - Test loss: 0.57085\n",
      "Epoch 11590 - lr: 0.01000 - Train loss: 0.62970 - Test loss: 0.57109\n",
      "Epoch 11591 - lr: 0.01000 - Train loss: 0.62856 - Test loss: 0.57138\n",
      "Epoch 11592 - lr: 0.01000 - Train loss: 0.63801 - Test loss: 0.57163\n",
      "Epoch 11593 - lr: 0.01000 - Train loss: 0.65242 - Test loss: 0.57110\n",
      "Epoch 11594 - lr: 0.01000 - Train loss: 0.63502 - Test loss: 0.57067\n",
      "Epoch 11595 - lr: 0.01000 - Train loss: 0.63393 - Test loss: 0.57093\n",
      "Epoch 11596 - lr: 0.01000 - Train loss: 0.63705 - Test loss: 0.57058\n",
      "Epoch 11597 - lr: 0.01000 - Train loss: 0.63769 - Test loss: 0.57066\n",
      "Epoch 11598 - lr: 0.01000 - Train loss: 0.64084 - Test loss: 0.57043\n",
      "Epoch 11599 - lr: 0.01000 - Train loss: 0.63695 - Test loss: 0.57012\n",
      "Epoch 11600 - lr: 0.01000 - Train loss: 0.63744 - Test loss: 0.57023\n",
      "Epoch 11601 - lr: 0.01000 - Train loss: 0.63898 - Test loss: 0.57001\n",
      "Epoch 11602 - lr: 0.01000 - Train loss: 0.63363 - Test loss: 0.56984\n",
      "Epoch 11603 - lr: 0.01000 - Train loss: 0.63168 - Test loss: 0.57021\n",
      "Epoch 11604 - lr: 0.01000 - Train loss: 0.63386 - Test loss: 0.57008\n",
      "Epoch 11605 - lr: 0.01000 - Train loss: 0.63371 - Test loss: 0.57045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11606 - lr: 0.01000 - Train loss: 0.63668 - Test loss: 0.57021\n",
      "Epoch 11607 - lr: 0.01000 - Train loss: 0.63755 - Test loss: 0.57038\n",
      "Epoch 11608 - lr: 0.01000 - Train loss: 0.64197 - Test loss: 0.57028\n",
      "Epoch 11609 - lr: 0.01000 - Train loss: 0.64193 - Test loss: 0.57019\n",
      "Epoch 11610 - lr: 0.01000 - Train loss: 0.64297 - Test loss: 0.57012\n",
      "Epoch 11611 - lr: 0.01000 - Train loss: 0.63744 - Test loss: 0.56992\n",
      "Epoch 11612 - lr: 0.01000 - Train loss: 0.63174 - Test loss: 0.57016\n",
      "Epoch 11613 - lr: 0.01000 - Train loss: 0.63780 - Test loss: 0.57055\n",
      "Epoch 11614 - lr: 0.01000 - Train loss: 0.64427 - Test loss: 0.57021\n",
      "Epoch 11615 - lr: 0.01000 - Train loss: 0.63832 - Test loss: 0.57047\n",
      "Epoch 11616 - lr: 0.01000 - Train loss: 0.63737 - Test loss: 0.57027\n",
      "Epoch 11617 - lr: 0.01000 - Train loss: 0.63274 - Test loss: 0.57047\n",
      "Epoch 11618 - lr: 0.01000 - Train loss: 0.63839 - Test loss: 0.57071\n",
      "Epoch 11619 - lr: 0.01000 - Train loss: 0.64030 - Test loss: 0.57055\n",
      "Epoch 11620 - lr: 0.01000 - Train loss: 0.63367 - Test loss: 0.57025\n",
      "Epoch 11621 - lr: 0.01000 - Train loss: 0.62569 - Test loss: 0.57064\n",
      "Epoch 11622 - lr: 0.01000 - Train loss: 0.63267 - Test loss: 0.57086\n",
      "Epoch 11623 - lr: 0.01000 - Train loss: 0.63846 - Test loss: 0.57109\n",
      "Epoch 11624 - lr: 0.01000 - Train loss: 0.63658 - Test loss: 0.57087\n",
      "Epoch 11625 - lr: 0.01000 - Train loss: 0.63331 - Test loss: 0.57111\n",
      "Epoch 11626 - lr: 0.01000 - Train loss: 0.63442 - Test loss: 0.57076\n",
      "Epoch 11627 - lr: 0.01000 - Train loss: 0.62801 - Test loss: 0.57106\n",
      "Epoch 11628 - lr: 0.01000 - Train loss: 0.63646 - Test loss: 0.57136\n",
      "Epoch 11629 - lr: 0.01000 - Train loss: 0.63968 - Test loss: 0.57112\n",
      "Epoch 11630 - lr: 0.01000 - Train loss: 0.63396 - Test loss: 0.57082\n",
      "Epoch 11631 - lr: 0.01000 - Train loss: 0.63276 - Test loss: 0.57110\n",
      "Epoch 11632 - lr: 0.01000 - Train loss: 0.63398 - Test loss: 0.57078\n",
      "Epoch 11633 - lr: 0.01000 - Train loss: 0.62694 - Test loss: 0.57109\n",
      "Epoch 11634 - lr: 0.01000 - Train loss: 0.62888 - Test loss: 0.57137\n",
      "Epoch 11635 - lr: 0.01000 - Train loss: 0.63849 - Test loss: 0.57160\n",
      "Epoch 11636 - lr: 0.01000 - Train loss: 0.64544 - Test loss: 0.57111\n",
      "Epoch 11637 - lr: 0.01000 - Train loss: 0.63807 - Test loss: 0.57116\n",
      "Epoch 11638 - lr: 0.01000 - Train loss: 0.64321 - Test loss: 0.57101\n",
      "Epoch 11639 - lr: 0.01000 - Train loss: 0.63734 - Test loss: 0.57071\n",
      "Epoch 11640 - lr: 0.01000 - Train loss: 0.63297 - Test loss: 0.57087\n",
      "Epoch 11641 - lr: 0.01000 - Train loss: 0.63816 - Test loss: 0.57103\n",
      "Epoch 11642 - lr: 0.01000 - Train loss: 0.64346 - Test loss: 0.57093\n",
      "Epoch 11643 - lr: 0.01000 - Train loss: 0.64083 - Test loss: 0.57056\n",
      "Epoch 11644 - lr: 0.01000 - Train loss: 0.62883 - Test loss: 0.57084\n",
      "Epoch 11645 - lr: 0.01000 - Train loss: 0.63855 - Test loss: 0.57113\n",
      "Epoch 11646 - lr: 0.01000 - Train loss: 0.64413 - Test loss: 0.57073\n",
      "Epoch 11647 - lr: 0.01000 - Train loss: 0.63847 - Test loss: 0.57099\n",
      "Epoch 11648 - lr: 0.01000 - Train loss: 0.64318 - Test loss: 0.57060\n",
      "Epoch 11649 - lr: 0.01000 - Train loss: 0.63731 - Test loss: 0.57091\n",
      "Epoch 11650 - lr: 0.01000 - Train loss: 0.63635 - Test loss: 0.57072\n",
      "Epoch 11651 - lr: 0.01000 - Train loss: 0.62638 - Test loss: 0.57102\n",
      "Epoch 11652 - lr: 0.01000 - Train loss: 0.62701 - Test loss: 0.57132\n",
      "Epoch 11653 - lr: 0.01000 - Train loss: 0.63325 - Test loss: 0.57150\n",
      "Epoch 11654 - lr: 0.01000 - Train loss: 0.63844 - Test loss: 0.57159\n",
      "Epoch 11655 - lr: 0.01000 - Train loss: 0.64322 - Test loss: 0.57137\n",
      "Epoch 11656 - lr: 0.01000 - Train loss: 0.64239 - Test loss: 0.57114\n",
      "Epoch 11657 - lr: 0.01000 - Train loss: 0.64003 - Test loss: 0.57076\n",
      "Epoch 11658 - lr: 0.01000 - Train loss: 0.63337 - Test loss: 0.57047\n",
      "Epoch 11659 - lr: 0.01000 - Train loss: 0.62572 - Test loss: 0.57083\n",
      "Epoch 11660 - lr: 0.01000 - Train loss: 0.63516 - Test loss: 0.57105\n",
      "Epoch 11661 - lr: 0.01000 - Train loss: 0.63469 - Test loss: 0.57092\n",
      "Epoch 11662 - lr: 0.01000 - Train loss: 0.63849 - Test loss: 0.57117\n",
      "Epoch 11663 - lr: 0.01000 - Train loss: 0.63652 - Test loss: 0.57096\n",
      "Epoch 11664 - lr: 0.01000 - Train loss: 0.63062 - Test loss: 0.57121\n",
      "Epoch 11665 - lr: 0.01000 - Train loss: 0.63588 - Test loss: 0.57111\n",
      "Epoch 11666 - lr: 0.01000 - Train loss: 0.63516 - Test loss: 0.57099\n",
      "Epoch 11667 - lr: 0.01000 - Train loss: 0.63730 - Test loss: 0.57104\n",
      "Epoch 11668 - lr: 0.01000 - Train loss: 0.63473 - Test loss: 0.57071\n",
      "Epoch 11669 - lr: 0.01000 - Train loss: 0.63058 - Test loss: 0.57102\n",
      "Epoch 11670 - lr: 0.01000 - Train loss: 0.63527 - Test loss: 0.57094\n",
      "Epoch 11671 - lr: 0.01000 - Train loss: 0.63660 - Test loss: 0.57096\n",
      "Epoch 11672 - lr: 0.01000 - Train loss: 0.63342 - Test loss: 0.57071\n",
      "Epoch 11673 - lr: 0.01000 - Train loss: 0.62576 - Test loss: 0.57110\n",
      "Epoch 11674 - lr: 0.01000 - Train loss: 0.62849 - Test loss: 0.57139\n",
      "Epoch 11675 - lr: 0.01000 - Train loss: 0.62521 - Test loss: 0.57177\n",
      "Epoch 11676 - lr: 0.01000 - Train loss: 0.63640 - Test loss: 0.57195\n",
      "Epoch 11677 - lr: 0.01000 - Train loss: 0.63380 - Test loss: 0.57167\n",
      "Epoch 11678 - lr: 0.01000 - Train loss: 0.63229 - Test loss: 0.57193\n",
      "Epoch 11679 - lr: 0.01000 - Train loss: 0.63395 - Test loss: 0.57159\n",
      "Epoch 11680 - lr: 0.01000 - Train loss: 0.62997 - Test loss: 0.57185\n",
      "Epoch 11681 - lr: 0.01000 - Train loss: 0.63635 - Test loss: 0.57176\n",
      "Epoch 11682 - lr: 0.01000 - Train loss: 0.63376 - Test loss: 0.57147\n",
      "Epoch 11683 - lr: 0.01000 - Train loss: 0.63211 - Test loss: 0.57174\n",
      "Epoch 11684 - lr: 0.01000 - Train loss: 0.63400 - Test loss: 0.57145\n",
      "Epoch 11685 - lr: 0.01000 - Train loss: 0.63242 - Test loss: 0.57173\n",
      "Epoch 11686 - lr: 0.01000 - Train loss: 0.63442 - Test loss: 0.57143\n",
      "Epoch 11687 - lr: 0.01000 - Train loss: 0.63434 - Test loss: 0.57173\n",
      "Epoch 11688 - lr: 0.01000 - Train loss: 0.64132 - Test loss: 0.57154\n",
      "Epoch 11689 - lr: 0.01000 - Train loss: 0.64249 - Test loss: 0.57141\n",
      "Epoch 11690 - lr: 0.01000 - Train loss: 0.63884 - Test loss: 0.57109\n",
      "Epoch 11691 - lr: 0.01000 - Train loss: 0.62552 - Test loss: 0.57142\n",
      "Epoch 11692 - lr: 0.01000 - Train loss: 0.63699 - Test loss: 0.57166\n",
      "Epoch 11693 - lr: 0.01000 - Train loss: 0.63391 - Test loss: 0.57140\n",
      "Epoch 11694 - lr: 0.01000 - Train loss: 0.63192 - Test loss: 0.57170\n",
      "Epoch 11695 - lr: 0.01000 - Train loss: 0.63391 - Test loss: 0.57143\n",
      "Epoch 11696 - lr: 0.01000 - Train loss: 0.63286 - Test loss: 0.57173\n",
      "Epoch 11697 - lr: 0.01000 - Train loss: 0.63546 - Test loss: 0.57142\n",
      "Epoch 11698 - lr: 0.01000 - Train loss: 0.63736 - Test loss: 0.57169\n",
      "Epoch 11699 - lr: 0.01000 - Train loss: 0.64269 - Test loss: 0.57130\n",
      "Epoch 11700 - lr: 0.01000 - Train loss: 0.63621 - Test loss: 0.57159\n",
      "Epoch 11701 - lr: 0.01000 - Train loss: 0.63521 - Test loss: 0.57141\n",
      "Epoch 11702 - lr: 0.01000 - Train loss: 0.62495 - Test loss: 0.57175\n",
      "Epoch 11703 - lr: 0.01000 - Train loss: 0.63784 - Test loss: 0.57197\n",
      "Epoch 11704 - lr: 0.01000 - Train loss: 0.63492 - Test loss: 0.57165\n",
      "Epoch 11705 - lr: 0.01000 - Train loss: 0.63621 - Test loss: 0.57195\n",
      "Epoch 11706 - lr: 0.01000 - Train loss: 0.63477 - Test loss: 0.57173\n",
      "Epoch 11707 - lr: 0.01000 - Train loss: 0.63397 - Test loss: 0.57186\n",
      "Epoch 11708 - lr: 0.01000 - Train loss: 0.63605 - Test loss: 0.57180\n",
      "Epoch 11709 - lr: 0.01000 - Train loss: 0.63375 - Test loss: 0.57153\n",
      "Epoch 11710 - lr: 0.01000 - Train loss: 0.63384 - Test loss: 0.57183\n",
      "Epoch 11711 - lr: 0.01000 - Train loss: 0.63956 - Test loss: 0.57156\n",
      "Epoch 11712 - lr: 0.01000 - Train loss: 0.63489 - Test loss: 0.57125\n",
      "Epoch 11713 - lr: 0.01000 - Train loss: 0.63645 - Test loss: 0.57157\n",
      "Epoch 11714 - lr: 0.01000 - Train loss: 0.63668 - Test loss: 0.57135\n",
      "Epoch 11715 - lr: 0.01000 - Train loss: 0.63308 - Test loss: 0.57154\n",
      "Epoch 11716 - lr: 0.01000 - Train loss: 0.63694 - Test loss: 0.57163\n",
      "Epoch 11717 - lr: 0.01000 - Train loss: 0.63855 - Test loss: 0.57137\n",
      "Epoch 11718 - lr: 0.01000 - Train loss: 0.63358 - Test loss: 0.57115\n",
      "Epoch 11719 - lr: 0.01000 - Train loss: 0.63335 - Test loss: 0.57148\n",
      "Epoch 11720 - lr: 0.01000 - Train loss: 0.63745 - Test loss: 0.57123\n",
      "Epoch 11721 - lr: 0.01000 - Train loss: 0.63453 - Test loss: 0.57114\n",
      "Epoch 11722 - lr: 0.01000 - Train loss: 0.63649 - Test loss: 0.57123\n",
      "Epoch 11723 - lr: 0.01000 - Train loss: 0.63670 - Test loss: 0.57101\n",
      "Epoch 11724 - lr: 0.01000 - Train loss: 0.63550 - Test loss: 0.57102\n",
      "Epoch 11725 - lr: 0.01000 - Train loss: 0.63359 - Test loss: 0.57089\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11726 - lr: 0.01000 - Train loss: 0.63541 - Test loss: 0.57128\n",
      "Epoch 11727 - lr: 0.01000 - Train loss: 0.63731 - Test loss: 0.57116\n",
      "Epoch 11728 - lr: 0.01000 - Train loss: 0.63635 - Test loss: 0.57120\n",
      "Epoch 11729 - lr: 0.01000 - Train loss: 0.63508 - Test loss: 0.57099\n",
      "Epoch 11730 - lr: 0.01000 - Train loss: 0.63701 - Test loss: 0.57131\n",
      "Epoch 11731 - lr: 0.01000 - Train loss: 0.63965 - Test loss: 0.57106\n",
      "Epoch 11732 - lr: 0.01000 - Train loss: 0.62612 - Test loss: 0.57143\n",
      "Epoch 11733 - lr: 0.01000 - Train loss: 0.62535 - Test loss: 0.57184\n",
      "Epoch 11734 - lr: 0.01000 - Train loss: 0.62761 - Test loss: 0.57214\n",
      "Epoch 11735 - lr: 0.01000 - Train loss: 0.62581 - Test loss: 0.57247\n",
      "Epoch 11736 - lr: 0.01000 - Train loss: 0.63588 - Test loss: 0.57263\n",
      "Epoch 11737 - lr: 0.01000 - Train loss: 0.63437 - Test loss: 0.57234\n",
      "Epoch 11738 - lr: 0.01000 - Train loss: 0.63715 - Test loss: 0.57254\n",
      "Epoch 11739 - lr: 0.01000 - Train loss: 0.64875 - Test loss: 0.57200\n",
      "Epoch 11740 - lr: 0.01000 - Train loss: 0.63392 - Test loss: 0.57170\n",
      "Epoch 11741 - lr: 0.01000 - Train loss: 0.63639 - Test loss: 0.57199\n",
      "Epoch 11742 - lr: 0.01000 - Train loss: 0.64217 - Test loss: 0.57160\n",
      "Epoch 11743 - lr: 0.01000 - Train loss: 0.63523 - Test loss: 0.57187\n",
      "Epoch 11744 - lr: 0.01000 - Train loss: 0.63892 - Test loss: 0.57170\n",
      "Epoch 11745 - lr: 0.01000 - Train loss: 0.63378 - Test loss: 0.57143\n",
      "Epoch 11746 - lr: 0.01000 - Train loss: 0.63433 - Test loss: 0.57176\n",
      "Epoch 11747 - lr: 0.01000 - Train loss: 0.64192 - Test loss: 0.57167\n",
      "Epoch 11748 - lr: 0.01000 - Train loss: 0.63788 - Test loss: 0.57139\n",
      "Epoch 11749 - lr: 0.01000 - Train loss: 0.62737 - Test loss: 0.57167\n",
      "Epoch 11750 - lr: 0.01000 - Train loss: 0.62642 - Test loss: 0.57202\n",
      "Epoch 11751 - lr: 0.01000 - Train loss: 0.63207 - Test loss: 0.57223\n",
      "Epoch 11752 - lr: 0.01000 - Train loss: 0.63754 - Test loss: 0.57244\n",
      "Epoch 11753 - lr: 0.01000 - Train loss: 0.63861 - Test loss: 0.57208\n",
      "Epoch 11754 - lr: 0.01000 - Train loss: 0.62537 - Test loss: 0.57236\n",
      "Epoch 11755 - lr: 0.01000 - Train loss: 0.63728 - Test loss: 0.57253\n",
      "Epoch 11756 - lr: 0.01000 - Train loss: 0.63485 - Test loss: 0.57219\n",
      "Epoch 11757 - lr: 0.01000 - Train loss: 0.63687 - Test loss: 0.57243\n",
      "Epoch 11758 - lr: 0.01000 - Train loss: 0.64783 - Test loss: 0.57193\n",
      "Epoch 11759 - lr: 0.01000 - Train loss: 0.63407 - Test loss: 0.57169\n",
      "Epoch 11760 - lr: 0.01000 - Train loss: 0.63722 - Test loss: 0.57192\n",
      "Epoch 11761 - lr: 0.01000 - Train loss: 0.63474 - Test loss: 0.57174\n",
      "Epoch 11762 - lr: 0.01000 - Train loss: 0.63635 - Test loss: 0.57192\n",
      "Epoch 11763 - lr: 0.01000 - Train loss: 0.63371 - Test loss: 0.57168\n",
      "Epoch 11764 - lr: 0.01000 - Train loss: 0.63389 - Test loss: 0.57201\n",
      "Epoch 11765 - lr: 0.01000 - Train loss: 0.64074 - Test loss: 0.57183\n",
      "Epoch 11766 - lr: 0.01000 - Train loss: 0.64189 - Test loss: 0.57170\n",
      "Epoch 11767 - lr: 0.01000 - Train loss: 0.63579 - Test loss: 0.57149\n",
      "Epoch 11768 - lr: 0.01000 - Train loss: 0.63489 - Test loss: 0.57168\n",
      "Epoch 11769 - lr: 0.01000 - Train loss: 0.63398 - Test loss: 0.57155\n",
      "Epoch 11770 - lr: 0.01000 - Train loss: 0.63728 - Test loss: 0.57183\n",
      "Epoch 11771 - lr: 0.01000 - Train loss: 0.63685 - Test loss: 0.57161\n",
      "Epoch 11772 - lr: 0.01000 - Train loss: 0.63161 - Test loss: 0.57181\n",
      "Epoch 11773 - lr: 0.01000 - Train loss: 0.63739 - Test loss: 0.57210\n",
      "Epoch 11774 - lr: 0.01000 - Train loss: 0.64311 - Test loss: 0.57171\n",
      "Epoch 11775 - lr: 0.01000 - Train loss: 0.63707 - Test loss: 0.57199\n",
      "Epoch 11776 - lr: 0.01000 - Train loss: 0.64383 - Test loss: 0.57159\n",
      "Epoch 11777 - lr: 0.01000 - Train loss: 0.63759 - Test loss: 0.57181\n",
      "Epoch 11778 - lr: 0.01000 - Train loss: 0.63561 - Test loss: 0.57163\n",
      "Epoch 11779 - lr: 0.01000 - Train loss: 0.63535 - Test loss: 0.57181\n",
      "Epoch 11780 - lr: 0.01000 - Train loss: 0.63354 - Test loss: 0.57164\n",
      "Epoch 11781 - lr: 0.01000 - Train loss: 0.63537 - Test loss: 0.57198\n",
      "Epoch 11782 - lr: 0.01000 - Train loss: 0.64129 - Test loss: 0.57186\n",
      "Epoch 11783 - lr: 0.01000 - Train loss: 0.64128 - Test loss: 0.57163\n",
      "Epoch 11784 - lr: 0.01000 - Train loss: 0.64128 - Test loss: 0.57143\n",
      "Epoch 11785 - lr: 0.01000 - Train loss: 0.64121 - Test loss: 0.57125\n",
      "Epoch 11786 - lr: 0.01000 - Train loss: 0.64036 - Test loss: 0.57104\n",
      "Epoch 11787 - lr: 0.01000 - Train loss: 0.63493 - Test loss: 0.57077\n",
      "Epoch 11788 - lr: 0.01000 - Train loss: 0.63539 - Test loss: 0.57117\n",
      "Epoch 11789 - lr: 0.01000 - Train loss: 0.64269 - Test loss: 0.57117\n",
      "Epoch 11790 - lr: 0.01000 - Train loss: 0.63532 - Test loss: 0.57104\n",
      "Epoch 11791 - lr: 0.01000 - Train loss: 0.63104 - Test loss: 0.57128\n",
      "Epoch 11792 - lr: 0.01000 - Train loss: 0.63715 - Test loss: 0.57169\n",
      "Epoch 11793 - lr: 0.01000 - Train loss: 0.64004 - Test loss: 0.57140\n",
      "Epoch 11794 - lr: 0.01000 - Train loss: 0.62703 - Test loss: 0.57172\n",
      "Epoch 11795 - lr: 0.01000 - Train loss: 0.63202 - Test loss: 0.57204\n",
      "Epoch 11796 - lr: 0.01000 - Train loss: 0.63318 - Test loss: 0.57175\n",
      "Epoch 11797 - lr: 0.01000 - Train loss: 0.62515 - Test loss: 0.57211\n",
      "Epoch 11798 - lr: 0.01000 - Train loss: 0.63225 - Test loss: 0.57229\n",
      "Epoch 11799 - lr: 0.01000 - Train loss: 0.63807 - Test loss: 0.57247\n",
      "Epoch 11800 - lr: 0.01000 - Train loss: 0.63613 - Test loss: 0.57220\n",
      "Epoch 11801 - lr: 0.01000 - Train loss: 0.63322 - Test loss: 0.57242\n",
      "Epoch 11802 - lr: 0.01000 - Train loss: 0.63466 - Test loss: 0.57201\n",
      "Epoch 11803 - lr: 0.01000 - Train loss: 0.63220 - Test loss: 0.57227\n",
      "Epoch 11804 - lr: 0.01000 - Train loss: 0.63337 - Test loss: 0.57192\n",
      "Epoch 11805 - lr: 0.01000 - Train loss: 0.62564 - Test loss: 0.57225\n",
      "Epoch 11806 - lr: 0.01000 - Train loss: 0.62538 - Test loss: 0.57257\n",
      "Epoch 11807 - lr: 0.01000 - Train loss: 0.63740 - Test loss: 0.57274\n",
      "Epoch 11808 - lr: 0.01000 - Train loss: 0.63385 - Test loss: 0.57236\n",
      "Epoch 11809 - lr: 0.01000 - Train loss: 0.62978 - Test loss: 0.57261\n",
      "Epoch 11810 - lr: 0.01000 - Train loss: 0.63564 - Test loss: 0.57246\n",
      "Epoch 11811 - lr: 0.01000 - Train loss: 0.63369 - Test loss: 0.57221\n",
      "Epoch 11812 - lr: 0.01000 - Train loss: 0.63645 - Test loss: 0.57250\n",
      "Epoch 11813 - lr: 0.01000 - Train loss: 0.63847 - Test loss: 0.57216\n",
      "Epoch 11814 - lr: 0.01000 - Train loss: 0.62557 - Test loss: 0.57244\n",
      "Epoch 11815 - lr: 0.01000 - Train loss: 0.63574 - Test loss: 0.57262\n",
      "Epoch 11816 - lr: 0.01000 - Train loss: 0.63347 - Test loss: 0.57235\n",
      "Epoch 11817 - lr: 0.01000 - Train loss: 0.63385 - Test loss: 0.57263\n",
      "Epoch 11818 - lr: 0.01000 - Train loss: 0.64016 - Test loss: 0.57236\n",
      "Epoch 11819 - lr: 0.01000 - Train loss: 0.63714 - Test loss: 0.57200\n",
      "Epoch 11820 - lr: 0.01000 - Train loss: 0.63484 - Test loss: 0.57189\n",
      "Epoch 11821 - lr: 0.01000 - Train loss: 0.63467 - Test loss: 0.57181\n",
      "Epoch 11822 - lr: 0.01000 - Train loss: 0.63502 - Test loss: 0.57177\n",
      "Epoch 11823 - lr: 0.01000 - Train loss: 0.63378 - Test loss: 0.57166\n",
      "Epoch 11824 - lr: 0.01000 - Train loss: 0.63702 - Test loss: 0.57193\n",
      "Epoch 11825 - lr: 0.01000 - Train loss: 0.63462 - Test loss: 0.57181\n",
      "Epoch 11826 - lr: 0.01000 - Train loss: 0.62473 - Test loss: 0.57220\n",
      "Epoch 11827 - lr: 0.01000 - Train loss: 0.63759 - Test loss: 0.57245\n",
      "Epoch 11828 - lr: 0.01000 - Train loss: 0.63510 - Test loss: 0.57217\n",
      "Epoch 11829 - lr: 0.01000 - Train loss: 0.63707 - Test loss: 0.57243\n",
      "Epoch 11830 - lr: 0.01000 - Train loss: 0.64096 - Test loss: 0.57207\n",
      "Epoch 11831 - lr: 0.01000 - Train loss: 0.63182 - Test loss: 0.57234\n",
      "Epoch 11832 - lr: 0.01000 - Train loss: 0.63368 - Test loss: 0.57206\n",
      "Epoch 11833 - lr: 0.01000 - Train loss: 0.63272 - Test loss: 0.57238\n",
      "Epoch 11834 - lr: 0.01000 - Train loss: 0.63591 - Test loss: 0.57208\n",
      "Epoch 11835 - lr: 0.01000 - Train loss: 0.63670 - Test loss: 0.57218\n",
      "Epoch 11836 - lr: 0.01000 - Train loss: 0.64078 - Test loss: 0.57203\n",
      "Epoch 11837 - lr: 0.01000 - Train loss: 0.64185 - Test loss: 0.57193\n",
      "Epoch 11838 - lr: 0.01000 - Train loss: 0.63645 - Test loss: 0.57171\n",
      "Epoch 11839 - lr: 0.01000 - Train loss: 0.63198 - Test loss: 0.57192\n",
      "Epoch 11840 - lr: 0.01000 - Train loss: 0.63725 - Test loss: 0.57215\n",
      "Epoch 11841 - lr: 0.01000 - Train loss: 0.63892 - Test loss: 0.57200\n",
      "Epoch 11842 - lr: 0.01000 - Train loss: 0.63313 - Test loss: 0.57175\n",
      "Epoch 11843 - lr: 0.01000 - Train loss: 0.63007 - Test loss: 0.57209\n",
      "Epoch 11844 - lr: 0.01000 - Train loss: 0.63434 - Test loss: 0.57198\n",
      "Epoch 11845 - lr: 0.01000 - Train loss: 0.63651 - Test loss: 0.57207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11846 - lr: 0.01000 - Train loss: 0.63742 - Test loss: 0.57181\n",
      "Epoch 11847 - lr: 0.01000 - Train loss: 0.63367 - Test loss: 0.57168\n",
      "Epoch 11848 - lr: 0.01000 - Train loss: 0.63700 - Test loss: 0.57197\n",
      "Epoch 11849 - lr: 0.01000 - Train loss: 0.63559 - Test loss: 0.57182\n",
      "Epoch 11850 - lr: 0.01000 - Train loss: 0.63440 - Test loss: 0.57202\n",
      "Epoch 11851 - lr: 0.01000 - Train loss: 0.63397 - Test loss: 0.57193\n",
      "Epoch 11852 - lr: 0.01000 - Train loss: 0.63695 - Test loss: 0.57213\n",
      "Epoch 11853 - lr: 0.01000 - Train loss: 0.64122 - Test loss: 0.57205\n",
      "Epoch 11854 - lr: 0.01000 - Train loss: 0.64110 - Test loss: 0.57193\n",
      "Epoch 11855 - lr: 0.01000 - Train loss: 0.64217 - Test loss: 0.57182\n",
      "Epoch 11856 - lr: 0.01000 - Train loss: 0.63731 - Test loss: 0.57159\n",
      "Epoch 11857 - lr: 0.01000 - Train loss: 0.62865 - Test loss: 0.57186\n",
      "Epoch 11858 - lr: 0.01000 - Train loss: 0.62737 - Test loss: 0.57224\n",
      "Epoch 11859 - lr: 0.01000 - Train loss: 0.63589 - Test loss: 0.57258\n",
      "Epoch 11860 - lr: 0.01000 - Train loss: 0.63520 - Test loss: 0.57238\n",
      "Epoch 11861 - lr: 0.01000 - Train loss: 0.62615 - Test loss: 0.57266\n",
      "Epoch 11862 - lr: 0.01000 - Train loss: 0.62618 - Test loss: 0.57296\n",
      "Epoch 11863 - lr: 0.01000 - Train loss: 0.62656 - Test loss: 0.57322\n",
      "Epoch 11864 - lr: 0.01000 - Train loss: 0.63001 - Test loss: 0.57342\n",
      "Epoch 11865 - lr: 0.01000 - Train loss: 0.63528 - Test loss: 0.57317\n",
      "Epoch 11866 - lr: 0.01000 - Train loss: 0.63483 - Test loss: 0.57293\n",
      "Epoch 11867 - lr: 0.01000 - Train loss: 0.63552 - Test loss: 0.57277\n",
      "Epoch 11868 - lr: 0.01000 - Train loss: 0.63332 - Test loss: 0.57246\n",
      "Epoch 11869 - lr: 0.01000 - Train loss: 0.63432 - Test loss: 0.57273\n",
      "Epoch 11870 - lr: 0.01000 - Train loss: 0.64207 - Test loss: 0.57260\n",
      "Epoch 11871 - lr: 0.01000 - Train loss: 0.64428 - Test loss: 0.57212\n",
      "Epoch 11872 - lr: 0.01000 - Train loss: 0.63710 - Test loss: 0.57222\n",
      "Epoch 11873 - lr: 0.01000 - Train loss: 0.64209 - Test loss: 0.57214\n",
      "Epoch 11874 - lr: 0.01000 - Train loss: 0.63722 - Test loss: 0.57187\n",
      "Epoch 11875 - lr: 0.01000 - Train loss: 0.62909 - Test loss: 0.57210\n",
      "Epoch 11876 - lr: 0.01000 - Train loss: 0.62999 - Test loss: 0.57244\n",
      "Epoch 11877 - lr: 0.01000 - Train loss: 0.63458 - Test loss: 0.57232\n",
      "Epoch 11878 - lr: 0.01000 - Train loss: 0.63603 - Test loss: 0.57232\n",
      "Epoch 11879 - lr: 0.01000 - Train loss: 0.63347 - Test loss: 0.57203\n",
      "Epoch 11880 - lr: 0.01000 - Train loss: 0.63034 - Test loss: 0.57235\n",
      "Epoch 11881 - lr: 0.01000 - Train loss: 0.63366 - Test loss: 0.57218\n",
      "Epoch 11882 - lr: 0.01000 - Train loss: 0.63697 - Test loss: 0.57249\n",
      "Epoch 11883 - lr: 0.01000 - Train loss: 0.64464 - Test loss: 0.57207\n",
      "Epoch 11884 - lr: 0.01000 - Train loss: 0.63667 - Test loss: 0.57214\n",
      "Epoch 11885 - lr: 0.01000 - Train loss: 0.63880 - Test loss: 0.57191\n",
      "Epoch 11886 - lr: 0.01000 - Train loss: 0.63307 - Test loss: 0.57166\n",
      "Epoch 11887 - lr: 0.01000 - Train loss: 0.62858 - Test loss: 0.57204\n",
      "Epoch 11888 - lr: 0.01000 - Train loss: 0.63682 - Test loss: 0.57220\n",
      "Epoch 11889 - lr: 0.01000 - Train loss: 0.64098 - Test loss: 0.57207\n",
      "Epoch 11890 - lr: 0.01000 - Train loss: 0.64200 - Test loss: 0.57200\n",
      "Epoch 11891 - lr: 0.01000 - Train loss: 0.63669 - Test loss: 0.57178\n",
      "Epoch 11892 - lr: 0.01000 - Train loss: 0.63066 - Test loss: 0.57201\n",
      "Epoch 11893 - lr: 0.01000 - Train loss: 0.63654 - Test loss: 0.57239\n",
      "Epoch 11894 - lr: 0.01000 - Train loss: 0.64077 - Test loss: 0.57206\n",
      "Epoch 11895 - lr: 0.01000 - Train loss: 0.63152 - Test loss: 0.57236\n",
      "Epoch 11896 - lr: 0.01000 - Train loss: 0.63291 - Test loss: 0.57210\n",
      "Epoch 11897 - lr: 0.01000 - Train loss: 0.62677 - Test loss: 0.57246\n",
      "Epoch 11898 - lr: 0.01000 - Train loss: 0.63340 - Test loss: 0.57278\n",
      "Epoch 11899 - lr: 0.01000 - Train loss: 0.63743 - Test loss: 0.57244\n",
      "Epoch 11900 - lr: 0.01000 - Train loss: 0.63419 - Test loss: 0.57228\n",
      "Epoch 11901 - lr: 0.01000 - Train loss: 0.63655 - Test loss: 0.57235\n",
      "Epoch 11902 - lr: 0.01000 - Train loss: 0.63748 - Test loss: 0.57207\n",
      "Epoch 11903 - lr: 0.01000 - Train loss: 0.63345 - Test loss: 0.57191\n",
      "Epoch 11904 - lr: 0.01000 - Train loss: 0.63690 - Test loss: 0.57224\n",
      "Epoch 11905 - lr: 0.01000 - Train loss: 0.64151 - Test loss: 0.57192\n",
      "Epoch 11906 - lr: 0.01000 - Train loss: 0.63469 - Test loss: 0.57225\n",
      "Epoch 11907 - lr: 0.01000 - Train loss: 0.64227 - Test loss: 0.57220\n",
      "Epoch 11908 - lr: 0.01000 - Train loss: 0.63998 - Test loss: 0.57188\n",
      "Epoch 11909 - lr: 0.01000 - Train loss: 0.62848 - Test loss: 0.57220\n",
      "Epoch 11910 - lr: 0.01000 - Train loss: 0.63742 - Test loss: 0.57240\n",
      "Epoch 11911 - lr: 0.01000 - Train loss: 0.64175 - Test loss: 0.57230\n",
      "Epoch 11912 - lr: 0.01000 - Train loss: 0.64195 - Test loss: 0.57217\n",
      "Epoch 11913 - lr: 0.01000 - Train loss: 0.64182 - Test loss: 0.57204\n",
      "Epoch 11914 - lr: 0.01000 - Train loss: 0.64290 - Test loss: 0.57193\n",
      "Epoch 11915 - lr: 0.01000 - Train loss: 0.63966 - Test loss: 0.57162\n",
      "Epoch 11916 - lr: 0.01000 - Train loss: 0.62704 - Test loss: 0.57196\n",
      "Epoch 11917 - lr: 0.01000 - Train loss: 0.63398 - Test loss: 0.57232\n",
      "Epoch 11918 - lr: 0.01000 - Train loss: 0.63712 - Test loss: 0.57200\n",
      "Epoch 11919 - lr: 0.01000 - Train loss: 0.63628 - Test loss: 0.57202\n",
      "Epoch 11920 - lr: 0.01000 - Train loss: 0.63282 - Test loss: 0.57172\n",
      "Epoch 11921 - lr: 0.01000 - Train loss: 0.62525 - Test loss: 0.57212\n",
      "Epoch 11922 - lr: 0.01000 - Train loss: 0.63501 - Test loss: 0.57236\n",
      "Epoch 11923 - lr: 0.01000 - Train loss: 0.63301 - Test loss: 0.57218\n",
      "Epoch 11924 - lr: 0.01000 - Train loss: 0.63293 - Test loss: 0.57252\n",
      "Epoch 11925 - lr: 0.01000 - Train loss: 0.63447 - Test loss: 0.57218\n",
      "Epoch 11926 - lr: 0.01000 - Train loss: 0.63327 - Test loss: 0.57251\n",
      "Epoch 11927 - lr: 0.01000 - Train loss: 0.63573 - Test loss: 0.57217\n",
      "Epoch 11928 - lr: 0.01000 - Train loss: 0.63750 - Test loss: 0.57243\n",
      "Epoch 11929 - lr: 0.01000 - Train loss: 0.63609 - Test loss: 0.57222\n",
      "Epoch 11930 - lr: 0.01000 - Train loss: 0.63320 - Test loss: 0.57239\n",
      "Epoch 11931 - lr: 0.01000 - Train loss: 0.63570 - Test loss: 0.57239\n",
      "Epoch 11932 - lr: 0.01000 - Train loss: 0.63258 - Test loss: 0.57214\n",
      "Epoch 11933 - lr: 0.01000 - Train loss: 0.62560 - Test loss: 0.57253\n",
      "Epoch 11934 - lr: 0.01000 - Train loss: 0.62467 - Test loss: 0.57292\n",
      "Epoch 11935 - lr: 0.01000 - Train loss: 0.63303 - Test loss: 0.57310\n",
      "Epoch 11936 - lr: 0.01000 - Train loss: 0.63621 - Test loss: 0.57307\n",
      "Epoch 11937 - lr: 0.01000 - Train loss: 0.63358 - Test loss: 0.57270\n",
      "Epoch 11938 - lr: 0.01000 - Train loss: 0.62990 - Test loss: 0.57298\n",
      "Epoch 11939 - lr: 0.01000 - Train loss: 0.63411 - Test loss: 0.57280\n",
      "Epoch 11940 - lr: 0.01000 - Train loss: 0.63677 - Test loss: 0.57288\n",
      "Epoch 11941 - lr: 0.01000 - Train loss: 0.64070 - Test loss: 0.57267\n",
      "Epoch 11942 - lr: 0.01000 - Train loss: 0.64134 - Test loss: 0.57247\n",
      "Epoch 11943 - lr: 0.01000 - Train loss: 0.64111 - Test loss: 0.57233\n",
      "Epoch 11944 - lr: 0.01000 - Train loss: 0.64193 - Test loss: 0.57219\n",
      "Epoch 11945 - lr: 0.01000 - Train loss: 0.63716 - Test loss: 0.57202\n",
      "Epoch 11946 - lr: 0.01000 - Train loss: 0.63671 - Test loss: 0.57210\n",
      "Epoch 11947 - lr: 0.01000 - Train loss: 0.63594 - Test loss: 0.57183\n",
      "Epoch 11948 - lr: 0.01000 - Train loss: 0.63709 - Test loss: 0.57205\n",
      "Epoch 11949 - lr: 0.01000 - Train loss: 0.64240 - Test loss: 0.57202\n",
      "Epoch 11950 - lr: 0.01000 - Train loss: 0.63698 - Test loss: 0.57181\n",
      "Epoch 11951 - lr: 0.01000 - Train loss: 0.62916 - Test loss: 0.57208\n",
      "Epoch 11952 - lr: 0.01000 - Train loss: 0.63149 - Test loss: 0.57246\n",
      "Epoch 11953 - lr: 0.01000 - Train loss: 0.63261 - Test loss: 0.57222\n",
      "Epoch 11954 - lr: 0.01000 - Train loss: 0.62465 - Test loss: 0.57263\n",
      "Epoch 11955 - lr: 0.01000 - Train loss: 0.63275 - Test loss: 0.57285\n",
      "Epoch 11956 - lr: 0.01000 - Train loss: 0.63643 - Test loss: 0.57288\n",
      "Epoch 11957 - lr: 0.01000 - Train loss: 0.63423 - Test loss: 0.57252\n",
      "Epoch 11958 - lr: 0.01000 - Train loss: 0.63335 - Test loss: 0.57283\n",
      "Epoch 11959 - lr: 0.01000 - Train loss: 0.63718 - Test loss: 0.57251\n",
      "Epoch 11960 - lr: 0.01000 - Train loss: 0.63439 - Test loss: 0.57239\n",
      "Epoch 11961 - lr: 0.01000 - Train loss: 0.63510 - Test loss: 0.57237\n",
      "Epoch 11962 - lr: 0.01000 - Train loss: 0.63272 - Test loss: 0.57219\n",
      "Epoch 11963 - lr: 0.01000 - Train loss: 0.63250 - Test loss: 0.57256\n",
      "Epoch 11964 - lr: 0.01000 - Train loss: 0.63443 - Test loss: 0.57227\n",
      "Epoch 11965 - lr: 0.01000 - Train loss: 0.63537 - Test loss: 0.57264\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11966 - lr: 0.01000 - Train loss: 0.63675 - Test loss: 0.57249\n",
      "Epoch 11967 - lr: 0.01000 - Train loss: 0.63697 - Test loss: 0.57260\n",
      "Epoch 11968 - lr: 0.01000 - Train loss: 0.64176 - Test loss: 0.57249\n",
      "Epoch 11969 - lr: 0.01000 - Train loss: 0.63635 - Test loss: 0.57230\n",
      "Epoch 11970 - lr: 0.01000 - Train loss: 0.63736 - Test loss: 0.57257\n",
      "Epoch 11971 - lr: 0.01000 - Train loss: 0.64158 - Test loss: 0.57223\n",
      "Epoch 11972 - lr: 0.01000 - Train loss: 0.63509 - Test loss: 0.57256\n",
      "Epoch 11973 - lr: 0.01000 - Train loss: 0.64240 - Test loss: 0.57249\n",
      "Epoch 11974 - lr: 0.01000 - Train loss: 0.63490 - Test loss: 0.57230\n",
      "Epoch 11975 - lr: 0.01000 - Train loss: 0.62752 - Test loss: 0.57256\n",
      "Epoch 11976 - lr: 0.01000 - Train loss: 0.62468 - Test loss: 0.57296\n",
      "Epoch 11977 - lr: 0.01000 - Train loss: 0.63667 - Test loss: 0.57315\n",
      "Epoch 11978 - lr: 0.01000 - Train loss: 0.63274 - Test loss: 0.57281\n",
      "Epoch 11979 - lr: 0.01000 - Train loss: 0.62439 - Test loss: 0.57317\n",
      "Epoch 11980 - lr: 0.01000 - Train loss: 0.63545 - Test loss: 0.57333\n",
      "Epoch 11981 - lr: 0.01000 - Train loss: 0.63274 - Test loss: 0.57303\n",
      "Epoch 11982 - lr: 0.01000 - Train loss: 0.62901 - Test loss: 0.57331\n",
      "Epoch 11983 - lr: 0.01000 - Train loss: 0.63593 - Test loss: 0.57324\n",
      "Epoch 11984 - lr: 0.01000 - Train loss: 0.63288 - Test loss: 0.57288\n",
      "Epoch 11985 - lr: 0.01000 - Train loss: 0.62636 - Test loss: 0.57320\n",
      "Epoch 11986 - lr: 0.01000 - Train loss: 0.63204 - Test loss: 0.57348\n",
      "Epoch 11987 - lr: 0.01000 - Train loss: 0.63364 - Test loss: 0.57310\n",
      "Epoch 11988 - lr: 0.01000 - Train loss: 0.63107 - Test loss: 0.57337\n",
      "Epoch 11989 - lr: 0.01000 - Train loss: 0.63288 - Test loss: 0.57305\n",
      "Epoch 11990 - lr: 0.01000 - Train loss: 0.62953 - Test loss: 0.57334\n",
      "Epoch 11991 - lr: 0.01000 - Train loss: 0.63437 - Test loss: 0.57318\n",
      "Epoch 11992 - lr: 0.01000 - Train loss: 0.63514 - Test loss: 0.57310\n",
      "Epoch 11993 - lr: 0.01000 - Train loss: 0.63281 - Test loss: 0.57284\n",
      "Epoch 11994 - lr: 0.01000 - Train loss: 0.63278 - Test loss: 0.57316\n",
      "Epoch 11995 - lr: 0.01000 - Train loss: 0.63741 - Test loss: 0.57287\n",
      "Epoch 11996 - lr: 0.01000 - Train loss: 0.63293 - Test loss: 0.57266\n",
      "Epoch 11997 - lr: 0.01000 - Train loss: 0.63512 - Test loss: 0.57301\n",
      "Epoch 11998 - lr: 0.01000 - Train loss: 0.63411 - Test loss: 0.57287\n",
      "Epoch 11999 - lr: 0.01000 - Train loss: 0.62427 - Test loss: 0.57323\n",
      "Epoch 12000 - lr: 0.01000 - Train loss: 0.63757 - Test loss: 0.57347\n",
      "Epoch 12001 - lr: 0.01000 - Train loss: 0.63574 - Test loss: 0.57314\n",
      "Epoch 12002 - lr: 0.01000 - Train loss: 0.63584 - Test loss: 0.57315\n",
      "Epoch 12003 - lr: 0.01000 - Train loss: 0.63565 - Test loss: 0.57284\n",
      "Epoch 12004 - lr: 0.01000 - Train loss: 0.63562 - Test loss: 0.57287\n",
      "Epoch 12005 - lr: 0.01000 - Train loss: 0.63483 - Test loss: 0.57260\n",
      "Epoch 12006 - lr: 0.01000 - Train loss: 0.63641 - Test loss: 0.57281\n",
      "Epoch 12007 - lr: 0.01000 - Train loss: 0.63742 - Test loss: 0.57268\n",
      "Epoch 12008 - lr: 0.01000 - Train loss: 0.63339 - Test loss: 0.57253\n",
      "Epoch 12009 - lr: 0.01000 - Train loss: 0.63650 - Test loss: 0.57279\n",
      "Epoch 12010 - lr: 0.01000 - Train loss: 0.63422 - Test loss: 0.57268\n",
      "Epoch 12011 - lr: 0.01000 - Train loss: 0.62477 - Test loss: 0.57305\n",
      "Epoch 12012 - lr: 0.01000 - Train loss: 0.62547 - Test loss: 0.57341\n",
      "Epoch 12013 - lr: 0.01000 - Train loss: 0.63280 - Test loss: 0.57361\n",
      "Epoch 12014 - lr: 0.01000 - Train loss: 0.63554 - Test loss: 0.57356\n",
      "Epoch 12015 - lr: 0.01000 - Train loss: 0.63402 - Test loss: 0.57323\n",
      "Epoch 12016 - lr: 0.01000 - Train loss: 0.63605 - Test loss: 0.57351\n",
      "Epoch 12017 - lr: 0.01000 - Train loss: 0.64746 - Test loss: 0.57303\n",
      "Epoch 12018 - lr: 0.01000 - Train loss: 0.63295 - Test loss: 0.57278\n",
      "Epoch 12019 - lr: 0.01000 - Train loss: 0.63551 - Test loss: 0.57312\n",
      "Epoch 12020 - lr: 0.01000 - Train loss: 0.63823 - Test loss: 0.57284\n",
      "Epoch 12021 - lr: 0.01000 - Train loss: 0.62384 - Test loss: 0.57323\n",
      "Epoch 12022 - lr: 0.01000 - Train loss: 0.63603 - Test loss: 0.57346\n",
      "Epoch 12023 - lr: 0.01000 - Train loss: 0.63342 - Test loss: 0.57317\n",
      "Epoch 12024 - lr: 0.01000 - Train loss: 0.63445 - Test loss: 0.57349\n",
      "Epoch 12025 - lr: 0.01000 - Train loss: 0.63721 - Test loss: 0.57329\n",
      "Epoch 12026 - lr: 0.01000 - Train loss: 0.63400 - Test loss: 0.57310\n",
      "Epoch 12027 - lr: 0.01000 - Train loss: 0.63536 - Test loss: 0.57310\n",
      "Epoch 12028 - lr: 0.01000 - Train loss: 0.63376 - Test loss: 0.57282\n",
      "Epoch 12029 - lr: 0.01000 - Train loss: 0.63551 - Test loss: 0.57316\n",
      "Epoch 12030 - lr: 0.01000 - Train loss: 0.63971 - Test loss: 0.57284\n",
      "Epoch 12031 - lr: 0.01000 - Train loss: 0.62891 - Test loss: 0.57314\n",
      "Epoch 12032 - lr: 0.01000 - Train loss: 0.63503 - Test loss: 0.57311\n",
      "Epoch 12033 - lr: 0.01000 - Train loss: 0.63289 - Test loss: 0.57288\n",
      "Epoch 12034 - lr: 0.01000 - Train loss: 0.63392 - Test loss: 0.57323\n",
      "Epoch 12035 - lr: 0.01000 - Train loss: 0.64044 - Test loss: 0.57313\n",
      "Epoch 12036 - lr: 0.01000 - Train loss: 0.63953 - Test loss: 0.57297\n",
      "Epoch 12037 - lr: 0.01000 - Train loss: 0.63735 - Test loss: 0.57268\n",
      "Epoch 12038 - lr: 0.01000 - Train loss: 0.63265 - Test loss: 0.57249\n",
      "Epoch 12039 - lr: 0.01000 - Train loss: 0.63388 - Test loss: 0.57287\n",
      "Epoch 12040 - lr: 0.01000 - Train loss: 0.64121 - Test loss: 0.57283\n",
      "Epoch 12041 - lr: 0.01000 - Train loss: 0.63690 - Test loss: 0.57258\n",
      "Epoch 12042 - lr: 0.01000 - Train loss: 0.62712 - Test loss: 0.57288\n",
      "Epoch 12043 - lr: 0.01000 - Train loss: 0.62388 - Test loss: 0.57333\n",
      "Epoch 12044 - lr: 0.01000 - Train loss: 0.63778 - Test loss: 0.57357\n",
      "Epoch 12045 - lr: 0.01000 - Train loss: 0.63654 - Test loss: 0.57325\n",
      "Epoch 12046 - lr: 0.01000 - Train loss: 0.63392 - Test loss: 0.57310\n",
      "Epoch 12047 - lr: 0.01000 - Train loss: 0.63516 - Test loss: 0.57308\n",
      "Epoch 12048 - lr: 0.01000 - Train loss: 0.63300 - Test loss: 0.57281\n",
      "Epoch 12049 - lr: 0.01000 - Train loss: 0.63293 - Test loss: 0.57315\n",
      "Epoch 12050 - lr: 0.01000 - Train loss: 0.63946 - Test loss: 0.57296\n",
      "Epoch 12051 - lr: 0.01000 - Train loss: 0.63930 - Test loss: 0.57276\n",
      "Epoch 12052 - lr: 0.01000 - Train loss: 0.63810 - Test loss: 0.57253\n",
      "Epoch 12053 - lr: 0.01000 - Train loss: 0.63301 - Test loss: 0.57229\n",
      "Epoch 12054 - lr: 0.01000 - Train loss: 0.63234 - Test loss: 0.57269\n",
      "Epoch 12055 - lr: 0.01000 - Train loss: 0.63628 - Test loss: 0.57247\n",
      "Epoch 12056 - lr: 0.01000 - Train loss: 0.63372 - Test loss: 0.57243\n",
      "Epoch 12057 - lr: 0.01000 - Train loss: 0.63482 - Test loss: 0.57249\n",
      "Epoch 12058 - lr: 0.01000 - Train loss: 0.63263 - Test loss: 0.57233\n",
      "Epoch 12059 - lr: 0.01000 - Train loss: 0.63256 - Test loss: 0.57274\n",
      "Epoch 12060 - lr: 0.01000 - Train loss: 0.63805 - Test loss: 0.57257\n",
      "Epoch 12061 - lr: 0.01000 - Train loss: 0.63354 - Test loss: 0.57236\n",
      "Epoch 12062 - lr: 0.01000 - Train loss: 0.63513 - Test loss: 0.57276\n",
      "Epoch 12063 - lr: 0.01000 - Train loss: 0.63613 - Test loss: 0.57260\n",
      "Epoch 12064 - lr: 0.01000 - Train loss: 0.62912 - Test loss: 0.57287\n",
      "Epoch 12065 - lr: 0.01000 - Train loss: 0.63256 - Test loss: 0.57325\n",
      "Epoch 12066 - lr: 0.01000 - Train loss: 0.63805 - Test loss: 0.57302\n",
      "Epoch 12067 - lr: 0.01000 - Train loss: 0.63340 - Test loss: 0.57274\n",
      "Epoch 12068 - lr: 0.01000 - Train loss: 0.63462 - Test loss: 0.57310\n",
      "Epoch 12069 - lr: 0.01000 - Train loss: 0.63363 - Test loss: 0.57298\n",
      "Epoch 12070 - lr: 0.01000 - Train loss: 0.62433 - Test loss: 0.57335\n",
      "Epoch 12071 - lr: 0.01000 - Train loss: 0.63680 - Test loss: 0.57358\n",
      "Epoch 12072 - lr: 0.01000 - Train loss: 0.63494 - Test loss: 0.57327\n",
      "Epoch 12073 - lr: 0.01000 - Train loss: 0.63612 - Test loss: 0.57337\n",
      "Epoch 12074 - lr: 0.01000 - Train loss: 0.64096 - Test loss: 0.57327\n",
      "Epoch 12075 - lr: 0.01000 - Train loss: 0.64306 - Test loss: 0.57283\n",
      "Epoch 12076 - lr: 0.01000 - Train loss: 0.63648 - Test loss: 0.57301\n",
      "Epoch 12077 - lr: 0.01000 - Train loss: 0.63472 - Test loss: 0.57287\n",
      "Epoch 12078 - lr: 0.01000 - Train loss: 0.63232 - Test loss: 0.57316\n",
      "Epoch 12079 - lr: 0.01000 - Train loss: 0.63559 - Test loss: 0.57287\n",
      "Epoch 12080 - lr: 0.01000 - Train loss: 0.63567 - Test loss: 0.57293\n",
      "Epoch 12081 - lr: 0.01000 - Train loss: 0.63531 - Test loss: 0.57266\n",
      "Epoch 12082 - lr: 0.01000 - Train loss: 0.63575 - Test loss: 0.57276\n",
      "Epoch 12083 - lr: 0.01000 - Train loss: 0.63707 - Test loss: 0.57254\n",
      "Epoch 12084 - lr: 0.01000 - Train loss: 0.63244 - Test loss: 0.57239\n",
      "Epoch 12085 - lr: 0.01000 - Train loss: 0.63359 - Test loss: 0.57279\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12086 - lr: 0.01000 - Train loss: 0.64113 - Test loss: 0.57277\n",
      "Epoch 12087 - lr: 0.01000 - Train loss: 0.64197 - Test loss: 0.57243\n",
      "Epoch 12088 - lr: 0.01000 - Train loss: 0.63623 - Test loss: 0.57276\n",
      "Epoch 12089 - lr: 0.01000 - Train loss: 0.64462 - Test loss: 0.57242\n",
      "Epoch 12090 - lr: 0.01000 - Train loss: 0.63509 - Test loss: 0.57245\n",
      "Epoch 12091 - lr: 0.01000 - Train loss: 0.63223 - Test loss: 0.57227\n",
      "Epoch 12092 - lr: 0.01000 - Train loss: 0.62682 - Test loss: 0.57271\n",
      "Epoch 12093 - lr: 0.01000 - Train loss: 0.63578 - Test loss: 0.57310\n",
      "Epoch 12094 - lr: 0.01000 - Train loss: 0.64149 - Test loss: 0.57277\n",
      "Epoch 12095 - lr: 0.01000 - Train loss: 0.63546 - Test loss: 0.57311\n",
      "Epoch 12096 - lr: 0.01000 - Train loss: 0.63479 - Test loss: 0.57295\n",
      "Epoch 12097 - lr: 0.01000 - Train loss: 0.63473 - Test loss: 0.57314\n",
      "Epoch 12098 - lr: 0.01000 - Train loss: 0.63240 - Test loss: 0.57294\n",
      "Epoch 12099 - lr: 0.01000 - Train loss: 0.63142 - Test loss: 0.57328\n",
      "Epoch 12100 - lr: 0.01000 - Train loss: 0.63292 - Test loss: 0.57299\n",
      "Epoch 12101 - lr: 0.01000 - Train loss: 0.62918 - Test loss: 0.57332\n",
      "Epoch 12102 - lr: 0.01000 - Train loss: 0.63403 - Test loss: 0.57321\n",
      "Epoch 12103 - lr: 0.01000 - Train loss: 0.63465 - Test loss: 0.57316\n",
      "Epoch 12104 - lr: 0.01000 - Train loss: 0.63250 - Test loss: 0.57296\n",
      "Epoch 12105 - lr: 0.01000 - Train loss: 0.63374 - Test loss: 0.57332\n",
      "Epoch 12106 - lr: 0.01000 - Train loss: 0.64132 - Test loss: 0.57325\n",
      "Epoch 12107 - lr: 0.01000 - Train loss: 0.64172 - Test loss: 0.57286\n",
      "Epoch 12108 - lr: 0.01000 - Train loss: 0.63588 - Test loss: 0.57318\n",
      "Epoch 12109 - lr: 0.01000 - Train loss: 0.64135 - Test loss: 0.57283\n",
      "Epoch 12110 - lr: 0.01000 - Train loss: 0.63525 - Test loss: 0.57317\n",
      "Epoch 12111 - lr: 0.01000 - Train loss: 0.63431 - Test loss: 0.57303\n",
      "Epoch 12112 - lr: 0.01000 - Train loss: 0.62411 - Test loss: 0.57340\n",
      "Epoch 12113 - lr: 0.01000 - Train loss: 0.63373 - Test loss: 0.57360\n",
      "Epoch 12114 - lr: 0.01000 - Train loss: 0.63353 - Test loss: 0.57344\n",
      "Epoch 12115 - lr: 0.01000 - Train loss: 0.63656 - Test loss: 0.57356\n",
      "Epoch 12116 - lr: 0.01000 - Train loss: 0.64174 - Test loss: 0.57345\n",
      "Epoch 12117 - lr: 0.01000 - Train loss: 0.64340 - Test loss: 0.57300\n",
      "Epoch 12118 - lr: 0.01000 - Train loss: 0.63679 - Test loss: 0.57314\n",
      "Epoch 12119 - lr: 0.01000 - Train loss: 0.64079 - Test loss: 0.57304\n",
      "Epoch 12120 - lr: 0.01000 - Train loss: 0.64167 - Test loss: 0.57290\n",
      "Epoch 12121 - lr: 0.01000 - Train loss: 0.63608 - Test loss: 0.57272\n",
      "Epoch 12122 - lr: 0.01000 - Train loss: 0.63716 - Test loss: 0.57301\n",
      "Epoch 12123 - lr: 0.01000 - Train loss: 0.64250 - Test loss: 0.57266\n",
      "Epoch 12124 - lr: 0.01000 - Train loss: 0.63713 - Test loss: 0.57296\n",
      "Epoch 12125 - lr: 0.01000 - Train loss: 0.64302 - Test loss: 0.57261\n",
      "Epoch 12126 - lr: 0.01000 - Train loss: 0.63733 - Test loss: 0.57287\n",
      "Epoch 12127 - lr: 0.01000 - Train loss: 0.63509 - Test loss: 0.57274\n",
      "Epoch 12128 - lr: 0.01000 - Train loss: 0.62695 - Test loss: 0.57307\n",
      "Epoch 12129 - lr: 0.01000 - Train loss: 0.63592 - Test loss: 0.57344\n",
      "Epoch 12130 - lr: 0.01000 - Train loss: 0.63541 - Test loss: 0.57324\n",
      "Epoch 12131 - lr: 0.01000 - Train loss: 0.63015 - Test loss: 0.57350\n",
      "Epoch 12132 - lr: 0.01000 - Train loss: 0.63291 - Test loss: 0.57326\n",
      "Epoch 12133 - lr: 0.01000 - Train loss: 0.63471 - Test loss: 0.57360\n",
      "Epoch 12134 - lr: 0.01000 - Train loss: 0.64291 - Test loss: 0.57346\n",
      "Epoch 12135 - lr: 0.01000 - Train loss: 0.64412 - Test loss: 0.57297\n",
      "Epoch 12136 - lr: 0.01000 - Train loss: 0.63708 - Test loss: 0.57306\n",
      "Epoch 12137 - lr: 0.01000 - Train loss: 0.63925 - Test loss: 0.57277\n",
      "Epoch 12138 - lr: 0.01000 - Train loss: 0.63214 - Test loss: 0.57245\n",
      "Epoch 12139 - lr: 0.01000 - Train loss: 0.62709 - Test loss: 0.57280\n",
      "Epoch 12140 - lr: 0.01000 - Train loss: 0.62452 - Test loss: 0.57324\n",
      "Epoch 12141 - lr: 0.01000 - Train loss: 0.63276 - Test loss: 0.57346\n",
      "Epoch 12142 - lr: 0.01000 - Train loss: 0.63539 - Test loss: 0.57343\n",
      "Epoch 12143 - lr: 0.01000 - Train loss: 0.63199 - Test loss: 0.57314\n",
      "Epoch 12144 - lr: 0.01000 - Train loss: 0.62427 - Test loss: 0.57353\n",
      "Epoch 12145 - lr: 0.01000 - Train loss: 0.63485 - Test loss: 0.57373\n",
      "Epoch 12146 - lr: 0.01000 - Train loss: 0.63220 - Test loss: 0.57347\n",
      "Epoch 12147 - lr: 0.01000 - Train loss: 0.62734 - Test loss: 0.57380\n",
      "Epoch 12148 - lr: 0.01000 - Train loss: 0.63732 - Test loss: 0.57405\n",
      "Epoch 12149 - lr: 0.01000 - Train loss: 0.64523 - Test loss: 0.57356\n",
      "Epoch 12150 - lr: 0.01000 - Train loss: 0.63532 - Test loss: 0.57347\n",
      "Epoch 12151 - lr: 0.01000 - Train loss: 0.63194 - Test loss: 0.57318\n",
      "Epoch 12152 - lr: 0.01000 - Train loss: 0.62413 - Test loss: 0.57359\n",
      "Epoch 12153 - lr: 0.01000 - Train loss: 0.63443 - Test loss: 0.57380\n",
      "Epoch 12154 - lr: 0.01000 - Train loss: 0.63241 - Test loss: 0.57357\n",
      "Epoch 12155 - lr: 0.01000 - Train loss: 0.63231 - Test loss: 0.57391\n",
      "Epoch 12156 - lr: 0.01000 - Train loss: 0.63393 - Test loss: 0.57353\n",
      "Epoch 12157 - lr: 0.01000 - Train loss: 0.63313 - Test loss: 0.57385\n",
      "Epoch 12158 - lr: 0.01000 - Train loss: 0.63748 - Test loss: 0.57353\n",
      "Epoch 12159 - lr: 0.01000 - Train loss: 0.63245 - Test loss: 0.57331\n",
      "Epoch 12160 - lr: 0.01000 - Train loss: 0.63404 - Test loss: 0.57367\n",
      "Epoch 12161 - lr: 0.01000 - Train loss: 0.64182 - Test loss: 0.57356\n",
      "Epoch 12162 - lr: 0.01000 - Train loss: 0.63438 - Test loss: 0.57337\n",
      "Epoch 12163 - lr: 0.01000 - Train loss: 0.63260 - Test loss: 0.57354\n",
      "Epoch 12164 - lr: 0.01000 - Train loss: 0.63501 - Test loss: 0.57353\n",
      "Epoch 12165 - lr: 0.01000 - Train loss: 0.63194 - Test loss: 0.57329\n",
      "Epoch 12166 - lr: 0.01000 - Train loss: 0.62552 - Test loss: 0.57368\n",
      "Epoch 12167 - lr: 0.01000 - Train loss: 0.62817 - Test loss: 0.57402\n",
      "Epoch 12168 - lr: 0.01000 - Train loss: 0.63638 - Test loss: 0.57407\n",
      "Epoch 12169 - lr: 0.01000 - Train loss: 0.63784 - Test loss: 0.57374\n",
      "Epoch 12170 - lr: 0.01000 - Train loss: 0.63204 - Test loss: 0.57345\n",
      "Epoch 12171 - lr: 0.01000 - Train loss: 0.62754 - Test loss: 0.57380\n",
      "Epoch 12172 - lr: 0.01000 - Train loss: 0.63680 - Test loss: 0.57399\n",
      "Epoch 12173 - lr: 0.01000 - Train loss: 0.63753 - Test loss: 0.57379\n",
      "Epoch 12174 - lr: 0.01000 - Train loss: 0.63312 - Test loss: 0.57357\n",
      "Epoch 12175 - lr: 0.01000 - Train loss: 0.63673 - Test loss: 0.57383\n",
      "Epoch 12176 - lr: 0.01000 - Train loss: 0.63541 - Test loss: 0.57360\n",
      "Epoch 12177 - lr: 0.01000 - Train loss: 0.63259 - Test loss: 0.57377\n",
      "Epoch 12178 - lr: 0.01000 - Train loss: 0.63483 - Test loss: 0.57373\n",
      "Epoch 12179 - lr: 0.01000 - Train loss: 0.63203 - Test loss: 0.57349\n",
      "Epoch 12180 - lr: 0.01000 - Train loss: 0.62821 - Test loss: 0.57384\n",
      "Epoch 12181 - lr: 0.01000 - Train loss: 0.63567 - Test loss: 0.57387\n",
      "Epoch 12182 - lr: 0.01000 - Train loss: 0.63369 - Test loss: 0.57354\n",
      "Epoch 12183 - lr: 0.01000 - Train loss: 0.63409 - Test loss: 0.57388\n",
      "Epoch 12184 - lr: 0.01000 - Train loss: 0.64139 - Test loss: 0.57379\n",
      "Epoch 12185 - lr: 0.01000 - Train loss: 0.63393 - Test loss: 0.57359\n",
      "Epoch 12186 - lr: 0.01000 - Train loss: 0.62893 - Test loss: 0.57380\n",
      "Epoch 12187 - lr: 0.01000 - Train loss: 0.63257 - Test loss: 0.57413\n",
      "Epoch 12188 - lr: 0.01000 - Train loss: 0.63582 - Test loss: 0.57377\n",
      "Epoch 12189 - lr: 0.01000 - Train loss: 0.63572 - Test loss: 0.57379\n",
      "Epoch 12190 - lr: 0.01000 - Train loss: 0.63413 - Test loss: 0.57345\n",
      "Epoch 12191 - lr: 0.01000 - Train loss: 0.63564 - Test loss: 0.57379\n",
      "Epoch 12192 - lr: 0.01000 - Train loss: 0.63729 - Test loss: 0.57352\n",
      "Epoch 12193 - lr: 0.01000 - Train loss: 0.62491 - Test loss: 0.57385\n",
      "Epoch 12194 - lr: 0.01000 - Train loss: 0.63300 - Test loss: 0.57406\n",
      "Epoch 12195 - lr: 0.01000 - Train loss: 0.63405 - Test loss: 0.57395\n",
      "Epoch 12196 - lr: 0.01000 - Train loss: 0.63378 - Test loss: 0.57383\n",
      "Epoch 12197 - lr: 0.01000 - Train loss: 0.63438 - Test loss: 0.57376\n",
      "Epoch 12198 - lr: 0.01000 - Train loss: 0.63232 - Test loss: 0.57356\n",
      "Epoch 12199 - lr: 0.01000 - Train loss: 0.63419 - Test loss: 0.57393\n",
      "Epoch 12200 - lr: 0.01000 - Train loss: 0.63984 - Test loss: 0.57380\n",
      "Epoch 12201 - lr: 0.01000 - Train loss: 0.63846 - Test loss: 0.57351\n",
      "Epoch 12202 - lr: 0.01000 - Train loss: 0.63249 - Test loss: 0.57321\n",
      "Epoch 12203 - lr: 0.01000 - Train loss: 0.62761 - Test loss: 0.57360\n",
      "Epoch 12204 - lr: 0.01000 - Train loss: 0.63630 - Test loss: 0.57378\n",
      "Epoch 12205 - lr: 0.01000 - Train loss: 0.64141 - Test loss: 0.57371\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12206 - lr: 0.01000 - Train loss: 0.63935 - Test loss: 0.57337\n",
      "Epoch 12207 - lr: 0.01000 - Train loss: 0.62850 - Test loss: 0.57369\n",
      "Epoch 12208 - lr: 0.01000 - Train loss: 0.63502 - Test loss: 0.57368\n",
      "Epoch 12209 - lr: 0.01000 - Train loss: 0.63196 - Test loss: 0.57343\n",
      "Epoch 12210 - lr: 0.01000 - Train loss: 0.62556 - Test loss: 0.57382\n",
      "Epoch 12211 - lr: 0.01000 - Train loss: 0.62995 - Test loss: 0.57415\n",
      "Epoch 12212 - lr: 0.01000 - Train loss: 0.63241 - Test loss: 0.57391\n",
      "Epoch 12213 - lr: 0.01000 - Train loss: 0.63320 - Test loss: 0.57423\n",
      "Epoch 12214 - lr: 0.01000 - Train loss: 0.64056 - Test loss: 0.57403\n",
      "Epoch 12215 - lr: 0.01000 - Train loss: 0.64043 - Test loss: 0.57386\n",
      "Epoch 12216 - lr: 0.01000 - Train loss: 0.64163 - Test loss: 0.57371\n",
      "Epoch 12217 - lr: 0.01000 - Train loss: 0.64098 - Test loss: 0.57332\n",
      "Epoch 12218 - lr: 0.01000 - Train loss: 0.63467 - Test loss: 0.57364\n",
      "Epoch 12219 - lr: 0.01000 - Train loss: 0.63986 - Test loss: 0.57352\n",
      "Epoch 12220 - lr: 0.01000 - Train loss: 0.63511 - Test loss: 0.57316\n",
      "Epoch 12221 - lr: 0.01000 - Train loss: 0.63674 - Test loss: 0.57343\n",
      "Epoch 12222 - lr: 0.01000 - Train loss: 0.63459 - Test loss: 0.57330\n",
      "Epoch 12223 - lr: 0.01000 - Train loss: 0.62712 - Test loss: 0.57363\n",
      "Epoch 12224 - lr: 0.01000 - Train loss: 0.63689 - Test loss: 0.57395\n",
      "Epoch 12225 - lr: 0.01000 - Train loss: 0.64829 - Test loss: 0.57349\n",
      "Epoch 12226 - lr: 0.01000 - Train loss: 0.63171 - Test loss: 0.57318\n",
      "Epoch 12227 - lr: 0.01000 - Train loss: 0.62401 - Test loss: 0.57362\n",
      "Epoch 12228 - lr: 0.01000 - Train loss: 0.63213 - Test loss: 0.57385\n",
      "Epoch 12229 - lr: 0.01000 - Train loss: 0.63549 - Test loss: 0.57387\n",
      "Epoch 12230 - lr: 0.01000 - Train loss: 0.63227 - Test loss: 0.57352\n",
      "Epoch 12231 - lr: 0.01000 - Train loss: 0.62396 - Test loss: 0.57393\n",
      "Epoch 12232 - lr: 0.01000 - Train loss: 0.63060 - Test loss: 0.57415\n",
      "Epoch 12233 - lr: 0.01000 - Train loss: 0.63686 - Test loss: 0.57440\n",
      "Epoch 12234 - lr: 0.01000 - Train loss: 0.64242 - Test loss: 0.57394\n",
      "Epoch 12235 - lr: 0.01000 - Train loss: 0.63678 - Test loss: 0.57417\n",
      "Epoch 12236 - lr: 0.01000 - Train loss: 0.64465 - Test loss: 0.57371\n",
      "Epoch 12237 - lr: 0.01000 - Train loss: 0.63525 - Test loss: 0.57367\n",
      "Epoch 12238 - lr: 0.01000 - Train loss: 0.63186 - Test loss: 0.57337\n",
      "Epoch 12239 - lr: 0.01000 - Train loss: 0.62386 - Test loss: 0.57380\n",
      "Epoch 12240 - lr: 0.01000 - Train loss: 0.63480 - Test loss: 0.57404\n",
      "Epoch 12241 - lr: 0.01000 - Train loss: 0.63183 - Test loss: 0.57379\n",
      "Epoch 12242 - lr: 0.01000 - Train loss: 0.62614 - Test loss: 0.57416\n",
      "Epoch 12243 - lr: 0.01000 - Train loss: 0.63428 - Test loss: 0.57448\n",
      "Epoch 12244 - lr: 0.01000 - Train loss: 0.64137 - Test loss: 0.57434\n",
      "Epoch 12245 - lr: 0.01000 - Train loss: 0.63687 - Test loss: 0.57407\n",
      "Epoch 12246 - lr: 0.01000 - Train loss: 0.63511 - Test loss: 0.57397\n",
      "Epoch 12247 - lr: 0.01000 - Train loss: 0.63176 - Test loss: 0.57366\n",
      "Epoch 12248 - lr: 0.01000 - Train loss: 0.62391 - Test loss: 0.57407\n",
      "Epoch 12249 - lr: 0.01000 - Train loss: 0.63023 - Test loss: 0.57429\n",
      "Epoch 12250 - lr: 0.01000 - Train loss: 0.63658 - Test loss: 0.57458\n",
      "Epoch 12251 - lr: 0.01000 - Train loss: 0.64811 - Test loss: 0.57408\n",
      "Epoch 12252 - lr: 0.01000 - Train loss: 0.63177 - Test loss: 0.57374\n",
      "Epoch 12253 - lr: 0.01000 - Train loss: 0.62559 - Test loss: 0.57411\n",
      "Epoch 12254 - lr: 0.01000 - Train loss: 0.63059 - Test loss: 0.57443\n",
      "Epoch 12255 - lr: 0.01000 - Train loss: 0.63198 - Test loss: 0.57411\n",
      "Epoch 12256 - lr: 0.01000 - Train loss: 0.62516 - Test loss: 0.57446\n",
      "Epoch 12257 - lr: 0.01000 - Train loss: 0.62693 - Test loss: 0.57475\n",
      "Epoch 12258 - lr: 0.01000 - Train loss: 0.63667 - Test loss: 0.57498\n",
      "Epoch 12259 - lr: 0.01000 - Train loss: 0.65046 - Test loss: 0.57443\n",
      "Epoch 12260 - lr: 0.01000 - Train loss: 0.63274 - Test loss: 0.57398\n",
      "Epoch 12261 - lr: 0.01000 - Train loss: 0.62788 - Test loss: 0.57430\n",
      "Epoch 12262 - lr: 0.01000 - Train loss: 0.63585 - Test loss: 0.57434\n",
      "Epoch 12263 - lr: 0.01000 - Train loss: 0.63586 - Test loss: 0.57398\n",
      "Epoch 12264 - lr: 0.01000 - Train loss: 0.63461 - Test loss: 0.57393\n",
      "Epoch 12265 - lr: 0.01000 - Train loss: 0.63175 - Test loss: 0.57368\n",
      "Epoch 12266 - lr: 0.01000 - Train loss: 0.62684 - Test loss: 0.57407\n",
      "Epoch 12267 - lr: 0.01000 - Train loss: 0.63636 - Test loss: 0.57436\n",
      "Epoch 12268 - lr: 0.01000 - Train loss: 0.64253 - Test loss: 0.57395\n",
      "Epoch 12269 - lr: 0.01000 - Train loss: 0.63647 - Test loss: 0.57417\n",
      "Epoch 12270 - lr: 0.01000 - Train loss: 0.63469 - Test loss: 0.57397\n",
      "Epoch 12271 - lr: 0.01000 - Train loss: 0.63347 - Test loss: 0.57415\n",
      "Epoch 12272 - lr: 0.01000 - Train loss: 0.63270 - Test loss: 0.57399\n",
      "Epoch 12273 - lr: 0.01000 - Train loss: 0.63638 - Test loss: 0.57428\n",
      "Epoch 12274 - lr: 0.01000 - Train loss: 0.63862 - Test loss: 0.57395\n",
      "Epoch 12275 - lr: 0.01000 - Train loss: 0.62543 - Test loss: 0.57428\n",
      "Epoch 12276 - lr: 0.01000 - Train loss: 0.62908 - Test loss: 0.57459\n",
      "Epoch 12277 - lr: 0.01000 - Train loss: 0.63340 - Test loss: 0.57441\n",
      "Epoch 12278 - lr: 0.01000 - Train loss: 0.63580 - Test loss: 0.57445\n",
      "Epoch 12279 - lr: 0.01000 - Train loss: 0.63682 - Test loss: 0.57410\n",
      "Epoch 12280 - lr: 0.01000 - Train loss: 0.63228 - Test loss: 0.57389\n",
      "Epoch 12281 - lr: 0.01000 - Train loss: 0.63515 - Test loss: 0.57425\n",
      "Epoch 12282 - lr: 0.01000 - Train loss: 0.63448 - Test loss: 0.57405\n",
      "Epoch 12283 - lr: 0.01000 - Train loss: 0.63381 - Test loss: 0.57423\n",
      "Epoch 12284 - lr: 0.01000 - Train loss: 0.63226 - Test loss: 0.57404\n",
      "Epoch 12285 - lr: 0.01000 - Train loss: 0.63472 - Test loss: 0.57439\n",
      "Epoch 12286 - lr: 0.01000 - Train loss: 0.63438 - Test loss: 0.57421\n",
      "Epoch 12287 - lr: 0.01000 - Train loss: 0.62925 - Test loss: 0.57448\n",
      "Epoch 12288 - lr: 0.01000 - Train loss: 0.63293 - Test loss: 0.57428\n",
      "Epoch 12289 - lr: 0.01000 - Train loss: 0.63641 - Test loss: 0.57449\n",
      "Epoch 12290 - lr: 0.01000 - Train loss: 0.63348 - Test loss: 0.57430\n",
      "Epoch 12291 - lr: 0.01000 - Train loss: 0.63102 - Test loss: 0.57445\n",
      "Epoch 12292 - lr: 0.01000 - Train loss: 0.63637 - Test loss: 0.57459\n",
      "Epoch 12293 - lr: 0.01000 - Train loss: 0.64033 - Test loss: 0.57443\n",
      "Epoch 12294 - lr: 0.01000 - Train loss: 0.64141 - Test loss: 0.57423\n",
      "Epoch 12295 - lr: 0.01000 - Train loss: 0.63402 - Test loss: 0.57401\n",
      "Epoch 12296 - lr: 0.01000 - Train loss: 0.63349 - Test loss: 0.57416\n",
      "Epoch 12297 - lr: 0.01000 - Train loss: 0.63258 - Test loss: 0.57399\n",
      "Epoch 12298 - lr: 0.01000 - Train loss: 0.63628 - Test loss: 0.57431\n",
      "Epoch 12299 - lr: 0.01000 - Train loss: 0.64415 - Test loss: 0.57388\n",
      "Epoch 12300 - lr: 0.01000 - Train loss: 0.63536 - Test loss: 0.57389\n",
      "Epoch 12301 - lr: 0.01000 - Train loss: 0.63246 - Test loss: 0.57357\n",
      "Epoch 12302 - lr: 0.01000 - Train loss: 0.62587 - Test loss: 0.57398\n",
      "Epoch 12303 - lr: 0.01000 - Train loss: 0.63329 - Test loss: 0.57434\n",
      "Epoch 12304 - lr: 0.01000 - Train loss: 0.63982 - Test loss: 0.57411\n",
      "Epoch 12305 - lr: 0.01000 - Train loss: 0.63697 - Test loss: 0.57378\n",
      "Epoch 12306 - lr: 0.01000 - Train loss: 0.63211 - Test loss: 0.57360\n",
      "Epoch 12307 - lr: 0.01000 - Train loss: 0.63443 - Test loss: 0.57400\n",
      "Epoch 12308 - lr: 0.01000 - Train loss: 0.63954 - Test loss: 0.57389\n",
      "Epoch 12309 - lr: 0.01000 - Train loss: 0.63426 - Test loss: 0.57353\n",
      "Epoch 12310 - lr: 0.01000 - Train loss: 0.63584 - Test loss: 0.57390\n",
      "Epoch 12311 - lr: 0.01000 - Train loss: 0.63929 - Test loss: 0.57361\n",
      "Epoch 12312 - lr: 0.01000 - Train loss: 0.62907 - Test loss: 0.57394\n",
      "Epoch 12313 - lr: 0.01000 - Train loss: 0.63304 - Test loss: 0.57383\n",
      "Epoch 12314 - lr: 0.01000 - Train loss: 0.63638 - Test loss: 0.57403\n",
      "Epoch 12315 - lr: 0.01000 - Train loss: 0.64176 - Test loss: 0.57396\n",
      "Epoch 12316 - lr: 0.01000 - Train loss: 0.63848 - Test loss: 0.57363\n",
      "Epoch 12317 - lr: 0.01000 - Train loss: 0.62538 - Test loss: 0.57398\n",
      "Epoch 12318 - lr: 0.01000 - Train loss: 0.62826 - Test loss: 0.57433\n",
      "Epoch 12319 - lr: 0.01000 - Train loss: 0.63536 - Test loss: 0.57431\n",
      "Epoch 12320 - lr: 0.01000 - Train loss: 0.63186 - Test loss: 0.57395\n",
      "Epoch 12321 - lr: 0.01000 - Train loss: 0.62372 - Test loss: 0.57435\n",
      "Epoch 12322 - lr: 0.01000 - Train loss: 0.63542 - Test loss: 0.57455\n",
      "Epoch 12323 - lr: 0.01000 - Train loss: 0.63170 - Test loss: 0.57421\n",
      "Epoch 12324 - lr: 0.01000 - Train loss: 0.62350 - Test loss: 0.57460\n",
      "Epoch 12325 - lr: 0.01000 - Train loss: 0.63301 - Test loss: 0.57478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12326 - lr: 0.01000 - Train loss: 0.63334 - Test loss: 0.57459\n",
      "Epoch 12327 - lr: 0.01000 - Train loss: 0.63532 - Test loss: 0.57458\n",
      "Epoch 12328 - lr: 0.01000 - Train loss: 0.63330 - Test loss: 0.57420\n",
      "Epoch 12329 - lr: 0.01000 - Train loss: 0.63328 - Test loss: 0.57453\n",
      "Epoch 12330 - lr: 0.01000 - Train loss: 0.64097 - Test loss: 0.57437\n",
      "Epoch 12331 - lr: 0.01000 - Train loss: 0.63455 - Test loss: 0.57415\n",
      "Epoch 12332 - lr: 0.01000 - Train loss: 0.63143 - Test loss: 0.57443\n",
      "Epoch 12333 - lr: 0.01000 - Train loss: 0.63253 - Test loss: 0.57407\n",
      "Epoch 12334 - lr: 0.01000 - Train loss: 0.62681 - Test loss: 0.57442\n",
      "Epoch 12335 - lr: 0.01000 - Train loss: 0.63651 - Test loss: 0.57468\n",
      "Epoch 12336 - lr: 0.01000 - Train loss: 0.64261 - Test loss: 0.57424\n",
      "Epoch 12337 - lr: 0.01000 - Train loss: 0.63652 - Test loss: 0.57442\n",
      "Epoch 12338 - lr: 0.01000 - Train loss: 0.63364 - Test loss: 0.57423\n",
      "Epoch 12339 - lr: 0.01000 - Train loss: 0.62874 - Test loss: 0.57442\n",
      "Epoch 12340 - lr: 0.01000 - Train loss: 0.63310 - Test loss: 0.57475\n",
      "Epoch 12341 - lr: 0.01000 - Train loss: 0.63941 - Test loss: 0.57445\n",
      "Epoch 12342 - lr: 0.01000 - Train loss: 0.63449 - Test loss: 0.57404\n",
      "Epoch 12343 - lr: 0.01000 - Train loss: 0.63637 - Test loss: 0.57433\n",
      "Epoch 12344 - lr: 0.01000 - Train loss: 0.64176 - Test loss: 0.57393\n",
      "Epoch 12345 - lr: 0.01000 - Train loss: 0.63630 - Test loss: 0.57422\n",
      "Epoch 12346 - lr: 0.01000 - Train loss: 0.64337 - Test loss: 0.57382\n",
      "Epoch 12347 - lr: 0.01000 - Train loss: 0.63616 - Test loss: 0.57394\n",
      "Epoch 12348 - lr: 0.01000 - Train loss: 0.63890 - Test loss: 0.57371\n",
      "Epoch 12349 - lr: 0.01000 - Train loss: 0.63205 - Test loss: 0.57338\n",
      "Epoch 12350 - lr: 0.01000 - Train loss: 0.62374 - Test loss: 0.57385\n",
      "Epoch 12351 - lr: 0.01000 - Train loss: 0.63286 - Test loss: 0.57412\n",
      "Epoch 12352 - lr: 0.01000 - Train loss: 0.63309 - Test loss: 0.57403\n",
      "Epoch 12353 - lr: 0.01000 - Train loss: 0.63574 - Test loss: 0.57415\n",
      "Epoch 12354 - lr: 0.01000 - Train loss: 0.63570 - Test loss: 0.57384\n",
      "Epoch 12355 - lr: 0.01000 - Train loss: 0.63474 - Test loss: 0.57387\n",
      "Epoch 12356 - lr: 0.01000 - Train loss: 0.63144 - Test loss: 0.57362\n",
      "Epoch 12357 - lr: 0.01000 - Train loss: 0.62349 - Test loss: 0.57409\n",
      "Epoch 12358 - lr: 0.01000 - Train loss: 0.63338 - Test loss: 0.57435\n",
      "Epoch 12359 - lr: 0.01000 - Train loss: 0.63225 - Test loss: 0.57420\n",
      "Epoch 12360 - lr: 0.01000 - Train loss: 0.63561 - Test loss: 0.57456\n",
      "Epoch 12361 - lr: 0.01000 - Train loss: 0.64140 - Test loss: 0.57417\n",
      "Epoch 12362 - lr: 0.01000 - Train loss: 0.63574 - Test loss: 0.57448\n",
      "Epoch 12363 - lr: 0.01000 - Train loss: 0.64053 - Test loss: 0.57411\n",
      "Epoch 12364 - lr: 0.01000 - Train loss: 0.63388 - Test loss: 0.57443\n",
      "Epoch 12365 - lr: 0.01000 - Train loss: 0.64176 - Test loss: 0.57434\n",
      "Epoch 12366 - lr: 0.01000 - Train loss: 0.63899 - Test loss: 0.57398\n",
      "Epoch 12367 - lr: 0.01000 - Train loss: 0.62767 - Test loss: 0.57429\n",
      "Epoch 12368 - lr: 0.01000 - Train loss: 0.63614 - Test loss: 0.57440\n",
      "Epoch 12369 - lr: 0.01000 - Train loss: 0.63746 - Test loss: 0.57408\n",
      "Epoch 12370 - lr: 0.01000 - Train loss: 0.63148 - Test loss: 0.57384\n",
      "Epoch 12371 - lr: 0.01000 - Train loss: 0.62658 - Test loss: 0.57423\n",
      "Epoch 12372 - lr: 0.01000 - Train loss: 0.63643 - Test loss: 0.57457\n",
      "Epoch 12373 - lr: 0.01000 - Train loss: 0.64512 - Test loss: 0.57412\n",
      "Epoch 12374 - lr: 0.01000 - Train loss: 0.63377 - Test loss: 0.57401\n",
      "Epoch 12375 - lr: 0.01000 - Train loss: 0.63298 - Test loss: 0.57392\n",
      "Epoch 12376 - lr: 0.01000 - Train loss: 0.63570 - Test loss: 0.57406\n",
      "Epoch 12377 - lr: 0.01000 - Train loss: 0.63522 - Test loss: 0.57376\n",
      "Epoch 12378 - lr: 0.01000 - Train loss: 0.63584 - Test loss: 0.57392\n",
      "Epoch 12379 - lr: 0.01000 - Train loss: 0.63745 - Test loss: 0.57368\n",
      "Epoch 12380 - lr: 0.01000 - Train loss: 0.63126 - Test loss: 0.57349\n",
      "Epoch 12381 - lr: 0.01000 - Train loss: 0.62455 - Test loss: 0.57397\n",
      "Epoch 12382 - lr: 0.01000 - Train loss: 0.62390 - Test loss: 0.57443\n",
      "Epoch 12383 - lr: 0.01000 - Train loss: 0.62754 - Test loss: 0.57471\n",
      "Epoch 12384 - lr: 0.01000 - Train loss: 0.62712 - Test loss: 0.57505\n",
      "Epoch 12385 - lr: 0.01000 - Train loss: 0.63664 - Test loss: 0.57520\n",
      "Epoch 12386 - lr: 0.01000 - Train loss: 0.63481 - Test loss: 0.57493\n",
      "Epoch 12387 - lr: 0.01000 - Train loss: 0.63378 - Test loss: 0.57516\n",
      "Epoch 12388 - lr: 0.01000 - Train loss: 0.64188 - Test loss: 0.57498\n",
      "Epoch 12389 - lr: 0.01000 - Train loss: 0.64438 - Test loss: 0.57443\n",
      "Epoch 12390 - lr: 0.01000 - Train loss: 0.63499 - Test loss: 0.57436\n",
      "Epoch 12391 - lr: 0.01000 - Train loss: 0.63150 - Test loss: 0.57402\n",
      "Epoch 12392 - lr: 0.01000 - Train loss: 0.62385 - Test loss: 0.57443\n",
      "Epoch 12393 - lr: 0.01000 - Train loss: 0.63421 - Test loss: 0.57466\n",
      "Epoch 12394 - lr: 0.01000 - Train loss: 0.63150 - Test loss: 0.57440\n",
      "Epoch 12395 - lr: 0.01000 - Train loss: 0.62680 - Test loss: 0.57475\n",
      "Epoch 12396 - lr: 0.01000 - Train loss: 0.63654 - Test loss: 0.57498\n",
      "Epoch 12397 - lr: 0.01000 - Train loss: 0.63584 - Test loss: 0.57468\n",
      "Epoch 12398 - lr: 0.01000 - Train loss: 0.62934 - Test loss: 0.57484\n",
      "Epoch 12399 - lr: 0.01000 - Train loss: 0.63547 - Test loss: 0.57516\n",
      "Epoch 12400 - lr: 0.01000 - Train loss: 0.63897 - Test loss: 0.57475\n",
      "Epoch 12401 - lr: 0.01000 - Train loss: 0.62697 - Test loss: 0.57501\n",
      "Epoch 12402 - lr: 0.01000 - Train loss: 0.63674 - Test loss: 0.57519\n",
      "Epoch 12403 - lr: 0.01000 - Train loss: 0.63496 - Test loss: 0.57488\n",
      "Epoch 12404 - lr: 0.01000 - Train loss: 0.63295 - Test loss: 0.57499\n",
      "Epoch 12405 - lr: 0.01000 - Train loss: 0.63300 - Test loss: 0.57479\n",
      "Epoch 12406 - lr: 0.01000 - Train loss: 0.63596 - Test loss: 0.57487\n",
      "Epoch 12407 - lr: 0.01000 - Train loss: 0.63918 - Test loss: 0.57457\n",
      "Epoch 12408 - lr: 0.01000 - Train loss: 0.63363 - Test loss: 0.57417\n",
      "Epoch 12409 - lr: 0.01000 - Train loss: 0.63428 - Test loss: 0.57453\n",
      "Epoch 12410 - lr: 0.01000 - Train loss: 0.64000 - Test loss: 0.57441\n",
      "Epoch 12411 - lr: 0.01000 - Train loss: 0.63744 - Test loss: 0.57407\n",
      "Epoch 12412 - lr: 0.01000 - Train loss: 0.63129 - Test loss: 0.57384\n",
      "Epoch 12413 - lr: 0.01000 - Train loss: 0.62530 - Test loss: 0.57427\n",
      "Epoch 12414 - lr: 0.01000 - Train loss: 0.63066 - Test loss: 0.57464\n",
      "Epoch 12415 - lr: 0.01000 - Train loss: 0.63153 - Test loss: 0.57432\n",
      "Epoch 12416 - lr: 0.01000 - Train loss: 0.62331 - Test loss: 0.57474\n",
      "Epoch 12417 - lr: 0.01000 - Train loss: 0.63482 - Test loss: 0.57495\n",
      "Epoch 12418 - lr: 0.01000 - Train loss: 0.63143 - Test loss: 0.57463\n",
      "Epoch 12419 - lr: 0.01000 - Train loss: 0.62400 - Test loss: 0.57501\n",
      "Epoch 12420 - lr: 0.01000 - Train loss: 0.62351 - Test loss: 0.57537\n",
      "Epoch 12421 - lr: 0.01000 - Train loss: 0.63637 - Test loss: 0.57554\n",
      "Epoch 12422 - lr: 0.01000 - Train loss: 0.63299 - Test loss: 0.57510\n",
      "Epoch 12423 - lr: 0.01000 - Train loss: 0.63250 - Test loss: 0.57537\n",
      "Epoch 12424 - lr: 0.01000 - Train loss: 0.63885 - Test loss: 0.57504\n",
      "Epoch 12425 - lr: 0.01000 - Train loss: 0.63481 - Test loss: 0.57462\n",
      "Epoch 12426 - lr: 0.01000 - Train loss: 0.63549 - Test loss: 0.57470\n",
      "Epoch 12427 - lr: 0.01000 - Train loss: 0.63866 - Test loss: 0.57445\n",
      "Epoch 12428 - lr: 0.01000 - Train loss: 0.63427 - Test loss: 0.57411\n",
      "Epoch 12429 - lr: 0.01000 - Train loss: 0.63585 - Test loss: 0.57436\n",
      "Epoch 12430 - lr: 0.01000 - Train loss: 0.63657 - Test loss: 0.57423\n",
      "Epoch 12431 - lr: 0.01000 - Train loss: 0.63279 - Test loss: 0.57410\n",
      "Epoch 12432 - lr: 0.01000 - Train loss: 0.63524 - Test loss: 0.57424\n",
      "Epoch 12433 - lr: 0.01000 - Train loss: 0.63575 - Test loss: 0.57398\n",
      "Epoch 12434 - lr: 0.01000 - Train loss: 0.63292 - Test loss: 0.57393\n",
      "Epoch 12435 - lr: 0.01000 - Train loss: 0.63406 - Test loss: 0.57399\n",
      "Epoch 12436 - lr: 0.01000 - Train loss: 0.63121 - Test loss: 0.57382\n",
      "Epoch 12437 - lr: 0.01000 - Train loss: 0.62604 - Test loss: 0.57427\n",
      "Epoch 12438 - lr: 0.01000 - Train loss: 0.63525 - Test loss: 0.57466\n",
      "Epoch 12439 - lr: 0.01000 - Train loss: 0.64183 - Test loss: 0.57429\n",
      "Epoch 12440 - lr: 0.01000 - Train loss: 0.63594 - Test loss: 0.57455\n",
      "Epoch 12441 - lr: 0.01000 - Train loss: 0.63740 - Test loss: 0.57428\n",
      "Epoch 12442 - lr: 0.01000 - Train loss: 0.62324 - Test loss: 0.57469\n",
      "Epoch 12443 - lr: 0.01000 - Train loss: 0.63275 - Test loss: 0.57490\n",
      "Epoch 12444 - lr: 0.01000 - Train loss: 0.63276 - Test loss: 0.57475\n",
      "Epoch 12445 - lr: 0.01000 - Train loss: 0.63545 - Test loss: 0.57484\n",
      "Epoch 12446 - lr: 0.01000 - Train loss: 0.63858 - Test loss: 0.57458\n",
      "Epoch 12447 - lr: 0.01000 - Train loss: 0.63430 - Test loss: 0.57423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12448 - lr: 0.01000 - Train loss: 0.63576 - Test loss: 0.57445\n",
      "Epoch 12449 - lr: 0.01000 - Train loss: 0.63873 - Test loss: 0.57434\n",
      "Epoch 12450 - lr: 0.01000 - Train loss: 0.63366 - Test loss: 0.57400\n",
      "Epoch 12451 - lr: 0.01000 - Train loss: 0.63532 - Test loss: 0.57437\n",
      "Epoch 12452 - lr: 0.01000 - Train loss: 0.64017 - Test loss: 0.57406\n",
      "Epoch 12453 - lr: 0.01000 - Train loss: 0.63369 - Test loss: 0.57442\n",
      "Epoch 12454 - lr: 0.01000 - Train loss: 0.64106 - Test loss: 0.57437\n",
      "Epoch 12455 - lr: 0.01000 - Train loss: 0.63377 - Test loss: 0.57419\n",
      "Epoch 12456 - lr: 0.01000 - Train loss: 0.63268 - Test loss: 0.57439\n",
      "Epoch 12457 - lr: 0.01000 - Train loss: 0.63269 - Test loss: 0.57428\n",
      "Epoch 12458 - lr: 0.01000 - Train loss: 0.63576 - Test loss: 0.57445\n",
      "Epoch 12459 - lr: 0.01000 - Train loss: 0.63992 - Test loss: 0.57427\n",
      "Epoch 12460 - lr: 0.01000 - Train loss: 0.64002 - Test loss: 0.57408\n",
      "Epoch 12461 - lr: 0.01000 - Train loss: 0.64009 - Test loss: 0.57391\n",
      "Epoch 12462 - lr: 0.01000 - Train loss: 0.64004 - Test loss: 0.57375\n",
      "Epoch 12463 - lr: 0.01000 - Train loss: 0.63917 - Test loss: 0.57356\n",
      "Epoch 12464 - lr: 0.01000 - Train loss: 0.63298 - Test loss: 0.57328\n",
      "Epoch 12465 - lr: 0.01000 - Train loss: 0.63038 - Test loss: 0.57373\n",
      "Epoch 12466 - lr: 0.01000 - Train loss: 0.63112 - Test loss: 0.57352\n",
      "Epoch 12467 - lr: 0.01000 - Train loss: 0.62420 - Test loss: 0.57400\n",
      "Epoch 12468 - lr: 0.01000 - Train loss: 0.63185 - Test loss: 0.57430\n",
      "Epoch 12469 - lr: 0.01000 - Train loss: 0.63404 - Test loss: 0.57430\n",
      "Epoch 12470 - lr: 0.01000 - Train loss: 0.63128 - Test loss: 0.57411\n",
      "Epoch 12471 - lr: 0.01000 - Train loss: 0.62798 - Test loss: 0.57450\n",
      "Epoch 12472 - lr: 0.01000 - Train loss: 0.63399 - Test loss: 0.57447\n",
      "Epoch 12473 - lr: 0.01000 - Train loss: 0.63141 - Test loss: 0.57426\n",
      "Epoch 12474 - lr: 0.01000 - Train loss: 0.63028 - Test loss: 0.57463\n",
      "Epoch 12475 - lr: 0.01000 - Train loss: 0.63137 - Test loss: 0.57434\n",
      "Epoch 12476 - lr: 0.01000 - Train loss: 0.62343 - Test loss: 0.57477\n",
      "Epoch 12477 - lr: 0.01000 - Train loss: 0.62748 - Test loss: 0.57504\n",
      "Epoch 12478 - lr: 0.01000 - Train loss: 0.62801 - Test loss: 0.57536\n",
      "Epoch 12479 - lr: 0.01000 - Train loss: 0.63404 - Test loss: 0.57521\n",
      "Epoch 12480 - lr: 0.01000 - Train loss: 0.63166 - Test loss: 0.57492\n",
      "Epoch 12481 - lr: 0.01000 - Train loss: 0.63235 - Test loss: 0.57523\n",
      "Epoch 12482 - lr: 0.01000 - Train loss: 0.63927 - Test loss: 0.57497\n",
      "Epoch 12483 - lr: 0.01000 - Train loss: 0.63969 - Test loss: 0.57472\n",
      "Epoch 12484 - lr: 0.01000 - Train loss: 0.64086 - Test loss: 0.57459\n",
      "Epoch 12485 - lr: 0.01000 - Train loss: 0.64041 - Test loss: 0.57420\n",
      "Epoch 12486 - lr: 0.01000 - Train loss: 0.63441 - Test loss: 0.57454\n",
      "Epoch 12487 - lr: 0.01000 - Train loss: 0.63473 - Test loss: 0.57439\n",
      "Epoch 12488 - lr: 0.01000 - Train loss: 0.63521 - Test loss: 0.57471\n",
      "Epoch 12489 - lr: 0.01000 - Train loss: 0.63785 - Test loss: 0.57439\n",
      "Epoch 12490 - lr: 0.01000 - Train loss: 0.62407 - Test loss: 0.57475\n",
      "Epoch 12491 - lr: 0.01000 - Train loss: 0.62349 - Test loss: 0.57513\n",
      "Epoch 12492 - lr: 0.01000 - Train loss: 0.63588 - Test loss: 0.57531\n",
      "Epoch 12493 - lr: 0.01000 - Train loss: 0.63189 - Test loss: 0.57489\n",
      "Epoch 12494 - lr: 0.01000 - Train loss: 0.62413 - Test loss: 0.57524\n",
      "Epoch 12495 - lr: 0.01000 - Train loss: 0.62305 - Test loss: 0.57559\n",
      "Epoch 12496 - lr: 0.01000 - Train loss: 0.63341 - Test loss: 0.57570\n",
      "Epoch 12497 - lr: 0.01000 - Train loss: 0.63208 - Test loss: 0.57537\n",
      "Epoch 12498 - lr: 0.01000 - Train loss: 0.63519 - Test loss: 0.57564\n",
      "Epoch 12499 - lr: 0.01000 - Train loss: 0.64579 - Test loss: 0.57508\n",
      "Epoch 12500 - lr: 0.01000 - Train loss: 0.63220 - Test loss: 0.57480\n",
      "Epoch 12501 - lr: 0.01000 - Train loss: 0.63592 - Test loss: 0.57506\n",
      "Epoch 12502 - lr: 0.01000 - Train loss: 0.64029 - Test loss: 0.57465\n",
      "Epoch 12503 - lr: 0.01000 - Train loss: 0.63391 - Test loss: 0.57496\n",
      "Epoch 12504 - lr: 0.01000 - Train loss: 0.63936 - Test loss: 0.57481\n",
      "Epoch 12505 - lr: 0.01000 - Train loss: 0.63581 - Test loss: 0.57443\n",
      "Epoch 12506 - lr: 0.01000 - Train loss: 0.63335 - Test loss: 0.57435\n",
      "Epoch 12507 - lr: 0.01000 - Train loss: 0.63247 - Test loss: 0.57425\n",
      "Epoch 12508 - lr: 0.01000 - Train loss: 0.63543 - Test loss: 0.57443\n",
      "Epoch 12509 - lr: 0.01000 - Train loss: 0.63865 - Test loss: 0.57424\n",
      "Epoch 12510 - lr: 0.01000 - Train loss: 0.63338 - Test loss: 0.57393\n",
      "Epoch 12511 - lr: 0.01000 - Train loss: 0.63464 - Test loss: 0.57435\n",
      "Epoch 12512 - lr: 0.01000 - Train loss: 0.63327 - Test loss: 0.57426\n",
      "Epoch 12513 - lr: 0.01000 - Train loss: 0.62640 - Test loss: 0.57456\n",
      "Epoch 12514 - lr: 0.01000 - Train loss: 0.62333 - Test loss: 0.57500\n",
      "Epoch 12515 - lr: 0.01000 - Train loss: 0.62966 - Test loss: 0.57522\n",
      "Epoch 12516 - lr: 0.01000 - Train loss: 0.63609 - Test loss: 0.57550\n",
      "Epoch 12517 - lr: 0.01000 - Train loss: 0.64999 - Test loss: 0.57497\n",
      "Epoch 12518 - lr: 0.01000 - Train loss: 0.63204 - Test loss: 0.57453\n",
      "Epoch 12519 - lr: 0.01000 - Train loss: 0.62528 - Test loss: 0.57491\n",
      "Epoch 12520 - lr: 0.01000 - Train loss: 0.63243 - Test loss: 0.57524\n",
      "Epoch 12521 - lr: 0.01000 - Train loss: 0.63763 - Test loss: 0.57490\n",
      "Epoch 12522 - lr: 0.01000 - Train loss: 0.63114 - Test loss: 0.57454\n",
      "Epoch 12523 - lr: 0.01000 - Train loss: 0.62301 - Test loss: 0.57497\n",
      "Epoch 12524 - lr: 0.01000 - Train loss: 0.63235 - Test loss: 0.57518\n",
      "Epoch 12525 - lr: 0.01000 - Train loss: 0.63293 - Test loss: 0.57503\n",
      "Epoch 12526 - lr: 0.01000 - Train loss: 0.63426 - Test loss: 0.57499\n",
      "Epoch 12527 - lr: 0.01000 - Train loss: 0.63127 - Test loss: 0.57467\n",
      "Epoch 12528 - lr: 0.01000 - Train loss: 0.62463 - Test loss: 0.57507\n",
      "Epoch 12529 - lr: 0.01000 - Train loss: 0.62815 - Test loss: 0.57539\n",
      "Epoch 12530 - lr: 0.01000 - Train loss: 0.63323 - Test loss: 0.57523\n",
      "Epoch 12531 - lr: 0.01000 - Train loss: 0.63315 - Test loss: 0.57509\n",
      "Epoch 12532 - lr: 0.01000 - Train loss: 0.63280 - Test loss: 0.57495\n",
      "Epoch 12533 - lr: 0.01000 - Train loss: 0.63372 - Test loss: 0.57490\n",
      "Epoch 12534 - lr: 0.01000 - Train loss: 0.63130 - Test loss: 0.57466\n",
      "Epoch 12535 - lr: 0.01000 - Train loss: 0.63031 - Test loss: 0.57503\n",
      "Epoch 12536 - lr: 0.01000 - Train loss: 0.63234 - Test loss: 0.57473\n",
      "Epoch 12537 - lr: 0.01000 - Train loss: 0.63288 - Test loss: 0.57509\n",
      "Epoch 12538 - lr: 0.01000 - Train loss: 0.63952 - Test loss: 0.57500\n",
      "Epoch 12539 - lr: 0.01000 - Train loss: 0.63850 - Test loss: 0.57483\n",
      "Epoch 12540 - lr: 0.01000 - Train loss: 0.63554 - Test loss: 0.57449\n",
      "Epoch 12541 - lr: 0.01000 - Train loss: 0.63235 - Test loss: 0.57439\n",
      "Epoch 12542 - lr: 0.01000 - Train loss: 0.63474 - Test loss: 0.57452\n",
      "Epoch 12543 - lr: 0.01000 - Train loss: 0.63606 - Test loss: 0.57429\n",
      "Epoch 12544 - lr: 0.01000 - Train loss: 0.63122 - Test loss: 0.57413\n",
      "Epoch 12545 - lr: 0.01000 - Train loss: 0.63194 - Test loss: 0.57456\n",
      "Epoch 12546 - lr: 0.01000 - Train loss: 0.63824 - Test loss: 0.57440\n",
      "Epoch 12547 - lr: 0.01000 - Train loss: 0.63591 - Test loss: 0.57416\n",
      "Epoch 12548 - lr: 0.01000 - Train loss: 0.63133 - Test loss: 0.57403\n",
      "Epoch 12549 - lr: 0.01000 - Train loss: 0.63336 - Test loss: 0.57447\n",
      "Epoch 12550 - lr: 0.01000 - Train loss: 0.63687 - Test loss: 0.57439\n",
      "Epoch 12551 - lr: 0.01000 - Train loss: 0.63114 - Test loss: 0.57417\n",
      "Epoch 12552 - lr: 0.01000 - Train loss: 0.62816 - Test loss: 0.57459\n",
      "Epoch 12553 - lr: 0.01000 - Train loss: 0.63245 - Test loss: 0.57450\n",
      "Epoch 12554 - lr: 0.01000 - Train loss: 0.63459 - Test loss: 0.57461\n",
      "Epoch 12555 - lr: 0.01000 - Train loss: 0.63508 - Test loss: 0.57435\n",
      "Epoch 12556 - lr: 0.01000 - Train loss: 0.63259 - Test loss: 0.57429\n",
      "Epoch 12557 - lr: 0.01000 - Train loss: 0.63336 - Test loss: 0.57432\n",
      "Epoch 12558 - lr: 0.01000 - Train loss: 0.63122 - Test loss: 0.57417\n",
      "Epoch 12559 - lr: 0.01000 - Train loss: 0.63215 - Test loss: 0.57460\n",
      "Epoch 12560 - lr: 0.01000 - Train loss: 0.63973 - Test loss: 0.57455\n",
      "Epoch 12561 - lr: 0.01000 - Train loss: 0.63431 - Test loss: 0.57437\n",
      "Epoch 12562 - lr: 0.01000 - Train loss: 0.63066 - Test loss: 0.57461\n",
      "Epoch 12563 - lr: 0.01000 - Train loss: 0.63477 - Test loss: 0.57474\n",
      "Epoch 12564 - lr: 0.01000 - Train loss: 0.63699 - Test loss: 0.57451\n",
      "Epoch 12565 - lr: 0.01000 - Train loss: 0.63169 - Test loss: 0.57424\n",
      "Epoch 12566 - lr: 0.01000 - Train loss: 0.62996 - Test loss: 0.57464\n",
      "Epoch 12567 - lr: 0.01000 - Train loss: 0.63174 - Test loss: 0.57439\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12568 - lr: 0.01000 - Train loss: 0.63057 - Test loss: 0.57478\n",
      "Epoch 12569 - lr: 0.01000 - Train loss: 0.63329 - Test loss: 0.57450\n",
      "Epoch 12570 - lr: 0.01000 - Train loss: 0.63521 - Test loss: 0.57479\n",
      "Epoch 12571 - lr: 0.01000 - Train loss: 0.63786 - Test loss: 0.57450\n",
      "Epoch 12572 - lr: 0.01000 - Train loss: 0.62593 - Test loss: 0.57485\n",
      "Epoch 12573 - lr: 0.01000 - Train loss: 0.63524 - Test loss: 0.57517\n",
      "Epoch 12574 - lr: 0.01000 - Train loss: 0.65052 - Test loss: 0.57473\n",
      "Epoch 12575 - lr: 0.01000 - Train loss: 0.63552 - Test loss: 0.57439\n",
      "Epoch 12576 - lr: 0.01000 - Train loss: 0.63185 - Test loss: 0.57427\n",
      "Epoch 12577 - lr: 0.01000 - Train loss: 0.63531 - Test loss: 0.57457\n",
      "Epoch 12578 - lr: 0.01000 - Train loss: 0.63245 - Test loss: 0.57448\n",
      "Epoch 12579 - lr: 0.01000 - Train loss: 0.63202 - Test loss: 0.57470\n",
      "Epoch 12580 - lr: 0.01000 - Train loss: 0.63267 - Test loss: 0.57463\n",
      "Epoch 12581 - lr: 0.01000 - Train loss: 0.63353 - Test loss: 0.57462\n",
      "Epoch 12582 - lr: 0.01000 - Train loss: 0.63110 - Test loss: 0.57442\n",
      "Epoch 12583 - lr: 0.01000 - Train loss: 0.63017 - Test loss: 0.57482\n",
      "Epoch 12584 - lr: 0.01000 - Train loss: 0.63211 - Test loss: 0.57455\n",
      "Epoch 12585 - lr: 0.01000 - Train loss: 0.63223 - Test loss: 0.57493\n",
      "Epoch 12586 - lr: 0.01000 - Train loss: 0.63999 - Test loss: 0.57484\n",
      "Epoch 12587 - lr: 0.01000 - Train loss: 0.64016 - Test loss: 0.57446\n",
      "Epoch 12588 - lr: 0.01000 - Train loss: 0.63424 - Test loss: 0.57481\n",
      "Epoch 12589 - lr: 0.01000 - Train loss: 0.63527 - Test loss: 0.57459\n",
      "Epoch 12590 - lr: 0.01000 - Train loss: 0.62733 - Test loss: 0.57485\n",
      "Epoch 12591 - lr: 0.01000 - Train loss: 0.62878 - Test loss: 0.57521\n",
      "Epoch 12592 - lr: 0.01000 - Train loss: 0.63159 - Test loss: 0.57497\n",
      "Epoch 12593 - lr: 0.01000 - Train loss: 0.63379 - Test loss: 0.57532\n",
      "Epoch 12594 - lr: 0.01000 - Train loss: 0.63238 - Test loss: 0.57514\n",
      "Epoch 12595 - lr: 0.01000 - Train loss: 0.63153 - Test loss: 0.57528\n",
      "Epoch 12596 - lr: 0.01000 - Train loss: 0.63378 - Test loss: 0.57519\n",
      "Epoch 12597 - lr: 0.01000 - Train loss: 0.63116 - Test loss: 0.57490\n",
      "Epoch 12598 - lr: 0.01000 - Train loss: 0.62896 - Test loss: 0.57524\n",
      "Epoch 12599 - lr: 0.01000 - Train loss: 0.63137 - Test loss: 0.57497\n",
      "Epoch 12600 - lr: 0.01000 - Train loss: 0.63180 - Test loss: 0.57531\n",
      "Epoch 12601 - lr: 0.01000 - Train loss: 0.63878 - Test loss: 0.57508\n",
      "Epoch 12602 - lr: 0.01000 - Train loss: 0.64002 - Test loss: 0.57493\n",
      "Epoch 12603 - lr: 0.01000 - Train loss: 0.63740 - Test loss: 0.57459\n",
      "Epoch 12604 - lr: 0.01000 - Train loss: 0.62386 - Test loss: 0.57497\n",
      "Epoch 12605 - lr: 0.01000 - Train loss: 0.62274 - Test loss: 0.57540\n",
      "Epoch 12606 - lr: 0.01000 - Train loss: 0.63152 - Test loss: 0.57557\n",
      "Epoch 12607 - lr: 0.01000 - Train loss: 0.63388 - Test loss: 0.57547\n",
      "Epoch 12608 - lr: 0.01000 - Train loss: 0.63132 - Test loss: 0.57513\n",
      "Epoch 12609 - lr: 0.01000 - Train loss: 0.62955 - Test loss: 0.57544\n",
      "Epoch 12610 - lr: 0.01000 - Train loss: 0.63157 - Test loss: 0.57511\n",
      "Epoch 12611 - lr: 0.01000 - Train loss: 0.63042 - Test loss: 0.57543\n",
      "Epoch 12612 - lr: 0.01000 - Train loss: 0.63332 - Test loss: 0.57507\n",
      "Epoch 12613 - lr: 0.01000 - Train loss: 0.63522 - Test loss: 0.57528\n",
      "Epoch 12614 - lr: 0.01000 - Train loss: 0.63651 - Test loss: 0.57497\n",
      "Epoch 12615 - lr: 0.01000 - Train loss: 0.62234 - Test loss: 0.57537\n",
      "Epoch 12616 - lr: 0.01000 - Train loss: 0.63665 - Test loss: 0.57560\n",
      "Epoch 12617 - lr: 0.01000 - Train loss: 0.63673 - Test loss: 0.57529\n",
      "Epoch 12618 - lr: 0.01000 - Train loss: 0.63181 - Test loss: 0.57494\n",
      "Epoch 12619 - lr: 0.01000 - Train loss: 0.63189 - Test loss: 0.57529\n",
      "Epoch 12620 - lr: 0.01000 - Train loss: 0.63958 - Test loss: 0.57514\n",
      "Epoch 12621 - lr: 0.01000 - Train loss: 0.63335 - Test loss: 0.57493\n",
      "Epoch 12622 - lr: 0.01000 - Train loss: 0.63368 - Test loss: 0.57511\n",
      "Epoch 12623 - lr: 0.01000 - Train loss: 0.63118 - Test loss: 0.57486\n",
      "Epoch 12624 - lr: 0.01000 - Train loss: 0.63028 - Test loss: 0.57522\n",
      "Epoch 12625 - lr: 0.01000 - Train loss: 0.63280 - Test loss: 0.57490\n",
      "Epoch 12626 - lr: 0.01000 - Train loss: 0.63475 - Test loss: 0.57523\n",
      "Epoch 12627 - lr: 0.01000 - Train loss: 0.64844 - Test loss: 0.57478\n",
      "Epoch 12628 - lr: 0.01000 - Train loss: 0.63199 - Test loss: 0.57445\n",
      "Epoch 12629 - lr: 0.01000 - Train loss: 0.63186 - Test loss: 0.57485\n",
      "Epoch 12630 - lr: 0.01000 - Train loss: 0.63884 - Test loss: 0.57471\n",
      "Epoch 12631 - lr: 0.01000 - Train loss: 0.64009 - Test loss: 0.57464\n",
      "Epoch 12632 - lr: 0.01000 - Train loss: 0.64137 - Test loss: 0.57427\n",
      "Epoch 12633 - lr: 0.01000 - Train loss: 0.63543 - Test loss: 0.57454\n",
      "Epoch 12634 - lr: 0.01000 - Train loss: 0.63265 - Test loss: 0.57447\n",
      "Epoch 12635 - lr: 0.01000 - Train loss: 0.63194 - Test loss: 0.57469\n",
      "Epoch 12636 - lr: 0.01000 - Train loss: 0.63250 - Test loss: 0.57463\n",
      "Epoch 12637 - lr: 0.01000 - Train loss: 0.63389 - Test loss: 0.57467\n",
      "Epoch 12638 - lr: 0.01000 - Train loss: 0.63094 - Test loss: 0.57442\n",
      "Epoch 12639 - lr: 0.01000 - Train loss: 0.62444 - Test loss: 0.57487\n",
      "Epoch 12640 - lr: 0.01000 - Train loss: 0.62862 - Test loss: 0.57524\n",
      "Epoch 12641 - lr: 0.01000 - Train loss: 0.63151 - Test loss: 0.57502\n",
      "Epoch 12642 - lr: 0.01000 - Train loss: 0.63415 - Test loss: 0.57538\n",
      "Epoch 12643 - lr: 0.01000 - Train loss: 0.63832 - Test loss: 0.57501\n",
      "Epoch 12644 - lr: 0.01000 - Train loss: 0.62773 - Test loss: 0.57531\n",
      "Epoch 12645 - lr: 0.01000 - Train loss: 0.63310 - Test loss: 0.57518\n",
      "Epoch 12646 - lr: 0.01000 - Train loss: 0.63210 - Test loss: 0.57501\n",
      "Epoch 12647 - lr: 0.01000 - Train loss: 0.63496 - Test loss: 0.57514\n",
      "Epoch 12648 - lr: 0.01000 - Train loss: 0.64003 - Test loss: 0.57504\n",
      "Epoch 12649 - lr: 0.01000 - Train loss: 0.63987 - Test loss: 0.57464\n",
      "Epoch 12650 - lr: 0.01000 - Train loss: 0.63381 - Test loss: 0.57499\n",
      "Epoch 12651 - lr: 0.01000 - Train loss: 0.63289 - Test loss: 0.57486\n",
      "Epoch 12652 - lr: 0.01000 - Train loss: 0.62271 - Test loss: 0.57525\n",
      "Epoch 12653 - lr: 0.01000 - Train loss: 0.63261 - Test loss: 0.57544\n",
      "Epoch 12654 - lr: 0.01000 - Train loss: 0.63180 - Test loss: 0.57523\n",
      "Epoch 12655 - lr: 0.01000 - Train loss: 0.63544 - Test loss: 0.57550\n",
      "Epoch 12656 - lr: 0.01000 - Train loss: 0.64737 - Test loss: 0.57501\n",
      "Epoch 12657 - lr: 0.01000 - Train loss: 0.63078 - Test loss: 0.57467\n",
      "Epoch 12658 - lr: 0.01000 - Train loss: 0.62369 - Test loss: 0.57510\n",
      "Epoch 12659 - lr: 0.01000 - Train loss: 0.62272 - Test loss: 0.57554\n",
      "Epoch 12660 - lr: 0.01000 - Train loss: 0.63041 - Test loss: 0.57572\n",
      "Epoch 12661 - lr: 0.01000 - Train loss: 0.63523 - Test loss: 0.57579\n",
      "Epoch 12662 - lr: 0.01000 - Train loss: 0.64040 - Test loss: 0.57562\n",
      "Epoch 12663 - lr: 0.01000 - Train loss: 0.64601 - Test loss: 0.57508\n",
      "Epoch 12664 - lr: 0.01000 - Train loss: 0.63100 - Test loss: 0.57479\n",
      "Epoch 12665 - lr: 0.01000 - Train loss: 0.63070 - Test loss: 0.57517\n",
      "Epoch 12666 - lr: 0.01000 - Train loss: 0.63262 - Test loss: 0.57483\n",
      "Epoch 12667 - lr: 0.01000 - Train loss: 0.63334 - Test loss: 0.57521\n",
      "Epoch 12668 - lr: 0.01000 - Train loss: 0.63693 - Test loss: 0.57505\n",
      "Epoch 12669 - lr: 0.01000 - Train loss: 0.63081 - Test loss: 0.57475\n",
      "Epoch 12670 - lr: 0.01000 - Train loss: 0.62501 - Test loss: 0.57516\n",
      "Epoch 12671 - lr: 0.01000 - Train loss: 0.63289 - Test loss: 0.57551\n",
      "Epoch 12672 - lr: 0.01000 - Train loss: 0.64030 - Test loss: 0.57539\n",
      "Epoch 12673 - lr: 0.01000 - Train loss: 0.63427 - Test loss: 0.57511\n",
      "Epoch 12674 - lr: 0.01000 - Train loss: 0.63159 - Test loss: 0.57527\n",
      "Epoch 12675 - lr: 0.01000 - Train loss: 0.63320 - Test loss: 0.57517\n",
      "Epoch 12676 - lr: 0.01000 - Train loss: 0.63152 - Test loss: 0.57498\n",
      "Epoch 12677 - lr: 0.01000 - Train loss: 0.63507 - Test loss: 0.57532\n",
      "Epoch 12678 - lr: 0.01000 - Train loss: 0.64709 - Test loss: 0.57487\n",
      "Epoch 12679 - lr: 0.01000 - Train loss: 0.63060 - Test loss: 0.57456\n",
      "Epoch 12680 - lr: 0.01000 - Train loss: 0.62315 - Test loss: 0.57503\n",
      "Epoch 12681 - lr: 0.01000 - Train loss: 0.62461 - Test loss: 0.57541\n",
      "Epoch 12682 - lr: 0.01000 - Train loss: 0.62697 - Test loss: 0.57568\n",
      "Epoch 12683 - lr: 0.01000 - Train loss: 0.62735 - Test loss: 0.57599\n",
      "Epoch 12684 - lr: 0.01000 - Train loss: 0.63395 - Test loss: 0.57586\n",
      "Epoch 12685 - lr: 0.01000 - Train loss: 0.63107 - Test loss: 0.57548\n",
      "Epoch 12686 - lr: 0.01000 - Train loss: 0.62679 - Test loss: 0.57580\n",
      "Epoch 12687 - lr: 0.01000 - Train loss: 0.63470 - Test loss: 0.57579\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12688 - lr: 0.01000 - Train loss: 0.63588 - Test loss: 0.57542\n",
      "Epoch 12689 - lr: 0.01000 - Train loss: 0.63109 - Test loss: 0.57514\n",
      "Epoch 12690 - lr: 0.01000 - Train loss: 0.63216 - Test loss: 0.57550\n",
      "Epoch 12691 - lr: 0.01000 - Train loss: 0.64005 - Test loss: 0.57538\n",
      "Epoch 12692 - lr: 0.01000 - Train loss: 0.64282 - Test loss: 0.57492\n",
      "Epoch 12693 - lr: 0.01000 - Train loss: 0.63428 - Test loss: 0.57494\n",
      "Epoch 12694 - lr: 0.01000 - Train loss: 0.63263 - Test loss: 0.57465\n",
      "Epoch 12695 - lr: 0.01000 - Train loss: 0.63410 - Test loss: 0.57505\n",
      "Epoch 12696 - lr: 0.01000 - Train loss: 0.63532 - Test loss: 0.57484\n",
      "Epoch 12697 - lr: 0.01000 - Train loss: 0.62569 - Test loss: 0.57515\n",
      "Epoch 12698 - lr: 0.01000 - Train loss: 0.62233 - Test loss: 0.57562\n",
      "Epoch 12699 - lr: 0.01000 - Train loss: 0.63458 - Test loss: 0.57582\n",
      "Epoch 12700 - lr: 0.01000 - Train loss: 0.63131 - Test loss: 0.57546\n",
      "Epoch 12701 - lr: 0.01000 - Train loss: 0.62753 - Test loss: 0.57578\n",
      "Epoch 12702 - lr: 0.01000 - Train loss: 0.63295 - Test loss: 0.57563\n",
      "Epoch 12703 - lr: 0.01000 - Train loss: 0.63207 - Test loss: 0.57543\n",
      "Epoch 12704 - lr: 0.01000 - Train loss: 0.63455 - Test loss: 0.57550\n",
      "Epoch 12705 - lr: 0.01000 - Train loss: 0.63787 - Test loss: 0.57526\n",
      "Epoch 12706 - lr: 0.01000 - Train loss: 0.63635 - Test loss: 0.57497\n",
      "Epoch 12707 - lr: 0.01000 - Train loss: 0.63089 - Test loss: 0.57470\n",
      "Epoch 12708 - lr: 0.01000 - Train loss: 0.62764 - Test loss: 0.57511\n",
      "Epoch 12709 - lr: 0.01000 - Train loss: 0.63230 - Test loss: 0.57503\n",
      "Epoch 12710 - lr: 0.01000 - Train loss: 0.63335 - Test loss: 0.57504\n",
      "Epoch 12711 - lr: 0.01000 - Train loss: 0.63100 - Test loss: 0.57482\n",
      "Epoch 12712 - lr: 0.01000 - Train loss: 0.63012 - Test loss: 0.57522\n",
      "Epoch 12713 - lr: 0.01000 - Train loss: 0.63295 - Test loss: 0.57495\n",
      "Epoch 12714 - lr: 0.01000 - Train loss: 0.63479 - Test loss: 0.57522\n",
      "Epoch 12715 - lr: 0.01000 - Train loss: 0.63273 - Test loss: 0.57509\n",
      "Epoch 12716 - lr: 0.01000 - Train loss: 0.63382 - Test loss: 0.57531\n",
      "Epoch 12717 - lr: 0.01000 - Train loss: 0.63118 - Test loss: 0.57506\n",
      "Epoch 12718 - lr: 0.01000 - Train loss: 0.63077 - Test loss: 0.57544\n",
      "Epoch 12719 - lr: 0.01000 - Train loss: 0.63577 - Test loss: 0.57517\n",
      "Epoch 12720 - lr: 0.01000 - Train loss: 0.63103 - Test loss: 0.57493\n",
      "Epoch 12721 - lr: 0.01000 - Train loss: 0.63157 - Test loss: 0.57532\n",
      "Epoch 12722 - lr: 0.01000 - Train loss: 0.63921 - Test loss: 0.57522\n",
      "Epoch 12723 - lr: 0.01000 - Train loss: 0.63402 - Test loss: 0.57501\n",
      "Epoch 12724 - lr: 0.01000 - Train loss: 0.63006 - Test loss: 0.57523\n",
      "Epoch 12725 - lr: 0.01000 - Train loss: 0.63462 - Test loss: 0.57537\n",
      "Epoch 12726 - lr: 0.01000 - Train loss: 0.63957 - Test loss: 0.57527\n",
      "Epoch 12727 - lr: 0.01000 - Train loss: 0.64046 - Test loss: 0.57487\n",
      "Epoch 12728 - lr: 0.01000 - Train loss: 0.63482 - Test loss: 0.57518\n",
      "Epoch 12729 - lr: 0.01000 - Train loss: 0.64663 - Test loss: 0.57476\n",
      "Epoch 12730 - lr: 0.01000 - Train loss: 0.63054 - Test loss: 0.57450\n",
      "Epoch 12731 - lr: 0.01000 - Train loss: 0.62495 - Test loss: 0.57497\n",
      "Epoch 12732 - lr: 0.01000 - Train loss: 0.63324 - Test loss: 0.57537\n",
      "Epoch 12733 - lr: 0.01000 - Train loss: 0.63480 - Test loss: 0.57521\n",
      "Epoch 12734 - lr: 0.01000 - Train loss: 0.63485 - Test loss: 0.57526\n",
      "Epoch 12735 - lr: 0.01000 - Train loss: 0.63794 - Test loss: 0.57503\n",
      "Epoch 12736 - lr: 0.01000 - Train loss: 0.63429 - Test loss: 0.57470\n",
      "Epoch 12737 - lr: 0.01000 - Train loss: 0.63393 - Test loss: 0.57476\n",
      "Epoch 12738 - lr: 0.01000 - Train loss: 0.63140 - Test loss: 0.57449\n",
      "Epoch 12739 - lr: 0.01000 - Train loss: 0.62802 - Test loss: 0.57492\n",
      "Epoch 12740 - lr: 0.01000 - Train loss: 0.63147 - Test loss: 0.57480\n",
      "Epoch 12741 - lr: 0.01000 - Train loss: 0.63503 - Test loss: 0.57514\n",
      "Epoch 12742 - lr: 0.01000 - Train loss: 0.64254 - Test loss: 0.57476\n",
      "Epoch 12743 - lr: 0.01000 - Train loss: 0.63417 - Test loss: 0.57481\n",
      "Epoch 12744 - lr: 0.01000 - Train loss: 0.63257 - Test loss: 0.57454\n",
      "Epoch 12745 - lr: 0.01000 - Train loss: 0.63415 - Test loss: 0.57496\n",
      "Epoch 12746 - lr: 0.01000 - Train loss: 0.63835 - Test loss: 0.57468\n",
      "Epoch 12747 - lr: 0.01000 - Train loss: 0.62949 - Test loss: 0.57504\n",
      "Epoch 12748 - lr: 0.01000 - Train loss: 0.63068 - Test loss: 0.57478\n",
      "Epoch 12749 - lr: 0.01000 - Train loss: 0.62353 - Test loss: 0.57524\n",
      "Epoch 12750 - lr: 0.01000 - Train loss: 0.62269 - Test loss: 0.57568\n",
      "Epoch 12751 - lr: 0.01000 - Train loss: 0.62764 - Test loss: 0.57591\n",
      "Epoch 12752 - lr: 0.01000 - Train loss: 0.63140 - Test loss: 0.57620\n",
      "Epoch 12753 - lr: 0.01000 - Train loss: 0.63758 - Test loss: 0.57584\n",
      "Epoch 12754 - lr: 0.01000 - Train loss: 0.63330 - Test loss: 0.57540\n",
      "Epoch 12755 - lr: 0.01000 - Train loss: 0.63518 - Test loss: 0.57560\n",
      "Epoch 12756 - lr: 0.01000 - Train loss: 0.63621 - Test loss: 0.57528\n",
      "Epoch 12757 - lr: 0.01000 - Train loss: 0.62232 - Test loss: 0.57566\n",
      "Epoch 12758 - lr: 0.01000 - Train loss: 0.63609 - Test loss: 0.57588\n",
      "Epoch 12759 - lr: 0.01000 - Train loss: 0.63448 - Test loss: 0.57550\n",
      "Epoch 12760 - lr: 0.01000 - Train loss: 0.63330 - Test loss: 0.57541\n",
      "Epoch 12761 - lr: 0.01000 - Train loss: 0.63072 - Test loss: 0.57514\n",
      "Epoch 12762 - lr: 0.01000 - Train loss: 0.62868 - Test loss: 0.57551\n",
      "Epoch 12763 - lr: 0.01000 - Train loss: 0.63091 - Test loss: 0.57525\n",
      "Epoch 12764 - lr: 0.01000 - Train loss: 0.63053 - Test loss: 0.57561\n",
      "Epoch 12765 - lr: 0.01000 - Train loss: 0.63434 - Test loss: 0.57528\n",
      "Epoch 12766 - lr: 0.01000 - Train loss: 0.63292 - Test loss: 0.57521\n",
      "Epoch 12767 - lr: 0.01000 - Train loss: 0.63093 - Test loss: 0.57502\n",
      "Epoch 12768 - lr: 0.01000 - Train loss: 0.63295 - Test loss: 0.57542\n",
      "Epoch 12769 - lr: 0.01000 - Train loss: 0.63196 - Test loss: 0.57530\n",
      "Epoch 12770 - lr: 0.01000 - Train loss: 0.62250 - Test loss: 0.57569\n",
      "Epoch 12771 - lr: 0.01000 - Train loss: 0.63559 - Test loss: 0.57593\n",
      "Epoch 12772 - lr: 0.01000 - Train loss: 0.63437 - Test loss: 0.57558\n",
      "Epoch 12773 - lr: 0.01000 - Train loss: 0.63262 - Test loss: 0.57545\n",
      "Epoch 12774 - lr: 0.01000 - Train loss: 0.63159 - Test loss: 0.57529\n",
      "Epoch 12775 - lr: 0.01000 - Train loss: 0.63451 - Test loss: 0.57546\n",
      "Epoch 12776 - lr: 0.01000 - Train loss: 0.63716 - Test loss: 0.57534\n",
      "Epoch 12777 - lr: 0.01000 - Train loss: 0.63383 - Test loss: 0.57501\n",
      "Epoch 12778 - lr: 0.01000 - Train loss: 0.63348 - Test loss: 0.57504\n",
      "Epoch 12779 - lr: 0.01000 - Train loss: 0.63175 - Test loss: 0.57480\n",
      "Epoch 12780 - lr: 0.01000 - Train loss: 0.63322 - Test loss: 0.57521\n",
      "Epoch 12781 - lr: 0.01000 - Train loss: 0.63352 - Test loss: 0.57507\n",
      "Epoch 12782 - lr: 0.01000 - Train loss: 0.63102 - Test loss: 0.57529\n",
      "Epoch 12783 - lr: 0.01000 - Train loss: 0.63288 - Test loss: 0.57527\n",
      "Epoch 12784 - lr: 0.01000 - Train loss: 0.63096 - Test loss: 0.57508\n",
      "Epoch 12785 - lr: 0.01000 - Train loss: 0.63320 - Test loss: 0.57547\n",
      "Epoch 12786 - lr: 0.01000 - Train loss: 0.63433 - Test loss: 0.57526\n",
      "Epoch 12787 - lr: 0.01000 - Train loss: 0.62799 - Test loss: 0.57550\n",
      "Epoch 12788 - lr: 0.01000 - Train loss: 0.63319 - Test loss: 0.57586\n",
      "Epoch 12789 - lr: 0.01000 - Train loss: 0.63331 - Test loss: 0.57564\n",
      "Epoch 12790 - lr: 0.01000 - Train loss: 0.63282 - Test loss: 0.57580\n",
      "Epoch 12791 - lr: 0.01000 - Train loss: 0.63108 - Test loss: 0.57554\n",
      "Epoch 12792 - lr: 0.01000 - Train loss: 0.63329 - Test loss: 0.57588\n",
      "Epoch 12793 - lr: 0.01000 - Train loss: 0.63554 - Test loss: 0.57557\n",
      "Epoch 12794 - lr: 0.01000 - Train loss: 0.62327 - Test loss: 0.57590\n",
      "Epoch 12795 - lr: 0.01000 - Train loss: 0.63217 - Test loss: 0.57609\n",
      "Epoch 12796 - lr: 0.01000 - Train loss: 0.63169 - Test loss: 0.57585\n",
      "Epoch 12797 - lr: 0.01000 - Train loss: 0.63469 - Test loss: 0.57598\n",
      "Epoch 12798 - lr: 0.01000 - Train loss: 0.63312 - Test loss: 0.57577\n",
      "Epoch 12799 - lr: 0.01000 - Train loss: 0.63315 - Test loss: 0.57604\n",
      "Epoch 12800 - lr: 0.01000 - Train loss: 0.63197 - Test loss: 0.57583\n",
      "Epoch 12801 - lr: 0.01000 - Train loss: 0.63546 - Test loss: 0.57599\n",
      "Epoch 12802 - lr: 0.01000 - Train loss: 0.63386 - Test loss: 0.57562\n",
      "Epoch 12803 - lr: 0.01000 - Train loss: 0.63363 - Test loss: 0.57559\n",
      "Epoch 12804 - lr: 0.01000 - Train loss: 0.63214 - Test loss: 0.57527\n",
      "Epoch 12805 - lr: 0.01000 - Train loss: 0.63410 - Test loss: 0.57561\n",
      "Epoch 12806 - lr: 0.01000 - Train loss: 0.64728 - Test loss: 0.57517\n",
      "Epoch 12807 - lr: 0.01000 - Train loss: 0.63105 - Test loss: 0.57486\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12808 - lr: 0.01000 - Train loss: 0.62924 - Test loss: 0.57527\n",
      "Epoch 12809 - lr: 0.01000 - Train loss: 0.63106 - Test loss: 0.57503\n",
      "Epoch 12810 - lr: 0.01000 - Train loss: 0.62978 - Test loss: 0.57543\n",
      "Epoch 12811 - lr: 0.01000 - Train loss: 0.63223 - Test loss: 0.57514\n",
      "Epoch 12812 - lr: 0.01000 - Train loss: 0.63417 - Test loss: 0.57550\n",
      "Epoch 12813 - lr: 0.01000 - Train loss: 0.64741 - Test loss: 0.57508\n",
      "Epoch 12814 - lr: 0.01000 - Train loss: 0.63112 - Test loss: 0.57479\n",
      "Epoch 12815 - lr: 0.01000 - Train loss: 0.62952 - Test loss: 0.57521\n",
      "Epoch 12816 - lr: 0.01000 - Train loss: 0.63139 - Test loss: 0.57496\n",
      "Epoch 12817 - lr: 0.01000 - Train loss: 0.63107 - Test loss: 0.57537\n",
      "Epoch 12818 - lr: 0.01000 - Train loss: 0.63741 - Test loss: 0.57518\n",
      "Epoch 12819 - lr: 0.01000 - Train loss: 0.63559 - Test loss: 0.57492\n",
      "Epoch 12820 - lr: 0.01000 - Train loss: 0.63051 - Test loss: 0.57472\n",
      "Epoch 12821 - lr: 0.01000 - Train loss: 0.62931 - Test loss: 0.57516\n",
      "Epoch 12822 - lr: 0.01000 - Train loss: 0.63119 - Test loss: 0.57492\n",
      "Epoch 12823 - lr: 0.01000 - Train loss: 0.63064 - Test loss: 0.57533\n",
      "Epoch 12824 - lr: 0.01000 - Train loss: 0.63576 - Test loss: 0.57510\n",
      "Epoch 12825 - lr: 0.01000 - Train loss: 0.63072 - Test loss: 0.57486\n",
      "Epoch 12826 - lr: 0.01000 - Train loss: 0.62974 - Test loss: 0.57528\n",
      "Epoch 12827 - lr: 0.01000 - Train loss: 0.63245 - Test loss: 0.57501\n",
      "Epoch 12828 - lr: 0.01000 - Train loss: 0.63448 - Test loss: 0.57533\n",
      "Epoch 12829 - lr: 0.01000 - Train loss: 0.64371 - Test loss: 0.57494\n",
      "Epoch 12830 - lr: 0.01000 - Train loss: 0.63177 - Test loss: 0.57484\n",
      "Epoch 12831 - lr: 0.01000 - Train loss: 0.63334 - Test loss: 0.57493\n",
      "Epoch 12832 - lr: 0.01000 - Train loss: 0.63155 - Test loss: 0.57471\n",
      "Epoch 12833 - lr: 0.01000 - Train loss: 0.63273 - Test loss: 0.57515\n",
      "Epoch 12834 - lr: 0.01000 - Train loss: 0.63189 - Test loss: 0.57509\n",
      "Epoch 12835 - lr: 0.01000 - Train loss: 0.62181 - Test loss: 0.57555\n",
      "Epoch 12836 - lr: 0.01000 - Train loss: 0.63464 - Test loss: 0.57580\n",
      "Epoch 12837 - lr: 0.01000 - Train loss: 0.63239 - Test loss: 0.57547\n",
      "Epoch 12838 - lr: 0.01000 - Train loss: 0.63453 - Test loss: 0.57576\n",
      "Epoch 12839 - lr: 0.01000 - Train loss: 0.64927 - Test loss: 0.57530\n",
      "Epoch 12840 - lr: 0.01000 - Train loss: 0.63451 - Test loss: 0.57498\n",
      "Epoch 12841 - lr: 0.01000 - Train loss: 0.63163 - Test loss: 0.57489\n",
      "Epoch 12842 - lr: 0.01000 - Train loss: 0.63370 - Test loss: 0.57501\n",
      "Epoch 12843 - lr: 0.01000 - Train loss: 0.63370 - Test loss: 0.57478\n",
      "Epoch 12844 - lr: 0.01000 - Train loss: 0.63309 - Test loss: 0.57485\n",
      "Epoch 12845 - lr: 0.01000 - Train loss: 0.63078 - Test loss: 0.57466\n",
      "Epoch 12846 - lr: 0.01000 - Train loss: 0.62964 - Test loss: 0.57511\n",
      "Epoch 12847 - lr: 0.01000 - Train loss: 0.63213 - Test loss: 0.57488\n",
      "Epoch 12848 - lr: 0.01000 - Train loss: 0.63415 - Test loss: 0.57527\n",
      "Epoch 12849 - lr: 0.01000 - Train loss: 0.64719 - Test loss: 0.57489\n",
      "Epoch 12850 - lr: 0.01000 - Train loss: 0.63109 - Test loss: 0.57462\n",
      "Epoch 12851 - lr: 0.01000 - Train loss: 0.63025 - Test loss: 0.57507\n",
      "Epoch 12852 - lr: 0.01000 - Train loss: 0.63376 - Test loss: 0.57485\n",
      "Epoch 12853 - lr: 0.01000 - Train loss: 0.63304 - Test loss: 0.57490\n",
      "Epoch 12854 - lr: 0.01000 - Train loss: 0.63069 - Test loss: 0.57472\n",
      "Epoch 12855 - lr: 0.01000 - Train loss: 0.62945 - Test loss: 0.57516\n",
      "Epoch 12856 - lr: 0.01000 - Train loss: 0.63173 - Test loss: 0.57493\n",
      "Epoch 12857 - lr: 0.01000 - Train loss: 0.63337 - Test loss: 0.57534\n",
      "Epoch 12858 - lr: 0.01000 - Train loss: 0.64015 - Test loss: 0.57501\n",
      "Epoch 12859 - lr: 0.01000 - Train loss: 0.63445 - Test loss: 0.57532\n",
      "Epoch 12860 - lr: 0.01000 - Train loss: 0.64683 - Test loss: 0.57493\n",
      "Epoch 12861 - lr: 0.01000 - Train loss: 0.63059 - Test loss: 0.57466\n",
      "Epoch 12862 - lr: 0.01000 - Train loss: 0.62661 - Test loss: 0.57512\n",
      "Epoch 12863 - lr: 0.01000 - Train loss: 0.63333 - Test loss: 0.57518\n",
      "Epoch 12864 - lr: 0.01000 - Train loss: 0.63092 - Test loss: 0.57494\n",
      "Epoch 12865 - lr: 0.01000 - Train loss: 0.62933 - Test loss: 0.57535\n",
      "Epoch 12866 - lr: 0.01000 - Train loss: 0.63132 - Test loss: 0.57509\n",
      "Epoch 12867 - lr: 0.01000 - Train loss: 0.63149 - Test loss: 0.57549\n",
      "Epoch 12868 - lr: 0.01000 - Train loss: 0.63930 - Test loss: 0.57543\n",
      "Epoch 12869 - lr: 0.01000 - Train loss: 0.64515 - Test loss: 0.57498\n",
      "Epoch 12870 - lr: 0.01000 - Train loss: 0.63044 - Test loss: 0.57478\n",
      "Epoch 12871 - lr: 0.01000 - Train loss: 0.63078 - Test loss: 0.57521\n",
      "Epoch 12872 - lr: 0.01000 - Train loss: 0.63577 - Test loss: 0.57499\n",
      "Epoch 12873 - lr: 0.01000 - Train loss: 0.63034 - Test loss: 0.57477\n",
      "Epoch 12874 - lr: 0.01000 - Train loss: 0.62690 - Test loss: 0.57522\n",
      "Epoch 12875 - lr: 0.01000 - Train loss: 0.63263 - Test loss: 0.57520\n",
      "Epoch 12876 - lr: 0.01000 - Train loss: 0.63075 - Test loss: 0.57504\n",
      "Epoch 12877 - lr: 0.01000 - Train loss: 0.63342 - Test loss: 0.57546\n",
      "Epoch 12878 - lr: 0.01000 - Train loss: 0.64008 - Test loss: 0.57511\n",
      "Epoch 12879 - lr: 0.01000 - Train loss: 0.63448 - Test loss: 0.57542\n",
      "Epoch 12880 - lr: 0.01000 - Train loss: 0.64809 - Test loss: 0.57502\n",
      "Epoch 12881 - lr: 0.01000 - Train loss: 0.63150 - Test loss: 0.57471\n",
      "Epoch 12882 - lr: 0.01000 - Train loss: 0.63096 - Test loss: 0.57515\n",
      "Epoch 12883 - lr: 0.01000 - Train loss: 0.63609 - Test loss: 0.57494\n",
      "Epoch 12884 - lr: 0.01000 - Train loss: 0.63030 - Test loss: 0.57471\n",
      "Epoch 12885 - lr: 0.01000 - Train loss: 0.62463 - Test loss: 0.57519\n",
      "Epoch 12886 - lr: 0.01000 - Train loss: 0.63310 - Test loss: 0.57560\n",
      "Epoch 12887 - lr: 0.01000 - Train loss: 0.63163 - Test loss: 0.57548\n",
      "Epoch 12888 - lr: 0.01000 - Train loss: 0.63172 - Test loss: 0.57566\n",
      "Epoch 12889 - lr: 0.01000 - Train loss: 0.63149 - Test loss: 0.57550\n",
      "Epoch 12890 - lr: 0.01000 - Train loss: 0.63461 - Test loss: 0.57567\n",
      "Epoch 12891 - lr: 0.01000 - Train loss: 0.63830 - Test loss: 0.57555\n",
      "Epoch 12892 - lr: 0.01000 - Train loss: 0.63950 - Test loss: 0.57539\n",
      "Epoch 12893 - lr: 0.01000 - Train loss: 0.63415 - Test loss: 0.57515\n",
      "Epoch 12894 - lr: 0.01000 - Train loss: 0.62852 - Test loss: 0.57538\n",
      "Epoch 12895 - lr: 0.01000 - Train loss: 0.63484 - Test loss: 0.57573\n",
      "Epoch 12896 - lr: 0.01000 - Train loss: 0.65105 - Test loss: 0.57529\n",
      "Epoch 12897 - lr: 0.01000 - Train loss: 0.63686 - Test loss: 0.57498\n",
      "Epoch 12898 - lr: 0.01000 - Train loss: 0.63038 - Test loss: 0.57468\n",
      "Epoch 12899 - lr: 0.01000 - Train loss: 0.62216 - Test loss: 0.57520\n",
      "Epoch 12900 - lr: 0.01000 - Train loss: 0.63154 - Test loss: 0.57548\n",
      "Epoch 12901 - lr: 0.01000 - Train loss: 0.63159 - Test loss: 0.57536\n",
      "Epoch 12902 - lr: 0.01000 - Train loss: 0.63456 - Test loss: 0.57552\n",
      "Epoch 12903 - lr: 0.01000 - Train loss: 0.63933 - Test loss: 0.57538\n",
      "Epoch 12904 - lr: 0.01000 - Train loss: 0.63414 - Test loss: 0.57522\n",
      "Epoch 12905 - lr: 0.01000 - Train loss: 0.63522 - Test loss: 0.57543\n",
      "Epoch 12906 - lr: 0.01000 - Train loss: 0.63215 - Test loss: 0.57530\n",
      "Epoch 12907 - lr: 0.01000 - Train loss: 0.63088 - Test loss: 0.57549\n",
      "Epoch 12908 - lr: 0.01000 - Train loss: 0.63294 - Test loss: 0.57545\n",
      "Epoch 12909 - lr: 0.01000 - Train loss: 0.63035 - Test loss: 0.57522\n",
      "Epoch 12910 - lr: 0.01000 - Train loss: 0.62875 - Test loss: 0.57562\n",
      "Epoch 12911 - lr: 0.01000 - Train loss: 0.63021 - Test loss: 0.57533\n",
      "Epoch 12912 - lr: 0.01000 - Train loss: 0.62332 - Test loss: 0.57577\n",
      "Epoch 12913 - lr: 0.01000 - Train loss: 0.62314 - Test loss: 0.57616\n",
      "Epoch 12914 - lr: 0.01000 - Train loss: 0.62183 - Test loss: 0.57655\n",
      "Epoch 12915 - lr: 0.01000 - Train loss: 0.63416 - Test loss: 0.57667\n",
      "Epoch 12916 - lr: 0.01000 - Train loss: 0.63094 - Test loss: 0.57623\n",
      "Epoch 12917 - lr: 0.01000 - Train loss: 0.62718 - Test loss: 0.57650\n",
      "Epoch 12918 - lr: 0.01000 - Train loss: 0.63230 - Test loss: 0.57627\n",
      "Epoch 12919 - lr: 0.01000 - Train loss: 0.63252 - Test loss: 0.57610\n",
      "Epoch 12920 - lr: 0.01000 - Train loss: 0.63120 - Test loss: 0.57586\n",
      "Epoch 12921 - lr: 0.01000 - Train loss: 0.63463 - Test loss: 0.57608\n",
      "Epoch 12922 - lr: 0.01000 - Train loss: 0.63765 - Test loss: 0.57570\n",
      "Epoch 12923 - lr: 0.01000 - Train loss: 0.62744 - Test loss: 0.57601\n",
      "Epoch 12924 - lr: 0.01000 - Train loss: 0.63161 - Test loss: 0.57582\n",
      "Epoch 12925 - lr: 0.01000 - Train loss: 0.63415 - Test loss: 0.57591\n",
      "Epoch 12926 - lr: 0.01000 - Train loss: 0.63844 - Test loss: 0.57573\n",
      "Epoch 12927 - lr: 0.01000 - Train loss: 0.63752 - Test loss: 0.57558\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12928 - lr: 0.01000 - Train loss: 0.63485 - Test loss: 0.57524\n",
      "Epoch 12929 - lr: 0.01000 - Train loss: 0.63097 - Test loss: 0.57510\n",
      "Epoch 12930 - lr: 0.01000 - Train loss: 0.63453 - Test loss: 0.57545\n",
      "Epoch 12931 - lr: 0.01000 - Train loss: 0.64042 - Test loss: 0.57511\n",
      "Epoch 12932 - lr: 0.01000 - Train loss: 0.63472 - Test loss: 0.57540\n",
      "Epoch 12933 - lr: 0.01000 - Train loss: 0.63819 - Test loss: 0.57511\n",
      "Epoch 12934 - lr: 0.01000 - Train loss: 0.63030 - Test loss: 0.57548\n",
      "Epoch 12935 - lr: 0.01000 - Train loss: 0.63231 - Test loss: 0.57517\n",
      "Epoch 12936 - lr: 0.01000 - Train loss: 0.63381 - Test loss: 0.57557\n",
      "Epoch 12937 - lr: 0.01000 - Train loss: 0.63930 - Test loss: 0.57523\n",
      "Epoch 12938 - lr: 0.01000 - Train loss: 0.63352 - Test loss: 0.57559\n",
      "Epoch 12939 - lr: 0.01000 - Train loss: 0.63232 - Test loss: 0.57545\n",
      "Epoch 12940 - lr: 0.01000 - Train loss: 0.63285 - Test loss: 0.57565\n",
      "Epoch 12941 - lr: 0.01000 - Train loss: 0.63014 - Test loss: 0.57540\n",
      "Epoch 12942 - lr: 0.01000 - Train loss: 0.62524 - Test loss: 0.57580\n",
      "Epoch 12943 - lr: 0.01000 - Train loss: 0.63506 - Test loss: 0.57610\n",
      "Epoch 12944 - lr: 0.01000 - Train loss: 0.64977 - Test loss: 0.57562\n",
      "Epoch 12945 - lr: 0.01000 - Train loss: 0.63283 - Test loss: 0.57520\n",
      "Epoch 12946 - lr: 0.01000 - Train loss: 0.63476 - Test loss: 0.57556\n",
      "Epoch 12947 - lr: 0.01000 - Train loss: 0.64562 - Test loss: 0.57515\n",
      "Epoch 12948 - lr: 0.01000 - Train loss: 0.62998 - Test loss: 0.57491\n",
      "Epoch 12949 - lr: 0.01000 - Train loss: 0.62431 - Test loss: 0.57538\n",
      "Epoch 12950 - lr: 0.01000 - Train loss: 0.63158 - Test loss: 0.57579\n",
      "Epoch 12951 - lr: 0.01000 - Train loss: 0.63719 - Test loss: 0.57551\n",
      "Epoch 12952 - lr: 0.01000 - Train loss: 0.63054 - Test loss: 0.57515\n",
      "Epoch 12953 - lr: 0.01000 - Train loss: 0.62210 - Test loss: 0.57564\n",
      "Epoch 12954 - lr: 0.01000 - Train loss: 0.63112 - Test loss: 0.57588\n",
      "Epoch 12955 - lr: 0.01000 - Train loss: 0.63218 - Test loss: 0.57577\n",
      "Epoch 12956 - lr: 0.01000 - Train loss: 0.63225 - Test loss: 0.57567\n",
      "Epoch 12957 - lr: 0.01000 - Train loss: 0.63161 - Test loss: 0.57555\n",
      "Epoch 12958 - lr: 0.01000 - Train loss: 0.63382 - Test loss: 0.57563\n",
      "Epoch 12959 - lr: 0.01000 - Train loss: 0.63243 - Test loss: 0.57531\n",
      "Epoch 12960 - lr: 0.01000 - Train loss: 0.63431 - Test loss: 0.57569\n",
      "Epoch 12961 - lr: 0.01000 - Train loss: 0.64473 - Test loss: 0.57528\n",
      "Epoch 12962 - lr: 0.01000 - Train loss: 0.63042 - Test loss: 0.57509\n",
      "Epoch 12963 - lr: 0.01000 - Train loss: 0.63223 - Test loss: 0.57554\n",
      "Epoch 12964 - lr: 0.01000 - Train loss: 0.63984 - Test loss: 0.57551\n",
      "Epoch 12965 - lr: 0.01000 - Train loss: 0.63707 - Test loss: 0.57520\n",
      "Epoch 12966 - lr: 0.01000 - Train loss: 0.62499 - Test loss: 0.57559\n",
      "Epoch 12967 - lr: 0.01000 - Train loss: 0.63450 - Test loss: 0.57597\n",
      "Epoch 12968 - lr: 0.01000 - Train loss: 0.64557 - Test loss: 0.57551\n",
      "Epoch 12969 - lr: 0.01000 - Train loss: 0.63007 - Test loss: 0.57524\n",
      "Epoch 12970 - lr: 0.01000 - Train loss: 0.62565 - Test loss: 0.57566\n",
      "Epoch 12971 - lr: 0.01000 - Train loss: 0.63520 - Test loss: 0.57590\n",
      "Epoch 12972 - lr: 0.01000 - Train loss: 0.63393 - Test loss: 0.57573\n",
      "Epoch 12973 - lr: 0.01000 - Train loss: 0.63515 - Test loss: 0.57600\n",
      "Epoch 12974 - lr: 0.01000 - Train loss: 0.64891 - Test loss: 0.57553\n",
      "Epoch 12975 - lr: 0.01000 - Train loss: 0.63070 - Test loss: 0.57511\n",
      "Epoch 12976 - lr: 0.01000 - Train loss: 0.62225 - Test loss: 0.57559\n",
      "Epoch 12977 - lr: 0.01000 - Train loss: 0.63282 - Test loss: 0.57585\n",
      "Epoch 12978 - lr: 0.01000 - Train loss: 0.63001 - Test loss: 0.57558\n",
      "Epoch 12979 - lr: 0.01000 - Train loss: 0.62352 - Test loss: 0.57601\n",
      "Epoch 12980 - lr: 0.01000 - Train loss: 0.62535 - Test loss: 0.57636\n",
      "Epoch 12981 - lr: 0.01000 - Train loss: 0.63537 - Test loss: 0.57661\n",
      "Epoch 12982 - lr: 0.01000 - Train loss: 0.64997 - Test loss: 0.57606\n",
      "Epoch 12983 - lr: 0.01000 - Train loss: 0.63209 - Test loss: 0.57558\n",
      "Epoch 12984 - lr: 0.01000 - Train loss: 0.63115 - Test loss: 0.57595\n",
      "Epoch 12985 - lr: 0.01000 - Train loss: 0.63443 - Test loss: 0.57559\n",
      "Epoch 12986 - lr: 0.01000 - Train loss: 0.63341 - Test loss: 0.57558\n",
      "Epoch 12987 - lr: 0.01000 - Train loss: 0.62993 - Test loss: 0.57528\n",
      "Epoch 12988 - lr: 0.01000 - Train loss: 0.62247 - Test loss: 0.57575\n",
      "Epoch 12989 - lr: 0.01000 - Train loss: 0.63285 - Test loss: 0.57601\n",
      "Epoch 12990 - lr: 0.01000 - Train loss: 0.62999 - Test loss: 0.57574\n",
      "Epoch 12991 - lr: 0.01000 - Train loss: 0.62398 - Test loss: 0.57616\n",
      "Epoch 12992 - lr: 0.01000 - Train loss: 0.63011 - Test loss: 0.57650\n",
      "Epoch 12993 - lr: 0.01000 - Train loss: 0.63172 - Test loss: 0.57606\n",
      "Epoch 12994 - lr: 0.01000 - Train loss: 0.63028 - Test loss: 0.57639\n",
      "Epoch 12995 - lr: 0.01000 - Train loss: 0.63255 - Test loss: 0.57598\n",
      "Epoch 12996 - lr: 0.01000 - Train loss: 0.63437 - Test loss: 0.57630\n",
      "Epoch 12997 - lr: 0.01000 - Train loss: 0.64672 - Test loss: 0.57581\n",
      "Epoch 12998 - lr: 0.01000 - Train loss: 0.62990 - Test loss: 0.57546\n",
      "Epoch 12999 - lr: 0.01000 - Train loss: 0.62191 - Test loss: 0.57595\n",
      "Epoch 13000 - lr: 0.01000 - Train loss: 0.63109 - Test loss: 0.57619\n",
      "Epoch 13001 - lr: 0.01000 - Train loss: 0.63191 - Test loss: 0.57606\n",
      "Epoch 13002 - lr: 0.01000 - Train loss: 0.63264 - Test loss: 0.57600\n",
      "Epoch 13003 - lr: 0.01000 - Train loss: 0.63017 - Test loss: 0.57576\n",
      "Epoch 13004 - lr: 0.01000 - Train loss: 0.62944 - Test loss: 0.57615\n",
      "Epoch 13005 - lr: 0.01000 - Train loss: 0.63095 - Test loss: 0.57581\n",
      "Epoch 13006 - lr: 0.01000 - Train loss: 0.62783 - Test loss: 0.57618\n",
      "Epoch 13007 - lr: 0.01000 - Train loss: 0.63049 - Test loss: 0.57595\n",
      "Epoch 13008 - lr: 0.01000 - Train loss: 0.63221 - Test loss: 0.57632\n",
      "Epoch 13009 - lr: 0.01000 - Train loss: 0.63745 - Test loss: 0.57617\n",
      "Epoch 13010 - lr: 0.01000 - Train loss: 0.63382 - Test loss: 0.57578\n",
      "Epoch 13011 - lr: 0.01000 - Train loss: 0.63317 - Test loss: 0.57578\n",
      "Epoch 13012 - lr: 0.01000 - Train loss: 0.63045 - Test loss: 0.57549\n",
      "Epoch 13013 - lr: 0.01000 - Train loss: 0.62580 - Test loss: 0.57592\n",
      "Epoch 13014 - lr: 0.01000 - Train loss: 0.63414 - Test loss: 0.57605\n",
      "Epoch 13015 - lr: 0.01000 - Train loss: 0.63845 - Test loss: 0.57588\n",
      "Epoch 13016 - lr: 0.01000 - Train loss: 0.63849 - Test loss: 0.57576\n",
      "Epoch 13017 - lr: 0.01000 - Train loss: 0.63958 - Test loss: 0.57567\n",
      "Epoch 13018 - lr: 0.01000 - Train loss: 0.63791 - Test loss: 0.57533\n",
      "Epoch 13019 - lr: 0.01000 - Train loss: 0.62936 - Test loss: 0.57569\n",
      "Epoch 13020 - lr: 0.01000 - Train loss: 0.63022 - Test loss: 0.57539\n",
      "Epoch 13021 - lr: 0.01000 - Train loss: 0.62194 - Test loss: 0.57588\n",
      "Epoch 13022 - lr: 0.01000 - Train loss: 0.62954 - Test loss: 0.57612\n",
      "Epoch 13023 - lr: 0.01000 - Train loss: 0.63440 - Test loss: 0.57623\n",
      "Epoch 13024 - lr: 0.01000 - Train loss: 0.63891 - Test loss: 0.57602\n",
      "Epoch 13025 - lr: 0.01000 - Train loss: 0.63811 - Test loss: 0.57585\n",
      "Epoch 13026 - lr: 0.01000 - Train loss: 0.63615 - Test loss: 0.57551\n",
      "Epoch 13027 - lr: 0.01000 - Train loss: 0.62977 - Test loss: 0.57522\n",
      "Epoch 13028 - lr: 0.01000 - Train loss: 0.62187 - Test loss: 0.57573\n",
      "Epoch 13029 - lr: 0.01000 - Train loss: 0.63077 - Test loss: 0.57598\n",
      "Epoch 13030 - lr: 0.01000 - Train loss: 0.63223 - Test loss: 0.57590\n",
      "Epoch 13031 - lr: 0.01000 - Train loss: 0.63108 - Test loss: 0.57574\n",
      "Epoch 13032 - lr: 0.01000 - Train loss: 0.63457 - Test loss: 0.57596\n",
      "Epoch 13033 - lr: 0.01000 - Train loss: 0.63416 - Test loss: 0.57580\n",
      "Epoch 13034 - lr: 0.01000 - Train loss: 0.63427 - Test loss: 0.57587\n",
      "Epoch 13035 - lr: 0.01000 - Train loss: 0.63713 - Test loss: 0.57562\n",
      "Epoch 13036 - lr: 0.01000 - Train loss: 0.63182 - Test loss: 0.57527\n",
      "Epoch 13037 - lr: 0.01000 - Train loss: 0.63251 - Test loss: 0.57569\n",
      "Epoch 13038 - lr: 0.01000 - Train loss: 0.63701 - Test loss: 0.57559\n",
      "Epoch 13039 - lr: 0.01000 - Train loss: 0.63061 - Test loss: 0.57523\n",
      "Epoch 13040 - lr: 0.01000 - Train loss: 0.62337 - Test loss: 0.57569\n",
      "Epoch 13041 - lr: 0.01000 - Train loss: 0.62554 - Test loss: 0.57609\n",
      "Epoch 13042 - lr: 0.01000 - Train loss: 0.63494 - Test loss: 0.57628\n",
      "Epoch 13043 - lr: 0.01000 - Train loss: 0.63182 - Test loss: 0.57608\n",
      "Epoch 13044 - lr: 0.01000 - Train loss: 0.62328 - Test loss: 0.57637\n",
      "Epoch 13045 - lr: 0.01000 - Train loss: 0.63013 - Test loss: 0.57654\n",
      "Epoch 13046 - lr: 0.01000 - Train loss: 0.63371 - Test loss: 0.57648\n",
      "Epoch 13047 - lr: 0.01000 - Train loss: 0.63153 - Test loss: 0.57604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13048 - lr: 0.01000 - Train loss: 0.63111 - Test loss: 0.57637\n",
      "Epoch 13049 - lr: 0.01000 - Train loss: 0.63805 - Test loss: 0.57609\n",
      "Epoch 13050 - lr: 0.01000 - Train loss: 0.63814 - Test loss: 0.57582\n",
      "Epoch 13051 - lr: 0.01000 - Train loss: 0.63844 - Test loss: 0.57560\n",
      "Epoch 13052 - lr: 0.01000 - Train loss: 0.63962 - Test loss: 0.57548\n",
      "Epoch 13053 - lr: 0.01000 - Train loss: 0.63583 - Test loss: 0.57520\n",
      "Epoch 13054 - lr: 0.01000 - Train loss: 0.62198 - Test loss: 0.57565\n",
      "Epoch 13055 - lr: 0.01000 - Train loss: 0.63188 - Test loss: 0.57589\n",
      "Epoch 13056 - lr: 0.01000 - Train loss: 0.63044 - Test loss: 0.57569\n",
      "Epoch 13057 - lr: 0.01000 - Train loss: 0.63269 - Test loss: 0.57608\n",
      "Epoch 13058 - lr: 0.01000 - Train loss: 0.63597 - Test loss: 0.57590\n",
      "Epoch 13059 - lr: 0.01000 - Train loss: 0.63001 - Test loss: 0.57558\n",
      "Epoch 13060 - lr: 0.01000 - Train loss: 0.62567 - Test loss: 0.57597\n",
      "Epoch 13061 - lr: 0.01000 - Train loss: 0.63477 - Test loss: 0.57612\n",
      "Epoch 13062 - lr: 0.01000 - Train loss: 0.63965 - Test loss: 0.57600\n",
      "Epoch 13063 - lr: 0.01000 - Train loss: 0.63256 - Test loss: 0.57576\n",
      "Epoch 13064 - lr: 0.01000 - Train loss: 0.63261 - Test loss: 0.57592\n",
      "Epoch 13065 - lr: 0.01000 - Train loss: 0.62984 - Test loss: 0.57565\n",
      "Epoch 13066 - lr: 0.01000 - Train loss: 0.62372 - Test loss: 0.57606\n",
      "Epoch 13067 - lr: 0.01000 - Train loss: 0.62889 - Test loss: 0.57640\n",
      "Epoch 13068 - lr: 0.01000 - Train loss: 0.62999 - Test loss: 0.57600\n",
      "Epoch 13069 - lr: 0.01000 - Train loss: 0.62158 - Test loss: 0.57642\n",
      "Epoch 13070 - lr: 0.01000 - Train loss: 0.63377 - Test loss: 0.57659\n",
      "Epoch 13071 - lr: 0.01000 - Train loss: 0.63018 - Test loss: 0.57618\n",
      "Epoch 13072 - lr: 0.01000 - Train loss: 0.62241 - Test loss: 0.57656\n",
      "Epoch 13073 - lr: 0.01000 - Train loss: 0.62200 - Test loss: 0.57692\n",
      "Epoch 13074 - lr: 0.01000 - Train loss: 0.63553 - Test loss: 0.57707\n",
      "Epoch 13075 - lr: 0.01000 - Train loss: 0.63385 - Test loss: 0.57660\n",
      "Epoch 13076 - lr: 0.01000 - Train loss: 0.63300 - Test loss: 0.57647\n",
      "Epoch 13077 - lr: 0.01000 - Train loss: 0.63021 - Test loss: 0.57609\n",
      "Epoch 13078 - lr: 0.01000 - Train loss: 0.62572 - Test loss: 0.57644\n",
      "Epoch 13079 - lr: 0.01000 - Train loss: 0.63403 - Test loss: 0.57649\n",
      "Epoch 13080 - lr: 0.01000 - Train loss: 0.63817 - Test loss: 0.57625\n",
      "Epoch 13081 - lr: 0.01000 - Train loss: 0.63879 - Test loss: 0.57610\n",
      "Epoch 13082 - lr: 0.01000 - Train loss: 0.63262 - Test loss: 0.57590\n",
      "Epoch 13083 - lr: 0.01000 - Train loss: 0.63052 - Test loss: 0.57620\n",
      "Epoch 13084 - lr: 0.01000 - Train loss: 0.63441 - Test loss: 0.57584\n",
      "Epoch 13085 - lr: 0.01000 - Train loss: 0.63148 - Test loss: 0.57570\n",
      "Epoch 13086 - lr: 0.01000 - Train loss: 0.63294 - Test loss: 0.57572\n",
      "Epoch 13087 - lr: 0.01000 - Train loss: 0.63004 - Test loss: 0.57544\n",
      "Epoch 13088 - lr: 0.01000 - Train loss: 0.62383 - Test loss: 0.57589\n",
      "Epoch 13089 - lr: 0.01000 - Train loss: 0.63084 - Test loss: 0.57627\n",
      "Epoch 13090 - lr: 0.01000 - Train loss: 0.63740 - Test loss: 0.57602\n",
      "Epoch 13091 - lr: 0.01000 - Train loss: 0.63584 - Test loss: 0.57570\n",
      "Epoch 13092 - lr: 0.01000 - Train loss: 0.62995 - Test loss: 0.57540\n",
      "Epoch 13093 - lr: 0.01000 - Train loss: 0.62376 - Test loss: 0.57585\n",
      "Epoch 13094 - lr: 0.01000 - Train loss: 0.63048 - Test loss: 0.57623\n",
      "Epoch 13095 - lr: 0.01000 - Train loss: 0.63576 - Test loss: 0.57592\n",
      "Epoch 13096 - lr: 0.01000 - Train loss: 0.63005 - Test loss: 0.57560\n",
      "Epoch 13097 - lr: 0.01000 - Train loss: 0.62514 - Test loss: 0.57601\n",
      "Epoch 13098 - lr: 0.01000 - Train loss: 0.63446 - Test loss: 0.57624\n",
      "Epoch 13099 - lr: 0.01000 - Train loss: 0.63382 - Test loss: 0.57598\n",
      "Epoch 13100 - lr: 0.01000 - Train loss: 0.62763 - Test loss: 0.57618\n",
      "Epoch 13101 - lr: 0.01000 - Train loss: 0.63356 - Test loss: 0.57653\n",
      "Epoch 13102 - lr: 0.01000 - Train loss: 0.64766 - Test loss: 0.57601\n",
      "Epoch 13103 - lr: 0.01000 - Train loss: 0.63078 - Test loss: 0.57561\n",
      "Epoch 13104 - lr: 0.01000 - Train loss: 0.62791 - Test loss: 0.57600\n",
      "Epoch 13105 - lr: 0.01000 - Train loss: 0.62997 - Test loss: 0.57575\n",
      "Epoch 13106 - lr: 0.01000 - Train loss: 0.62796 - Test loss: 0.57613\n",
      "Epoch 13107 - lr: 0.01000 - Train loss: 0.63003 - Test loss: 0.57586\n",
      "Epoch 13108 - lr: 0.01000 - Train loss: 0.62823 - Test loss: 0.57623\n",
      "Epoch 13109 - lr: 0.01000 - Train loss: 0.63012 - Test loss: 0.57593\n",
      "Epoch 13110 - lr: 0.01000 - Train loss: 0.62752 - Test loss: 0.57629\n",
      "Epoch 13111 - lr: 0.01000 - Train loss: 0.63040 - Test loss: 0.57604\n",
      "Epoch 13112 - lr: 0.01000 - Train loss: 0.63282 - Test loss: 0.57640\n",
      "Epoch 13113 - lr: 0.01000 - Train loss: 0.63835 - Test loss: 0.57600\n",
      "Epoch 13114 - lr: 0.01000 - Train loss: 0.63155 - Test loss: 0.57631\n",
      "Epoch 13115 - lr: 0.01000 - Train loss: 0.63873 - Test loss: 0.57620\n",
      "Epoch 13116 - lr: 0.01000 - Train loss: 0.63129 - Test loss: 0.57600\n",
      "Epoch 13117 - lr: 0.01000 - Train loss: 0.63284 - Test loss: 0.57617\n",
      "Epoch 13118 - lr: 0.01000 - Train loss: 0.62988 - Test loss: 0.57587\n",
      "Epoch 13119 - lr: 0.01000 - Train loss: 0.62515 - Test loss: 0.57626\n",
      "Epoch 13120 - lr: 0.01000 - Train loss: 0.63451 - Test loss: 0.57645\n",
      "Epoch 13121 - lr: 0.01000 - Train loss: 0.63240 - Test loss: 0.57621\n",
      "Epoch 13122 - lr: 0.01000 - Train loss: 0.63296 - Test loss: 0.57637\n",
      "Epoch 13123 - lr: 0.01000 - Train loss: 0.62997 - Test loss: 0.57603\n",
      "Epoch 13124 - lr: 0.01000 - Train loss: 0.62553 - Test loss: 0.57640\n",
      "Epoch 13125 - lr: 0.01000 - Train loss: 0.63409 - Test loss: 0.57647\n",
      "Epoch 13126 - lr: 0.01000 - Train loss: 0.63918 - Test loss: 0.57632\n",
      "Epoch 13127 - lr: 0.01000 - Train loss: 0.64699 - Test loss: 0.57580\n",
      "Epoch 13128 - lr: 0.01000 - Train loss: 0.63009 - Test loss: 0.57544\n",
      "Epoch 13129 - lr: 0.01000 - Train loss: 0.62356 - Test loss: 0.57589\n",
      "Epoch 13130 - lr: 0.01000 - Train loss: 0.62937 - Test loss: 0.57627\n",
      "Epoch 13131 - lr: 0.01000 - Train loss: 0.63113 - Test loss: 0.57590\n",
      "Epoch 13132 - lr: 0.01000 - Train loss: 0.63040 - Test loss: 0.57627\n",
      "Epoch 13133 - lr: 0.01000 - Train loss: 0.63575 - Test loss: 0.57596\n",
      "Epoch 13134 - lr: 0.01000 - Train loss: 0.62994 - Test loss: 0.57563\n",
      "Epoch 13135 - lr: 0.01000 - Train loss: 0.62438 - Test loss: 0.57605\n",
      "Epoch 13136 - lr: 0.01000 - Train loss: 0.63372 - Test loss: 0.57641\n",
      "Epoch 13137 - lr: 0.01000 - Train loss: 0.64995 - Test loss: 0.57593\n",
      "Epoch 13138 - lr: 0.01000 - Train loss: 0.63587 - Test loss: 0.57561\n",
      "Epoch 13139 - lr: 0.01000 - Train loss: 0.62979 - Test loss: 0.57531\n",
      "Epoch 13140 - lr: 0.01000 - Train loss: 0.62259 - Test loss: 0.57581\n",
      "Epoch 13141 - lr: 0.01000 - Train loss: 0.62161 - Test loss: 0.57630\n",
      "Epoch 13142 - lr: 0.01000 - Train loss: 0.62906 - Test loss: 0.57651\n",
      "Epoch 13143 - lr: 0.01000 - Train loss: 0.63429 - Test loss: 0.57662\n",
      "Epoch 13144 - lr: 0.01000 - Train loss: 0.63702 - Test loss: 0.57642\n",
      "Epoch 13145 - lr: 0.01000 - Train loss: 0.63355 - Test loss: 0.57599\n",
      "Epoch 13146 - lr: 0.01000 - Train loss: 0.63285 - Test loss: 0.57596\n",
      "Epoch 13147 - lr: 0.01000 - Train loss: 0.63008 - Test loss: 0.57565\n",
      "Epoch 13148 - lr: 0.01000 - Train loss: 0.62533 - Test loss: 0.57606\n",
      "Epoch 13149 - lr: 0.01000 - Train loss: 0.63411 - Test loss: 0.57621\n",
      "Epoch 13150 - lr: 0.01000 - Train loss: 0.63821 - Test loss: 0.57610\n",
      "Epoch 13151 - lr: 0.01000 - Train loss: 0.63746 - Test loss: 0.57594\n",
      "Epoch 13152 - lr: 0.01000 - Train loss: 0.63582 - Test loss: 0.57561\n",
      "Epoch 13153 - lr: 0.01000 - Train loss: 0.62968 - Test loss: 0.57531\n",
      "Epoch 13154 - lr: 0.01000 - Train loss: 0.62222 - Test loss: 0.57581\n",
      "Epoch 13155 - lr: 0.01000 - Train loss: 0.62175 - Test loss: 0.57628\n",
      "Epoch 13156 - lr: 0.01000 - Train loss: 0.63474 - Test loss: 0.57651\n",
      "Epoch 13157 - lr: 0.01000 - Train loss: 0.63254 - Test loss: 0.57612\n",
      "Epoch 13158 - lr: 0.01000 - Train loss: 0.63426 - Test loss: 0.57630\n",
      "Epoch 13159 - lr: 0.01000 - Train loss: 0.63116 - Test loss: 0.57613\n",
      "Epoch 13160 - lr: 0.01000 - Train loss: 0.62363 - Test loss: 0.57642\n",
      "Epoch 13161 - lr: 0.01000 - Train loss: 0.62543 - Test loss: 0.57668\n",
      "Epoch 13162 - lr: 0.01000 - Train loss: 0.62339 - Test loss: 0.57701\n",
      "Epoch 13163 - lr: 0.01000 - Train loss: 0.62831 - Test loss: 0.57725\n",
      "Epoch 13164 - lr: 0.01000 - Train loss: 0.63046 - Test loss: 0.57679\n",
      "Epoch 13165 - lr: 0.01000 - Train loss: 0.62929 - Test loss: 0.57704\n",
      "Epoch 13166 - lr: 0.01000 - Train loss: 0.63260 - Test loss: 0.57658\n",
      "Epoch 13167 - lr: 0.01000 - Train loss: 0.63401 - Test loss: 0.57664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13168 - lr: 0.01000 - Train loss: 0.63621 - Test loss: 0.57643\n",
      "Epoch 13169 - lr: 0.01000 - Train loss: 0.63151 - Test loss: 0.57600\n",
      "Epoch 13170 - lr: 0.01000 - Train loss: 0.63327 - Test loss: 0.57634\n",
      "Epoch 13171 - lr: 0.01000 - Train loss: 0.64768 - Test loss: 0.57586\n",
      "Epoch 13172 - lr: 0.01000 - Train loss: 0.63139 - Test loss: 0.57549\n",
      "Epoch 13173 - lr: 0.01000 - Train loss: 0.63279 - Test loss: 0.57591\n",
      "Epoch 13174 - lr: 0.01000 - Train loss: 0.63512 - Test loss: 0.57566\n",
      "Epoch 13175 - lr: 0.01000 - Train loss: 0.62139 - Test loss: 0.57611\n",
      "Epoch 13176 - lr: 0.01000 - Train loss: 0.63491 - Test loss: 0.57637\n",
      "Epoch 13177 - lr: 0.01000 - Train loss: 0.63307 - Test loss: 0.57601\n",
      "Epoch 13178 - lr: 0.01000 - Train loss: 0.63334 - Test loss: 0.57606\n",
      "Epoch 13179 - lr: 0.01000 - Train loss: 0.63358 - Test loss: 0.57574\n",
      "Epoch 13180 - lr: 0.01000 - Train loss: 0.63180 - Test loss: 0.57568\n",
      "Epoch 13181 - lr: 0.01000 - Train loss: 0.63025 - Test loss: 0.57555\n",
      "Epoch 13182 - lr: 0.01000 - Train loss: 0.63373 - Test loss: 0.57594\n",
      "Epoch 13183 - lr: 0.01000 - Train loss: 0.64776 - Test loss: 0.57554\n",
      "Epoch 13184 - lr: 0.01000 - Train loss: 0.63141 - Test loss: 0.57522\n",
      "Epoch 13185 - lr: 0.01000 - Train loss: 0.63285 - Test loss: 0.57567\n",
      "Epoch 13186 - lr: 0.01000 - Train loss: 0.63454 - Test loss: 0.57549\n",
      "Epoch 13187 - lr: 0.01000 - Train loss: 0.62269 - Test loss: 0.57591\n",
      "Epoch 13188 - lr: 0.01000 - Train loss: 0.62986 - Test loss: 0.57618\n",
      "Epoch 13189 - lr: 0.01000 - Train loss: 0.63272 - Test loss: 0.57617\n",
      "Epoch 13190 - lr: 0.01000 - Train loss: 0.62992 - Test loss: 0.57586\n",
      "Epoch 13191 - lr: 0.01000 - Train loss: 0.62513 - Test loss: 0.57626\n",
      "Epoch 13192 - lr: 0.01000 - Train loss: 0.63415 - Test loss: 0.57643\n",
      "Epoch 13193 - lr: 0.01000 - Train loss: 0.63476 - Test loss: 0.57624\n",
      "Epoch 13194 - lr: 0.01000 - Train loss: 0.63060 - Test loss: 0.57599\n",
      "Epoch 13195 - lr: 0.01000 - Train loss: 0.63416 - Test loss: 0.57627\n",
      "Epoch 13196 - lr: 0.01000 - Train loss: 0.64373 - Test loss: 0.57582\n",
      "Epoch 13197 - lr: 0.01000 - Train loss: 0.63030 - Test loss: 0.57562\n",
      "Epoch 13198 - lr: 0.01000 - Train loss: 0.63389 - Test loss: 0.57600\n",
      "Epoch 13199 - lr: 0.01000 - Train loss: 0.64737 - Test loss: 0.57559\n",
      "Epoch 13200 - lr: 0.01000 - Train loss: 0.63031 - Test loss: 0.57526\n",
      "Epoch 13201 - lr: 0.01000 - Train loss: 0.62538 - Test loss: 0.57573\n",
      "Epoch 13202 - lr: 0.01000 - Train loss: 0.63391 - Test loss: 0.57592\n",
      "Epoch 13203 - lr: 0.01000 - Train loss: 0.63861 - Test loss: 0.57580\n",
      "Epoch 13204 - lr: 0.01000 - Train loss: 0.63343 - Test loss: 0.57566\n",
      "Epoch 13205 - lr: 0.01000 - Train loss: 0.63452 - Test loss: 0.57589\n",
      "Epoch 13206 - lr: 0.01000 - Train loss: 0.63148 - Test loss: 0.57577\n",
      "Epoch 13207 - lr: 0.01000 - Train loss: 0.63125 - Test loss: 0.57598\n",
      "Epoch 13208 - lr: 0.01000 - Train loss: 0.63037 - Test loss: 0.57581\n",
      "Epoch 13209 - lr: 0.01000 - Train loss: 0.63410 - Test loss: 0.57618\n",
      "Epoch 13210 - lr: 0.01000 - Train loss: 0.64926 - Test loss: 0.57574\n",
      "Epoch 13211 - lr: 0.01000 - Train loss: 0.63240 - Test loss: 0.57536\n",
      "Epoch 13212 - lr: 0.01000 - Train loss: 0.63438 - Test loss: 0.57573\n",
      "Epoch 13213 - lr: 0.01000 - Train loss: 0.64291 - Test loss: 0.57536\n",
      "Epoch 13214 - lr: 0.01000 - Train loss: 0.63132 - Test loss: 0.57529\n",
      "Epoch 13215 - lr: 0.01000 - Train loss: 0.63253 - Test loss: 0.57537\n",
      "Epoch 13216 - lr: 0.01000 - Train loss: 0.62921 - Test loss: 0.57517\n",
      "Epoch 13217 - lr: 0.01000 - Train loss: 0.62153 - Test loss: 0.57572\n",
      "Epoch 13218 - lr: 0.01000 - Train loss: 0.63228 - Test loss: 0.57602\n",
      "Epoch 13219 - lr: 0.01000 - Train loss: 0.62941 - Test loss: 0.57578\n",
      "Epoch 13220 - lr: 0.01000 - Train loss: 0.62304 - Test loss: 0.57624\n",
      "Epoch 13221 - lr: 0.01000 - Train loss: 0.62602 - Test loss: 0.57661\n",
      "Epoch 13222 - lr: 0.01000 - Train loss: 0.63310 - Test loss: 0.57655\n",
      "Epoch 13223 - lr: 0.01000 - Train loss: 0.63001 - Test loss: 0.57615\n",
      "Epoch 13224 - lr: 0.01000 - Train loss: 0.62283 - Test loss: 0.57656\n",
      "Epoch 13225 - lr: 0.01000 - Train loss: 0.62442 - Test loss: 0.57690\n",
      "Epoch 13226 - lr: 0.01000 - Train loss: 0.63410 - Test loss: 0.57717\n",
      "Epoch 13227 - lr: 0.01000 - Train loss: 0.65495 - Test loss: 0.57666\n",
      "Epoch 13228 - lr: 0.01000 - Train loss: 0.63142 - Test loss: 0.57639\n",
      "Epoch 13229 - lr: 0.01000 - Train loss: 0.62404 - Test loss: 0.57664\n",
      "Epoch 13230 - lr: 0.01000 - Train loss: 0.62259 - Test loss: 0.57698\n",
      "Epoch 13231 - lr: 0.01000 - Train loss: 0.63106 - Test loss: 0.57712\n",
      "Epoch 13232 - lr: 0.01000 - Train loss: 0.63103 - Test loss: 0.57684\n",
      "Epoch 13233 - lr: 0.01000 - Train loss: 0.63414 - Test loss: 0.57690\n",
      "Epoch 13234 - lr: 0.01000 - Train loss: 0.63941 - Test loss: 0.57674\n",
      "Epoch 13235 - lr: 0.01000 - Train loss: 0.64561 - Test loss: 0.57618\n",
      "Epoch 13236 - lr: 0.01000 - Train loss: 0.62934 - Test loss: 0.57584\n",
      "Epoch 13237 - lr: 0.01000 - Train loss: 0.62194 - Test loss: 0.57631\n",
      "Epoch 13238 - lr: 0.01000 - Train loss: 0.62316 - Test loss: 0.57668\n",
      "Epoch 13239 - lr: 0.01000 - Train loss: 0.62722 - Test loss: 0.57690\n",
      "Epoch 13240 - lr: 0.01000 - Train loss: 0.63313 - Test loss: 0.57722\n",
      "Epoch 13241 - lr: 0.01000 - Train loss: 0.63871 - Test loss: 0.57671\n",
      "Epoch 13242 - lr: 0.01000 - Train loss: 0.63240 - Test loss: 0.57698\n",
      "Epoch 13243 - lr: 0.01000 - Train loss: 0.63459 - Test loss: 0.57672\n",
      "Epoch 13244 - lr: 0.01000 - Train loss: 0.63226 - Test loss: 0.57653\n",
      "Epoch 13245 - lr: 0.01000 - Train loss: 0.62992 - Test loss: 0.57625\n",
      "Epoch 13246 - lr: 0.01000 - Train loss: 0.63124 - Test loss: 0.57662\n",
      "Epoch 13247 - lr: 0.01000 - Train loss: 0.63915 - Test loss: 0.57644\n",
      "Epoch 13248 - lr: 0.01000 - Train loss: 0.63143 - Test loss: 0.57623\n",
      "Epoch 13249 - lr: 0.01000 - Train loss: 0.62899 - Test loss: 0.57641\n",
      "Epoch 13250 - lr: 0.01000 - Train loss: 0.63422 - Test loss: 0.57653\n",
      "Epoch 13251 - lr: 0.01000 - Train loss: 0.63931 - Test loss: 0.57635\n",
      "Epoch 13252 - lr: 0.01000 - Train loss: 0.63159 - Test loss: 0.57614\n",
      "Epoch 13253 - lr: 0.01000 - Train loss: 0.62970 - Test loss: 0.57632\n",
      "Epoch 13254 - lr: 0.01000 - Train loss: 0.63316 - Test loss: 0.57632\n",
      "Epoch 13255 - lr: 0.01000 - Train loss: 0.62976 - Test loss: 0.57593\n",
      "Epoch 13256 - lr: 0.01000 - Train loss: 0.62139 - Test loss: 0.57640\n",
      "Epoch 13257 - lr: 0.01000 - Train loss: 0.63366 - Test loss: 0.57663\n",
      "Epoch 13258 - lr: 0.01000 - Train loss: 0.62976 - Test loss: 0.57624\n",
      "Epoch 13259 - lr: 0.01000 - Train loss: 0.62127 - Test loss: 0.57669\n",
      "Epoch 13260 - lr: 0.01000 - Train loss: 0.63091 - Test loss: 0.57688\n",
      "Epoch 13261 - lr: 0.01000 - Train loss: 0.63089 - Test loss: 0.57665\n",
      "Epoch 13262 - lr: 0.01000 - Train loss: 0.63400 - Test loss: 0.57675\n",
      "Epoch 13263 - lr: 0.01000 - Train loss: 0.63935 - Test loss: 0.57661\n",
      "Epoch 13264 - lr: 0.01000 - Train loss: 0.64493 - Test loss: 0.57609\n",
      "Epoch 13265 - lr: 0.01000 - Train loss: 0.62938 - Test loss: 0.57580\n",
      "Epoch 13266 - lr: 0.01000 - Train loss: 0.62495 - Test loss: 0.57624\n",
      "Epoch 13267 - lr: 0.01000 - Train loss: 0.63441 - Test loss: 0.57647\n",
      "Epoch 13268 - lr: 0.01000 - Train loss: 0.63291 - Test loss: 0.57630\n",
      "Epoch 13269 - lr: 0.01000 - Train loss: 0.63361 - Test loss: 0.57661\n",
      "Epoch 13270 - lr: 0.01000 - Train loss: 0.64149 - Test loss: 0.57615\n",
      "Epoch 13271 - lr: 0.01000 - Train loss: 0.63372 - Test loss: 0.57618\n",
      "Epoch 13272 - lr: 0.01000 - Train loss: 0.63291 - Test loss: 0.57582\n",
      "Epoch 13273 - lr: 0.01000 - Train loss: 0.63429 - Test loss: 0.57604\n",
      "Epoch 13274 - lr: 0.01000 - Train loss: 0.63903 - Test loss: 0.57599\n",
      "Epoch 13275 - lr: 0.01000 - Train loss: 0.63713 - Test loss: 0.57585\n",
      "Epoch 13276 - lr: 0.01000 - Train loss: 0.63006 - Test loss: 0.57545\n",
      "Epoch 13277 - lr: 0.01000 - Train loss: 0.62175 - Test loss: 0.57594\n",
      "Epoch 13278 - lr: 0.01000 - Train loss: 0.63239 - Test loss: 0.57622\n",
      "Epoch 13279 - lr: 0.01000 - Train loss: 0.62922 - Test loss: 0.57594\n",
      "Epoch 13280 - lr: 0.01000 - Train loss: 0.62138 - Test loss: 0.57642\n",
      "Epoch 13281 - lr: 0.01000 - Train loss: 0.63096 - Test loss: 0.57663\n",
      "Epoch 13282 - lr: 0.01000 - Train loss: 0.63062 - Test loss: 0.57643\n",
      "Epoch 13283 - lr: 0.01000 - Train loss: 0.63450 - Test loss: 0.57666\n",
      "Epoch 13284 - lr: 0.01000 - Train loss: 0.63286 - Test loss: 0.57641\n",
      "Epoch 13285 - lr: 0.01000 - Train loss: 0.63051 - Test loss: 0.57656\n",
      "Epoch 13286 - lr: 0.01000 - Train loss: 0.63134 - Test loss: 0.57642\n",
      "Epoch 13287 - lr: 0.01000 - Train loss: 0.63233 - Test loss: 0.57636\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13288 - lr: 0.01000 - Train loss: 0.62926 - Test loss: 0.57607\n",
      "Epoch 13289 - lr: 0.01000 - Train loss: 0.62268 - Test loss: 0.57652\n",
      "Epoch 13290 - lr: 0.01000 - Train loss: 0.62399 - Test loss: 0.57690\n",
      "Epoch 13291 - lr: 0.01000 - Train loss: 0.63306 - Test loss: 0.57721\n",
      "Epoch 13292 - lr: 0.01000 - Train loss: 0.63658 - Test loss: 0.57676\n",
      "Epoch 13293 - lr: 0.01000 - Train loss: 0.62376 - Test loss: 0.57705\n",
      "Epoch 13294 - lr: 0.01000 - Train loss: 0.63195 - Test loss: 0.57733\n",
      "Epoch 13295 - lr: 0.01000 - Train loss: 0.63831 - Test loss: 0.57709\n",
      "Epoch 13296 - lr: 0.01000 - Train loss: 0.63953 - Test loss: 0.57681\n",
      "Epoch 13297 - lr: 0.01000 - Train loss: 0.63517 - Test loss: 0.57640\n",
      "Epoch 13298 - lr: 0.01000 - Train loss: 0.62180 - Test loss: 0.57673\n",
      "Epoch 13299 - lr: 0.01000 - Train loss: 0.63358 - Test loss: 0.57692\n",
      "Epoch 13300 - lr: 0.01000 - Train loss: 0.62956 - Test loss: 0.57647\n",
      "Epoch 13301 - lr: 0.01000 - Train loss: 0.62137 - Test loss: 0.57688\n",
      "Epoch 13302 - lr: 0.01000 - Train loss: 0.63435 - Test loss: 0.57707\n",
      "Epoch 13303 - lr: 0.01000 - Train loss: 0.63057 - Test loss: 0.57661\n",
      "Epoch 13304 - lr: 0.01000 - Train loss: 0.62587 - Test loss: 0.57694\n",
      "Epoch 13305 - lr: 0.01000 - Train loss: 0.63273 - Test loss: 0.57684\n",
      "Epoch 13306 - lr: 0.01000 - Train loss: 0.62947 - Test loss: 0.57644\n",
      "Epoch 13307 - lr: 0.01000 - Train loss: 0.62153 - Test loss: 0.57687\n",
      "Epoch 13308 - lr: 0.01000 - Train loss: 0.62485 - Test loss: 0.57713\n",
      "Epoch 13309 - lr: 0.01000 - Train loss: 0.62182 - Test loss: 0.57748\n",
      "Epoch 13310 - lr: 0.01000 - Train loss: 0.62193 - Test loss: 0.57777\n",
      "Epoch 13311 - lr: 0.01000 - Train loss: 0.63357 - Test loss: 0.57785\n",
      "Epoch 13312 - lr: 0.01000 - Train loss: 0.63057 - Test loss: 0.57733\n",
      "Epoch 13313 - lr: 0.01000 - Train loss: 0.62969 - Test loss: 0.57758\n",
      "Epoch 13314 - lr: 0.01000 - Train loss: 0.63465 - Test loss: 0.57712\n",
      "Epoch 13315 - lr: 0.01000 - Train loss: 0.62981 - Test loss: 0.57676\n",
      "Epoch 13316 - lr: 0.01000 - Train loss: 0.63079 - Test loss: 0.57708\n",
      "Epoch 13317 - lr: 0.01000 - Train loss: 0.63881 - Test loss: 0.57693\n",
      "Epoch 13318 - lr: 0.01000 - Train loss: 0.64738 - Test loss: 0.57638\n",
      "Epoch 13319 - lr: 0.01000 - Train loss: 0.63044 - Test loss: 0.57599\n",
      "Epoch 13320 - lr: 0.01000 - Train loss: 0.62857 - Test loss: 0.57639\n",
      "Epoch 13321 - lr: 0.01000 - Train loss: 0.62999 - Test loss: 0.57608\n",
      "Epoch 13322 - lr: 0.01000 - Train loss: 0.62579 - Test loss: 0.57649\n",
      "Epoch 13323 - lr: 0.01000 - Train loss: 0.63222 - Test loss: 0.57646\n",
      "Epoch 13324 - lr: 0.01000 - Train loss: 0.62947 - Test loss: 0.57617\n",
      "Epoch 13325 - lr: 0.01000 - Train loss: 0.62607 - Test loss: 0.57658\n",
      "Epoch 13326 - lr: 0.01000 - Train loss: 0.63145 - Test loss: 0.57647\n",
      "Epoch 13327 - lr: 0.01000 - Train loss: 0.63056 - Test loss: 0.57632\n",
      "Epoch 13328 - lr: 0.01000 - Train loss: 0.63315 - Test loss: 0.57644\n",
      "Epoch 13329 - lr: 0.01000 - Train loss: 0.63740 - Test loss: 0.57628\n",
      "Epoch 13330 - lr: 0.01000 - Train loss: 0.63721 - Test loss: 0.57618\n",
      "Epoch 13331 - lr: 0.01000 - Train loss: 0.63835 - Test loss: 0.57604\n",
      "Epoch 13332 - lr: 0.01000 - Train loss: 0.63167 - Test loss: 0.57590\n",
      "Epoch 13333 - lr: 0.01000 - Train loss: 0.63183 - Test loss: 0.57614\n",
      "Epoch 13334 - lr: 0.01000 - Train loss: 0.62930 - Test loss: 0.57592\n",
      "Epoch 13335 - lr: 0.01000 - Train loss: 0.62633 - Test loss: 0.57635\n",
      "Epoch 13336 - lr: 0.01000 - Train loss: 0.63087 - Test loss: 0.57622\n",
      "Epoch 13337 - lr: 0.01000 - Train loss: 0.63265 - Test loss: 0.57626\n",
      "Epoch 13338 - lr: 0.01000 - Train loss: 0.63081 - Test loss: 0.57594\n",
      "Epoch 13339 - lr: 0.01000 - Train loss: 0.63181 - Test loss: 0.57636\n",
      "Epoch 13340 - lr: 0.01000 - Train loss: 0.63158 - Test loss: 0.57623\n",
      "Epoch 13341 - lr: 0.01000 - Train loss: 0.62683 - Test loss: 0.57656\n",
      "Epoch 13342 - lr: 0.01000 - Train loss: 0.63022 - Test loss: 0.57634\n",
      "Epoch 13343 - lr: 0.01000 - Train loss: 0.63377 - Test loss: 0.57664\n",
      "Epoch 13344 - lr: 0.01000 - Train loss: 0.64935 - Test loss: 0.57618\n",
      "Epoch 13345 - lr: 0.01000 - Train loss: 0.63516 - Test loss: 0.57585\n",
      "Epoch 13346 - lr: 0.01000 - Train loss: 0.62915 - Test loss: 0.57559\n",
      "Epoch 13347 - lr: 0.01000 - Train loss: 0.62213 - Test loss: 0.57610\n",
      "Epoch 13348 - lr: 0.01000 - Train loss: 0.62122 - Test loss: 0.57660\n",
      "Epoch 13349 - lr: 0.01000 - Train loss: 0.62738 - Test loss: 0.57683\n",
      "Epoch 13350 - lr: 0.01000 - Train loss: 0.63357 - Test loss: 0.57714\n",
      "Epoch 13351 - lr: 0.01000 - Train loss: 0.65442 - Test loss: 0.57666\n",
      "Epoch 13352 - lr: 0.01000 - Train loss: 0.63110 - Test loss: 0.57641\n",
      "Epoch 13353 - lr: 0.01000 - Train loss: 0.63270 - Test loss: 0.57659\n",
      "Epoch 13354 - lr: 0.01000 - Train loss: 0.62930 - Test loss: 0.57625\n",
      "Epoch 13355 - lr: 0.01000 - Train loss: 0.62190 - Test loss: 0.57669\n",
      "Epoch 13356 - lr: 0.01000 - Train loss: 0.62076 - Test loss: 0.57713\n",
      "Epoch 13357 - lr: 0.01000 - Train loss: 0.63542 - Test loss: 0.57733\n",
      "Epoch 13358 - lr: 0.01000 - Train loss: 0.63537 - Test loss: 0.57692\n",
      "Epoch 13359 - lr: 0.01000 - Train loss: 0.62963 - Test loss: 0.57650\n",
      "Epoch 13360 - lr: 0.01000 - Train loss: 0.62470 - Test loss: 0.57687\n",
      "Epoch 13361 - lr: 0.01000 - Train loss: 0.63396 - Test loss: 0.57701\n",
      "Epoch 13362 - lr: 0.01000 - Train loss: 0.63086 - Test loss: 0.57680\n",
      "Epoch 13363 - lr: 0.01000 - Train loss: 0.62148 - Test loss: 0.57713\n",
      "Epoch 13364 - lr: 0.01000 - Train loss: 0.63409 - Test loss: 0.57730\n",
      "Epoch 13365 - lr: 0.01000 - Train loss: 0.63166 - Test loss: 0.57685\n",
      "Epoch 13366 - lr: 0.01000 - Train loss: 0.63381 - Test loss: 0.57709\n",
      "Epoch 13367 - lr: 0.01000 - Train loss: 0.65043 - Test loss: 0.57657\n",
      "Epoch 13368 - lr: 0.01000 - Train loss: 0.63776 - Test loss: 0.57632\n",
      "Epoch 13369 - lr: 0.01000 - Train loss: 0.63873 - Test loss: 0.57622\n",
      "Epoch 13370 - lr: 0.01000 - Train loss: 0.63361 - Test loss: 0.57596\n",
      "Epoch 13371 - lr: 0.01000 - Train loss: 0.62556 - Test loss: 0.57625\n",
      "Epoch 13372 - lr: 0.01000 - Train loss: 0.62720 - Test loss: 0.57664\n",
      "Epoch 13373 - lr: 0.01000 - Train loss: 0.62955 - Test loss: 0.57636\n",
      "Epoch 13374 - lr: 0.01000 - Train loss: 0.62895 - Test loss: 0.57673\n",
      "Epoch 13375 - lr: 0.01000 - Train loss: 0.63046 - Test loss: 0.57632\n",
      "Epoch 13376 - lr: 0.01000 - Train loss: 0.62745 - Test loss: 0.57669\n",
      "Epoch 13377 - lr: 0.01000 - Train loss: 0.62933 - Test loss: 0.57637\n",
      "Epoch 13378 - lr: 0.01000 - Train loss: 0.62536 - Test loss: 0.57675\n",
      "Epoch 13379 - lr: 0.01000 - Train loss: 0.63302 - Test loss: 0.57674\n",
      "Epoch 13380 - lr: 0.01000 - Train loss: 0.63192 - Test loss: 0.57634\n",
      "Epoch 13381 - lr: 0.01000 - Train loss: 0.63387 - Test loss: 0.57659\n",
      "Epoch 13382 - lr: 0.01000 - Train loss: 0.63746 - Test loss: 0.57621\n",
      "Epoch 13383 - lr: 0.01000 - Train loss: 0.63003 - Test loss: 0.57655\n",
      "Epoch 13384 - lr: 0.01000 - Train loss: 0.63499 - Test loss: 0.57623\n",
      "Epoch 13385 - lr: 0.01000 - Train loss: 0.62909 - Test loss: 0.57593\n",
      "Epoch 13386 - lr: 0.01000 - Train loss: 0.62297 - Test loss: 0.57639\n",
      "Epoch 13387 - lr: 0.01000 - Train loss: 0.62872 - Test loss: 0.57677\n",
      "Epoch 13388 - lr: 0.01000 - Train loss: 0.63040 - Test loss: 0.57637\n",
      "Epoch 13389 - lr: 0.01000 - Train loss: 0.62865 - Test loss: 0.57673\n",
      "Epoch 13390 - lr: 0.01000 - Train loss: 0.63054 - Test loss: 0.57634\n",
      "Epoch 13391 - lr: 0.01000 - Train loss: 0.63008 - Test loss: 0.57670\n",
      "Epoch 13392 - lr: 0.01000 - Train loss: 0.63683 - Test loss: 0.57645\n",
      "Epoch 13393 - lr: 0.01000 - Train loss: 0.63634 - Test loss: 0.57616\n",
      "Epoch 13394 - lr: 0.01000 - Train loss: 0.63279 - Test loss: 0.57582\n",
      "Epoch 13395 - lr: 0.01000 - Train loss: 0.63235 - Test loss: 0.57587\n",
      "Epoch 13396 - lr: 0.01000 - Train loss: 0.62972 - Test loss: 0.57560\n",
      "Epoch 13397 - lr: 0.01000 - Train loss: 0.62569 - Test loss: 0.57607\n",
      "Epoch 13398 - lr: 0.01000 - Train loss: 0.63172 - Test loss: 0.57606\n",
      "Epoch 13399 - lr: 0.01000 - Train loss: 0.62928 - Test loss: 0.57585\n",
      "Epoch 13400 - lr: 0.01000 - Train loss: 0.62884 - Test loss: 0.57629\n",
      "Epoch 13401 - lr: 0.01000 - Train loss: 0.63161 - Test loss: 0.57598\n",
      "Epoch 13402 - lr: 0.01000 - Train loss: 0.63355 - Test loss: 0.57627\n",
      "Epoch 13403 - lr: 0.01000 - Train loss: 0.63712 - Test loss: 0.57594\n",
      "Epoch 13404 - lr: 0.01000 - Train loss: 0.62947 - Test loss: 0.57631\n",
      "Epoch 13405 - lr: 0.01000 - Train loss: 0.63356 - Test loss: 0.57600\n",
      "Epoch 13406 - lr: 0.01000 - Train loss: 0.63034 - Test loss: 0.57588\n",
      "Epoch 13407 - lr: 0.01000 - Train loss: 0.63311 - Test loss: 0.57605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13408 - lr: 0.01000 - Train loss: 0.63772 - Test loss: 0.57595\n",
      "Epoch 13409 - lr: 0.01000 - Train loss: 0.63215 - Test loss: 0.57583\n",
      "Epoch 13410 - lr: 0.01000 - Train loss: 0.63289 - Test loss: 0.57620\n",
      "Epoch 13411 - lr: 0.01000 - Train loss: 0.64351 - Test loss: 0.57578\n",
      "Epoch 13412 - lr: 0.01000 - Train loss: 0.62946 - Test loss: 0.57558\n",
      "Epoch 13413 - lr: 0.01000 - Train loss: 0.63155 - Test loss: 0.57604\n",
      "Epoch 13414 - lr: 0.01000 - Train loss: 0.63588 - Test loss: 0.57595\n",
      "Epoch 13415 - lr: 0.01000 - Train loss: 0.62945 - Test loss: 0.57560\n",
      "Epoch 13416 - lr: 0.01000 - Train loss: 0.62127 - Test loss: 0.57611\n",
      "Epoch 13417 - lr: 0.01000 - Train loss: 0.62561 - Test loss: 0.57643\n",
      "Epoch 13418 - lr: 0.01000 - Train loss: 0.62773 - Test loss: 0.57679\n",
      "Epoch 13419 - lr: 0.01000 - Train loss: 0.62931 - Test loss: 0.57642\n",
      "Epoch 13420 - lr: 0.01000 - Train loss: 0.62318 - Test loss: 0.57680\n",
      "Epoch 13421 - lr: 0.01000 - Train loss: 0.63066 - Test loss: 0.57711\n",
      "Epoch 13422 - lr: 0.01000 - Train loss: 0.63892 - Test loss: 0.57692\n",
      "Epoch 13423 - lr: 0.01000 - Train loss: 0.65220 - Test loss: 0.57638\n",
      "Epoch 13424 - lr: 0.01000 - Train loss: 0.63805 - Test loss: 0.57620\n",
      "Epoch 13425 - lr: 0.01000 - Train loss: 0.63863 - Test loss: 0.57607\n",
      "Epoch 13426 - lr: 0.01000 - Train loss: 0.63331 - Test loss: 0.57589\n",
      "Epoch 13427 - lr: 0.01000 - Train loss: 0.63435 - Test loss: 0.57607\n",
      "Epoch 13428 - lr: 0.01000 - Train loss: 0.63405 - Test loss: 0.57592\n",
      "Epoch 13429 - lr: 0.01000 - Train loss: 0.63257 - Test loss: 0.57587\n",
      "Epoch 13430 - lr: 0.01000 - Train loss: 0.62886 - Test loss: 0.57555\n",
      "Epoch 13431 - lr: 0.01000 - Train loss: 0.62291 - Test loss: 0.57599\n",
      "Epoch 13432 - lr: 0.01000 - Train loss: 0.62546 - Test loss: 0.57631\n",
      "Epoch 13433 - lr: 0.01000 - Train loss: 0.62709 - Test loss: 0.57668\n",
      "Epoch 13434 - lr: 0.01000 - Train loss: 0.62944 - Test loss: 0.57637\n",
      "Epoch 13435 - lr: 0.01000 - Train loss: 0.62810 - Test loss: 0.57672\n",
      "Epoch 13436 - lr: 0.01000 - Train loss: 0.62915 - Test loss: 0.57631\n",
      "Epoch 13437 - lr: 0.01000 - Train loss: 0.62078 - Test loss: 0.57675\n",
      "Epoch 13438 - lr: 0.01000 - Train loss: 0.63456 - Test loss: 0.57696\n",
      "Epoch 13439 - lr: 0.01000 - Train loss: 0.63192 - Test loss: 0.57651\n",
      "Epoch 13440 - lr: 0.01000 - Train loss: 0.63403 - Test loss: 0.57676\n",
      "Epoch 13441 - lr: 0.01000 - Train loss: 0.64833 - Test loss: 0.57626\n",
      "Epoch 13442 - lr: 0.01000 - Train loss: 0.63135 - Test loss: 0.57583\n",
      "Epoch 13443 - lr: 0.01000 - Train loss: 0.63274 - Test loss: 0.57624\n",
      "Epoch 13444 - lr: 0.01000 - Train loss: 0.63548 - Test loss: 0.57596\n",
      "Epoch 13445 - lr: 0.01000 - Train loss: 0.62187 - Test loss: 0.57637\n",
      "Epoch 13446 - lr: 0.01000 - Train loss: 0.62122 - Test loss: 0.57680\n",
      "Epoch 13447 - lr: 0.01000 - Train loss: 0.63424 - Test loss: 0.57700\n",
      "Epoch 13448 - lr: 0.01000 - Train loss: 0.63067 - Test loss: 0.57654\n",
      "Epoch 13449 - lr: 0.01000 - Train loss: 0.62870 - Test loss: 0.57687\n",
      "Epoch 13450 - lr: 0.01000 - Train loss: 0.62992 - Test loss: 0.57644\n",
      "Epoch 13451 - lr: 0.01000 - Train loss: 0.62324 - Test loss: 0.57682\n",
      "Epoch 13452 - lr: 0.01000 - Train loss: 0.63111 - Test loss: 0.57714\n",
      "Epoch 13453 - lr: 0.01000 - Train loss: 0.63917 - Test loss: 0.57697\n",
      "Epoch 13454 - lr: 0.01000 - Train loss: 0.64836 - Test loss: 0.57639\n",
      "Epoch 13455 - lr: 0.01000 - Train loss: 0.63089 - Test loss: 0.57593\n",
      "Epoch 13456 - lr: 0.01000 - Train loss: 0.62990 - Test loss: 0.57634\n",
      "Epoch 13457 - lr: 0.01000 - Train loss: 0.63316 - Test loss: 0.57598\n",
      "Epoch 13458 - lr: 0.01000 - Train loss: 0.63254 - Test loss: 0.57602\n",
      "Epoch 13459 - lr: 0.01000 - Train loss: 0.62919 - Test loss: 0.57570\n",
      "Epoch 13460 - lr: 0.01000 - Train loss: 0.62087 - Test loss: 0.57622\n",
      "Epoch 13461 - lr: 0.01000 - Train loss: 0.63277 - Test loss: 0.57650\n",
      "Epoch 13462 - lr: 0.01000 - Train loss: 0.62907 - Test loss: 0.57616\n",
      "Epoch 13463 - lr: 0.01000 - Train loss: 0.62073 - Test loss: 0.57665\n",
      "Epoch 13464 - lr: 0.01000 - Train loss: 0.63114 - Test loss: 0.57687\n",
      "Epoch 13465 - lr: 0.01000 - Train loss: 0.62946 - Test loss: 0.57659\n",
      "Epoch 13466 - lr: 0.01000 - Train loss: 0.63089 - Test loss: 0.57696\n",
      "Epoch 13467 - lr: 0.01000 - Train loss: 0.63873 - Test loss: 0.57682\n",
      "Epoch 13468 - lr: 0.01000 - Train loss: 0.64258 - Test loss: 0.57629\n",
      "Epoch 13469 - lr: 0.01000 - Train loss: 0.63066 - Test loss: 0.57612\n",
      "Epoch 13470 - lr: 0.01000 - Train loss: 0.63239 - Test loss: 0.57617\n",
      "Epoch 13471 - lr: 0.01000 - Train loss: 0.62934 - Test loss: 0.57586\n",
      "Epoch 13472 - lr: 0.01000 - Train loss: 0.62163 - Test loss: 0.57636\n",
      "Epoch 13473 - lr: 0.01000 - Train loss: 0.62077 - Test loss: 0.57684\n",
      "Epoch 13474 - lr: 0.01000 - Train loss: 0.63459 - Test loss: 0.57708\n",
      "Epoch 13475 - lr: 0.01000 - Train loss: 0.63284 - Test loss: 0.57667\n",
      "Epoch 13476 - lr: 0.01000 - Train loss: 0.63227 - Test loss: 0.57662\n",
      "Epoch 13477 - lr: 0.01000 - Train loss: 0.62952 - Test loss: 0.57627\n",
      "Epoch 13478 - lr: 0.01000 - Train loss: 0.62467 - Test loss: 0.57668\n",
      "Epoch 13479 - lr: 0.01000 - Train loss: 0.63343 - Test loss: 0.57680\n",
      "Epoch 13480 - lr: 0.01000 - Train loss: 0.63842 - Test loss: 0.57669\n",
      "Epoch 13481 - lr: 0.01000 - Train loss: 0.64170 - Test loss: 0.57620\n",
      "Epoch 13482 - lr: 0.01000 - Train loss: 0.63164 - Test loss: 0.57613\n",
      "Epoch 13483 - lr: 0.01000 - Train loss: 0.62890 - Test loss: 0.57592\n",
      "Epoch 13484 - lr: 0.01000 - Train loss: 0.62584 - Test loss: 0.57637\n",
      "Epoch 13485 - lr: 0.01000 - Train loss: 0.63090 - Test loss: 0.57628\n",
      "Epoch 13486 - lr: 0.01000 - Train loss: 0.63100 - Test loss: 0.57622\n",
      "Epoch 13487 - lr: 0.01000 - Train loss: 0.63022 - Test loss: 0.57611\n",
      "Epoch 13488 - lr: 0.01000 - Train loss: 0.63282 - Test loss: 0.57626\n",
      "Epoch 13489 - lr: 0.01000 - Train loss: 0.63559 - Test loss: 0.57605\n",
      "Epoch 13490 - lr: 0.01000 - Train loss: 0.63075 - Test loss: 0.57575\n",
      "Epoch 13491 - lr: 0.01000 - Train loss: 0.63231 - Test loss: 0.57620\n",
      "Epoch 13492 - lr: 0.01000 - Train loss: 0.63767 - Test loss: 0.57589\n",
      "Epoch 13493 - lr: 0.01000 - Train loss: 0.63167 - Test loss: 0.57630\n",
      "Epoch 13494 - lr: 0.01000 - Train loss: 0.63240 - Test loss: 0.57618\n",
      "Epoch 13495 - lr: 0.01000 - Train loss: 0.63367 - Test loss: 0.57649\n",
      "Epoch 13496 - lr: 0.01000 - Train loss: 0.64973 - Test loss: 0.57606\n",
      "Epoch 13497 - lr: 0.01000 - Train loss: 0.63550 - Test loss: 0.57577\n",
      "Epoch 13498 - lr: 0.01000 - Train loss: 0.62886 - Test loss: 0.57547\n",
      "Epoch 13499 - lr: 0.01000 - Train loss: 0.62098 - Test loss: 0.57603\n",
      "Epoch 13500 - lr: 0.01000 - Train loss: 0.63260 - Test loss: 0.57634\n",
      "Epoch 13501 - lr: 0.01000 - Train loss: 0.62893 - Test loss: 0.57604\n",
      "Epoch 13502 - lr: 0.01000 - Train loss: 0.62066 - Test loss: 0.57656\n",
      "Epoch 13503 - lr: 0.01000 - Train loss: 0.63064 - Test loss: 0.57679\n",
      "Epoch 13504 - lr: 0.01000 - Train loss: 0.62974 - Test loss: 0.57657\n",
      "Epoch 13505 - lr: 0.01000 - Train loss: 0.63338 - Test loss: 0.57690\n",
      "Epoch 13506 - lr: 0.01000 - Train loss: 0.65133 - Test loss: 0.57644\n",
      "Epoch 13507 - lr: 0.01000 - Train loss: 0.63883 - Test loss: 0.57631\n",
      "Epoch 13508 - lr: 0.01000 - Train loss: 0.64281 - Test loss: 0.57586\n",
      "Epoch 13509 - lr: 0.01000 - Train loss: 0.62989 - Test loss: 0.57572\n",
      "Epoch 13510 - lr: 0.01000 - Train loss: 0.63385 - Test loss: 0.57609\n",
      "Epoch 13511 - lr: 0.01000 - Train loss: 0.63305 - Test loss: 0.57593\n",
      "Epoch 13512 - lr: 0.01000 - Train loss: 0.62636 - Test loss: 0.57621\n",
      "Epoch 13513 - lr: 0.01000 - Train loss: 0.63220 - Test loss: 0.57664\n",
      "Epoch 13514 - lr: 0.01000 - Train loss: 0.63241 - Test loss: 0.57647\n",
      "Epoch 13515 - lr: 0.01000 - Train loss: 0.63262 - Test loss: 0.57679\n",
      "Epoch 13516 - lr: 0.01000 - Train loss: 0.63114 - Test loss: 0.57659\n",
      "Epoch 13517 - lr: 0.01000 - Train loss: 0.62963 - Test loss: 0.57673\n",
      "Epoch 13518 - lr: 0.01000 - Train loss: 0.63184 - Test loss: 0.57663\n",
      "Epoch 13519 - lr: 0.01000 - Train loss: 0.62895 - Test loss: 0.57633\n",
      "Epoch 13520 - lr: 0.01000 - Train loss: 0.62469 - Test loss: 0.57673\n",
      "Epoch 13521 - lr: 0.01000 - Train loss: 0.63398 - Test loss: 0.57686\n",
      "Epoch 13522 - lr: 0.01000 - Train loss: 0.63930 - Test loss: 0.57673\n",
      "Epoch 13523 - lr: 0.01000 - Train loss: 0.64150 - Test loss: 0.57621\n",
      "Epoch 13524 - lr: 0.01000 - Train loss: 0.63249 - Test loss: 0.57618\n",
      "Epoch 13525 - lr: 0.01000 - Train loss: 0.62879 - Test loss: 0.57583\n",
      "Epoch 13526 - lr: 0.01000 - Train loss: 0.62353 - Test loss: 0.57625\n",
      "Epoch 13527 - lr: 0.01000 - Train loss: 0.62113 - Test loss: 0.57672\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13528 - lr: 0.01000 - Train loss: 0.63305 - Test loss: 0.57695\n",
      "Epoch 13529 - lr: 0.01000 - Train loss: 0.62898 - Test loss: 0.57653\n",
      "Epoch 13530 - lr: 0.01000 - Train loss: 0.62144 - Test loss: 0.57694\n",
      "Epoch 13531 - lr: 0.01000 - Train loss: 0.63196 - Test loss: 0.57714\n",
      "Epoch 13532 - lr: 0.01000 - Train loss: 0.62881 - Test loss: 0.57676\n",
      "Epoch 13533 - lr: 0.01000 - Train loss: 0.62105 - Test loss: 0.57718\n",
      "Epoch 13534 - lr: 0.01000 - Train loss: 0.62498 - Test loss: 0.57741\n",
      "Epoch 13535 - lr: 0.01000 - Train loss: 0.62470 - Test loss: 0.57771\n",
      "Epoch 13536 - lr: 0.01000 - Train loss: 0.63385 - Test loss: 0.57771\n",
      "Epoch 13537 - lr: 0.01000 - Train loss: 0.63832 - Test loss: 0.57747\n",
      "Epoch 13538 - lr: 0.01000 - Train loss: 0.63238 - Test loss: 0.57714\n",
      "Epoch 13539 - lr: 0.01000 - Train loss: 0.63318 - Test loss: 0.57738\n",
      "Epoch 13540 - lr: 0.01000 - Train loss: 0.64617 - Test loss: 0.57680\n",
      "Epoch 13541 - lr: 0.01000 - Train loss: 0.62879 - Test loss: 0.57635\n",
      "Epoch 13542 - lr: 0.01000 - Train loss: 0.62150 - Test loss: 0.57678\n",
      "Epoch 13543 - lr: 0.01000 - Train loss: 0.63129 - Test loss: 0.57700\n",
      "Epoch 13544 - lr: 0.01000 - Train loss: 0.62897 - Test loss: 0.57669\n",
      "Epoch 13545 - lr: 0.01000 - Train loss: 0.62599 - Test loss: 0.57706\n",
      "Epoch 13546 - lr: 0.01000 - Train loss: 0.63052 - Test loss: 0.57686\n",
      "Epoch 13547 - lr: 0.01000 - Train loss: 0.63284 - Test loss: 0.57688\n",
      "Epoch 13548 - lr: 0.01000 - Train loss: 0.63215 - Test loss: 0.57649\n",
      "Epoch 13549 - lr: 0.01000 - Train loss: 0.63340 - Test loss: 0.57665\n",
      "Epoch 13550 - lr: 0.01000 - Train loss: 0.63858 - Test loss: 0.57658\n",
      "Epoch 13551 - lr: 0.01000 - Train loss: 0.63674 - Test loss: 0.57620\n",
      "Epoch 13552 - lr: 0.01000 - Train loss: 0.62809 - Test loss: 0.57657\n",
      "Epoch 13553 - lr: 0.01000 - Train loss: 0.62887 - Test loss: 0.57622\n",
      "Epoch 13554 - lr: 0.01000 - Train loss: 0.62076 - Test loss: 0.57671\n",
      "Epoch 13555 - lr: 0.01000 - Train loss: 0.63330 - Test loss: 0.57696\n",
      "Epoch 13556 - lr: 0.01000 - Train loss: 0.62950 - Test loss: 0.57656\n",
      "Epoch 13557 - lr: 0.01000 - Train loss: 0.62254 - Test loss: 0.57698\n",
      "Epoch 13558 - lr: 0.01000 - Train loss: 0.62769 - Test loss: 0.57731\n",
      "Epoch 13559 - lr: 0.01000 - Train loss: 0.62905 - Test loss: 0.57690\n",
      "Epoch 13560 - lr: 0.01000 - Train loss: 0.62161 - Test loss: 0.57729\n",
      "Epoch 13561 - lr: 0.01000 - Train loss: 0.62035 - Test loss: 0.57768\n",
      "Epoch 13562 - lr: 0.01000 - Train loss: 0.63231 - Test loss: 0.57779\n",
      "Epoch 13563 - lr: 0.01000 - Train loss: 0.62932 - Test loss: 0.57733\n",
      "Epoch 13564 - lr: 0.01000 - Train loss: 0.62549 - Test loss: 0.57762\n",
      "Epoch 13565 - lr: 0.01000 - Train loss: 0.63143 - Test loss: 0.57740\n",
      "Epoch 13566 - lr: 0.01000 - Train loss: 0.62950 - Test loss: 0.57708\n",
      "Epoch 13567 - lr: 0.01000 - Train loss: 0.63242 - Test loss: 0.57739\n",
      "Epoch 13568 - lr: 0.01000 - Train loss: 0.64692 - Test loss: 0.57685\n",
      "Epoch 13569 - lr: 0.01000 - Train loss: 0.63048 - Test loss: 0.57643\n",
      "Epoch 13570 - lr: 0.01000 - Train loss: 0.63169 - Test loss: 0.57683\n",
      "Epoch 13571 - lr: 0.01000 - Train loss: 0.63047 - Test loss: 0.57669\n",
      "Epoch 13572 - lr: 0.01000 - Train loss: 0.63252 - Test loss: 0.57689\n",
      "Epoch 13573 - lr: 0.01000 - Train loss: 0.62920 - Test loss: 0.57654\n",
      "Epoch 13574 - lr: 0.01000 - Train loss: 0.62427 - Test loss: 0.57695\n",
      "Epoch 13575 - lr: 0.01000 - Train loss: 0.63342 - Test loss: 0.57711\n",
      "Epoch 13576 - lr: 0.01000 - Train loss: 0.63284 - Test loss: 0.57691\n",
      "Epoch 13577 - lr: 0.01000 - Train loss: 0.63306 - Test loss: 0.57692\n",
      "Epoch 13578 - lr: 0.01000 - Train loss: 0.63707 - Test loss: 0.57669\n",
      "Epoch 13579 - lr: 0.01000 - Train loss: 0.63846 - Test loss: 0.57656\n",
      "Epoch 13580 - lr: 0.01000 - Train loss: 0.64448 - Test loss: 0.57608\n",
      "Epoch 13581 - lr: 0.01000 - Train loss: 0.62849 - Test loss: 0.57581\n",
      "Epoch 13582 - lr: 0.01000 - Train loss: 0.62137 - Test loss: 0.57634\n",
      "Epoch 13583 - lr: 0.01000 - Train loss: 0.62077 - Test loss: 0.57683\n",
      "Epoch 13584 - lr: 0.01000 - Train loss: 0.63372 - Test loss: 0.57708\n",
      "Epoch 13585 - lr: 0.01000 - Train loss: 0.63066 - Test loss: 0.57666\n",
      "Epoch 13586 - lr: 0.01000 - Train loss: 0.63164 - Test loss: 0.57703\n",
      "Epoch 13587 - lr: 0.01000 - Train loss: 0.63044 - Test loss: 0.57685\n",
      "Epoch 13588 - lr: 0.01000 - Train loss: 0.62322 - Test loss: 0.57711\n",
      "Epoch 13589 - lr: 0.01000 - Train loss: 0.62171 - Test loss: 0.57745\n",
      "Epoch 13590 - lr: 0.01000 - Train loss: 0.63065 - Test loss: 0.57759\n",
      "Epoch 13591 - lr: 0.01000 - Train loss: 0.62970 - Test loss: 0.57725\n",
      "Epoch 13592 - lr: 0.01000 - Train loss: 0.63330 - Test loss: 0.57751\n",
      "Epoch 13593 - lr: 0.01000 - Train loss: 0.65452 - Test loss: 0.57702\n",
      "Epoch 13594 - lr: 0.01000 - Train loss: 0.63214 - Test loss: 0.57670\n",
      "Epoch 13595 - lr: 0.01000 - Train loss: 0.62944 - Test loss: 0.57688\n",
      "Epoch 13596 - lr: 0.01000 - Train loss: 0.63117 - Test loss: 0.57677\n",
      "Epoch 13597 - lr: 0.01000 - Train loss: 0.62938 - Test loss: 0.57655\n",
      "Epoch 13598 - lr: 0.01000 - Train loss: 0.63278 - Test loss: 0.57694\n",
      "Epoch 13599 - lr: 0.01000 - Train loss: 0.64518 - Test loss: 0.57646\n",
      "Epoch 13600 - lr: 0.01000 - Train loss: 0.62853 - Test loss: 0.57612\n",
      "Epoch 13601 - lr: 0.01000 - Train loss: 0.62049 - Test loss: 0.57665\n",
      "Epoch 13602 - lr: 0.01000 - Train loss: 0.63024 - Test loss: 0.57691\n",
      "Epoch 13603 - lr: 0.01000 - Train loss: 0.62970 - Test loss: 0.57671\n",
      "Epoch 13604 - lr: 0.01000 - Train loss: 0.63350 - Test loss: 0.57700\n",
      "Epoch 13605 - lr: 0.01000 - Train loss: 0.64046 - Test loss: 0.57655\n",
      "Epoch 13606 - lr: 0.01000 - Train loss: 0.63285 - Test loss: 0.57661\n",
      "Epoch 13607 - lr: 0.01000 - Train loss: 0.63411 - Test loss: 0.57631\n",
      "Epoch 13608 - lr: 0.01000 - Train loss: 0.62870 - Test loss: 0.57609\n",
      "Epoch 13609 - lr: 0.01000 - Train loss: 0.62686 - Test loss: 0.57655\n",
      "Epoch 13610 - lr: 0.01000 - Train loss: 0.62870 - Test loss: 0.57631\n",
      "Epoch 13611 - lr: 0.01000 - Train loss: 0.62486 - Test loss: 0.57675\n",
      "Epoch 13612 - lr: 0.01000 - Train loss: 0.63237 - Test loss: 0.57679\n",
      "Epoch 13613 - lr: 0.01000 - Train loss: 0.63056 - Test loss: 0.57642\n",
      "Epoch 13614 - lr: 0.01000 - Train loss: 0.63183 - Test loss: 0.57682\n",
      "Epoch 13615 - lr: 0.01000 - Train loss: 0.63160 - Test loss: 0.57664\n",
      "Epoch 13616 - lr: 0.01000 - Train loss: 0.63070 - Test loss: 0.57683\n",
      "Epoch 13617 - lr: 0.01000 - Train loss: 0.62908 - Test loss: 0.57660\n",
      "Epoch 13618 - lr: 0.01000 - Train loss: 0.63091 - Test loss: 0.57699\n",
      "Epoch 13619 - lr: 0.01000 - Train loss: 0.63651 - Test loss: 0.57685\n",
      "Epoch 13620 - lr: 0.01000 - Train loss: 0.63427 - Test loss: 0.57647\n",
      "Epoch 13621 - lr: 0.01000 - Train loss: 0.62862 - Test loss: 0.57620\n",
      "Epoch 13622 - lr: 0.01000 - Train loss: 0.62495 - Test loss: 0.57664\n",
      "Epoch 13623 - lr: 0.01000 - Train loss: 0.63206 - Test loss: 0.57665\n",
      "Epoch 13624 - lr: 0.01000 - Train loss: 0.62926 - Test loss: 0.57631\n",
      "Epoch 13625 - lr: 0.01000 - Train loss: 0.62428 - Test loss: 0.57674\n",
      "Epoch 13626 - lr: 0.01000 - Train loss: 0.63321 - Test loss: 0.57690\n",
      "Epoch 13627 - lr: 0.01000 - Train loss: 0.63710 - Test loss: 0.57677\n",
      "Epoch 13628 - lr: 0.01000 - Train loss: 0.63846 - Test loss: 0.57663\n",
      "Epoch 13629 - lr: 0.01000 - Train loss: 0.64518 - Test loss: 0.57614\n",
      "Epoch 13630 - lr: 0.01000 - Train loss: 0.62842 - Test loss: 0.57581\n",
      "Epoch 13631 - lr: 0.01000 - Train loss: 0.62048 - Test loss: 0.57636\n",
      "Epoch 13632 - lr: 0.01000 - Train loss: 0.63264 - Test loss: 0.57666\n",
      "Epoch 13633 - lr: 0.01000 - Train loss: 0.62893 - Test loss: 0.57632\n",
      "Epoch 13634 - lr: 0.01000 - Train loss: 0.62102 - Test loss: 0.57680\n",
      "Epoch 13635 - lr: 0.01000 - Train loss: 0.62172 - Test loss: 0.57719\n",
      "Epoch 13636 - lr: 0.01000 - Train loss: 0.62959 - Test loss: 0.57737\n",
      "Epoch 13637 - lr: 0.01000 - Train loss: 0.63085 - Test loss: 0.57717\n",
      "Epoch 13638 - lr: 0.01000 - Train loss: 0.63025 - Test loss: 0.57696\n",
      "Epoch 13639 - lr: 0.01000 - Train loss: 0.63208 - Test loss: 0.57694\n",
      "Epoch 13640 - lr: 0.01000 - Train loss: 0.63042 - Test loss: 0.57656\n",
      "Epoch 13641 - lr: 0.01000 - Train loss: 0.63209 - Test loss: 0.57694\n",
      "Epoch 13642 - lr: 0.01000 - Train loss: 0.64231 - Test loss: 0.57647\n",
      "Epoch 13643 - lr: 0.01000 - Train loss: 0.62979 - Test loss: 0.57629\n",
      "Epoch 13644 - lr: 0.01000 - Train loss: 0.63286 - Test loss: 0.57650\n",
      "Epoch 13645 - lr: 0.01000 - Train loss: 0.63804 - Test loss: 0.57646\n",
      "Epoch 13646 - lr: 0.01000 - Train loss: 0.64216 - Test loss: 0.57602\n",
      "Epoch 13647 - lr: 0.01000 - Train loss: 0.62981 - Test loss: 0.57591\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13648 - lr: 0.01000 - Train loss: 0.63285 - Test loss: 0.57615\n",
      "Epoch 13649 - lr: 0.01000 - Train loss: 0.63759 - Test loss: 0.57607\n",
      "Epoch 13650 - lr: 0.01000 - Train loss: 0.63477 - Test loss: 0.57598\n",
      "Epoch 13651 - lr: 0.01000 - Train loss: 0.62836 - Test loss: 0.57572\n",
      "Epoch 13652 - lr: 0.01000 - Train loss: 0.62073 - Test loss: 0.57628\n",
      "Epoch 13653 - lr: 0.01000 - Train loss: 0.62582 - Test loss: 0.57661\n",
      "Epoch 13654 - lr: 0.01000 - Train loss: 0.63097 - Test loss: 0.57701\n",
      "Epoch 13655 - lr: 0.01000 - Train loss: 0.63697 - Test loss: 0.57686\n",
      "Epoch 13656 - lr: 0.01000 - Train loss: 0.63686 - Test loss: 0.57654\n",
      "Epoch 13657 - lr: 0.01000 - Train loss: 0.63520 - Test loss: 0.57621\n",
      "Epoch 13658 - lr: 0.01000 - Train loss: 0.62852 - Test loss: 0.57586\n",
      "Epoch 13659 - lr: 0.01000 - Train loss: 0.62065 - Test loss: 0.57638\n",
      "Epoch 13660 - lr: 0.01000 - Train loss: 0.63270 - Test loss: 0.57667\n",
      "Epoch 13661 - lr: 0.01000 - Train loss: 0.62892 - Test loss: 0.57631\n",
      "Epoch 13662 - lr: 0.01000 - Train loss: 0.62077 - Test loss: 0.57679\n",
      "Epoch 13663 - lr: 0.01000 - Train loss: 0.62406 - Test loss: 0.57709\n",
      "Epoch 13664 - lr: 0.01000 - Train loss: 0.62114 - Test loss: 0.57749\n",
      "Epoch 13665 - lr: 0.01000 - Train loss: 0.62053 - Test loss: 0.57783\n",
      "Epoch 13666 - lr: 0.01000 - Train loss: 0.63528 - Test loss: 0.57797\n",
      "Epoch 13667 - lr: 0.01000 - Train loss: 0.63667 - Test loss: 0.57757\n",
      "Epoch 13668 - lr: 0.01000 - Train loss: 0.63769 - Test loss: 0.57725\n",
      "Epoch 13669 - lr: 0.01000 - Train loss: 0.63086 - Test loss: 0.57698\n",
      "Epoch 13670 - lr: 0.01000 - Train loss: 0.62360 - Test loss: 0.57726\n",
      "Epoch 13671 - lr: 0.01000 - Train loss: 0.63350 - Test loss: 0.57751\n",
      "Epoch 13672 - lr: 0.01000 - Train loss: 0.65768 - Test loss: 0.57704\n",
      "Epoch 13673 - lr: 0.01000 - Train loss: 0.65162 - Test loss: 0.57650\n",
      "Epoch 13674 - lr: 0.01000 - Train loss: 0.63892 - Test loss: 0.57636\n",
      "Epoch 13675 - lr: 0.01000 - Train loss: 0.64233 - Test loss: 0.57591\n",
      "Epoch 13676 - lr: 0.01000 - Train loss: 0.62987 - Test loss: 0.57579\n",
      "Epoch 13677 - lr: 0.01000 - Train loss: 0.63354 - Test loss: 0.57608\n",
      "Epoch 13678 - lr: 0.01000 - Train loss: 0.63910 - Test loss: 0.57607\n",
      "Epoch 13679 - lr: 0.01000 - Train loss: 0.63855 - Test loss: 0.57571\n",
      "Epoch 13680 - lr: 0.01000 - Train loss: 0.63382 - Test loss: 0.57611\n",
      "Epoch 13681 - lr: 0.01000 - Train loss: 0.64410 - Test loss: 0.57572\n",
      "Epoch 13682 - lr: 0.01000 - Train loss: 0.62838 - Test loss: 0.57548\n",
      "Epoch 13683 - lr: 0.01000 - Train loss: 0.62104 - Test loss: 0.57604\n",
      "Epoch 13684 - lr: 0.01000 - Train loss: 0.62758 - Test loss: 0.57636\n",
      "Epoch 13685 - lr: 0.01000 - Train loss: 0.63415 - Test loss: 0.57663\n",
      "Epoch 13686 - lr: 0.01000 - Train loss: 0.63389 - Test loss: 0.57646\n",
      "Epoch 13687 - lr: 0.01000 - Train loss: 0.63221 - Test loss: 0.57639\n",
      "Epoch 13688 - lr: 0.01000 - Train loss: 0.62841 - Test loss: 0.57602\n",
      "Epoch 13689 - lr: 0.01000 - Train loss: 0.62461 - Test loss: 0.57639\n",
      "Epoch 13690 - lr: 0.01000 - Train loss: 0.62561 - Test loss: 0.57681\n",
      "Epoch 13691 - lr: 0.01000 - Train loss: 0.63097 - Test loss: 0.57667\n",
      "Epoch 13692 - lr: 0.01000 - Train loss: 0.63052 - Test loss: 0.57653\n",
      "Epoch 13693 - lr: 0.01000 - Train loss: 0.63188 - Test loss: 0.57651\n",
      "Epoch 13694 - lr: 0.01000 - Train loss: 0.62826 - Test loss: 0.57617\n",
      "Epoch 13695 - lr: 0.01000 - Train loss: 0.62254 - Test loss: 0.57659\n",
      "Epoch 13696 - lr: 0.01000 - Train loss: 0.62353 - Test loss: 0.57695\n",
      "Epoch 13697 - lr: 0.01000 - Train loss: 0.62042 - Test loss: 0.57738\n",
      "Epoch 13698 - lr: 0.01000 - Train loss: 0.63251 - Test loss: 0.57754\n",
      "Epoch 13699 - lr: 0.01000 - Train loss: 0.62857 - Test loss: 0.57706\n",
      "Epoch 13700 - lr: 0.01000 - Train loss: 0.62093 - Test loss: 0.57744\n",
      "Epoch 13701 - lr: 0.01000 - Train loss: 0.63243 - Test loss: 0.57761\n",
      "Epoch 13702 - lr: 0.01000 - Train loss: 0.62866 - Test loss: 0.57714\n",
      "Epoch 13703 - lr: 0.01000 - Train loss: 0.62008 - Test loss: 0.57756\n",
      "Epoch 13704 - lr: 0.01000 - Train loss: 0.63331 - Test loss: 0.57773\n",
      "Epoch 13705 - lr: 0.01000 - Train loss: 0.62976 - Test loss: 0.57724\n",
      "Epoch 13706 - lr: 0.01000 - Train loss: 0.62644 - Test loss: 0.57756\n",
      "Epoch 13707 - lr: 0.01000 - Train loss: 0.62890 - Test loss: 0.57721\n",
      "Epoch 13708 - lr: 0.01000 - Train loss: 0.62869 - Test loss: 0.57754\n",
      "Epoch 13709 - lr: 0.01000 - Train loss: 0.63173 - Test loss: 0.57709\n",
      "Epoch 13710 - lr: 0.01000 - Train loss: 0.63312 - Test loss: 0.57721\n",
      "Epoch 13711 - lr: 0.01000 - Train loss: 0.63629 - Test loss: 0.57705\n",
      "Epoch 13712 - lr: 0.01000 - Train loss: 0.63365 - Test loss: 0.57664\n",
      "Epoch 13713 - lr: 0.01000 - Train loss: 0.62892 - Test loss: 0.57642\n",
      "Epoch 13714 - lr: 0.01000 - Train loss: 0.63150 - Test loss: 0.57685\n",
      "Epoch 13715 - lr: 0.01000 - Train loss: 0.63017 - Test loss: 0.57672\n",
      "Epoch 13716 - lr: 0.01000 - Train loss: 0.62448 - Test loss: 0.57698\n",
      "Epoch 13717 - lr: 0.01000 - Train loss: 0.62421 - Test loss: 0.57736\n",
      "Epoch 13718 - lr: 0.01000 - Train loss: 0.63335 - Test loss: 0.57744\n",
      "Epoch 13719 - lr: 0.01000 - Train loss: 0.63706 - Test loss: 0.57724\n",
      "Epoch 13720 - lr: 0.01000 - Train loss: 0.63833 - Test loss: 0.57700\n",
      "Epoch 13721 - lr: 0.01000 - Train loss: 0.63396 - Test loss: 0.57664\n",
      "Epoch 13722 - lr: 0.01000 - Train loss: 0.62105 - Test loss: 0.57700\n",
      "Epoch 13723 - lr: 0.01000 - Train loss: 0.63182 - Test loss: 0.57720\n",
      "Epoch 13724 - lr: 0.01000 - Train loss: 0.62837 - Test loss: 0.57681\n",
      "Epoch 13725 - lr: 0.01000 - Train loss: 0.62007 - Test loss: 0.57726\n",
      "Epoch 13726 - lr: 0.01000 - Train loss: 0.63238 - Test loss: 0.57745\n",
      "Epoch 13727 - lr: 0.01000 - Train loss: 0.62876 - Test loss: 0.57701\n",
      "Epoch 13728 - lr: 0.01000 - Train loss: 0.62075 - Test loss: 0.57742\n",
      "Epoch 13729 - lr: 0.01000 - Train loss: 0.62201 - Test loss: 0.57773\n",
      "Epoch 13730 - lr: 0.01000 - Train loss: 0.62701 - Test loss: 0.57787\n",
      "Epoch 13731 - lr: 0.01000 - Train loss: 0.63345 - Test loss: 0.57806\n",
      "Epoch 13732 - lr: 0.01000 - Train loss: 0.65513 - Test loss: 0.57751\n",
      "Epoch 13733 - lr: 0.01000 - Train loss: 0.63566 - Test loss: 0.57701\n",
      "Epoch 13734 - lr: 0.01000 - Train loss: 0.62390 - Test loss: 0.57732\n",
      "Epoch 13735 - lr: 0.01000 - Train loss: 0.63366 - Test loss: 0.57750\n",
      "Epoch 13736 - lr: 0.01000 - Train loss: 0.63371 - Test loss: 0.57712\n",
      "Epoch 13737 - lr: 0.01000 - Train loss: 0.62211 - Test loss: 0.57740\n",
      "Epoch 13738 - lr: 0.01000 - Train loss: 0.62633 - Test loss: 0.57759\n",
      "Epoch 13739 - lr: 0.01000 - Train loss: 0.63272 - Test loss: 0.57788\n",
      "Epoch 13740 - lr: 0.01000 - Train loss: 0.64839 - Test loss: 0.57727\n",
      "Epoch 13741 - lr: 0.01000 - Train loss: 0.63124 - Test loss: 0.57676\n",
      "Epoch 13742 - lr: 0.01000 - Train loss: 0.63324 - Test loss: 0.57709\n",
      "Epoch 13743 - lr: 0.01000 - Train loss: 0.64767 - Test loss: 0.57660\n",
      "Epoch 13744 - lr: 0.01000 - Train loss: 0.62966 - Test loss: 0.57617\n",
      "Epoch 13745 - lr: 0.01000 - Train loss: 0.62374 - Test loss: 0.57663\n",
      "Epoch 13746 - lr: 0.01000 - Train loss: 0.63370 - Test loss: 0.57694\n",
      "Epoch 13747 - lr: 0.01000 - Train loss: 0.63667 - Test loss: 0.57657\n",
      "Epoch 13748 - lr: 0.01000 - Train loss: 0.62876 - Test loss: 0.57692\n",
      "Epoch 13749 - lr: 0.01000 - Train loss: 0.62954 - Test loss: 0.57649\n",
      "Epoch 13750 - lr: 0.01000 - Train loss: 0.62157 - Test loss: 0.57694\n",
      "Epoch 13751 - lr: 0.01000 - Train loss: 0.62128 - Test loss: 0.57735\n",
      "Epoch 13752 - lr: 0.01000 - Train loss: 0.62031 - Test loss: 0.57772\n",
      "Epoch 13753 - lr: 0.01000 - Train loss: 0.63431 - Test loss: 0.57787\n",
      "Epoch 13754 - lr: 0.01000 - Train loss: 0.63122 - Test loss: 0.57733\n",
      "Epoch 13755 - lr: 0.01000 - Train loss: 0.63317 - Test loss: 0.57761\n",
      "Epoch 13756 - lr: 0.01000 - Train loss: 0.65123 - Test loss: 0.57707\n",
      "Epoch 13757 - lr: 0.01000 - Train loss: 0.63818 - Test loss: 0.57680\n",
      "Epoch 13758 - lr: 0.01000 - Train loss: 0.63658 - Test loss: 0.57663\n",
      "Epoch 13759 - lr: 0.01000 - Train loss: 0.62998 - Test loss: 0.57617\n",
      "Epoch 13760 - lr: 0.01000 - Train loss: 0.62410 - Test loss: 0.57661\n",
      "Epoch 13761 - lr: 0.01000 - Train loss: 0.63369 - Test loss: 0.57683\n",
      "Epoch 13762 - lr: 0.01000 - Train loss: 0.63757 - Test loss: 0.57671\n",
      "Epoch 13763 - lr: 0.01000 - Train loss: 0.63656 - Test loss: 0.57638\n",
      "Epoch 13764 - lr: 0.01000 - Train loss: 0.62948 - Test loss: 0.57595\n",
      "Epoch 13765 - lr: 0.01000 - Train loss: 0.62073 - Test loss: 0.57646\n",
      "Epoch 13766 - lr: 0.01000 - Train loss: 0.62696 - Test loss: 0.57674\n",
      "Epoch 13767 - lr: 0.01000 - Train loss: 0.63392 - Test loss: 0.57706\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13768 - lr: 0.01000 - Train loss: 0.64433 - Test loss: 0.57655\n",
      "Epoch 13769 - lr: 0.01000 - Train loss: 0.62815 - Test loss: 0.57619\n",
      "Epoch 13770 - lr: 0.01000 - Train loss: 0.62057 - Test loss: 0.57669\n",
      "Epoch 13771 - lr: 0.01000 - Train loss: 0.63128 - Test loss: 0.57694\n",
      "Epoch 13772 - lr: 0.01000 - Train loss: 0.62816 - Test loss: 0.57661\n",
      "Epoch 13773 - lr: 0.01000 - Train loss: 0.62035 - Test loss: 0.57707\n",
      "Epoch 13774 - lr: 0.01000 - Train loss: 0.63202 - Test loss: 0.57728\n",
      "Epoch 13775 - lr: 0.01000 - Train loss: 0.62821 - Test loss: 0.57686\n",
      "Epoch 13776 - lr: 0.01000 - Train loss: 0.62143 - Test loss: 0.57725\n",
      "Epoch 13777 - lr: 0.01000 - Train loss: 0.62900 - Test loss: 0.57745\n",
      "Epoch 13778 - lr: 0.01000 - Train loss: 0.63120 - Test loss: 0.57731\n",
      "Epoch 13779 - lr: 0.01000 - Train loss: 0.62848 - Test loss: 0.57698\n",
      "Epoch 13780 - lr: 0.01000 - Train loss: 0.62603 - Test loss: 0.57735\n",
      "Epoch 13781 - lr: 0.01000 - Train loss: 0.62893 - Test loss: 0.57706\n",
      "Epoch 13782 - lr: 0.01000 - Train loss: 0.63115 - Test loss: 0.57742\n",
      "Epoch 13783 - lr: 0.01000 - Train loss: 0.63378 - Test loss: 0.57720\n",
      "Epoch 13784 - lr: 0.01000 - Train loss: 0.62971 - Test loss: 0.57693\n",
      "Epoch 13785 - lr: 0.01000 - Train loss: 0.63321 - Test loss: 0.57710\n",
      "Epoch 13786 - lr: 0.01000 - Train loss: 0.63614 - Test loss: 0.57695\n",
      "Epoch 13787 - lr: 0.01000 - Train loss: 0.63079 - Test loss: 0.57649\n",
      "Epoch 13788 - lr: 0.01000 - Train loss: 0.63214 - Test loss: 0.57689\n",
      "Epoch 13789 - lr: 0.01000 - Train loss: 0.63486 - Test loss: 0.57657\n",
      "Epoch 13790 - lr: 0.01000 - Train loss: 0.62123 - Test loss: 0.57698\n",
      "Epoch 13791 - lr: 0.01000 - Train loss: 0.62017 - Test loss: 0.57741\n",
      "Epoch 13792 - lr: 0.01000 - Train loss: 0.63331 - Test loss: 0.57759\n",
      "Epoch 13793 - lr: 0.01000 - Train loss: 0.62937 - Test loss: 0.57709\n",
      "Epoch 13794 - lr: 0.01000 - Train loss: 0.62273 - Test loss: 0.57745\n",
      "Epoch 13795 - lr: 0.01000 - Train loss: 0.63173 - Test loss: 0.57776\n",
      "Epoch 13796 - lr: 0.01000 - Train loss: 0.63066 - Test loss: 0.57746\n",
      "Epoch 13797 - lr: 0.01000 - Train loss: 0.63274 - Test loss: 0.57756\n",
      "Epoch 13798 - lr: 0.01000 - Train loss: 0.62872 - Test loss: 0.57708\n",
      "Epoch 13799 - lr: 0.01000 - Train loss: 0.61995 - Test loss: 0.57750\n",
      "Epoch 13800 - lr: 0.01000 - Train loss: 0.63156 - Test loss: 0.57766\n",
      "Epoch 13801 - lr: 0.01000 - Train loss: 0.62834 - Test loss: 0.57723\n",
      "Epoch 13802 - lr: 0.01000 - Train loss: 0.62075 - Test loss: 0.57763\n",
      "Epoch 13803 - lr: 0.01000 - Train loss: 0.62057 - Test loss: 0.57797\n",
      "Epoch 13804 - lr: 0.01000 - Train loss: 0.63337 - Test loss: 0.57810\n",
      "Epoch 13805 - lr: 0.01000 - Train loss: 0.63067 - Test loss: 0.57757\n",
      "Epoch 13806 - lr: 0.01000 - Train loss: 0.63270 - Test loss: 0.57783\n",
      "Epoch 13807 - lr: 0.01000 - Train loss: 0.65273 - Test loss: 0.57730\n",
      "Epoch 13808 - lr: 0.01000 - Train loss: 0.63374 - Test loss: 0.57704\n",
      "Epoch 13809 - lr: 0.01000 - Train loss: 0.62923 - Test loss: 0.57677\n",
      "Epoch 13810 - lr: 0.01000 - Train loss: 0.63309 - Test loss: 0.57710\n",
      "Epoch 13811 - lr: 0.01000 - Train loss: 0.64617 - Test loss: 0.57663\n",
      "Epoch 13812 - lr: 0.01000 - Train loss: 0.62852 - Test loss: 0.57623\n",
      "Epoch 13813 - lr: 0.01000 - Train loss: 0.62008 - Test loss: 0.57676\n",
      "Epoch 13814 - lr: 0.01000 - Train loss: 0.63148 - Test loss: 0.57704\n",
      "Epoch 13815 - lr: 0.01000 - Train loss: 0.62811 - Test loss: 0.57671\n",
      "Epoch 13816 - lr: 0.01000 - Train loss: 0.62004 - Test loss: 0.57721\n",
      "Epoch 13817 - lr: 0.01000 - Train loss: 0.62885 - Test loss: 0.57742\n",
      "Epoch 13818 - lr: 0.01000 - Train loss: 0.63097 - Test loss: 0.57730\n",
      "Epoch 13819 - lr: 0.01000 - Train loss: 0.62846 - Test loss: 0.57699\n",
      "Epoch 13820 - lr: 0.01000 - Train loss: 0.62779 - Test loss: 0.57737\n",
      "Epoch 13821 - lr: 0.01000 - Train loss: 0.62954 - Test loss: 0.57696\n",
      "Epoch 13822 - lr: 0.01000 - Train loss: 0.62820 - Test loss: 0.57733\n",
      "Epoch 13823 - lr: 0.01000 - Train loss: 0.63091 - Test loss: 0.57692\n",
      "Epoch 13824 - lr: 0.01000 - Train loss: 0.63292 - Test loss: 0.57719\n",
      "Epoch 13825 - lr: 0.01000 - Train loss: 0.64086 - Test loss: 0.57672\n",
      "Epoch 13826 - lr: 0.01000 - Train loss: 0.63097 - Test loss: 0.57666\n",
      "Epoch 13827 - lr: 0.01000 - Train loss: 0.62811 - Test loss: 0.57641\n",
      "Epoch 13828 - lr: 0.01000 - Train loss: 0.62348 - Test loss: 0.57688\n",
      "Epoch 13829 - lr: 0.01000 - Train loss: 0.63300 - Test loss: 0.57714\n",
      "Epoch 13830 - lr: 0.01000 - Train loss: 0.63349 - Test loss: 0.57684\n",
      "Epoch 13831 - lr: 0.01000 - Train loss: 0.62068 - Test loss: 0.57723\n",
      "Epoch 13832 - lr: 0.01000 - Train loss: 0.63157 - Test loss: 0.57745\n",
      "Epoch 13833 - lr: 0.01000 - Train loss: 0.62844 - Test loss: 0.57707\n",
      "Epoch 13834 - lr: 0.01000 - Train loss: 0.62288 - Test loss: 0.57745\n",
      "Epoch 13835 - lr: 0.01000 - Train loss: 0.63251 - Test loss: 0.57775\n",
      "Epoch 13836 - lr: 0.01000 - Train loss: 0.65268 - Test loss: 0.57724\n",
      "Epoch 13837 - lr: 0.01000 - Train loss: 0.63192 - Test loss: 0.57698\n",
      "Epoch 13838 - lr: 0.01000 - Train loss: 0.63328 - Test loss: 0.57720\n",
      "Epoch 13839 - lr: 0.01000 - Train loss: 0.64132 - Test loss: 0.57671\n",
      "Epoch 13840 - lr: 0.01000 - Train loss: 0.63055 - Test loss: 0.57659\n",
      "Epoch 13841 - lr: 0.01000 - Train loss: 0.62893 - Test loss: 0.57643\n",
      "Epoch 13842 - lr: 0.01000 - Train loss: 0.63280 - Test loss: 0.57683\n",
      "Epoch 13843 - lr: 0.01000 - Train loss: 0.64572 - Test loss: 0.57640\n",
      "Epoch 13844 - lr: 0.01000 - Train loss: 0.62822 - Test loss: 0.57604\n",
      "Epoch 13845 - lr: 0.01000 - Train loss: 0.62035 - Test loss: 0.57658\n",
      "Epoch 13846 - lr: 0.01000 - Train loss: 0.63154 - Test loss: 0.57689\n",
      "Epoch 13847 - lr: 0.01000 - Train loss: 0.62802 - Test loss: 0.57657\n",
      "Epoch 13848 - lr: 0.01000 - Train loss: 0.61982 - Test loss: 0.57709\n",
      "Epoch 13849 - lr: 0.01000 - Train loss: 0.63156 - Test loss: 0.57733\n",
      "Epoch 13850 - lr: 0.01000 - Train loss: 0.62821 - Test loss: 0.57696\n",
      "Epoch 13851 - lr: 0.01000 - Train loss: 0.62056 - Test loss: 0.57741\n",
      "Epoch 13852 - lr: 0.01000 - Train loss: 0.62066 - Test loss: 0.57779\n",
      "Epoch 13853 - lr: 0.01000 - Train loss: 0.63197 - Test loss: 0.57794\n",
      "Epoch 13854 - lr: 0.01000 - Train loss: 0.62881 - Test loss: 0.57748\n",
      "Epoch 13855 - lr: 0.01000 - Train loss: 0.62479 - Test loss: 0.57779\n",
      "Epoch 13856 - lr: 0.01000 - Train loss: 0.63085 - Test loss: 0.57760\n",
      "Epoch 13857 - lr: 0.01000 - Train loss: 0.62867 - Test loss: 0.57727\n",
      "Epoch 13858 - lr: 0.01000 - Train loss: 0.63050 - Test loss: 0.57761\n",
      "Epoch 13859 - lr: 0.01000 - Train loss: 0.63115 - Test loss: 0.57739\n",
      "Epoch 13860 - lr: 0.01000 - Train loss: 0.63190 - Test loss: 0.57765\n",
      "Epoch 13861 - lr: 0.01000 - Train loss: 0.64491 - Test loss: 0.57709\n",
      "Epoch 13862 - lr: 0.01000 - Train loss: 0.62829 - Test loss: 0.57669\n",
      "Epoch 13863 - lr: 0.01000 - Train loss: 0.62156 - Test loss: 0.57714\n",
      "Epoch 13864 - lr: 0.01000 - Train loss: 0.62492 - Test loss: 0.57751\n",
      "Epoch 13865 - lr: 0.01000 - Train loss: 0.63062 - Test loss: 0.57733\n",
      "Epoch 13866 - lr: 0.01000 - Train loss: 0.62894 - Test loss: 0.57707\n",
      "Epoch 13867 - lr: 0.01000 - Train loss: 0.63249 - Test loss: 0.57738\n",
      "Epoch 13868 - lr: 0.01000 - Train loss: 0.65094 - Test loss: 0.57692\n",
      "Epoch 13869 - lr: 0.01000 - Train loss: 0.63677 - Test loss: 0.57677\n",
      "Epoch 13870 - lr: 0.01000 - Train loss: 0.63771 - Test loss: 0.57667\n",
      "Epoch 13871 - lr: 0.01000 - Train loss: 0.63115 - Test loss: 0.57647\n",
      "Epoch 13872 - lr: 0.01000 - Train loss: 0.62966 - Test loss: 0.57669\n",
      "Epoch 13873 - lr: 0.01000 - Train loss: 0.62894 - Test loss: 0.57653\n",
      "Epoch 13874 - lr: 0.01000 - Train loss: 0.63286 - Test loss: 0.57691\n",
      "Epoch 13875 - lr: 0.01000 - Train loss: 0.64748 - Test loss: 0.57646\n",
      "Epoch 13876 - lr: 0.01000 - Train loss: 0.63010 - Test loss: 0.57606\n",
      "Epoch 13877 - lr: 0.01000 - Train loss: 0.63008 - Test loss: 0.57653\n",
      "Epoch 13878 - lr: 0.01000 - Train loss: 0.63783 - Test loss: 0.57643\n",
      "Epoch 13879 - lr: 0.01000 - Train loss: 0.63254 - Test loss: 0.57629\n",
      "Epoch 13880 - lr: 0.01000 - Train loss: 0.63306 - Test loss: 0.57645\n",
      "Epoch 13881 - lr: 0.01000 - Train loss: 0.63814 - Test loss: 0.57634\n",
      "Epoch 13882 - lr: 0.01000 - Train loss: 0.63207 - Test loss: 0.57619\n",
      "Epoch 13883 - lr: 0.01000 - Train loss: 0.63341 - Test loss: 0.57654\n",
      "Epoch 13884 - lr: 0.01000 - Train loss: 0.64768 - Test loss: 0.57611\n",
      "Epoch 13885 - lr: 0.01000 - Train loss: 0.62906 - Test loss: 0.57567\n",
      "Epoch 13886 - lr: 0.01000 - Train loss: 0.62062 - Test loss: 0.57621\n",
      "Epoch 13887 - lr: 0.01000 - Train loss: 0.63029 - Test loss: 0.57651\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13888 - lr: 0.01000 - Train loss: 0.62819 - Test loss: 0.57628\n",
      "Epoch 13889 - lr: 0.01000 - Train loss: 0.62472 - Test loss: 0.57673\n",
      "Epoch 13890 - lr: 0.01000 - Train loss: 0.63149 - Test loss: 0.57670\n",
      "Epoch 13891 - lr: 0.01000 - Train loss: 0.62781 - Test loss: 0.57634\n",
      "Epoch 13892 - lr: 0.01000 - Train loss: 0.62185 - Test loss: 0.57676\n",
      "Epoch 13893 - lr: 0.01000 - Train loss: 0.62475 - Test loss: 0.57706\n",
      "Epoch 13894 - lr: 0.01000 - Train loss: 0.62796 - Test loss: 0.57742\n",
      "Epoch 13895 - lr: 0.01000 - Train loss: 0.62868 - Test loss: 0.57692\n",
      "Epoch 13896 - lr: 0.01000 - Train loss: 0.62005 - Test loss: 0.57734\n",
      "Epoch 13897 - lr: 0.01000 - Train loss: 0.63322 - Test loss: 0.57754\n",
      "Epoch 13898 - lr: 0.01000 - Train loss: 0.62930 - Test loss: 0.57704\n",
      "Epoch 13899 - lr: 0.01000 - Train loss: 0.62339 - Test loss: 0.57740\n",
      "Epoch 13900 - lr: 0.01000 - Train loss: 0.63346 - Test loss: 0.57760\n",
      "Epoch 13901 - lr: 0.01000 - Train loss: 0.64209 - Test loss: 0.57703\n",
      "Epoch 13902 - lr: 0.01000 - Train loss: 0.62943 - Test loss: 0.57678\n",
      "Epoch 13903 - lr: 0.01000 - Train loss: 0.63298 - Test loss: 0.57696\n",
      "Epoch 13904 - lr: 0.01000 - Train loss: 0.63790 - Test loss: 0.57686\n",
      "Epoch 13905 - lr: 0.01000 - Train loss: 0.63119 - Test loss: 0.57666\n",
      "Epoch 13906 - lr: 0.01000 - Train loss: 0.62808 - Test loss: 0.57699\n",
      "Epoch 13907 - lr: 0.01000 - Train loss: 0.62858 - Test loss: 0.57654\n",
      "Epoch 13908 - lr: 0.01000 - Train loss: 0.62068 - Test loss: 0.57698\n",
      "Epoch 13909 - lr: 0.01000 - Train loss: 0.63088 - Test loss: 0.57720\n",
      "Epoch 13910 - lr: 0.01000 - Train loss: 0.62790 - Test loss: 0.57684\n",
      "Epoch 13911 - lr: 0.01000 - Train loss: 0.62019 - Test loss: 0.57729\n",
      "Epoch 13912 - lr: 0.01000 - Train loss: 0.62547 - Test loss: 0.57751\n",
      "Epoch 13913 - lr: 0.01000 - Train loss: 0.63105 - Test loss: 0.57783\n",
      "Epoch 13914 - lr: 0.01000 - Train loss: 0.63228 - Test loss: 0.57753\n",
      "Epoch 13915 - lr: 0.01000 - Train loss: 0.63349 - Test loss: 0.57759\n",
      "Epoch 13916 - lr: 0.01000 - Train loss: 0.63003 - Test loss: 0.57732\n",
      "Epoch 13917 - lr: 0.01000 - Train loss: 0.62507 - Test loss: 0.57746\n",
      "Epoch 13918 - lr: 0.01000 - Train loss: 0.62921 - Test loss: 0.57777\n",
      "Epoch 13919 - lr: 0.01000 - Train loss: 0.63369 - Test loss: 0.57727\n",
      "Epoch 13920 - lr: 0.01000 - Train loss: 0.62852 - Test loss: 0.57692\n",
      "Epoch 13921 - lr: 0.01000 - Train loss: 0.62998 - Test loss: 0.57730\n",
      "Epoch 13922 - lr: 0.01000 - Train loss: 0.63817 - Test loss: 0.57709\n",
      "Epoch 13923 - lr: 0.01000 - Train loss: 0.63336 - Test loss: 0.57673\n",
      "Epoch 13924 - lr: 0.01000 - Train loss: 0.62151 - Test loss: 0.57705\n",
      "Epoch 13925 - lr: 0.01000 - Train loss: 0.62729 - Test loss: 0.57726\n",
      "Epoch 13926 - lr: 0.01000 - Train loss: 0.63327 - Test loss: 0.57739\n",
      "Epoch 13927 - lr: 0.01000 - Train loss: 0.63614 - Test loss: 0.57718\n",
      "Epoch 13928 - lr: 0.01000 - Train loss: 0.63037 - Test loss: 0.57665\n",
      "Epoch 13929 - lr: 0.01000 - Train loss: 0.63020 - Test loss: 0.57703\n",
      "Epoch 13930 - lr: 0.01000 - Train loss: 0.63834 - Test loss: 0.57685\n",
      "Epoch 13931 - lr: 0.01000 - Train loss: 0.63144 - Test loss: 0.57658\n",
      "Epoch 13932 - lr: 0.01000 - Train loss: 0.62921 - Test loss: 0.57676\n",
      "Epoch 13933 - lr: 0.01000 - Train loss: 0.62971 - Test loss: 0.57661\n",
      "Epoch 13934 - lr: 0.01000 - Train loss: 0.63238 - Test loss: 0.57669\n",
      "Epoch 13935 - lr: 0.01000 - Train loss: 0.63059 - Test loss: 0.57628\n",
      "Epoch 13936 - lr: 0.01000 - Train loss: 0.63179 - Test loss: 0.57671\n",
      "Epoch 13937 - lr: 0.01000 - Train loss: 0.63066 - Test loss: 0.57654\n",
      "Epoch 13938 - lr: 0.01000 - Train loss: 0.63004 - Test loss: 0.57673\n",
      "Epoch 13939 - lr: 0.01000 - Train loss: 0.62830 - Test loss: 0.57649\n",
      "Epoch 13940 - lr: 0.01000 - Train loss: 0.62804 - Test loss: 0.57691\n",
      "Epoch 13941 - lr: 0.01000 - Train loss: 0.62863 - Test loss: 0.57647\n",
      "Epoch 13942 - lr: 0.01000 - Train loss: 0.62022 - Test loss: 0.57693\n",
      "Epoch 13943 - lr: 0.01000 - Train loss: 0.63203 - Test loss: 0.57716\n",
      "Epoch 13944 - lr: 0.01000 - Train loss: 0.62797 - Test loss: 0.57673\n",
      "Epoch 13945 - lr: 0.01000 - Train loss: 0.62124 - Test loss: 0.57713\n",
      "Epoch 13946 - lr: 0.01000 - Train loss: 0.62753 - Test loss: 0.57734\n",
      "Epoch 13947 - lr: 0.01000 - Train loss: 0.63269 - Test loss: 0.57741\n",
      "Epoch 13948 - lr: 0.01000 - Train loss: 0.63702 - Test loss: 0.57712\n",
      "Epoch 13949 - lr: 0.01000 - Train loss: 0.63840 - Test loss: 0.57695\n",
      "Epoch 13950 - lr: 0.01000 - Train loss: 0.64538 - Test loss: 0.57643\n",
      "Epoch 13951 - lr: 0.01000 - Train loss: 0.62784 - Test loss: 0.57602\n",
      "Epoch 13952 - lr: 0.01000 - Train loss: 0.62285 - Test loss: 0.57645\n",
      "Epoch 13953 - lr: 0.01000 - Train loss: 0.61994 - Test loss: 0.57697\n",
      "Epoch 13954 - lr: 0.01000 - Train loss: 0.63014 - Test loss: 0.57719\n",
      "Epoch 13955 - lr: 0.01000 - Train loss: 0.62821 - Test loss: 0.57689\n",
      "Epoch 13956 - lr: 0.01000 - Train loss: 0.62724 - Test loss: 0.57727\n",
      "Epoch 13957 - lr: 0.01000 - Train loss: 0.62809 - Test loss: 0.57684\n",
      "Epoch 13958 - lr: 0.01000 - Train loss: 0.61997 - Test loss: 0.57728\n",
      "Epoch 13959 - lr: 0.01000 - Train loss: 0.63282 - Test loss: 0.57750\n",
      "Epoch 13960 - lr: 0.01000 - Train loss: 0.62914 - Test loss: 0.57703\n",
      "Epoch 13961 - lr: 0.01000 - Train loss: 0.62483 - Test loss: 0.57739\n",
      "Epoch 13962 - lr: 0.01000 - Train loss: 0.63010 - Test loss: 0.57721\n",
      "Epoch 13963 - lr: 0.01000 - Train loss: 0.62992 - Test loss: 0.57704\n",
      "Epoch 13964 - lr: 0.01000 - Train loss: 0.63006 - Test loss: 0.57692\n",
      "Epoch 13965 - lr: 0.01000 - Train loss: 0.62919 - Test loss: 0.57676\n",
      "Epoch 13966 - lr: 0.01000 - Train loss: 0.63213 - Test loss: 0.57691\n",
      "Epoch 13967 - lr: 0.01000 - Train loss: 0.63677 - Test loss: 0.57675\n",
      "Epoch 13968 - lr: 0.01000 - Train loss: 0.63412 - Test loss: 0.57661\n",
      "Epoch 13969 - lr: 0.01000 - Train loss: 0.62774 - Test loss: 0.57628\n",
      "Epoch 13970 - lr: 0.01000 - Train loss: 0.61992 - Test loss: 0.57682\n",
      "Epoch 13971 - lr: 0.01000 - Train loss: 0.62513 - Test loss: 0.57712\n",
      "Epoch 13972 - lr: 0.01000 - Train loss: 0.63027 - Test loss: 0.57749\n",
      "Epoch 13973 - lr: 0.01000 - Train loss: 0.63483 - Test loss: 0.57728\n",
      "Epoch 13974 - lr: 0.01000 - Train loss: 0.62867 - Test loss: 0.57680\n",
      "Epoch 13975 - lr: 0.01000 - Train loss: 0.62224 - Test loss: 0.57720\n",
      "Epoch 13976 - lr: 0.01000 - Train loss: 0.63106 - Test loss: 0.57754\n",
      "Epoch 13977 - lr: 0.01000 - Train loss: 0.63075 - Test loss: 0.57726\n",
      "Epoch 13978 - lr: 0.01000 - Train loss: 0.63126 - Test loss: 0.57739\n",
      "Epoch 13979 - lr: 0.01000 - Train loss: 0.62793 - Test loss: 0.57698\n",
      "Epoch 13980 - lr: 0.01000 - Train loss: 0.62016 - Test loss: 0.57741\n",
      "Epoch 13981 - lr: 0.01000 - Train loss: 0.62140 - Test loss: 0.57773\n",
      "Epoch 13982 - lr: 0.01000 - Train loss: 0.62661 - Test loss: 0.57788\n",
      "Epoch 13983 - lr: 0.01000 - Train loss: 0.63295 - Test loss: 0.57804\n",
      "Epoch 13984 - lr: 0.01000 - Train loss: 0.65084 - Test loss: 0.57745\n",
      "Epoch 13985 - lr: 0.01000 - Train loss: 0.63733 - Test loss: 0.57723\n",
      "Epoch 13986 - lr: 0.01000 - Train loss: 0.62963 - Test loss: 0.57699\n",
      "Epoch 13987 - lr: 0.01000 - Train loss: 0.63009 - Test loss: 0.57714\n",
      "Epoch 13988 - lr: 0.01000 - Train loss: 0.62810 - Test loss: 0.57685\n",
      "Epoch 13989 - lr: 0.01000 - Train loss: 0.62737 - Test loss: 0.57724\n",
      "Epoch 13990 - lr: 0.01000 - Train loss: 0.62855 - Test loss: 0.57681\n",
      "Epoch 13991 - lr: 0.01000 - Train loss: 0.62132 - Test loss: 0.57723\n",
      "Epoch 13992 - lr: 0.01000 - Train loss: 0.62447 - Test loss: 0.57757\n",
      "Epoch 13993 - lr: 0.01000 - Train loss: 0.63098 - Test loss: 0.57741\n",
      "Epoch 13994 - lr: 0.01000 - Train loss: 0.62794 - Test loss: 0.57702\n",
      "Epoch 13995 - lr: 0.01000 - Train loss: 0.62224 - Test loss: 0.57741\n",
      "Epoch 13996 - lr: 0.01000 - Train loss: 0.63132 - Test loss: 0.57773\n",
      "Epoch 13997 - lr: 0.01000 - Train loss: 0.64032 - Test loss: 0.57716\n",
      "Epoch 13998 - lr: 0.01000 - Train loss: 0.63121 - Test loss: 0.57705\n",
      "Epoch 13999 - lr: 0.01000 - Train loss: 0.62822 - Test loss: 0.57667\n",
      "Epoch 14000 - lr: 0.01000 - Train loss: 0.62155 - Test loss: 0.57711\n",
      "Epoch 14001 - lr: 0.01000 - Train loss: 0.62714 - Test loss: 0.57747\n",
      "Epoch 14002 - lr: 0.01000 - Train loss: 0.62880 - Test loss: 0.57703\n",
      "Epoch 14003 - lr: 0.01000 - Train loss: 0.62578 - Test loss: 0.57738\n",
      "Epoch 14004 - lr: 0.01000 - Train loss: 0.62821 - Test loss: 0.57704\n",
      "Epoch 14005 - lr: 0.01000 - Train loss: 0.62824 - Test loss: 0.57740\n",
      "Epoch 14006 - lr: 0.01000 - Train loss: 0.63369 - Test loss: 0.57703\n",
      "Epoch 14007 - lr: 0.01000 - Train loss: 0.62814 - Test loss: 0.57666\n",
      "Epoch 14008 - lr: 0.01000 - Train loss: 0.62422 - Test loss: 0.57707\n",
      "Epoch 14009 - lr: 0.01000 - Train loss: 0.63065 - Test loss: 0.57699\n",
      "Epoch 14010 - lr: 0.01000 - Train loss: 0.62810 - Test loss: 0.57668\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14011 - lr: 0.01000 - Train loss: 0.62651 - Test loss: 0.57708\n",
      "Epoch 14012 - lr: 0.01000 - Train loss: 0.62861 - Test loss: 0.57674\n",
      "Epoch 14013 - lr: 0.01000 - Train loss: 0.62835 - Test loss: 0.57713\n",
      "Epoch 14014 - lr: 0.01000 - Train loss: 0.63549 - Test loss: 0.57691\n",
      "Epoch 14015 - lr: 0.01000 - Train loss: 0.63695 - Test loss: 0.57680\n",
      "Epoch 14016 - lr: 0.01000 - Train loss: 0.64798 - Test loss: 0.57633\n",
      "Epoch 14017 - lr: 0.01000 - Train loss: 0.63528 - Test loss: 0.57610\n",
      "Epoch 14018 - lr: 0.01000 - Train loss: 0.63358 - Test loss: 0.57587\n",
      "Epoch 14019 - lr: 0.01000 - Train loss: 0.62752 - Test loss: 0.57565\n",
      "Epoch 14020 - lr: 0.01000 - Train loss: 0.62001 - Test loss: 0.57625\n",
      "Epoch 14021 - lr: 0.01000 - Train loss: 0.62180 - Test loss: 0.57671\n",
      "Epoch 14022 - lr: 0.01000 - Train loss: 0.62217 - Test loss: 0.57709\n",
      "Epoch 14023 - lr: 0.01000 - Train loss: 0.62027 - Test loss: 0.57749\n",
      "Epoch 14024 - lr: 0.01000 - Train loss: 0.63149 - Test loss: 0.57766\n",
      "Epoch 14025 - lr: 0.01000 - Train loss: 0.62879 - Test loss: 0.57721\n",
      "Epoch 14026 - lr: 0.01000 - Train loss: 0.62847 - Test loss: 0.57753\n",
      "Epoch 14027 - lr: 0.01000 - Train loss: 0.63603 - Test loss: 0.57725\n",
      "Epoch 14028 - lr: 0.01000 - Train loss: 0.63474 - Test loss: 0.57703\n",
      "Epoch 14029 - lr: 0.01000 - Train loss: 0.63098 - Test loss: 0.57659\n",
      "Epoch 14030 - lr: 0.01000 - Train loss: 0.63187 - Test loss: 0.57669\n",
      "Epoch 14031 - lr: 0.01000 - Train loss: 0.63660 - Test loss: 0.57655\n",
      "Epoch 14032 - lr: 0.01000 - Train loss: 0.62925 - Test loss: 0.57641\n",
      "Epoch 14033 - lr: 0.01000 - Train loss: 0.63151 - Test loss: 0.57664\n",
      "Epoch 14034 - lr: 0.01000 - Train loss: 0.62810 - Test loss: 0.57631\n",
      "Epoch 14035 - lr: 0.01000 - Train loss: 0.62187 - Test loss: 0.57678\n",
      "Epoch 14036 - lr: 0.01000 - Train loss: 0.62989 - Test loss: 0.57716\n",
      "Epoch 14037 - lr: 0.01000 - Train loss: 0.63452 - Test loss: 0.57697\n",
      "Epoch 14038 - lr: 0.01000 - Train loss: 0.62886 - Test loss: 0.57650\n",
      "Epoch 14039 - lr: 0.01000 - Train loss: 0.62602 - Test loss: 0.57689\n",
      "Epoch 14040 - lr: 0.01000 - Train loss: 0.62784 - Test loss: 0.57656\n",
      "Epoch 14041 - lr: 0.01000 - Train loss: 0.62339 - Test loss: 0.57697\n",
      "Epoch 14042 - lr: 0.01000 - Train loss: 0.63229 - Test loss: 0.57707\n",
      "Epoch 14043 - lr: 0.01000 - Train loss: 0.63602 - Test loss: 0.57692\n",
      "Epoch 14044 - lr: 0.01000 - Train loss: 0.63730 - Test loss: 0.57676\n",
      "Epoch 14045 - lr: 0.01000 - Train loss: 0.64409 - Test loss: 0.57624\n",
      "Epoch 14046 - lr: 0.01000 - Train loss: 0.62750 - Test loss: 0.57588\n",
      "Epoch 14047 - lr: 0.01000 - Train loss: 0.61942 - Test loss: 0.57644\n",
      "Epoch 14048 - lr: 0.01000 - Train loss: 0.63268 - Test loss: 0.57674\n",
      "Epoch 14049 - lr: 0.01000 - Train loss: 0.62979 - Test loss: 0.57636\n",
      "Epoch 14050 - lr: 0.01000 - Train loss: 0.63134 - Test loss: 0.57677\n",
      "Epoch 14051 - lr: 0.01000 - Train loss: 0.64257 - Test loss: 0.57630\n",
      "Epoch 14052 - lr: 0.01000 - Train loss: 0.62765 - Test loss: 0.57603\n",
      "Epoch 14053 - lr: 0.01000 - Train loss: 0.62457 - Test loss: 0.57650\n",
      "Epoch 14054 - lr: 0.01000 - Train loss: 0.62972 - Test loss: 0.57641\n",
      "Epoch 14055 - lr: 0.01000 - Train loss: 0.62968 - Test loss: 0.57633\n",
      "Epoch 14056 - lr: 0.01000 - Train loss: 0.62937 - Test loss: 0.57625\n",
      "Epoch 14057 - lr: 0.01000 - Train loss: 0.63034 - Test loss: 0.57626\n",
      "Epoch 14058 - lr: 0.01000 - Train loss: 0.62754 - Test loss: 0.57604\n",
      "Epoch 14059 - lr: 0.01000 - Train loss: 0.62315 - Test loss: 0.57653\n",
      "Epoch 14060 - lr: 0.01000 - Train loss: 0.63232 - Test loss: 0.57675\n",
      "Epoch 14061 - lr: 0.01000 - Train loss: 0.63054 - Test loss: 0.57661\n",
      "Epoch 14062 - lr: 0.01000 - Train loss: 0.63101 - Test loss: 0.57695\n",
      "Epoch 14063 - lr: 0.01000 - Train loss: 0.63671 - Test loss: 0.57653\n",
      "Epoch 14064 - lr: 0.01000 - Train loss: 0.63118 - Test loss: 0.57688\n",
      "Epoch 14065 - lr: 0.01000 - Train loss: 0.63779 - Test loss: 0.57644\n",
      "Epoch 14066 - lr: 0.01000 - Train loss: 0.63273 - Test loss: 0.57673\n",
      "Epoch 14067 - lr: 0.01000 - Train loss: 0.65010 - Test loss: 0.57629\n",
      "Epoch 14068 - lr: 0.01000 - Train loss: 0.63793 - Test loss: 0.57618\n",
      "Epoch 14069 - lr: 0.01000 - Train loss: 0.64267 - Test loss: 0.57575\n",
      "Epoch 14070 - lr: 0.01000 - Train loss: 0.62746 - Test loss: 0.57553\n",
      "Epoch 14071 - lr: 0.01000 - Train loss: 0.62120 - Test loss: 0.57609\n",
      "Epoch 14072 - lr: 0.01000 - Train loss: 0.62348 - Test loss: 0.57657\n",
      "Epoch 14073 - lr: 0.01000 - Train loss: 0.63296 - Test loss: 0.57676\n",
      "Epoch 14074 - lr: 0.01000 - Train loss: 0.63667 - Test loss: 0.57662\n",
      "Epoch 14075 - lr: 0.01000 - Train loss: 0.63643 - Test loss: 0.57632\n",
      "Epoch 14076 - lr: 0.01000 - Train loss: 0.63274 - Test loss: 0.57593\n",
      "Epoch 14077 - lr: 0.01000 - Train loss: 0.62940 - Test loss: 0.57584\n",
      "Epoch 14078 - lr: 0.01000 - Train loss: 0.63142 - Test loss: 0.57596\n",
      "Epoch 14079 - lr: 0.01000 - Train loss: 0.62777 - Test loss: 0.57562\n",
      "Epoch 14080 - lr: 0.01000 - Train loss: 0.62251 - Test loss: 0.57608\n",
      "Epoch 14081 - lr: 0.01000 - Train loss: 0.61963 - Test loss: 0.57662\n",
      "Epoch 14082 - lr: 0.01000 - Train loss: 0.63103 - Test loss: 0.57686\n",
      "Epoch 14083 - lr: 0.01000 - Train loss: 0.62740 - Test loss: 0.57649\n",
      "Epoch 14084 - lr: 0.01000 - Train loss: 0.62058 - Test loss: 0.57692\n",
      "Epoch 14085 - lr: 0.01000 - Train loss: 0.62923 - Test loss: 0.57713\n",
      "Epoch 14086 - lr: 0.01000 - Train loss: 0.62861 - Test loss: 0.57687\n",
      "Epoch 14087 - lr: 0.01000 - Train loss: 0.63264 - Test loss: 0.57719\n",
      "Epoch 14088 - lr: 0.01000 - Train loss: 0.65376 - Test loss: 0.57675\n",
      "Epoch 14089 - lr: 0.01000 - Train loss: 0.63139 - Test loss: 0.57646\n",
      "Epoch 14090 - lr: 0.01000 - Train loss: 0.62719 - Test loss: 0.57668\n",
      "Epoch 14091 - lr: 0.01000 - Train loss: 0.63234 - Test loss: 0.57681\n",
      "Epoch 14092 - lr: 0.01000 - Train loss: 0.63485 - Test loss: 0.57650\n",
      "Epoch 14093 - lr: 0.01000 - Train loss: 0.62778 - Test loss: 0.57608\n",
      "Epoch 14094 - lr: 0.01000 - Train loss: 0.62123 - Test loss: 0.57655\n",
      "Epoch 14095 - lr: 0.01000 - Train loss: 0.62492 - Test loss: 0.57686\n",
      "Epoch 14096 - lr: 0.01000 - Train loss: 0.63059 - Test loss: 0.57726\n",
      "Epoch 14097 - lr: 0.01000 - Train loss: 0.63369 - Test loss: 0.57705\n",
      "Epoch 14098 - lr: 0.01000 - Train loss: 0.62783 - Test loss: 0.57667\n",
      "Epoch 14099 - lr: 0.01000 - Train loss: 0.62474 - Test loss: 0.57706\n",
      "Epoch 14100 - lr: 0.01000 - Train loss: 0.62940 - Test loss: 0.57687\n",
      "Epoch 14101 - lr: 0.01000 - Train loss: 0.63160 - Test loss: 0.57688\n",
      "Epoch 14102 - lr: 0.01000 - Train loss: 0.62909 - Test loss: 0.57646\n",
      "Epoch 14103 - lr: 0.01000 - Train loss: 0.62592 - Test loss: 0.57687\n",
      "Epoch 14104 - lr: 0.01000 - Train loss: 0.62746 - Test loss: 0.57656\n",
      "Epoch 14105 - lr: 0.01000 - Train loss: 0.62009 - Test loss: 0.57703\n",
      "Epoch 14106 - lr: 0.01000 - Train loss: 0.62030 - Test loss: 0.57743\n",
      "Epoch 14107 - lr: 0.01000 - Train loss: 0.63077 - Test loss: 0.57759\n",
      "Epoch 14108 - lr: 0.01000 - Train loss: 0.62765 - Test loss: 0.57717\n",
      "Epoch 14109 - lr: 0.01000 - Train loss: 0.62034 - Test loss: 0.57757\n",
      "Epoch 14110 - lr: 0.01000 - Train loss: 0.61901 - Test loss: 0.57796\n",
      "Epoch 14111 - lr: 0.01000 - Train loss: 0.63223 - Test loss: 0.57807\n",
      "Epoch 14112 - lr: 0.01000 - Train loss: 0.62940 - Test loss: 0.57753\n",
      "Epoch 14113 - lr: 0.01000 - Train loss: 0.63047 - Test loss: 0.57783\n",
      "Epoch 14114 - lr: 0.01000 - Train loss: 0.63070 - Test loss: 0.57749\n",
      "Epoch 14115 - lr: 0.01000 - Train loss: 0.63015 - Test loss: 0.57760\n",
      "Epoch 14116 - lr: 0.01000 - Train loss: 0.62784 - Test loss: 0.57722\n",
      "Epoch 14117 - lr: 0.01000 - Train loss: 0.62616 - Test loss: 0.57756\n",
      "Epoch 14118 - lr: 0.01000 - Train loss: 0.62796 - Test loss: 0.57715\n",
      "Epoch 14119 - lr: 0.01000 - Train loss: 0.62387 - Test loss: 0.57750\n",
      "Epoch 14120 - lr: 0.01000 - Train loss: 0.63083 - Test loss: 0.57739\n",
      "Epoch 14121 - lr: 0.01000 - Train loss: 0.62843 - Test loss: 0.57698\n",
      "Epoch 14122 - lr: 0.01000 - Train loss: 0.62744 - Test loss: 0.57735\n",
      "Epoch 14123 - lr: 0.01000 - Train loss: 0.63152 - Test loss: 0.57696\n",
      "Epoch 14124 - lr: 0.01000 - Train loss: 0.62957 - Test loss: 0.57683\n",
      "Epoch 14125 - lr: 0.01000 - Train loss: 0.62875 - Test loss: 0.57667\n",
      "Epoch 14126 - lr: 0.01000 - Train loss: 0.63132 - Test loss: 0.57680\n",
      "Epoch 14127 - lr: 0.01000 - Train loss: 0.63575 - Test loss: 0.57667\n",
      "Epoch 14128 - lr: 0.01000 - Train loss: 0.63034 - Test loss: 0.57654\n",
      "Epoch 14129 - lr: 0.01000 - Train loss: 0.63130 - Test loss: 0.57689\n",
      "Epoch 14130 - lr: 0.01000 - Train loss: 0.64821 - Test loss: 0.57646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14131 - lr: 0.01000 - Train loss: 0.63612 - Test loss: 0.57628\n",
      "Epoch 14132 - lr: 0.01000 - Train loss: 0.63545 - Test loss: 0.57621\n",
      "Epoch 14133 - lr: 0.01000 - Train loss: 0.63421 - Test loss: 0.57596\n",
      "Epoch 14134 - lr: 0.01000 - Train loss: 0.62775 - Test loss: 0.57566\n",
      "Epoch 14135 - lr: 0.01000 - Train loss: 0.61946 - Test loss: 0.57626\n",
      "Epoch 14136 - lr: 0.01000 - Train loss: 0.62645 - Test loss: 0.57659\n",
      "Epoch 14137 - lr: 0.01000 - Train loss: 0.63236 - Test loss: 0.57683\n",
      "Epoch 14138 - lr: 0.01000 - Train loss: 0.62912 - Test loss: 0.57669\n",
      "Epoch 14139 - lr: 0.01000 - Train loss: 0.62145 - Test loss: 0.57701\n",
      "Epoch 14140 - lr: 0.01000 - Train loss: 0.62356 - Test loss: 0.57728\n",
      "Epoch 14141 - lr: 0.01000 - Train loss: 0.62333 - Test loss: 0.57761\n",
      "Epoch 14142 - lr: 0.01000 - Train loss: 0.63235 - Test loss: 0.57762\n",
      "Epoch 14143 - lr: 0.01000 - Train loss: 0.63697 - Test loss: 0.57741\n",
      "Epoch 14144 - lr: 0.01000 - Train loss: 0.63272 - Test loss: 0.57698\n",
      "Epoch 14145 - lr: 0.01000 - Train loss: 0.62093 - Test loss: 0.57727\n",
      "Epoch 14146 - lr: 0.01000 - Train loss: 0.62715 - Test loss: 0.57743\n",
      "Epoch 14147 - lr: 0.01000 - Train loss: 0.63209 - Test loss: 0.57744\n",
      "Epoch 14148 - lr: 0.01000 - Train loss: 0.63668 - Test loss: 0.57716\n",
      "Epoch 14149 - lr: 0.01000 - Train loss: 0.63203 - Test loss: 0.57690\n",
      "Epoch 14150 - lr: 0.01000 - Train loss: 0.63132 - Test loss: 0.57682\n",
      "Epoch 14151 - lr: 0.01000 - Train loss: 0.62830 - Test loss: 0.57639\n",
      "Epoch 14152 - lr: 0.01000 - Train loss: 0.62138 - Test loss: 0.57684\n",
      "Epoch 14153 - lr: 0.01000 - Train loss: 0.62783 - Test loss: 0.57721\n",
      "Epoch 14154 - lr: 0.01000 - Train loss: 0.63061 - Test loss: 0.57674\n",
      "Epoch 14155 - lr: 0.01000 - Train loss: 0.63253 - Test loss: 0.57696\n",
      "Epoch 14156 - lr: 0.01000 - Train loss: 0.63645 - Test loss: 0.57653\n",
      "Epoch 14157 - lr: 0.01000 - Train loss: 0.63096 - Test loss: 0.57689\n",
      "Epoch 14158 - lr: 0.01000 - Train loss: 0.63320 - Test loss: 0.57656\n",
      "Epoch 14159 - lr: 0.01000 - Train loss: 0.61937 - Test loss: 0.57697\n",
      "Epoch 14160 - lr: 0.01000 - Train loss: 0.63303 - Test loss: 0.57720\n",
      "Epoch 14161 - lr: 0.01000 - Train loss: 0.63009 - Test loss: 0.57673\n",
      "Epoch 14162 - lr: 0.01000 - Train loss: 0.63210 - Test loss: 0.57707\n",
      "Epoch 14163 - lr: 0.01000 - Train loss: 0.65362 - Test loss: 0.57664\n",
      "Epoch 14164 - lr: 0.01000 - Train loss: 0.63276 - Test loss: 0.57630\n",
      "Epoch 14165 - lr: 0.01000 - Train loss: 0.62042 - Test loss: 0.57670\n",
      "Epoch 14166 - lr: 0.01000 - Train loss: 0.62929 - Test loss: 0.57693\n",
      "Epoch 14167 - lr: 0.01000 - Train loss: 0.62802 - Test loss: 0.57668\n",
      "Epoch 14168 - lr: 0.01000 - Train loss: 0.63035 - Test loss: 0.57708\n",
      "Epoch 14169 - lr: 0.01000 - Train loss: 0.63414 - Test loss: 0.57689\n",
      "Epoch 14170 - lr: 0.01000 - Train loss: 0.62725 - Test loss: 0.57645\n",
      "Epoch 14171 - lr: 0.01000 - Train loss: 0.62082 - Test loss: 0.57686\n",
      "Epoch 14172 - lr: 0.01000 - Train loss: 0.62666 - Test loss: 0.57710\n",
      "Epoch 14173 - lr: 0.01000 - Train loss: 0.63262 - Test loss: 0.57724\n",
      "Epoch 14174 - lr: 0.01000 - Train loss: 0.63500 - Test loss: 0.57703\n",
      "Epoch 14175 - lr: 0.01000 - Train loss: 0.62832 - Test loss: 0.57651\n",
      "Epoch 14176 - lr: 0.01000 - Train loss: 0.61945 - Test loss: 0.57698\n",
      "Epoch 14177 - lr: 0.01000 - Train loss: 0.62745 - Test loss: 0.57718\n",
      "Epoch 14178 - lr: 0.01000 - Train loss: 0.63148 - Test loss: 0.57716\n",
      "Epoch 14179 - lr: 0.01000 - Train loss: 0.62893 - Test loss: 0.57668\n",
      "Epoch 14180 - lr: 0.01000 - Train loss: 0.62577 - Test loss: 0.57707\n",
      "Epoch 14181 - lr: 0.01000 - Train loss: 0.62733 - Test loss: 0.57671\n",
      "Epoch 14182 - lr: 0.01000 - Train loss: 0.61981 - Test loss: 0.57717\n",
      "Epoch 14183 - lr: 0.01000 - Train loss: 0.62075 - Test loss: 0.57752\n",
      "Epoch 14184 - lr: 0.01000 - Train loss: 0.62750 - Test loss: 0.57766\n",
      "Epoch 14185 - lr: 0.01000 - Train loss: 0.63132 - Test loss: 0.57757\n",
      "Epoch 14186 - lr: 0.01000 - Train loss: 0.62969 - Test loss: 0.57707\n",
      "Epoch 14187 - lr: 0.01000 - Train loss: 0.63162 - Test loss: 0.57740\n",
      "Epoch 14188 - lr: 0.01000 - Train loss: 0.65201 - Test loss: 0.57692\n",
      "Epoch 14189 - lr: 0.01000 - Train loss: 0.62977 - Test loss: 0.57670\n",
      "Epoch 14190 - lr: 0.01000 - Train loss: 0.62229 - Test loss: 0.57706\n",
      "Epoch 14191 - lr: 0.01000 - Train loss: 0.63219 - Test loss: 0.57739\n",
      "Epoch 14192 - lr: 0.01000 - Train loss: 0.65418 - Test loss: 0.57692\n",
      "Epoch 14193 - lr: 0.01000 - Train loss: 0.63434 - Test loss: 0.57651\n",
      "Epoch 14194 - lr: 0.01000 - Train loss: 0.62218 - Test loss: 0.57689\n",
      "Epoch 14195 - lr: 0.01000 - Train loss: 0.63200 - Test loss: 0.57726\n",
      "Epoch 14196 - lr: 0.01000 - Train loss: 0.64649 - Test loss: 0.57672\n",
      "Epoch 14197 - lr: 0.01000 - Train loss: 0.62812 - Test loss: 0.57622\n",
      "Epoch 14198 - lr: 0.01000 - Train loss: 0.61984 - Test loss: 0.57671\n",
      "Epoch 14199 - lr: 0.01000 - Train loss: 0.63058 - Test loss: 0.57696\n",
      "Epoch 14200 - lr: 0.01000 - Train loss: 0.62709 - Test loss: 0.57660\n",
      "Epoch 14201 - lr: 0.01000 - Train loss: 0.62042 - Test loss: 0.57703\n",
      "Epoch 14202 - lr: 0.01000 - Train loss: 0.62825 - Test loss: 0.57725\n",
      "Epoch 14203 - lr: 0.01000 - Train loss: 0.62965 - Test loss: 0.57709\n",
      "Epoch 14204 - lr: 0.01000 - Train loss: 0.62917 - Test loss: 0.57691\n",
      "Epoch 14205 - lr: 0.01000 - Train loss: 0.63080 - Test loss: 0.57689\n",
      "Epoch 14206 - lr: 0.01000 - Train loss: 0.62733 - Test loss: 0.57651\n",
      "Epoch 14207 - lr: 0.01000 - Train loss: 0.61936 - Test loss: 0.57702\n",
      "Epoch 14208 - lr: 0.01000 - Train loss: 0.63191 - Test loss: 0.57728\n",
      "Epoch 14209 - lr: 0.01000 - Train loss: 0.62820 - Test loss: 0.57685\n",
      "Epoch 14210 - lr: 0.01000 - Train loss: 0.62202 - Test loss: 0.57727\n",
      "Epoch 14211 - lr: 0.01000 - Train loss: 0.63179 - Test loss: 0.57760\n",
      "Epoch 14212 - lr: 0.01000 - Train loss: 0.65124 - Test loss: 0.57709\n",
      "Epoch 14213 - lr: 0.01000 - Train loss: 0.63593 - Test loss: 0.57690\n",
      "Epoch 14214 - lr: 0.01000 - Train loss: 0.63492 - Test loss: 0.57657\n",
      "Epoch 14215 - lr: 0.01000 - Train loss: 0.62822 - Test loss: 0.57616\n",
      "Epoch 14216 - lr: 0.01000 - Train loss: 0.62013 - Test loss: 0.57668\n",
      "Epoch 14217 - lr: 0.01000 - Train loss: 0.61919 - Test loss: 0.57717\n",
      "Epoch 14218 - lr: 0.01000 - Train loss: 0.63228 - Test loss: 0.57740\n",
      "Epoch 14219 - lr: 0.01000 - Train loss: 0.62839 - Test loss: 0.57692\n",
      "Epoch 14220 - lr: 0.01000 - Train loss: 0.62202 - Test loss: 0.57732\n",
      "Epoch 14221 - lr: 0.01000 - Train loss: 0.63187 - Test loss: 0.57764\n",
      "Epoch 14222 - lr: 0.01000 - Train loss: 0.65118 - Test loss: 0.57711\n",
      "Epoch 14223 - lr: 0.01000 - Train loss: 0.63712 - Test loss: 0.57693\n",
      "Epoch 14224 - lr: 0.01000 - Train loss: 0.63312 - Test loss: 0.57672\n",
      "Epoch 14225 - lr: 0.01000 - Train loss: 0.62828 - Test loss: 0.57646\n",
      "Epoch 14226 - lr: 0.01000 - Train loss: 0.63232 - Test loss: 0.57687\n",
      "Epoch 14227 - lr: 0.01000 - Train loss: 0.64831 - Test loss: 0.57641\n",
      "Epoch 14228 - lr: 0.01000 - Train loss: 0.63128 - Test loss: 0.57600\n",
      "Epoch 14229 - lr: 0.01000 - Train loss: 0.63226 - Test loss: 0.57623\n",
      "Epoch 14230 - lr: 0.01000 - Train loss: 0.63629 - Test loss: 0.57607\n",
      "Epoch 14231 - lr: 0.01000 - Train loss: 0.63269 - Test loss: 0.57576\n",
      "Epoch 14232 - lr: 0.01000 - Train loss: 0.62835 - Test loss: 0.57569\n",
      "Epoch 14233 - lr: 0.01000 - Train loss: 0.63272 - Test loss: 0.57612\n",
      "Epoch 14234 - lr: 0.01000 - Train loss: 0.63284 - Test loss: 0.57594\n",
      "Epoch 14235 - lr: 0.01000 - Train loss: 0.62009 - Test loss: 0.57640\n",
      "Epoch 14236 - lr: 0.01000 - Train loss: 0.62943 - Test loss: 0.57668\n",
      "Epoch 14237 - lr: 0.01000 - Train loss: 0.62748 - Test loss: 0.57643\n",
      "Epoch 14238 - lr: 0.01000 - Train loss: 0.62487 - Test loss: 0.57687\n",
      "Epoch 14239 - lr: 0.01000 - Train loss: 0.62817 - Test loss: 0.57664\n",
      "Epoch 14240 - lr: 0.01000 - Train loss: 0.63189 - Test loss: 0.57705\n",
      "Epoch 14241 - lr: 0.01000 - Train loss: 0.64272 - Test loss: 0.57654\n",
      "Epoch 14242 - lr: 0.01000 - Train loss: 0.62717 - Test loss: 0.57622\n",
      "Epoch 14243 - lr: 0.01000 - Train loss: 0.62019 - Test loss: 0.57673\n",
      "Epoch 14244 - lr: 0.01000 - Train loss: 0.61953 - Test loss: 0.57718\n",
      "Epoch 14245 - lr: 0.01000 - Train loss: 0.63161 - Test loss: 0.57738\n",
      "Epoch 14246 - lr: 0.01000 - Train loss: 0.62732 - Test loss: 0.57688\n",
      "Epoch 14247 - lr: 0.01000 - Train loss: 0.62323 - Test loss: 0.57718\n",
      "Epoch 14248 - lr: 0.01000 - Train loss: 0.62340 - Test loss: 0.57755\n",
      "Epoch 14249 - lr: 0.01000 - Train loss: 0.63207 - Test loss: 0.57753\n",
      "Epoch 14250 - lr: 0.01000 - Train loss: 0.63285 - Test loss: 0.57706\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14251 - lr: 0.01000 - Train loss: 0.62784 - Test loss: 0.57677\n",
      "Epoch 14252 - lr: 0.01000 - Train loss: 0.63047 - Test loss: 0.57718\n",
      "Epoch 14253 - lr: 0.01000 - Train loss: 0.63191 - Test loss: 0.57698\n",
      "Epoch 14254 - lr: 0.01000 - Train loss: 0.63206 - Test loss: 0.57700\n",
      "Epoch 14255 - lr: 0.01000 - Train loss: 0.63325 - Test loss: 0.57661\n",
      "Epoch 14256 - lr: 0.01000 - Train loss: 0.62709 - Test loss: 0.57632\n",
      "Epoch 14257 - lr: 0.01000 - Train loss: 0.62077 - Test loss: 0.57681\n",
      "Epoch 14258 - lr: 0.01000 - Train loss: 0.62356 - Test loss: 0.57722\n",
      "Epoch 14259 - lr: 0.01000 - Train loss: 0.63147 - Test loss: 0.57719\n",
      "Epoch 14260 - lr: 0.01000 - Train loss: 0.62870 - Test loss: 0.57671\n",
      "Epoch 14261 - lr: 0.01000 - Train loss: 0.62381 - Test loss: 0.57711\n",
      "Epoch 14262 - lr: 0.01000 - Train loss: 0.63042 - Test loss: 0.57702\n",
      "Epoch 14263 - lr: 0.01000 - Train loss: 0.62698 - Test loss: 0.57666\n",
      "Epoch 14264 - lr: 0.01000 - Train loss: 0.61888 - Test loss: 0.57716\n",
      "Epoch 14265 - lr: 0.01000 - Train loss: 0.63158 - Test loss: 0.57738\n",
      "Epoch 14266 - lr: 0.01000 - Train loss: 0.62774 - Test loss: 0.57693\n",
      "Epoch 14267 - lr: 0.01000 - Train loss: 0.61944 - Test loss: 0.57738\n",
      "Epoch 14268 - lr: 0.01000 - Train loss: 0.62188 - Test loss: 0.57767\n",
      "Epoch 14269 - lr: 0.01000 - Train loss: 0.61920 - Test loss: 0.57804\n",
      "Epoch 14270 - lr: 0.01000 - Train loss: 0.63353 - Test loss: 0.57818\n",
      "Epoch 14271 - lr: 0.01000 - Train loss: 0.63371 - Test loss: 0.57771\n",
      "Epoch 14272 - lr: 0.01000 - Train loss: 0.62776 - Test loss: 0.57720\n",
      "Epoch 14273 - lr: 0.01000 - Train loss: 0.62170 - Test loss: 0.57757\n",
      "Epoch 14274 - lr: 0.01000 - Train loss: 0.63115 - Test loss: 0.57788\n",
      "Epoch 14275 - lr: 0.01000 - Train loss: 0.64871 - Test loss: 0.57730\n",
      "Epoch 14276 - lr: 0.01000 - Train loss: 0.63624 - Test loss: 0.57701\n",
      "Epoch 14277 - lr: 0.01000 - Train loss: 0.63525 - Test loss: 0.57684\n",
      "Epoch 14278 - lr: 0.01000 - Train loss: 0.63210 - Test loss: 0.57644\n",
      "Epoch 14279 - lr: 0.01000 - Train loss: 0.62844 - Test loss: 0.57630\n",
      "Epoch 14280 - lr: 0.01000 - Train loss: 0.63180 - Test loss: 0.57653\n",
      "Epoch 14281 - lr: 0.01000 - Train loss: 0.63728 - Test loss: 0.57648\n",
      "Epoch 14282 - lr: 0.01000 - Train loss: 0.64288 - Test loss: 0.57604\n",
      "Epoch 14283 - lr: 0.01000 - Train loss: 0.62676 - Test loss: 0.57577\n",
      "Epoch 14284 - lr: 0.01000 - Train loss: 0.61924 - Test loss: 0.57636\n",
      "Epoch 14285 - lr: 0.01000 - Train loss: 0.63066 - Test loss: 0.57668\n",
      "Epoch 14286 - lr: 0.01000 - Train loss: 0.62692 - Test loss: 0.57635\n",
      "Epoch 14287 - lr: 0.01000 - Train loss: 0.61981 - Test loss: 0.57684\n",
      "Epoch 14288 - lr: 0.01000 - Train loss: 0.62945 - Test loss: 0.57710\n",
      "Epoch 14289 - lr: 0.01000 - Train loss: 0.62722 - Test loss: 0.57681\n",
      "Epoch 14290 - lr: 0.01000 - Train loss: 0.62429 - Test loss: 0.57721\n",
      "Epoch 14291 - lr: 0.01000 - Train loss: 0.62866 - Test loss: 0.57700\n",
      "Epoch 14292 - lr: 0.01000 - Train loss: 0.63153 - Test loss: 0.57709\n",
      "Epoch 14293 - lr: 0.01000 - Train loss: 0.63558 - Test loss: 0.57685\n",
      "Epoch 14294 - lr: 0.01000 - Train loss: 0.63673 - Test loss: 0.57667\n",
      "Epoch 14295 - lr: 0.01000 - Train loss: 0.62950 - Test loss: 0.57650\n",
      "Epoch 14296 - lr: 0.01000 - Train loss: 0.63007 - Test loss: 0.57672\n",
      "Epoch 14297 - lr: 0.01000 - Train loss: 0.62685 - Test loss: 0.57642\n",
      "Epoch 14298 - lr: 0.01000 - Train loss: 0.61880 - Test loss: 0.57695\n",
      "Epoch 14299 - lr: 0.01000 - Train loss: 0.63056 - Test loss: 0.57719\n",
      "Epoch 14300 - lr: 0.01000 - Train loss: 0.62707 - Test loss: 0.57680\n",
      "Epoch 14301 - lr: 0.01000 - Train loss: 0.61865 - Test loss: 0.57730\n",
      "Epoch 14302 - lr: 0.01000 - Train loss: 0.63065 - Test loss: 0.57750\n",
      "Epoch 14303 - lr: 0.01000 - Train loss: 0.62732 - Test loss: 0.57707\n",
      "Epoch 14304 - lr: 0.01000 - Train loss: 0.61996 - Test loss: 0.57750\n",
      "Epoch 14305 - lr: 0.01000 - Train loss: 0.61899 - Test loss: 0.57790\n",
      "Epoch 14306 - lr: 0.01000 - Train loss: 0.62556 - Test loss: 0.57802\n",
      "Epoch 14307 - lr: 0.01000 - Train loss: 0.63204 - Test loss: 0.57822\n",
      "Epoch 14308 - lr: 0.01000 - Train loss: 0.66068 - Test loss: 0.57773\n",
      "Epoch 14309 - lr: 0.01000 - Train loss: 0.65883 - Test loss: 0.57724\n",
      "Epoch 14310 - lr: 0.01000 - Train loss: 0.65656 - Test loss: 0.57680\n",
      "Epoch 14311 - lr: 0.01000 - Train loss: 0.64815 - Test loss: 0.57633\n",
      "Epoch 14312 - lr: 0.01000 - Train loss: 0.63190 - Test loss: 0.57599\n",
      "Epoch 14313 - lr: 0.01000 - Train loss: 0.62917 - Test loss: 0.57600\n",
      "Epoch 14314 - lr: 0.01000 - Train loss: 0.62872 - Test loss: 0.57602\n",
      "Epoch 14315 - lr: 0.01000 - Train loss: 0.63057 - Test loss: 0.57617\n",
      "Epoch 14316 - lr: 0.01000 - Train loss: 0.62697 - Test loss: 0.57589\n",
      "Epoch 14317 - lr: 0.01000 - Train loss: 0.62132 - Test loss: 0.57640\n",
      "Epoch 14318 - lr: 0.01000 - Train loss: 0.62021 - Test loss: 0.57691\n",
      "Epoch 14319 - lr: 0.01000 - Train loss: 0.62721 - Test loss: 0.57718\n",
      "Epoch 14320 - lr: 0.01000 - Train loss: 0.63068 - Test loss: 0.57717\n",
      "Epoch 14321 - lr: 0.01000 - Train loss: 0.62722 - Test loss: 0.57676\n",
      "Epoch 14322 - lr: 0.01000 - Train loss: 0.61895 - Test loss: 0.57725\n",
      "Epoch 14323 - lr: 0.01000 - Train loss: 0.63191 - Test loss: 0.57750\n",
      "Epoch 14324 - lr: 0.01000 - Train loss: 0.62824 - Test loss: 0.57705\n",
      "Epoch 14325 - lr: 0.01000 - Train loss: 0.62383 - Test loss: 0.57744\n",
      "Epoch 14326 - lr: 0.01000 - Train loss: 0.62953 - Test loss: 0.57729\n",
      "Epoch 14327 - lr: 0.01000 - Train loss: 0.62795 - Test loss: 0.57704\n",
      "Epoch 14328 - lr: 0.01000 - Train loss: 0.63185 - Test loss: 0.57737\n",
      "Epoch 14329 - lr: 0.01000 - Train loss: 0.64790 - Test loss: 0.57689\n",
      "Epoch 14330 - lr: 0.01000 - Train loss: 0.63358 - Test loss: 0.57655\n",
      "Epoch 14331 - lr: 0.01000 - Train loss: 0.62690 - Test loss: 0.57622\n",
      "Epoch 14332 - lr: 0.01000 - Train loss: 0.61900 - Test loss: 0.57679\n",
      "Epoch 14333 - lr: 0.01000 - Train loss: 0.63110 - Test loss: 0.57710\n",
      "Epoch 14334 - lr: 0.01000 - Train loss: 0.62736 - Test loss: 0.57673\n",
      "Epoch 14335 - lr: 0.01000 - Train loss: 0.61924 - Test loss: 0.57724\n",
      "Epoch 14336 - lr: 0.01000 - Train loss: 0.62158 - Test loss: 0.57758\n",
      "Epoch 14337 - lr: 0.01000 - Train loss: 0.61918 - Test loss: 0.57798\n",
      "Epoch 14338 - lr: 0.01000 - Train loss: 0.63237 - Test loss: 0.57814\n",
      "Epoch 14339 - lr: 0.01000 - Train loss: 0.63046 - Test loss: 0.57762\n",
      "Epoch 14340 - lr: 0.01000 - Train loss: 0.63137 - Test loss: 0.57763\n",
      "Epoch 14341 - lr: 0.01000 - Train loss: 0.63618 - Test loss: 0.57740\n",
      "Epoch 14342 - lr: 0.01000 - Train loss: 0.62843 - Test loss: 0.57719\n",
      "Epoch 14343 - lr: 0.01000 - Train loss: 0.62716 - Test loss: 0.57735\n",
      "Epoch 14344 - lr: 0.01000 - Train loss: 0.63048 - Test loss: 0.57731\n",
      "Epoch 14345 - lr: 0.01000 - Train loss: 0.62769 - Test loss: 0.57690\n",
      "Epoch 14346 - lr: 0.01000 - Train loss: 0.62260 - Test loss: 0.57731\n",
      "Epoch 14347 - lr: 0.01000 - Train loss: 0.63171 - Test loss: 0.57744\n",
      "Epoch 14348 - lr: 0.01000 - Train loss: 0.63463 - Test loss: 0.57726\n",
      "Epoch 14349 - lr: 0.01000 - Train loss: 0.63178 - Test loss: 0.57682\n",
      "Epoch 14350 - lr: 0.01000 - Train loss: 0.62812 - Test loss: 0.57663\n",
      "Epoch 14351 - lr: 0.01000 - Train loss: 0.63164 - Test loss: 0.57688\n",
      "Epoch 14352 - lr: 0.01000 - Train loss: 0.63126 - Test loss: 0.57674\n",
      "Epoch 14353 - lr: 0.01000 - Train loss: 0.63092 - Test loss: 0.57676\n",
      "Epoch 14354 - lr: 0.01000 - Train loss: 0.62972 - Test loss: 0.57640\n",
      "Epoch 14355 - lr: 0.01000 - Train loss: 0.63179 - Test loss: 0.57674\n",
      "Epoch 14356 - lr: 0.01000 - Train loss: 0.64211 - Test loss: 0.57631\n",
      "Epoch 14357 - lr: 0.01000 - Train loss: 0.62674 - Test loss: 0.57605\n",
      "Epoch 14358 - lr: 0.01000 - Train loss: 0.62115 - Test loss: 0.57659\n",
      "Epoch 14359 - lr: 0.01000 - Train loss: 0.62921 - Test loss: 0.57703\n",
      "Epoch 14360 - lr: 0.01000 - Train loss: 0.63651 - Test loss: 0.57693\n",
      "Epoch 14361 - lr: 0.01000 - Train loss: 0.62880 - Test loss: 0.57673\n",
      "Epoch 14362 - lr: 0.01000 - Train loss: 0.62355 - Test loss: 0.57697\n",
      "Epoch 14363 - lr: 0.01000 - Train loss: 0.62617 - Test loss: 0.57734\n",
      "Epoch 14364 - lr: 0.01000 - Train loss: 0.62710 - Test loss: 0.57688\n",
      "Epoch 14365 - lr: 0.01000 - Train loss: 0.61876 - Test loss: 0.57732\n",
      "Epoch 14366 - lr: 0.01000 - Train loss: 0.63298 - Test loss: 0.57754\n",
      "Epoch 14367 - lr: 0.01000 - Train loss: 0.63174 - Test loss: 0.57708\n",
      "Epoch 14368 - lr: 0.01000 - Train loss: 0.62846 - Test loss: 0.57685\n",
      "Epoch 14369 - lr: 0.01000 - Train loss: 0.63115 - Test loss: 0.57693\n",
      "Epoch 14370 - lr: 0.01000 - Train loss: 0.63313 - Test loss: 0.57660\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14371 - lr: 0.01000 - Train loss: 0.62681 - Test loss: 0.57626\n",
      "Epoch 14372 - lr: 0.01000 - Train loss: 0.61852 - Test loss: 0.57682\n",
      "Epoch 14373 - lr: 0.01000 - Train loss: 0.62964 - Test loss: 0.57708\n",
      "Epoch 14374 - lr: 0.01000 - Train loss: 0.62694 - Test loss: 0.57675\n",
      "Epoch 14375 - lr: 0.01000 - Train loss: 0.62204 - Test loss: 0.57718\n",
      "Epoch 14376 - lr: 0.01000 - Train loss: 0.63190 - Test loss: 0.57743\n",
      "Epoch 14377 - lr: 0.01000 - Train loss: 0.64920 - Test loss: 0.57692\n",
      "Epoch 14378 - lr: 0.01000 - Train loss: 0.63679 - Test loss: 0.57677\n",
      "Epoch 14379 - lr: 0.01000 - Train loss: 0.64373 - Test loss: 0.57628\n",
      "Epoch 14380 - lr: 0.01000 - Train loss: 0.62670 - Test loss: 0.57592\n",
      "Epoch 14381 - lr: 0.01000 - Train loss: 0.61975 - Test loss: 0.57646\n",
      "Epoch 14382 - lr: 0.01000 - Train loss: 0.62807 - Test loss: 0.57677\n",
      "Epoch 14383 - lr: 0.01000 - Train loss: 0.62824 - Test loss: 0.57662\n",
      "Epoch 14384 - lr: 0.01000 - Train loss: 0.63151 - Test loss: 0.57680\n",
      "Epoch 14385 - lr: 0.01000 - Train loss: 0.63697 - Test loss: 0.57672\n",
      "Epoch 14386 - lr: 0.01000 - Train loss: 0.64615 - Test loss: 0.57624\n",
      "Epoch 14387 - lr: 0.01000 - Train loss: 0.62877 - Test loss: 0.57585\n",
      "Epoch 14388 - lr: 0.01000 - Train loss: 0.62785 - Test loss: 0.57636\n",
      "Epoch 14389 - lr: 0.01000 - Train loss: 0.63087 - Test loss: 0.57604\n",
      "Epoch 14390 - lr: 0.01000 - Train loss: 0.63090 - Test loss: 0.57618\n",
      "Epoch 14391 - lr: 0.01000 - Train loss: 0.62858 - Test loss: 0.57585\n",
      "Epoch 14392 - lr: 0.01000 - Train loss: 0.62682 - Test loss: 0.57635\n",
      "Epoch 14393 - lr: 0.01000 - Train loss: 0.62758 - Test loss: 0.57601\n",
      "Epoch 14394 - lr: 0.01000 - Train loss: 0.61911 - Test loss: 0.57657\n",
      "Epoch 14395 - lr: 0.01000 - Train loss: 0.62330 - Test loss: 0.57690\n",
      "Epoch 14396 - lr: 0.01000 - Train loss: 0.62542 - Test loss: 0.57729\n",
      "Epoch 14397 - lr: 0.01000 - Train loss: 0.62684 - Test loss: 0.57688\n",
      "Epoch 14398 - lr: 0.01000 - Train loss: 0.61872 - Test loss: 0.57734\n",
      "Epoch 14399 - lr: 0.01000 - Train loss: 0.62672 - Test loss: 0.57751\n",
      "Epoch 14400 - lr: 0.01000 - Train loss: 0.63114 - Test loss: 0.57748\n",
      "Epoch 14401 - lr: 0.01000 - Train loss: 0.63247 - Test loss: 0.57704\n",
      "Epoch 14402 - lr: 0.01000 - Train loss: 0.62687 - Test loss: 0.57668\n",
      "Epoch 14403 - lr: 0.01000 - Train loss: 0.62272 - Test loss: 0.57710\n",
      "Epoch 14404 - lr: 0.01000 - Train loss: 0.63123 - Test loss: 0.57716\n",
      "Epoch 14405 - lr: 0.01000 - Train loss: 0.63542 - Test loss: 0.57691\n",
      "Epoch 14406 - lr: 0.01000 - Train loss: 0.63626 - Test loss: 0.57678\n",
      "Epoch 14407 - lr: 0.01000 - Train loss: 0.63033 - Test loss: 0.57653\n",
      "Epoch 14408 - lr: 0.01000 - Train loss: 0.62733 - Test loss: 0.57673\n",
      "Epoch 14409 - lr: 0.01000 - Train loss: 0.62959 - Test loss: 0.57667\n",
      "Epoch 14410 - lr: 0.01000 - Train loss: 0.62681 - Test loss: 0.57640\n",
      "Epoch 14411 - lr: 0.01000 - Train loss: 0.62346 - Test loss: 0.57685\n",
      "Epoch 14412 - lr: 0.01000 - Train loss: 0.62951 - Test loss: 0.57676\n",
      "Epoch 14413 - lr: 0.01000 - Train loss: 0.62700 - Test loss: 0.57650\n",
      "Epoch 14414 - lr: 0.01000 - Train loss: 0.62663 - Test loss: 0.57693\n",
      "Epoch 14415 - lr: 0.01000 - Train loss: 0.62882 - Test loss: 0.57653\n",
      "Epoch 14416 - lr: 0.01000 - Train loss: 0.63032 - Test loss: 0.57694\n",
      "Epoch 14417 - lr: 0.01000 - Train loss: 0.64106 - Test loss: 0.57646\n",
      "Epoch 14418 - lr: 0.01000 - Train loss: 0.62721 - Test loss: 0.57622\n",
      "Epoch 14419 - lr: 0.01000 - Train loss: 0.62962 - Test loss: 0.57669\n",
      "Epoch 14420 - lr: 0.01000 - Train loss: 0.62862 - Test loss: 0.57658\n",
      "Epoch 14421 - lr: 0.01000 - Train loss: 0.61860 - Test loss: 0.57702\n",
      "Epoch 14422 - lr: 0.01000 - Train loss: 0.63286 - Test loss: 0.57726\n",
      "Epoch 14423 - lr: 0.01000 - Train loss: 0.63226 - Test loss: 0.57685\n",
      "Epoch 14424 - lr: 0.01000 - Train loss: 0.62689 - Test loss: 0.57653\n",
      "Epoch 14425 - lr: 0.01000 - Train loss: 0.62474 - Test loss: 0.57695\n",
      "Epoch 14426 - lr: 0.01000 - Train loss: 0.62699 - Test loss: 0.57665\n",
      "Epoch 14427 - lr: 0.01000 - Train loss: 0.62531 - Test loss: 0.57706\n",
      "Epoch 14428 - lr: 0.01000 - Train loss: 0.62691 - Test loss: 0.57669\n",
      "Epoch 14429 - lr: 0.01000 - Train loss: 0.62081 - Test loss: 0.57712\n",
      "Epoch 14430 - lr: 0.01000 - Train loss: 0.62840 - Test loss: 0.57747\n",
      "Epoch 14431 - lr: 0.01000 - Train loss: 0.63656 - Test loss: 0.57730\n",
      "Epoch 14432 - lr: 0.01000 - Train loss: 0.65223 - Test loss: 0.57678\n",
      "Epoch 14433 - lr: 0.01000 - Train loss: 0.63345 - Test loss: 0.57638\n",
      "Epoch 14434 - lr: 0.01000 - Train loss: 0.62090 - Test loss: 0.57680\n",
      "Epoch 14435 - lr: 0.01000 - Train loss: 0.62815 - Test loss: 0.57718\n",
      "Epoch 14436 - lr: 0.01000 - Train loss: 0.63477 - Test loss: 0.57684\n",
      "Epoch 14437 - lr: 0.01000 - Train loss: 0.63109 - Test loss: 0.57640\n",
      "Epoch 14438 - lr: 0.01000 - Train loss: 0.62971 - Test loss: 0.57635\n",
      "Epoch 14439 - lr: 0.01000 - Train loss: 0.62645 - Test loss: 0.57608\n",
      "Epoch 14440 - lr: 0.01000 - Train loss: 0.61862 - Test loss: 0.57663\n",
      "Epoch 14441 - lr: 0.01000 - Train loss: 0.62691 - Test loss: 0.57689\n",
      "Epoch 14442 - lr: 0.01000 - Train loss: 0.63018 - Test loss: 0.57686\n",
      "Epoch 14443 - lr: 0.01000 - Train loss: 0.62705 - Test loss: 0.57647\n",
      "Epoch 14444 - lr: 0.01000 - Train loss: 0.61913 - Test loss: 0.57696\n",
      "Epoch 14445 - lr: 0.01000 - Train loss: 0.61939 - Test loss: 0.57737\n",
      "Epoch 14446 - lr: 0.01000 - Train loss: 0.63019 - Test loss: 0.57754\n",
      "Epoch 14447 - lr: 0.01000 - Train loss: 0.62718 - Test loss: 0.57709\n",
      "Epoch 14448 - lr: 0.01000 - Train loss: 0.62231 - Test loss: 0.57745\n",
      "Epoch 14449 - lr: 0.01000 - Train loss: 0.63153 - Test loss: 0.57752\n",
      "Epoch 14450 - lr: 0.01000 - Train loss: 0.62965 - Test loss: 0.57726\n",
      "Epoch 14451 - lr: 0.01000 - Train loss: 0.63058 - Test loss: 0.57752\n",
      "Epoch 14452 - lr: 0.01000 - Train loss: 0.64940 - Test loss: 0.57697\n",
      "Epoch 14453 - lr: 0.01000 - Train loss: 0.63581 - Test loss: 0.57679\n",
      "Epoch 14454 - lr: 0.01000 - Train loss: 0.63305 - Test loss: 0.57659\n",
      "Epoch 14455 - lr: 0.01000 - Train loss: 0.62651 - Test loss: 0.57621\n",
      "Epoch 14456 - lr: 0.01000 - Train loss: 0.61843 - Test loss: 0.57673\n",
      "Epoch 14457 - lr: 0.01000 - Train loss: 0.63203 - Test loss: 0.57701\n",
      "Epoch 14458 - lr: 0.01000 - Train loss: 0.62927 - Test loss: 0.57657\n",
      "Epoch 14459 - lr: 0.01000 - Train loss: 0.63138 - Test loss: 0.57693\n",
      "Epoch 14460 - lr: 0.01000 - Train loss: 0.65442 - Test loss: 0.57654\n",
      "Epoch 14461 - lr: 0.01000 - Train loss: 0.64697 - Test loss: 0.57606\n",
      "Epoch 14462 - lr: 0.01000 - Train loss: 0.63073 - Test loss: 0.57570\n",
      "Epoch 14463 - lr: 0.01000 - Train loss: 0.63076 - Test loss: 0.57587\n",
      "Epoch 14464 - lr: 0.01000 - Train loss: 0.62804 - Test loss: 0.57556\n",
      "Epoch 14465 - lr: 0.01000 - Train loss: 0.62260 - Test loss: 0.57611\n",
      "Epoch 14466 - lr: 0.01000 - Train loss: 0.63148 - Test loss: 0.57632\n",
      "Epoch 14467 - lr: 0.01000 - Train loss: 0.63552 - Test loss: 0.57614\n",
      "Epoch 14468 - lr: 0.01000 - Train loss: 0.63494 - Test loss: 0.57592\n",
      "Epoch 14469 - lr: 0.01000 - Train loss: 0.62961 - Test loss: 0.57556\n",
      "Epoch 14470 - lr: 0.01000 - Train loss: 0.63161 - Test loss: 0.57604\n",
      "Epoch 14471 - lr: 0.01000 - Train loss: 0.64667 - Test loss: 0.57568\n",
      "Epoch 14472 - lr: 0.01000 - Train loss: 0.62890 - Test loss: 0.57532\n",
      "Epoch 14473 - lr: 0.01000 - Train loss: 0.62783 - Test loss: 0.57587\n",
      "Epoch 14474 - lr: 0.01000 - Train loss: 0.62937 - Test loss: 0.57554\n",
      "Epoch 14475 - lr: 0.01000 - Train loss: 0.63036 - Test loss: 0.57606\n",
      "Epoch 14476 - lr: 0.01000 - Train loss: 0.63008 - Test loss: 0.57598\n",
      "Epoch 14477 - lr: 0.01000 - Train loss: 0.62856 - Test loss: 0.57640\n",
      "Epoch 14478 - lr: 0.01000 - Train loss: 0.63265 - Test loss: 0.57603\n",
      "Epoch 14479 - lr: 0.01000 - Train loss: 0.62692 - Test loss: 0.57580\n",
      "Epoch 14480 - lr: 0.01000 - Train loss: 0.62545 - Test loss: 0.57629\n",
      "Epoch 14481 - lr: 0.01000 - Train loss: 0.62641 - Test loss: 0.57596\n",
      "Epoch 14482 - lr: 0.01000 - Train loss: 0.62183 - Test loss: 0.57636\n",
      "Epoch 14483 - lr: 0.01000 - Train loss: 0.61897 - Test loss: 0.57686\n",
      "Epoch 14484 - lr: 0.01000 - Train loss: 0.62550 - Test loss: 0.57706\n",
      "Epoch 14485 - lr: 0.01000 - Train loss: 0.63249 - Test loss: 0.57729\n",
      "Epoch 14486 - lr: 0.01000 - Train loss: 0.64556 - Test loss: 0.57670\n",
      "Epoch 14487 - lr: 0.01000 - Train loss: 0.62724 - Test loss: 0.57616\n",
      "Epoch 14488 - lr: 0.01000 - Train loss: 0.62099 - Test loss: 0.57657\n",
      "Epoch 14489 - lr: 0.01000 - Train loss: 0.62039 - Test loss: 0.57697\n",
      "Epoch 14490 - lr: 0.01000 - Train loss: 0.62475 - Test loss: 0.57719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14491 - lr: 0.01000 - Train loss: 0.63186 - Test loss: 0.57751\n",
      "Epoch 14492 - lr: 0.01000 - Train loss: 0.65662 - Test loss: 0.57703\n",
      "Epoch 14493 - lr: 0.01000 - Train loss: 0.64674 - Test loss: 0.57645\n",
      "Epoch 14494 - lr: 0.01000 - Train loss: 0.62807 - Test loss: 0.57594\n",
      "Epoch 14495 - lr: 0.01000 - Train loss: 0.61940 - Test loss: 0.57647\n",
      "Epoch 14496 - lr: 0.01000 - Train loss: 0.62059 - Test loss: 0.57687\n",
      "Epoch 14497 - lr: 0.01000 - Train loss: 0.62358 - Test loss: 0.57714\n",
      "Epoch 14498 - lr: 0.01000 - Train loss: 0.62787 - Test loss: 0.57749\n",
      "Epoch 14499 - lr: 0.01000 - Train loss: 0.63012 - Test loss: 0.57693\n",
      "Epoch 14500 - lr: 0.01000 - Train loss: 0.63226 - Test loss: 0.57723\n",
      "Epoch 14501 - lr: 0.01000 - Train loss: 0.65302 - Test loss: 0.57675\n",
      "Epoch 14502 - lr: 0.01000 - Train loss: 0.63169 - Test loss: 0.57651\n",
      "Epoch 14503 - lr: 0.01000 - Train loss: 0.63155 - Test loss: 0.57656\n",
      "Epoch 14504 - lr: 0.01000 - Train loss: 0.62891 - Test loss: 0.57609\n",
      "Epoch 14505 - lr: 0.01000 - Train loss: 0.62525 - Test loss: 0.57655\n",
      "Epoch 14506 - lr: 0.01000 - Train loss: 0.62644 - Test loss: 0.57621\n",
      "Epoch 14507 - lr: 0.01000 - Train loss: 0.62052 - Test loss: 0.57665\n",
      "Epoch 14508 - lr: 0.01000 - Train loss: 0.62315 - Test loss: 0.57696\n",
      "Epoch 14509 - lr: 0.01000 - Train loss: 0.62586 - Test loss: 0.57733\n",
      "Epoch 14510 - lr: 0.01000 - Train loss: 0.62658 - Test loss: 0.57682\n",
      "Epoch 14511 - lr: 0.01000 - Train loss: 0.62402 - Test loss: 0.57708\n",
      "Epoch 14512 - lr: 0.01000 - Train loss: 0.63032 - Test loss: 0.57746\n",
      "Epoch 14513 - lr: 0.01000 - Train loss: 0.62936 - Test loss: 0.57719\n",
      "Epoch 14514 - lr: 0.01000 - Train loss: 0.61896 - Test loss: 0.57751\n",
      "Epoch 14515 - lr: 0.01000 - Train loss: 0.62656 - Test loss: 0.57762\n",
      "Epoch 14516 - lr: 0.01000 - Train loss: 0.63153 - Test loss: 0.57758\n",
      "Epoch 14517 - lr: 0.01000 - Train loss: 0.63104 - Test loss: 0.57703\n",
      "Epoch 14518 - lr: 0.01000 - Train loss: 0.63115 - Test loss: 0.57702\n",
      "Epoch 14519 - lr: 0.01000 - Train loss: 0.62903 - Test loss: 0.57653\n",
      "Epoch 14520 - lr: 0.01000 - Train loss: 0.62885 - Test loss: 0.57695\n",
      "Epoch 14521 - lr: 0.01000 - Train loss: 0.63675 - Test loss: 0.57675\n",
      "Epoch 14522 - lr: 0.01000 - Train loss: 0.63459 - Test loss: 0.57657\n",
      "Epoch 14523 - lr: 0.01000 - Train loss: 0.62715 - Test loss: 0.57606\n",
      "Epoch 14524 - lr: 0.01000 - Train loss: 0.62232 - Test loss: 0.57644\n",
      "Epoch 14525 - lr: 0.01000 - Train loss: 0.62175 - Test loss: 0.57690\n",
      "Epoch 14526 - lr: 0.01000 - Train loss: 0.63232 - Test loss: 0.57722\n",
      "Epoch 14527 - lr: 0.01000 - Train loss: 0.65335 - Test loss: 0.57674\n",
      "Epoch 14528 - lr: 0.01000 - Train loss: 0.62984 - Test loss: 0.57650\n",
      "Epoch 14529 - lr: 0.01000 - Train loss: 0.62420 - Test loss: 0.57685\n",
      "Epoch 14530 - lr: 0.01000 - Train loss: 0.62784 - Test loss: 0.57661\n",
      "Epoch 14531 - lr: 0.01000 - Train loss: 0.63238 - Test loss: 0.57696\n",
      "Epoch 14532 - lr: 0.01000 - Train loss: 0.65021 - Test loss: 0.57648\n",
      "Epoch 14533 - lr: 0.01000 - Train loss: 0.63636 - Test loss: 0.57620\n",
      "Epoch 14534 - lr: 0.01000 - Train loss: 0.63310 - Test loss: 0.57584\n",
      "Epoch 14535 - lr: 0.01000 - Train loss: 0.62646 - Test loss: 0.57560\n",
      "Epoch 14536 - lr: 0.01000 - Train loss: 0.61917 - Test loss: 0.57617\n",
      "Epoch 14537 - lr: 0.01000 - Train loss: 0.62508 - Test loss: 0.57649\n",
      "Epoch 14538 - lr: 0.01000 - Train loss: 0.63262 - Test loss: 0.57686\n",
      "Epoch 14539 - lr: 0.01000 - Train loss: 0.64590 - Test loss: 0.57636\n",
      "Epoch 14540 - lr: 0.01000 - Train loss: 0.62735 - Test loss: 0.57581\n",
      "Epoch 14541 - lr: 0.01000 - Train loss: 0.62412 - Test loss: 0.57618\n",
      "Epoch 14542 - lr: 0.01000 - Train loss: 0.63128 - Test loss: 0.57668\n",
      "Epoch 14543 - lr: 0.01000 - Train loss: 0.63315 - Test loss: 0.57636\n",
      "Epoch 14544 - lr: 0.01000 - Train loss: 0.61957 - Test loss: 0.57677\n",
      "Epoch 14545 - lr: 0.01000 - Train loss: 0.62061 - Test loss: 0.57710\n",
      "Epoch 14546 - lr: 0.01000 - Train loss: 0.62374 - Test loss: 0.57731\n",
      "Epoch 14547 - lr: 0.01000 - Train loss: 0.62899 - Test loss: 0.57765\n",
      "Epoch 14548 - lr: 0.01000 - Train loss: 0.63486 - Test loss: 0.57716\n",
      "Epoch 14549 - lr: 0.01000 - Train loss: 0.62712 - Test loss: 0.57652\n",
      "Epoch 14550 - lr: 0.01000 - Train loss: 0.62565 - Test loss: 0.57678\n",
      "Epoch 14551 - lr: 0.01000 - Train loss: 0.63236 - Test loss: 0.57699\n",
      "Epoch 14552 - lr: 0.01000 - Train loss: 0.63705 - Test loss: 0.57685\n",
      "Epoch 14553 - lr: 0.01000 - Train loss: 0.63836 - Test loss: 0.57669\n",
      "Epoch 14554 - lr: 0.01000 - Train loss: 0.64301 - Test loss: 0.57615\n",
      "Epoch 14555 - lr: 0.01000 - Train loss: 0.62655 - Test loss: 0.57575\n",
      "Epoch 14556 - lr: 0.01000 - Train loss: 0.62231 - Test loss: 0.57617\n",
      "Epoch 14557 - lr: 0.01000 - Train loss: 0.62231 - Test loss: 0.57666\n",
      "Epoch 14558 - lr: 0.01000 - Train loss: 0.63279 - Test loss: 0.57690\n",
      "Epoch 14559 - lr: 0.01000 - Train loss: 0.63192 - Test loss: 0.57668\n",
      "Epoch 14560 - lr: 0.01000 - Train loss: 0.63187 - Test loss: 0.57670\n",
      "Epoch 14561 - lr: 0.01000 - Train loss: 0.62898 - Test loss: 0.57617\n",
      "Epoch 14562 - lr: 0.01000 - Train loss: 0.62316 - Test loss: 0.57660\n",
      "Epoch 14563 - lr: 0.01000 - Train loss: 0.63071 - Test loss: 0.57660\n",
      "Epoch 14564 - lr: 0.01000 - Train loss: 0.62701 - Test loss: 0.57609\n",
      "Epoch 14565 - lr: 0.01000 - Train loss: 0.62489 - Test loss: 0.57641\n",
      "Epoch 14566 - lr: 0.01000 - Train loss: 0.63262 - Test loss: 0.57680\n",
      "Epoch 14567 - lr: 0.01000 - Train loss: 0.64440 - Test loss: 0.57631\n",
      "Epoch 14568 - lr: 0.01000 - Train loss: 0.62700 - Test loss: 0.57580\n",
      "Epoch 14569 - lr: 0.01000 - Train loss: 0.62399 - Test loss: 0.57618\n",
      "Epoch 14570 - lr: 0.01000 - Train loss: 0.63113 - Test loss: 0.57669\n",
      "Epoch 14571 - lr: 0.01000 - Train loss: 0.62981 - Test loss: 0.57649\n",
      "Epoch 14572 - lr: 0.01000 - Train loss: 0.62585 - Test loss: 0.57668\n",
      "Epoch 14573 - lr: 0.01000 - Train loss: 0.63248 - Test loss: 0.57689\n",
      "Epoch 14574 - lr: 0.01000 - Train loss: 0.63859 - Test loss: 0.57676\n",
      "Epoch 14575 - lr: 0.01000 - Train loss: 0.64405 - Test loss: 0.57621\n",
      "Epoch 14576 - lr: 0.01000 - Train loss: 0.62705 - Test loss: 0.57571\n",
      "Epoch 14577 - lr: 0.01000 - Train loss: 0.62324 - Test loss: 0.57612\n",
      "Epoch 14578 - lr: 0.01000 - Train loss: 0.62813 - Test loss: 0.57663\n",
      "Epoch 14579 - lr: 0.01000 - Train loss: 0.62836 - Test loss: 0.57610\n",
      "Epoch 14580 - lr: 0.01000 - Train loss: 0.61926 - Test loss: 0.57657\n",
      "Epoch 14581 - lr: 0.01000 - Train loss: 0.62751 - Test loss: 0.57681\n",
      "Epoch 14582 - lr: 0.01000 - Train loss: 0.62890 - Test loss: 0.57667\n",
      "Epoch 14583 - lr: 0.01000 - Train loss: 0.62935 - Test loss: 0.57658\n",
      "Epoch 14584 - lr: 0.01000 - Train loss: 0.62710 - Test loss: 0.57635\n",
      "Epoch 14585 - lr: 0.01000 - Train loss: 0.62898 - Test loss: 0.57682\n",
      "Epoch 14586 - lr: 0.01000 - Train loss: 0.63414 - Test loss: 0.57646\n",
      "Epoch 14587 - lr: 0.01000 - Train loss: 0.62671 - Test loss: 0.57599\n",
      "Epoch 14588 - lr: 0.01000 - Train loss: 0.62418 - Test loss: 0.57635\n",
      "Epoch 14589 - lr: 0.01000 - Train loss: 0.63166 - Test loss: 0.57683\n",
      "Epoch 14590 - lr: 0.01000 - Train loss: 0.64059 - Test loss: 0.57636\n",
      "Epoch 14591 - lr: 0.01000 - Train loss: 0.62791 - Test loss: 0.57617\n",
      "Epoch 14592 - lr: 0.01000 - Train loss: 0.63242 - Test loss: 0.57650\n",
      "Epoch 14593 - lr: 0.01000 - Train loss: 0.63372 - Test loss: 0.57639\n",
      "Epoch 14594 - lr: 0.01000 - Train loss: 0.62647 - Test loss: 0.57598\n",
      "Epoch 14595 - lr: 0.01000 - Train loss: 0.62183 - Test loss: 0.57639\n",
      "Epoch 14596 - lr: 0.01000 - Train loss: 0.62008 - Test loss: 0.57688\n",
      "Epoch 14597 - lr: 0.01000 - Train loss: 0.62153 - Test loss: 0.57727\n",
      "Epoch 14598 - lr: 0.01000 - Train loss: 0.63228 - Test loss: 0.57759\n",
      "Epoch 14599 - lr: 0.01000 - Train loss: 0.65130 - Test loss: 0.57701\n",
      "Epoch 14600 - lr: 0.01000 - Train loss: 0.63700 - Test loss: 0.57667\n",
      "Epoch 14601 - lr: 0.01000 - Train loss: 0.63534 - Test loss: 0.57631\n",
      "Epoch 14602 - lr: 0.01000 - Train loss: 0.62766 - Test loss: 0.57574\n",
      "Epoch 14603 - lr: 0.01000 - Train loss: 0.62310 - Test loss: 0.57614\n",
      "Epoch 14604 - lr: 0.01000 - Train loss: 0.62770 - Test loss: 0.57665\n",
      "Epoch 14605 - lr: 0.01000 - Train loss: 0.62783 - Test loss: 0.57611\n",
      "Epoch 14606 - lr: 0.01000 - Train loss: 0.62189 - Test loss: 0.57649\n",
      "Epoch 14607 - lr: 0.01000 - Train loss: 0.62035 - Test loss: 0.57695\n",
      "Epoch 14608 - lr: 0.01000 - Train loss: 0.62443 - Test loss: 0.57732\n",
      "Epoch 14609 - lr: 0.01000 - Train loss: 0.62706 - Test loss: 0.57694\n",
      "Epoch 14610 - lr: 0.01000 - Train loss: 0.62619 - Test loss: 0.57731\n",
      "Epoch 14611 - lr: 0.01000 - Train loss: 0.62687 - Test loss: 0.57672\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14612 - lr: 0.01000 - Train loss: 0.62572 - Test loss: 0.57695\n",
      "Epoch 14613 - lr: 0.01000 - Train loss: 0.63202 - Test loss: 0.57711\n",
      "Epoch 14614 - lr: 0.01000 - Train loss: 0.63716 - Test loss: 0.57686\n",
      "Epoch 14615 - lr: 0.01000 - Train loss: 0.63786 - Test loss: 0.57672\n",
      "Epoch 14616 - lr: 0.01000 - Train loss: 0.62976 - Test loss: 0.57646\n",
      "Epoch 14617 - lr: 0.01000 - Train loss: 0.62548 - Test loss: 0.57665\n",
      "Epoch 14618 - lr: 0.01000 - Train loss: 0.63265 - Test loss: 0.57691\n",
      "Epoch 14619 - lr: 0.01000 - Train loss: 0.63276 - Test loss: 0.57670\n",
      "Epoch 14620 - lr: 0.01000 - Train loss: 0.62802 - Test loss: 0.57643\n",
      "Epoch 14621 - lr: 0.01000 - Train loss: 0.63269 - Test loss: 0.57673\n",
      "Epoch 14622 - lr: 0.01000 - Train loss: 0.62975 - Test loss: 0.57652\n",
      "Epoch 14623 - lr: 0.01000 - Train loss: 0.61923 - Test loss: 0.57688\n",
      "Epoch 14624 - lr: 0.01000 - Train loss: 0.62794 - Test loss: 0.57705\n",
      "Epoch 14625 - lr: 0.01000 - Train loss: 0.62792 - Test loss: 0.57679\n",
      "Epoch 14626 - lr: 0.01000 - Train loss: 0.63280 - Test loss: 0.57708\n",
      "Epoch 14627 - lr: 0.01000 - Train loss: 0.63789 - Test loss: 0.57656\n",
      "Epoch 14628 - lr: 0.01000 - Train loss: 0.63261 - Test loss: 0.57674\n",
      "Epoch 14629 - lr: 0.01000 - Train loss: 0.63801 - Test loss: 0.57664\n",
      "Epoch 14630 - lr: 0.01000 - Train loss: 0.63380 - Test loss: 0.57642\n",
      "Epoch 14631 - lr: 0.01000 - Train loss: 0.62660 - Test loss: 0.57599\n",
      "Epoch 14632 - lr: 0.01000 - Train loss: 0.62052 - Test loss: 0.57641\n",
      "Epoch 14633 - lr: 0.01000 - Train loss: 0.62195 - Test loss: 0.57675\n",
      "Epoch 14634 - lr: 0.01000 - Train loss: 0.62029 - Test loss: 0.57715\n",
      "Epoch 14635 - lr: 0.01000 - Train loss: 0.62343 - Test loss: 0.57746\n",
      "Epoch 14636 - lr: 0.01000 - Train loss: 0.62995 - Test loss: 0.57724\n",
      "Epoch 14637 - lr: 0.01000 - Train loss: 0.62649 - Test loss: 0.57679\n",
      "Epoch 14638 - lr: 0.01000 - Train loss: 0.61900 - Test loss: 0.57717\n",
      "Epoch 14639 - lr: 0.01000 - Train loss: 0.62878 - Test loss: 0.57733\n",
      "Epoch 14640 - lr: 0.01000 - Train loss: 0.62659 - Test loss: 0.57692\n",
      "Epoch 14641 - lr: 0.01000 - Train loss: 0.62043 - Test loss: 0.57730\n",
      "Epoch 14642 - lr: 0.01000 - Train loss: 0.62664 - Test loss: 0.57763\n",
      "Epoch 14643 - lr: 0.01000 - Train loss: 0.62708 - Test loss: 0.57695\n",
      "Epoch 14644 - lr: 0.01000 - Train loss: 0.62588 - Test loss: 0.57715\n",
      "Epoch 14645 - lr: 0.01000 - Train loss: 0.63169 - Test loss: 0.57723\n",
      "Epoch 14646 - lr: 0.01000 - Train loss: 0.63261 - Test loss: 0.57676\n",
      "Epoch 14647 - lr: 0.01000 - Train loss: 0.62663 - Test loss: 0.57644\n",
      "Epoch 14648 - lr: 0.01000 - Train loss: 0.62494 - Test loss: 0.57689\n",
      "Epoch 14649 - lr: 0.01000 - Train loss: 0.62614 - Test loss: 0.57650\n",
      "Epoch 14650 - lr: 0.01000 - Train loss: 0.62075 - Test loss: 0.57688\n",
      "Epoch 14651 - lr: 0.01000 - Train loss: 0.61953 - Test loss: 0.57728\n",
      "Epoch 14652 - lr: 0.01000 - Train loss: 0.62708 - Test loss: 0.57745\n",
      "Epoch 14653 - lr: 0.01000 - Train loss: 0.62916 - Test loss: 0.57726\n",
      "Epoch 14654 - lr: 0.01000 - Train loss: 0.62679 - Test loss: 0.57694\n",
      "Epoch 14655 - lr: 0.01000 - Train loss: 0.62793 - Test loss: 0.57734\n",
      "Epoch 14656 - lr: 0.01000 - Train loss: 0.63192 - Test loss: 0.57687\n",
      "Epoch 14657 - lr: 0.01000 - Train loss: 0.62731 - Test loss: 0.57662\n",
      "Epoch 14658 - lr: 0.01000 - Train loss: 0.63177 - Test loss: 0.57699\n",
      "Epoch 14659 - lr: 0.01000 - Train loss: 0.64517 - Test loss: 0.57650\n",
      "Epoch 14660 - lr: 0.01000 - Train loss: 0.62668 - Test loss: 0.57600\n",
      "Epoch 14661 - lr: 0.01000 - Train loss: 0.62264 - Test loss: 0.57641\n",
      "Epoch 14662 - lr: 0.01000 - Train loss: 0.62597 - Test loss: 0.57689\n",
      "Epoch 14663 - lr: 0.01000 - Train loss: 0.62639 - Test loss: 0.57643\n",
      "Epoch 14664 - lr: 0.01000 - Train loss: 0.62362 - Test loss: 0.57675\n",
      "Epoch 14665 - lr: 0.01000 - Train loss: 0.63015 - Test loss: 0.57719\n",
      "Epoch 14666 - lr: 0.01000 - Train loss: 0.62868 - Test loss: 0.57698\n",
      "Epoch 14667 - lr: 0.01000 - Train loss: 0.62491 - Test loss: 0.57714\n",
      "Epoch 14668 - lr: 0.01000 - Train loss: 0.63213 - Test loss: 0.57741\n",
      "Epoch 14669 - lr: 0.01000 - Train loss: 0.64333 - Test loss: 0.57683\n",
      "Epoch 14670 - lr: 0.01000 - Train loss: 0.62606 - Test loss: 0.57634\n",
      "Epoch 14671 - lr: 0.01000 - Train loss: 0.62324 - Test loss: 0.57669\n",
      "Epoch 14672 - lr: 0.01000 - Train loss: 0.62889 - Test loss: 0.57714\n",
      "Epoch 14673 - lr: 0.01000 - Train loss: 0.63723 - Test loss: 0.57695\n",
      "Epoch 14674 - lr: 0.01000 - Train loss: 0.62959 - Test loss: 0.57667\n",
      "Epoch 14675 - lr: 0.01000 - Train loss: 0.62741 - Test loss: 0.57684\n",
      "Epoch 14676 - lr: 0.01000 - Train loss: 0.62800 - Test loss: 0.57666\n",
      "Epoch 14677 - lr: 0.01000 - Train loss: 0.63100 - Test loss: 0.57677\n",
      "Epoch 14678 - lr: 0.01000 - Train loss: 0.62893 - Test loss: 0.57629\n",
      "Epoch 14679 - lr: 0.01000 - Train loss: 0.62895 - Test loss: 0.57675\n",
      "Epoch 14680 - lr: 0.01000 - Train loss: 0.63714 - Test loss: 0.57660\n",
      "Epoch 14681 - lr: 0.01000 - Train loss: 0.62962 - Test loss: 0.57640\n",
      "Epoch 14682 - lr: 0.01000 - Train loss: 0.62410 - Test loss: 0.57677\n",
      "Epoch 14683 - lr: 0.01000 - Train loss: 0.62683 - Test loss: 0.57648\n",
      "Epoch 14684 - lr: 0.01000 - Train loss: 0.62792 - Test loss: 0.57691\n",
      "Epoch 14685 - lr: 0.01000 - Train loss: 0.63027 - Test loss: 0.57642\n",
      "Epoch 14686 - lr: 0.01000 - Train loss: 0.63211 - Test loss: 0.57668\n",
      "Epoch 14687 - lr: 0.01000 - Train loss: 0.62911 - Test loss: 0.57650\n",
      "Epoch 14688 - lr: 0.01000 - Train loss: 0.61890 - Test loss: 0.57686\n",
      "Epoch 14689 - lr: 0.01000 - Train loss: 0.62917 - Test loss: 0.57705\n",
      "Epoch 14690 - lr: 0.01000 - Train loss: 0.62601 - Test loss: 0.57662\n",
      "Epoch 14691 - lr: 0.01000 - Train loss: 0.62012 - Test loss: 0.57699\n",
      "Epoch 14692 - lr: 0.01000 - Train loss: 0.62255 - Test loss: 0.57724\n",
      "Epoch 14693 - lr: 0.01000 - Train loss: 0.62445 - Test loss: 0.57757\n",
      "Epoch 14694 - lr: 0.01000 - Train loss: 0.62627 - Test loss: 0.57711\n",
      "Epoch 14695 - lr: 0.01000 - Train loss: 0.61858 - Test loss: 0.57749\n",
      "Epoch 14696 - lr: 0.01000 - Train loss: 0.62284 - Test loss: 0.57766\n",
      "Epoch 14697 - lr: 0.01000 - Train loss: 0.62572 - Test loss: 0.57794\n",
      "Epoch 14698 - lr: 0.01000 - Train loss: 0.62632 - Test loss: 0.57730\n",
      "Epoch 14699 - lr: 0.01000 - Train loss: 0.62338 - Test loss: 0.57749\n",
      "Epoch 14700 - lr: 0.01000 - Train loss: 0.62910 - Test loss: 0.57781\n",
      "Epoch 14701 - lr: 0.01000 - Train loss: 0.63589 - Test loss: 0.57755\n",
      "Epoch 14702 - lr: 0.01000 - Train loss: 0.63746 - Test loss: 0.57726\n",
      "Epoch 14703 - lr: 0.01000 - Train loss: 0.64648 - Test loss: 0.57665\n",
      "Epoch 14704 - lr: 0.01000 - Train loss: 0.62753 - Test loss: 0.57609\n",
      "Epoch 14705 - lr: 0.01000 - Train loss: 0.61845 - Test loss: 0.57659\n",
      "Epoch 14706 - lr: 0.01000 - Train loss: 0.62602 - Test loss: 0.57685\n",
      "Epoch 14707 - lr: 0.01000 - Train loss: 0.63064 - Test loss: 0.57688\n",
      "Epoch 14708 - lr: 0.01000 - Train loss: 0.62731 - Test loss: 0.57637\n",
      "Epoch 14709 - lr: 0.01000 - Train loss: 0.61821 - Test loss: 0.57684\n",
      "Epoch 14710 - lr: 0.01000 - Train loss: 0.62892 - Test loss: 0.57707\n",
      "Epoch 14711 - lr: 0.01000 - Train loss: 0.62587 - Test loss: 0.57668\n",
      "Epoch 14712 - lr: 0.01000 - Train loss: 0.61825 - Test loss: 0.57713\n",
      "Epoch 14713 - lr: 0.01000 - Train loss: 0.63059 - Test loss: 0.57734\n",
      "Epoch 14714 - lr: 0.01000 - Train loss: 0.62625 - Test loss: 0.57682\n",
      "Epoch 14715 - lr: 0.01000 - Train loss: 0.62108 - Test loss: 0.57714\n",
      "Epoch 14716 - lr: 0.01000 - Train loss: 0.61787 - Test loss: 0.57758\n",
      "Epoch 14717 - lr: 0.01000 - Train loss: 0.62940 - Test loss: 0.57771\n",
      "Epoch 14718 - lr: 0.01000 - Train loss: 0.62594 - Test loss: 0.57721\n",
      "Epoch 14719 - lr: 0.01000 - Train loss: 0.61796 - Test loss: 0.57762\n",
      "Epoch 14720 - lr: 0.01000 - Train loss: 0.63192 - Test loss: 0.57780\n",
      "Epoch 14721 - lr: 0.01000 - Train loss: 0.62937 - Test loss: 0.57725\n",
      "Epoch 14722 - lr: 0.01000 - Train loss: 0.63134 - Test loss: 0.57743\n",
      "Epoch 14723 - lr: 0.01000 - Train loss: 0.63353 - Test loss: 0.57698\n",
      "Epoch 14724 - lr: 0.01000 - Train loss: 0.62291 - Test loss: 0.57731\n",
      "Epoch 14725 - lr: 0.01000 - Train loss: 0.62860 - Test loss: 0.57712\n",
      "Epoch 14726 - lr: 0.01000 - Train loss: 0.62700 - Test loss: 0.57685\n",
      "Epoch 14727 - lr: 0.01000 - Train loss: 0.63112 - Test loss: 0.57718\n",
      "Epoch 14728 - lr: 0.01000 - Train loss: 0.64930 - Test loss: 0.57670\n",
      "Epoch 14729 - lr: 0.01000 - Train loss: 0.63660 - Test loss: 0.57657\n",
      "Epoch 14730 - lr: 0.01000 - Train loss: 0.64161 - Test loss: 0.57609\n",
      "Epoch 14731 - lr: 0.01000 - Train loss: 0.62567 - Test loss: 0.57580\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14732 - lr: 0.01000 - Train loss: 0.61811 - Test loss: 0.57638\n",
      "Epoch 14733 - lr: 0.01000 - Train loss: 0.62882 - Test loss: 0.57669\n",
      "Epoch 14734 - lr: 0.01000 - Train loss: 0.62569 - Test loss: 0.57637\n",
      "Epoch 14735 - lr: 0.01000 - Train loss: 0.61801 - Test loss: 0.57689\n",
      "Epoch 14736 - lr: 0.01000 - Train loss: 0.63043 - Test loss: 0.57714\n",
      "Epoch 14737 - lr: 0.01000 - Train loss: 0.62623 - Test loss: 0.57667\n",
      "Epoch 14738 - lr: 0.01000 - Train loss: 0.61867 - Test loss: 0.57711\n",
      "Epoch 14739 - lr: 0.01000 - Train loss: 0.62851 - Test loss: 0.57733\n",
      "Epoch 14740 - lr: 0.01000 - Train loss: 0.62598 - Test loss: 0.57695\n",
      "Epoch 14741 - lr: 0.01000 - Train loss: 0.62049 - Test loss: 0.57736\n",
      "Epoch 14742 - lr: 0.01000 - Train loss: 0.63006 - Test loss: 0.57769\n",
      "Epoch 14743 - lr: 0.01000 - Train loss: 0.64267 - Test loss: 0.57707\n",
      "Epoch 14744 - lr: 0.01000 - Train loss: 0.62576 - Test loss: 0.57660\n",
      "Epoch 14745 - lr: 0.01000 - Train loss: 0.61829 - Test loss: 0.57707\n",
      "Epoch 14746 - lr: 0.01000 - Train loss: 0.62982 - Test loss: 0.57731\n",
      "Epoch 14747 - lr: 0.01000 - Train loss: 0.62606 - Test loss: 0.57686\n",
      "Epoch 14748 - lr: 0.01000 - Train loss: 0.61747 - Test loss: 0.57734\n",
      "Epoch 14749 - lr: 0.01000 - Train loss: 0.63164 - Test loss: 0.57757\n",
      "Epoch 14750 - lr: 0.01000 - Train loss: 0.62969 - Test loss: 0.57709\n",
      "Epoch 14751 - lr: 0.01000 - Train loss: 0.63014 - Test loss: 0.57710\n",
      "Epoch 14752 - lr: 0.01000 - Train loss: 0.63133 - Test loss: 0.57672\n",
      "Epoch 14753 - lr: 0.01000 - Train loss: 0.62604 - Test loss: 0.57644\n",
      "Epoch 14754 - lr: 0.01000 - Train loss: 0.62510 - Test loss: 0.57689\n",
      "Epoch 14755 - lr: 0.01000 - Train loss: 0.62632 - Test loss: 0.57651\n",
      "Epoch 14756 - lr: 0.01000 - Train loss: 0.61902 - Test loss: 0.57700\n",
      "Epoch 14757 - lr: 0.01000 - Train loss: 0.61969 - Test loss: 0.57742\n",
      "Epoch 14758 - lr: 0.01000 - Train loss: 0.62525 - Test loss: 0.57772\n",
      "Epoch 14759 - lr: 0.01000 - Train loss: 0.62709 - Test loss: 0.57720\n",
      "Epoch 14760 - lr: 0.01000 - Train loss: 0.62502 - Test loss: 0.57752\n",
      "Epoch 14761 - lr: 0.01000 - Train loss: 0.62692 - Test loss: 0.57704\n",
      "Epoch 14762 - lr: 0.01000 - Train loss: 0.62490 - Test loss: 0.57739\n",
      "Epoch 14763 - lr: 0.01000 - Train loss: 0.62687 - Test loss: 0.57694\n",
      "Epoch 14764 - lr: 0.01000 - Train loss: 0.62543 - Test loss: 0.57730\n",
      "Epoch 14765 - lr: 0.01000 - Train loss: 0.62837 - Test loss: 0.57685\n",
      "Epoch 14766 - lr: 0.01000 - Train loss: 0.63058 - Test loss: 0.57710\n",
      "Epoch 14767 - lr: 0.01000 - Train loss: 0.64724 - Test loss: 0.57660\n",
      "Epoch 14768 - lr: 0.01000 - Train loss: 0.63554 - Test loss: 0.57647\n",
      "Epoch 14769 - lr: 0.01000 - Train loss: 0.64639 - Test loss: 0.57602\n",
      "Epoch 14770 - lr: 0.01000 - Train loss: 0.63290 - Test loss: 0.57577\n",
      "Epoch 14771 - lr: 0.01000 - Train loss: 0.62652 - Test loss: 0.57546\n",
      "Epoch 14772 - lr: 0.01000 - Train loss: 0.61910 - Test loss: 0.57606\n",
      "Epoch 14773 - lr: 0.01000 - Train loss: 0.62021 - Test loss: 0.57657\n",
      "Epoch 14774 - lr: 0.01000 - Train loss: 0.62881 - Test loss: 0.57698\n",
      "Epoch 14775 - lr: 0.01000 - Train loss: 0.62804 - Test loss: 0.57678\n",
      "Epoch 14776 - lr: 0.01000 - Train loss: 0.61836 - Test loss: 0.57715\n",
      "Epoch 14777 - lr: 0.01000 - Train loss: 0.61946 - Test loss: 0.57744\n",
      "Epoch 14778 - lr: 0.01000 - Train loss: 0.62506 - Test loss: 0.57756\n",
      "Epoch 14779 - lr: 0.01000 - Train loss: 0.63118 - Test loss: 0.57763\n",
      "Epoch 14780 - lr: 0.01000 - Train loss: 0.62919 - Test loss: 0.57724\n",
      "Epoch 14781 - lr: 0.01000 - Train loss: 0.62845 - Test loss: 0.57732\n",
      "Epoch 14782 - lr: 0.01000 - Train loss: 0.62597 - Test loss: 0.57690\n",
      "Epoch 14783 - lr: 0.01000 - Train loss: 0.62140 - Test loss: 0.57726\n",
      "Epoch 14784 - lr: 0.01000 - Train loss: 0.63101 - Test loss: 0.57736\n",
      "Epoch 14785 - lr: 0.01000 - Train loss: 0.62724 - Test loss: 0.57710\n",
      "Epoch 14786 - lr: 0.01000 - Train loss: 0.62507 - Test loss: 0.57721\n",
      "Epoch 14787 - lr: 0.01000 - Train loss: 0.63098 - Test loss: 0.57731\n",
      "Epoch 14788 - lr: 0.01000 - Train loss: 0.62805 - Test loss: 0.57705\n",
      "Epoch 14789 - lr: 0.01000 - Train loss: 0.61965 - Test loss: 0.57735\n",
      "Epoch 14790 - lr: 0.01000 - Train loss: 0.62475 - Test loss: 0.57762\n",
      "Epoch 14791 - lr: 0.01000 - Train loss: 0.62609 - Test loss: 0.57707\n",
      "Epoch 14792 - lr: 0.01000 - Train loss: 0.61744 - Test loss: 0.57747\n",
      "Epoch 14793 - lr: 0.01000 - Train loss: 0.62768 - Test loss: 0.57757\n",
      "Epoch 14794 - lr: 0.01000 - Train loss: 0.62677 - Test loss: 0.57718\n",
      "Epoch 14795 - lr: 0.01000 - Train loss: 0.63042 - Test loss: 0.57746\n",
      "Epoch 14796 - lr: 0.01000 - Train loss: 0.66049 - Test loss: 0.57703\n",
      "Epoch 14797 - lr: 0.01000 - Train loss: 0.65986 - Test loss: 0.57660\n",
      "Epoch 14798 - lr: 0.01000 - Train loss: 0.65868 - Test loss: 0.57624\n",
      "Epoch 14799 - lr: 0.01000 - Train loss: 0.65643 - Test loss: 0.57590\n",
      "Epoch 14800 - lr: 0.01000 - Train loss: 0.64931 - Test loss: 0.57553\n",
      "Epoch 14801 - lr: 0.01000 - Train loss: 0.63624 - Test loss: 0.57546\n",
      "Epoch 14802 - lr: 0.01000 - Train loss: 0.63617 - Test loss: 0.57550\n",
      "Epoch 14803 - lr: 0.01000 - Train loss: 0.63732 - Test loss: 0.57549\n",
      "Epoch 14804 - lr: 0.01000 - Train loss: 0.63314 - Test loss: 0.57525\n",
      "Epoch 14805 - lr: 0.01000 - Train loss: 0.62264 - Test loss: 0.57575\n",
      "Epoch 14806 - lr: 0.01000 - Train loss: 0.62962 - Test loss: 0.57582\n",
      "Epoch 14807 - lr: 0.01000 - Train loss: 0.62597 - Test loss: 0.57544\n",
      "Epoch 14808 - lr: 0.01000 - Train loss: 0.62321 - Test loss: 0.57585\n",
      "Epoch 14809 - lr: 0.01000 - Train loss: 0.63017 - Test loss: 0.57637\n",
      "Epoch 14810 - lr: 0.01000 - Train loss: 0.62976 - Test loss: 0.57621\n",
      "Epoch 14811 - lr: 0.01000 - Train loss: 0.62817 - Test loss: 0.57658\n",
      "Epoch 14812 - lr: 0.01000 - Train loss: 0.63090 - Test loss: 0.57609\n",
      "Epoch 14813 - lr: 0.01000 - Train loss: 0.63059 - Test loss: 0.57615\n",
      "Epoch 14814 - lr: 0.01000 - Train loss: 0.62689 - Test loss: 0.57562\n",
      "Epoch 14815 - lr: 0.01000 - Train loss: 0.62286 - Test loss: 0.57599\n",
      "Epoch 14816 - lr: 0.01000 - Train loss: 0.62881 - Test loss: 0.57649\n",
      "Epoch 14817 - lr: 0.01000 - Train loss: 0.63662 - Test loss: 0.57626\n",
      "Epoch 14818 - lr: 0.01000 - Train loss: 0.63783 - Test loss: 0.57612\n",
      "Epoch 14819 - lr: 0.01000 - Train loss: 0.64163 - Test loss: 0.57562\n",
      "Epoch 14820 - lr: 0.01000 - Train loss: 0.62583 - Test loss: 0.57529\n",
      "Epoch 14821 - lr: 0.01000 - Train loss: 0.61931 - Test loss: 0.57581\n",
      "Epoch 14822 - lr: 0.01000 - Train loss: 0.62442 - Test loss: 0.57614\n",
      "Epoch 14823 - lr: 0.01000 - Train loss: 0.63227 - Test loss: 0.57651\n",
      "Epoch 14824 - lr: 0.01000 - Train loss: 0.64159 - Test loss: 0.57600\n",
      "Epoch 14825 - lr: 0.01000 - Train loss: 0.62586 - Test loss: 0.57563\n",
      "Epoch 14826 - lr: 0.01000 - Train loss: 0.61879 - Test loss: 0.57613\n",
      "Epoch 14827 - lr: 0.01000 - Train loss: 0.62658 - Test loss: 0.57640\n",
      "Epoch 14828 - lr: 0.01000 - Train loss: 0.62872 - Test loss: 0.57630\n",
      "Epoch 14829 - lr: 0.01000 - Train loss: 0.62659 - Test loss: 0.57606\n",
      "Epoch 14830 - lr: 0.01000 - Train loss: 0.62952 - Test loss: 0.57654\n",
      "Epoch 14831 - lr: 0.01000 - Train loss: 0.63744 - Test loss: 0.57645\n",
      "Epoch 14832 - lr: 0.01000 - Train loss: 0.62922 - Test loss: 0.57619\n",
      "Epoch 14833 - lr: 0.01000 - Train loss: 0.61878 - Test loss: 0.57654\n",
      "Epoch 14834 - lr: 0.01000 - Train loss: 0.62751 - Test loss: 0.57673\n",
      "Epoch 14835 - lr: 0.01000 - Train loss: 0.62699 - Test loss: 0.57644\n",
      "Epoch 14836 - lr: 0.01000 - Train loss: 0.63154 - Test loss: 0.57684\n",
      "Epoch 14837 - lr: 0.01000 - Train loss: 0.64701 - Test loss: 0.57629\n",
      "Epoch 14838 - lr: 0.01000 - Train loss: 0.62734 - Test loss: 0.57566\n",
      "Epoch 14839 - lr: 0.01000 - Train loss: 0.62141 - Test loss: 0.57605\n",
      "Epoch 14840 - lr: 0.01000 - Train loss: 0.62079 - Test loss: 0.57653\n",
      "Epoch 14841 - lr: 0.01000 - Train loss: 0.63156 - Test loss: 0.57693\n",
      "Epoch 14842 - lr: 0.01000 - Train loss: 0.64559 - Test loss: 0.57635\n",
      "Epoch 14843 - lr: 0.01000 - Train loss: 0.62721 - Test loss: 0.57568\n",
      "Epoch 14844 - lr: 0.01000 - Train loss: 0.62211 - Test loss: 0.57606\n",
      "Epoch 14845 - lr: 0.01000 - Train loss: 0.62521 - Test loss: 0.57654\n",
      "Epoch 14846 - lr: 0.01000 - Train loss: 0.62630 - Test loss: 0.57602\n",
      "Epoch 14847 - lr: 0.01000 - Train loss: 0.62373 - Test loss: 0.57633\n",
      "Epoch 14848 - lr: 0.01000 - Train loss: 0.63170 - Test loss: 0.57677\n",
      "Epoch 14849 - lr: 0.01000 - Train loss: 0.64705 - Test loss: 0.57625\n",
      "Epoch 14850 - lr: 0.01000 - Train loss: 0.62733 - Test loss: 0.57563\n",
      "Epoch 14851 - lr: 0.01000 - Train loss: 0.62113 - Test loss: 0.57604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14852 - lr: 0.01000 - Train loss: 0.61964 - Test loss: 0.57654\n",
      "Epoch 14853 - lr: 0.01000 - Train loss: 0.62305 - Test loss: 0.57693\n",
      "Epoch 14854 - lr: 0.01000 - Train loss: 0.62824 - Test loss: 0.57670\n",
      "Epoch 14855 - lr: 0.01000 - Train loss: 0.62918 - Test loss: 0.57659\n",
      "Epoch 14856 - lr: 0.01000 - Train loss: 0.62567 - Test loss: 0.57619\n",
      "Epoch 14857 - lr: 0.01000 - Train loss: 0.61947 - Test loss: 0.57661\n",
      "Epoch 14858 - lr: 0.01000 - Train loss: 0.62321 - Test loss: 0.57687\n",
      "Epoch 14859 - lr: 0.01000 - Train loss: 0.63003 - Test loss: 0.57726\n",
      "Epoch 14860 - lr: 0.01000 - Train loss: 0.63062 - Test loss: 0.57697\n",
      "Epoch 14861 - lr: 0.01000 - Train loss: 0.63235 - Test loss: 0.57714\n",
      "Epoch 14862 - lr: 0.01000 - Train loss: 0.63868 - Test loss: 0.57655\n",
      "Epoch 14863 - lr: 0.01000 - Train loss: 0.62993 - Test loss: 0.57647\n",
      "Epoch 14864 - lr: 0.01000 - Train loss: 0.62639 - Test loss: 0.57592\n",
      "Epoch 14865 - lr: 0.01000 - Train loss: 0.62341 - Test loss: 0.57626\n",
      "Epoch 14866 - lr: 0.01000 - Train loss: 0.63109 - Test loss: 0.57674\n",
      "Epoch 14867 - lr: 0.01000 - Train loss: 0.63970 - Test loss: 0.57622\n",
      "Epoch 14868 - lr: 0.01000 - Train loss: 0.62764 - Test loss: 0.57604\n",
      "Epoch 14869 - lr: 0.01000 - Train loss: 0.63067 - Test loss: 0.57619\n",
      "Epoch 14870 - lr: 0.01000 - Train loss: 0.62713 - Test loss: 0.57567\n",
      "Epoch 14871 - lr: 0.01000 - Train loss: 0.62025 - Test loss: 0.57612\n",
      "Epoch 14872 - lr: 0.01000 - Train loss: 0.61874 - Test loss: 0.57659\n",
      "Epoch 14873 - lr: 0.01000 - Train loss: 0.62674 - Test loss: 0.57681\n",
      "Epoch 14874 - lr: 0.01000 - Train loss: 0.62794 - Test loss: 0.57663\n",
      "Epoch 14875 - lr: 0.01000 - Train loss: 0.62915 - Test loss: 0.57656\n",
      "Epoch 14876 - lr: 0.01000 - Train loss: 0.62550 - Test loss: 0.57615\n",
      "Epoch 14877 - lr: 0.01000 - Train loss: 0.62135 - Test loss: 0.57652\n",
      "Epoch 14878 - lr: 0.01000 - Train loss: 0.62094 - Test loss: 0.57696\n",
      "Epoch 14879 - lr: 0.01000 - Train loss: 0.63188 - Test loss: 0.57726\n",
      "Epoch 14880 - lr: 0.01000 - Train loss: 0.64769 - Test loss: 0.57668\n",
      "Epoch 14881 - lr: 0.01000 - Train loss: 0.62850 - Test loss: 0.57609\n",
      "Epoch 14882 - lr: 0.01000 - Train loss: 0.62699 - Test loss: 0.57655\n",
      "Epoch 14883 - lr: 0.01000 - Train loss: 0.62728 - Test loss: 0.57603\n",
      "Epoch 14884 - lr: 0.01000 - Train loss: 0.61816 - Test loss: 0.57651\n",
      "Epoch 14885 - lr: 0.01000 - Train loss: 0.62728 - Test loss: 0.57675\n",
      "Epoch 14886 - lr: 0.01000 - Train loss: 0.62654 - Test loss: 0.57649\n",
      "Epoch 14887 - lr: 0.01000 - Train loss: 0.63047 - Test loss: 0.57692\n",
      "Epoch 14888 - lr: 0.01000 - Train loss: 0.63504 - Test loss: 0.57646\n",
      "Epoch 14889 - lr: 0.01000 - Train loss: 0.62996 - Test loss: 0.57684\n",
      "Epoch 14890 - lr: 0.01000 - Train loss: 0.63044 - Test loss: 0.57661\n",
      "Epoch 14891 - lr: 0.01000 - Train loss: 0.63211 - Test loss: 0.57684\n",
      "Epoch 14892 - lr: 0.01000 - Train loss: 0.63774 - Test loss: 0.57631\n",
      "Epoch 14893 - lr: 0.01000 - Train loss: 0.63107 - Test loss: 0.57639\n",
      "Epoch 14894 - lr: 0.01000 - Train loss: 0.62913 - Test loss: 0.57589\n",
      "Epoch 14895 - lr: 0.01000 - Train loss: 0.63037 - Test loss: 0.57637\n",
      "Epoch 14896 - lr: 0.01000 - Train loss: 0.62907 - Test loss: 0.57619\n",
      "Epoch 14897 - lr: 0.01000 - Train loss: 0.61846 - Test loss: 0.57657\n",
      "Epoch 14898 - lr: 0.01000 - Train loss: 0.62476 - Test loss: 0.57677\n",
      "Epoch 14899 - lr: 0.01000 - Train loss: 0.63210 - Test loss: 0.57697\n",
      "Epoch 14900 - lr: 0.01000 - Train loss: 0.63195 - Test loss: 0.57670\n",
      "Epoch 14901 - lr: 0.01000 - Train loss: 0.62793 - Test loss: 0.57642\n",
      "Epoch 14902 - lr: 0.01000 - Train loss: 0.63003 - Test loss: 0.57641\n",
      "Epoch 14903 - lr: 0.01000 - Train loss: 0.62658 - Test loss: 0.57582\n",
      "Epoch 14904 - lr: 0.01000 - Train loss: 0.62293 - Test loss: 0.57616\n",
      "Epoch 14905 - lr: 0.01000 - Train loss: 0.62995 - Test loss: 0.57665\n",
      "Epoch 14906 - lr: 0.01000 - Train loss: 0.63236 - Test loss: 0.57645\n",
      "Epoch 14907 - lr: 0.01000 - Train loss: 0.62625 - Test loss: 0.57609\n",
      "Epoch 14908 - lr: 0.01000 - Train loss: 0.62588 - Test loss: 0.57654\n",
      "Epoch 14909 - lr: 0.01000 - Train loss: 0.62661 - Test loss: 0.57593\n",
      "Epoch 14910 - lr: 0.01000 - Train loss: 0.62351 - Test loss: 0.57624\n",
      "Epoch 14911 - lr: 0.01000 - Train loss: 0.63144 - Test loss: 0.57669\n",
      "Epoch 14912 - lr: 0.01000 - Train loss: 0.64664 - Test loss: 0.57617\n",
      "Epoch 14913 - lr: 0.01000 - Train loss: 0.62703 - Test loss: 0.57555\n",
      "Epoch 14914 - lr: 0.01000 - Train loss: 0.62123 - Test loss: 0.57596\n",
      "Epoch 14915 - lr: 0.01000 - Train loss: 0.62101 - Test loss: 0.57645\n",
      "Epoch 14916 - lr: 0.01000 - Train loss: 0.63211 - Test loss: 0.57678\n",
      "Epoch 14917 - lr: 0.01000 - Train loss: 0.64249 - Test loss: 0.57622\n",
      "Epoch 14918 - lr: 0.01000 - Train loss: 0.62577 - Test loss: 0.57572\n",
      "Epoch 14919 - lr: 0.01000 - Train loss: 0.62230 - Test loss: 0.57610\n",
      "Epoch 14920 - lr: 0.01000 - Train loss: 0.62733 - Test loss: 0.57659\n",
      "Epoch 14921 - lr: 0.01000 - Train loss: 0.62771 - Test loss: 0.57601\n",
      "Epoch 14922 - lr: 0.01000 - Train loss: 0.61860 - Test loss: 0.57647\n",
      "Epoch 14923 - lr: 0.01000 - Train loss: 0.61877 - Test loss: 0.57685\n",
      "Epoch 14924 - lr: 0.01000 - Train loss: 0.62664 - Test loss: 0.57701\n",
      "Epoch 14925 - lr: 0.01000 - Train loss: 0.62796 - Test loss: 0.57678\n",
      "Epoch 14926 - lr: 0.01000 - Train loss: 0.62871 - Test loss: 0.57664\n",
      "Epoch 14927 - lr: 0.01000 - Train loss: 0.62555 - Test loss: 0.57628\n",
      "Epoch 14928 - lr: 0.01000 - Train loss: 0.61854 - Test loss: 0.57674\n",
      "Epoch 14929 - lr: 0.01000 - Train loss: 0.61794 - Test loss: 0.57714\n",
      "Epoch 14930 - lr: 0.01000 - Train loss: 0.62899 - Test loss: 0.57728\n",
      "Epoch 14931 - lr: 0.01000 - Train loss: 0.62538 - Test loss: 0.57674\n",
      "Epoch 14932 - lr: 0.01000 - Train loss: 0.62266 - Test loss: 0.57698\n",
      "Epoch 14933 - lr: 0.01000 - Train loss: 0.62868 - Test loss: 0.57736\n",
      "Epoch 14934 - lr: 0.01000 - Train loss: 0.63722 - Test loss: 0.57715\n",
      "Epoch 14935 - lr: 0.01000 - Train loss: 0.64286 - Test loss: 0.57650\n",
      "Epoch 14936 - lr: 0.01000 - Train loss: 0.62556 - Test loss: 0.57597\n",
      "Epoch 14937 - lr: 0.01000 - Train loss: 0.62343 - Test loss: 0.57630\n",
      "Epoch 14938 - lr: 0.01000 - Train loss: 0.63116 - Test loss: 0.57674\n",
      "Epoch 14939 - lr: 0.01000 - Train loss: 0.64586 - Test loss: 0.57622\n",
      "Epoch 14940 - lr: 0.01000 - Train loss: 0.62656 - Test loss: 0.57564\n",
      "Epoch 14941 - lr: 0.01000 - Train loss: 0.62153 - Test loss: 0.57605\n",
      "Epoch 14942 - lr: 0.01000 - Train loss: 0.62336 - Test loss: 0.57654\n",
      "Epoch 14943 - lr: 0.01000 - Train loss: 0.62590 - Test loss: 0.57625\n",
      "Epoch 14944 - lr: 0.01000 - Train loss: 0.62463 - Test loss: 0.57669\n",
      "Epoch 14945 - lr: 0.01000 - Train loss: 0.62547 - Test loss: 0.57620\n",
      "Epoch 14946 - lr: 0.01000 - Train loss: 0.62360 - Test loss: 0.57650\n",
      "Epoch 14947 - lr: 0.01000 - Train loss: 0.63131 - Test loss: 0.57689\n",
      "Epoch 14948 - lr: 0.01000 - Train loss: 0.64709 - Test loss: 0.57636\n",
      "Epoch 14949 - lr: 0.01000 - Train loss: 0.62786 - Test loss: 0.57580\n",
      "Epoch 14950 - lr: 0.01000 - Train loss: 0.62372 - Test loss: 0.57629\n",
      "Epoch 14951 - lr: 0.01000 - Train loss: 0.62535 - Test loss: 0.57598\n",
      "Epoch 14952 - lr: 0.01000 - Train loss: 0.61780 - Test loss: 0.57649\n",
      "Epoch 14953 - lr: 0.01000 - Train loss: 0.62644 - Test loss: 0.57672\n",
      "Epoch 14954 - lr: 0.01000 - Train loss: 0.62752 - Test loss: 0.57655\n",
      "Epoch 14955 - lr: 0.01000 - Train loss: 0.62899 - Test loss: 0.57650\n",
      "Epoch 14956 - lr: 0.01000 - Train loss: 0.62520 - Test loss: 0.57607\n",
      "Epoch 14957 - lr: 0.01000 - Train loss: 0.62258 - Test loss: 0.57641\n",
      "Epoch 14958 - lr: 0.01000 - Train loss: 0.62892 - Test loss: 0.57687\n",
      "Epoch 14959 - lr: 0.01000 - Train loss: 0.63408 - Test loss: 0.57670\n",
      "Epoch 14960 - lr: 0.01000 - Train loss: 0.62660 - Test loss: 0.57609\n",
      "Epoch 14961 - lr: 0.01000 - Train loss: 0.61840 - Test loss: 0.57653\n",
      "Epoch 14962 - lr: 0.01000 - Train loss: 0.62637 - Test loss: 0.57677\n",
      "Epoch 14963 - lr: 0.01000 - Train loss: 0.62745 - Test loss: 0.57658\n",
      "Epoch 14964 - lr: 0.01000 - Train loss: 0.62879 - Test loss: 0.57652\n",
      "Epoch 14965 - lr: 0.01000 - Train loss: 0.62504 - Test loss: 0.57611\n",
      "Epoch 14966 - lr: 0.01000 - Train loss: 0.62104 - Test loss: 0.57649\n",
      "Epoch 14967 - lr: 0.01000 - Train loss: 0.62054 - Test loss: 0.57693\n",
      "Epoch 14968 - lr: 0.01000 - Train loss: 0.63117 - Test loss: 0.57721\n",
      "Epoch 14969 - lr: 0.01000 - Train loss: 0.64871 - Test loss: 0.57664\n",
      "Epoch 14970 - lr: 0.01000 - Train loss: 0.63483 - Test loss: 0.57631\n",
      "Epoch 14971 - lr: 0.01000 - Train loss: 0.63181 - Test loss: 0.57591\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14972 - lr: 0.01000 - Train loss: 0.62505 - Test loss: 0.57561\n",
      "Epoch 14973 - lr: 0.01000 - Train loss: 0.61762 - Test loss: 0.57617\n",
      "Epoch 14974 - lr: 0.01000 - Train loss: 0.62775 - Test loss: 0.57646\n",
      "Epoch 14975 - lr: 0.01000 - Train loss: 0.62515 - Test loss: 0.57614\n",
      "Epoch 14976 - lr: 0.01000 - Train loss: 0.61767 - Test loss: 0.57665\n",
      "Epoch 14977 - lr: 0.01000 - Train loss: 0.62210 - Test loss: 0.57692\n",
      "Epoch 14978 - lr: 0.01000 - Train loss: 0.62615 - Test loss: 0.57728\n",
      "Epoch 14979 - lr: 0.01000 - Train loss: 0.62748 - Test loss: 0.57667\n",
      "Epoch 14980 - lr: 0.01000 - Train loss: 0.62438 - Test loss: 0.57703\n",
      "Epoch 14981 - lr: 0.01000 - Train loss: 0.62514 - Test loss: 0.57652\n",
      "Epoch 14982 - lr: 0.01000 - Train loss: 0.62124 - Test loss: 0.57681\n",
      "Epoch 14983 - lr: 0.01000 - Train loss: 0.62158 - Test loss: 0.57719\n",
      "Epoch 14984 - lr: 0.01000 - Train loss: 0.62957 - Test loss: 0.57710\n",
      "Epoch 14985 - lr: 0.01000 - Train loss: 0.62646 - Test loss: 0.57654\n",
      "Epoch 14986 - lr: 0.01000 - Train loss: 0.61842 - Test loss: 0.57696\n",
      "Epoch 14987 - lr: 0.01000 - Train loss: 0.61859 - Test loss: 0.57734\n",
      "Epoch 14988 - lr: 0.01000 - Train loss: 0.61949 - Test loss: 0.57764\n",
      "Epoch 14989 - lr: 0.01000 - Train loss: 0.62741 - Test loss: 0.57787\n",
      "Epoch 14990 - lr: 0.01000 - Train loss: 0.63624 - Test loss: 0.57754\n",
      "Epoch 14991 - lr: 0.01000 - Train loss: 0.65391 - Test loss: 0.57693\n",
      "Epoch 14992 - lr: 0.01000 - Train loss: 0.63723 - Test loss: 0.57632\n",
      "Epoch 14993 - lr: 0.01000 - Train loss: 0.62990 - Test loss: 0.57635\n",
      "Epoch 14994 - lr: 0.01000 - Train loss: 0.62809 - Test loss: 0.57589\n",
      "Epoch 14995 - lr: 0.01000 - Train loss: 0.62947 - Test loss: 0.57635\n",
      "Epoch 14996 - lr: 0.01000 - Train loss: 0.63192 - Test loss: 0.57601\n",
      "Epoch 14997 - lr: 0.01000 - Train loss: 0.61862 - Test loss: 0.57644\n",
      "Epoch 14998 - lr: 0.01000 - Train loss: 0.61890 - Test loss: 0.57685\n",
      "Epoch 14999 - lr: 0.01000 - Train loss: 0.62163 - Test loss: 0.57717\n",
      "Epoch 15000 - lr: 0.01000 - Train loss: 0.62991 - Test loss: 0.57706\n",
      "Epoch 15001 - lr: 0.01000 - Train loss: 0.62672 - Test loss: 0.57644\n",
      "Epoch 15002 - lr: 0.01000 - Train loss: 0.61825 - Test loss: 0.57685\n",
      "Epoch 15003 - lr: 0.01000 - Train loss: 0.61727 - Test loss: 0.57724\n",
      "Epoch 15004 - lr: 0.01000 - Train loss: 0.62647 - Test loss: 0.57732\n",
      "Epoch 15005 - lr: 0.01000 - Train loss: 0.62722 - Test loss: 0.57700\n",
      "Epoch 15006 - lr: 0.01000 - Train loss: 0.62925 - Test loss: 0.57689\n",
      "Epoch 15007 - lr: 0.01000 - Train loss: 0.62577 - Test loss: 0.57635\n",
      "Epoch 15008 - lr: 0.01000 - Train loss: 0.61714 - Test loss: 0.57680\n",
      "Epoch 15009 - lr: 0.01000 - Train loss: 0.63058 - Test loss: 0.57702\n",
      "Epoch 15010 - lr: 0.01000 - Train loss: 0.62698 - Test loss: 0.57648\n",
      "Epoch 15011 - lr: 0.01000 - Train loss: 0.62487 - Test loss: 0.57686\n",
      "Epoch 15012 - lr: 0.01000 - Train loss: 0.62581 - Test loss: 0.57635\n",
      "Epoch 15013 - lr: 0.01000 - Train loss: 0.61680 - Test loss: 0.57683\n",
      "Epoch 15014 - lr: 0.01000 - Train loss: 0.62813 - Test loss: 0.57701\n",
      "Epoch 15015 - lr: 0.01000 - Train loss: 0.62506 - Test loss: 0.57657\n",
      "Epoch 15016 - lr: 0.01000 - Train loss: 0.61694 - Test loss: 0.57703\n",
      "Epoch 15017 - lr: 0.01000 - Train loss: 0.62456 - Test loss: 0.57719\n",
      "Epoch 15018 - lr: 0.01000 - Train loss: 0.63001 - Test loss: 0.57722\n",
      "Epoch 15019 - lr: 0.01000 - Train loss: 0.63479 - Test loss: 0.57701\n",
      "Epoch 15020 - lr: 0.01000 - Train loss: 0.63024 - Test loss: 0.57657\n",
      "Epoch 15021 - lr: 0.01000 - Train loss: 0.61913 - Test loss: 0.57685\n",
      "Epoch 15022 - lr: 0.01000 - Train loss: 0.62083 - Test loss: 0.57710\n",
      "Epoch 15023 - lr: 0.01000 - Train loss: 0.61882 - Test loss: 0.57744\n",
      "Epoch 15024 - lr: 0.01000 - Train loss: 0.62363 - Test loss: 0.57768\n",
      "Epoch 15025 - lr: 0.01000 - Train loss: 0.62542 - Test loss: 0.57710\n",
      "Epoch 15026 - lr: 0.01000 - Train loss: 0.61887 - Test loss: 0.57742\n",
      "Epoch 15027 - lr: 0.01000 - Train loss: 0.62478 - Test loss: 0.57766\n",
      "Epoch 15028 - lr: 0.01000 - Train loss: 0.62701 - Test loss: 0.57703\n",
      "Epoch 15029 - lr: 0.01000 - Train loss: 0.62754 - Test loss: 0.57732\n",
      "Epoch 15030 - lr: 0.01000 - Train loss: 0.62840 - Test loss: 0.57700\n",
      "Epoch 15031 - lr: 0.01000 - Train loss: 0.63001 - Test loss: 0.57721\n",
      "Epoch 15032 - lr: 0.01000 - Train loss: 0.66112 - Test loss: 0.57675\n",
      "Epoch 15033 - lr: 0.01000 - Train loss: 0.65306 - Test loss: 0.57623\n",
      "Epoch 15034 - lr: 0.01000 - Train loss: 0.64111 - Test loss: 0.57569\n",
      "Epoch 15035 - lr: 0.01000 - Train loss: 0.62464 - Test loss: 0.57534\n",
      "Epoch 15036 - lr: 0.01000 - Train loss: 0.61884 - Test loss: 0.57586\n",
      "Epoch 15037 - lr: 0.01000 - Train loss: 0.62112 - Test loss: 0.57625\n",
      "Epoch 15038 - lr: 0.01000 - Train loss: 0.62189 - Test loss: 0.57667\n",
      "Epoch 15039 - lr: 0.01000 - Train loss: 0.62778 - Test loss: 0.57650\n",
      "Epoch 15040 - lr: 0.01000 - Train loss: 0.62568 - Test loss: 0.57620\n",
      "Epoch 15041 - lr: 0.01000 - Train loss: 0.62867 - Test loss: 0.57662\n",
      "Epoch 15042 - lr: 0.01000 - Train loss: 0.63029 - Test loss: 0.57627\n",
      "Epoch 15043 - lr: 0.01000 - Train loss: 0.61870 - Test loss: 0.57659\n",
      "Epoch 15044 - lr: 0.01000 - Train loss: 0.62329 - Test loss: 0.57681\n",
      "Epoch 15045 - lr: 0.01000 - Train loss: 0.63042 - Test loss: 0.57709\n",
      "Epoch 15046 - lr: 0.01000 - Train loss: 0.65743 - Test loss: 0.57662\n",
      "Epoch 15047 - lr: 0.01000 - Train loss: 0.65126 - Test loss: 0.57608\n",
      "Epoch 15048 - lr: 0.01000 - Train loss: 0.62907 - Test loss: 0.57587\n",
      "Epoch 15049 - lr: 0.01000 - Train loss: 0.63058 - Test loss: 0.57624\n",
      "Epoch 15050 - lr: 0.01000 - Train loss: 0.64778 - Test loss: 0.57577\n",
      "Epoch 15051 - lr: 0.01000 - Train loss: 0.63208 - Test loss: 0.57539\n",
      "Epoch 15052 - lr: 0.01000 - Train loss: 0.62484 - Test loss: 0.57504\n",
      "Epoch 15053 - lr: 0.01000 - Train loss: 0.62149 - Test loss: 0.57550\n",
      "Epoch 15054 - lr: 0.01000 - Train loss: 0.62519 - Test loss: 0.57603\n",
      "Epoch 15055 - lr: 0.01000 - Train loss: 0.62549 - Test loss: 0.57553\n",
      "Epoch 15056 - lr: 0.01000 - Train loss: 0.62334 - Test loss: 0.57586\n",
      "Epoch 15057 - lr: 0.01000 - Train loss: 0.63088 - Test loss: 0.57626\n",
      "Epoch 15058 - lr: 0.01000 - Train loss: 0.64485 - Test loss: 0.57576\n",
      "Epoch 15059 - lr: 0.01000 - Train loss: 0.62591 - Test loss: 0.57522\n",
      "Epoch 15060 - lr: 0.01000 - Train loss: 0.62048 - Test loss: 0.57566\n",
      "Epoch 15061 - lr: 0.01000 - Train loss: 0.61911 - Test loss: 0.57618\n",
      "Epoch 15062 - lr: 0.01000 - Train loss: 0.62557 - Test loss: 0.57659\n",
      "Epoch 15063 - lr: 0.01000 - Train loss: 0.62581 - Test loss: 0.57599\n",
      "Epoch 15064 - lr: 0.01000 - Train loss: 0.62151 - Test loss: 0.57628\n",
      "Epoch 15065 - lr: 0.01000 - Train loss: 0.62474 - Test loss: 0.57669\n",
      "Epoch 15066 - lr: 0.01000 - Train loss: 0.62520 - Test loss: 0.57611\n",
      "Epoch 15067 - lr: 0.01000 - Train loss: 0.62376 - Test loss: 0.57636\n",
      "Epoch 15068 - lr: 0.01000 - Train loss: 0.63084 - Test loss: 0.57661\n",
      "Epoch 15069 - lr: 0.01000 - Train loss: 0.63116 - Test loss: 0.57621\n",
      "Epoch 15070 - lr: 0.01000 - Train loss: 0.61713 - Test loss: 0.57660\n",
      "Epoch 15071 - lr: 0.01000 - Train loss: 0.62868 - Test loss: 0.57676\n",
      "Epoch 15072 - lr: 0.01000 - Train loss: 0.62479 - Test loss: 0.57624\n",
      "Epoch 15073 - lr: 0.01000 - Train loss: 0.62232 - Test loss: 0.57650\n",
      "Epoch 15074 - lr: 0.01000 - Train loss: 0.62872 - Test loss: 0.57689\n",
      "Epoch 15075 - lr: 0.01000 - Train loss: 0.62733 - Test loss: 0.57662\n",
      "Epoch 15076 - lr: 0.01000 - Train loss: 0.62111 - Test loss: 0.57678\n",
      "Epoch 15077 - lr: 0.01000 - Train loss: 0.62125 - Test loss: 0.57710\n",
      "Epoch 15078 - lr: 0.01000 - Train loss: 0.62981 - Test loss: 0.57700\n",
      "Epoch 15079 - lr: 0.01000 - Train loss: 0.62771 - Test loss: 0.57638\n",
      "Epoch 15080 - lr: 0.01000 - Train loss: 0.62825 - Test loss: 0.57674\n",
      "Epoch 15081 - lr: 0.01000 - Train loss: 0.63187 - Test loss: 0.57649\n",
      "Epoch 15082 - lr: 0.01000 - Train loss: 0.62474 - Test loss: 0.57596\n",
      "Epoch 15083 - lr: 0.01000 - Train loss: 0.62040 - Test loss: 0.57629\n",
      "Epoch 15084 - lr: 0.01000 - Train loss: 0.61831 - Test loss: 0.57672\n",
      "Epoch 15085 - lr: 0.01000 - Train loss: 0.61930 - Test loss: 0.57706\n",
      "Epoch 15086 - lr: 0.01000 - Train loss: 0.62780 - Test loss: 0.57733\n",
      "Epoch 15087 - lr: 0.01000 - Train loss: 0.63558 - Test loss: 0.57704\n",
      "Epoch 15088 - lr: 0.01000 - Train loss: 0.62782 - Test loss: 0.57662\n",
      "Epoch 15089 - lr: 0.01000 - Train loss: 0.62889 - Test loss: 0.57670\n",
      "Epoch 15090 - lr: 0.01000 - Train loss: 0.62480 - Test loss: 0.57614\n",
      "Epoch 15091 - lr: 0.01000 - Train loss: 0.62310 - Test loss: 0.57638\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15092 - lr: 0.01000 - Train loss: 0.63057 - Test loss: 0.57671\n",
      "Epoch 15093 - lr: 0.01000 - Train loss: 0.65460 - Test loss: 0.57623\n",
      "Epoch 15094 - lr: 0.01000 - Train loss: 0.64007 - Test loss: 0.57564\n",
      "Epoch 15095 - lr: 0.01000 - Train loss: 0.62483 - Test loss: 0.57533\n",
      "Epoch 15096 - lr: 0.01000 - Train loss: 0.61836 - Test loss: 0.57587\n",
      "Epoch 15097 - lr: 0.01000 - Train loss: 0.61978 - Test loss: 0.57632\n",
      "Epoch 15098 - lr: 0.01000 - Train loss: 0.63021 - Test loss: 0.57668\n",
      "Epoch 15099 - lr: 0.01000 - Train loss: 0.64888 - Test loss: 0.57612\n",
      "Epoch 15100 - lr: 0.01000 - Train loss: 0.63537 - Test loss: 0.57581\n",
      "Epoch 15101 - lr: 0.01000 - Train loss: 0.63656 - Test loss: 0.57566\n",
      "Epoch 15102 - lr: 0.01000 - Train loss: 0.63550 - Test loss: 0.57521\n",
      "Epoch 15103 - lr: 0.01000 - Train loss: 0.63121 - Test loss: 0.57557\n",
      "Epoch 15104 - lr: 0.01000 - Train loss: 0.63636 - Test loss: 0.57516\n",
      "Epoch 15105 - lr: 0.01000 - Train loss: 0.63085 - Test loss: 0.57540\n",
      "Epoch 15106 - lr: 0.01000 - Train loss: 0.63587 - Test loss: 0.57524\n",
      "Epoch 15107 - lr: 0.01000 - Train loss: 0.63629 - Test loss: 0.57508\n",
      "Epoch 15108 - lr: 0.01000 - Train loss: 0.63724 - Test loss: 0.57505\n",
      "Epoch 15109 - lr: 0.01000 - Train loss: 0.63546 - Test loss: 0.57466\n",
      "Epoch 15110 - lr: 0.01000 - Train loss: 0.63165 - Test loss: 0.57509\n",
      "Epoch 15111 - lr: 0.01000 - Train loss: 0.63875 - Test loss: 0.57469\n",
      "Epoch 15112 - lr: 0.01000 - Train loss: 0.62700 - Test loss: 0.57460\n",
      "Epoch 15113 - lr: 0.01000 - Train loss: 0.63010 - Test loss: 0.57484\n",
      "Epoch 15114 - lr: 0.01000 - Train loss: 0.62705 - Test loss: 0.57428\n",
      "Epoch 15115 - lr: 0.01000 - Train loss: 0.61904 - Test loss: 0.57482\n",
      "Epoch 15116 - lr: 0.01000 - Train loss: 0.62006 - Test loss: 0.57529\n",
      "Epoch 15117 - lr: 0.01000 - Train loss: 0.61802 - Test loss: 0.57577\n",
      "Epoch 15118 - lr: 0.01000 - Train loss: 0.62084 - Test loss: 0.57605\n",
      "Epoch 15119 - lr: 0.01000 - Train loss: 0.62048 - Test loss: 0.57641\n",
      "Epoch 15120 - lr: 0.01000 - Train loss: 0.63209 - Test loss: 0.57665\n",
      "Epoch 15121 - lr: 0.01000 - Train loss: 0.65082 - Test loss: 0.57604\n",
      "Epoch 15122 - lr: 0.01000 - Train loss: 0.63589 - Test loss: 0.57564\n",
      "Epoch 15123 - lr: 0.01000 - Train loss: 0.62969 - Test loss: 0.57507\n",
      "Epoch 15124 - lr: 0.01000 - Train loss: 0.63174 - Test loss: 0.57543\n",
      "Epoch 15125 - lr: 0.01000 - Train loss: 0.62883 - Test loss: 0.57525\n",
      "Epoch 15126 - lr: 0.01000 - Train loss: 0.61821 - Test loss: 0.57564\n",
      "Epoch 15127 - lr: 0.01000 - Train loss: 0.62064 - Test loss: 0.57593\n",
      "Epoch 15128 - lr: 0.01000 - Train loss: 0.61951 - Test loss: 0.57630\n",
      "Epoch 15129 - lr: 0.01000 - Train loss: 0.62809 - Test loss: 0.57665\n",
      "Epoch 15130 - lr: 0.01000 - Train loss: 0.63170 - Test loss: 0.57599\n",
      "Epoch 15131 - lr: 0.01000 - Train loss: 0.62649 - Test loss: 0.57565\n",
      "Epoch 15132 - lr: 0.01000 - Train loss: 0.63187 - Test loss: 0.57603\n",
      "Epoch 15133 - lr: 0.01000 - Train loss: 0.64821 - Test loss: 0.57549\n",
      "Epoch 15134 - lr: 0.01000 - Train loss: 0.62767 - Test loss: 0.57484\n",
      "Epoch 15135 - lr: 0.01000 - Train loss: 0.61909 - Test loss: 0.57534\n",
      "Epoch 15136 - lr: 0.01000 - Train loss: 0.62461 - Test loss: 0.57582\n",
      "Epoch 15137 - lr: 0.01000 - Train loss: 0.62634 - Test loss: 0.57519\n",
      "Epoch 15138 - lr: 0.01000 - Train loss: 0.61973 - Test loss: 0.57557\n",
      "Epoch 15139 - lr: 0.01000 - Train loss: 0.61788 - Test loss: 0.57601\n",
      "Epoch 15140 - lr: 0.01000 - Train loss: 0.62487 - Test loss: 0.57618\n",
      "Epoch 15141 - lr: 0.01000 - Train loss: 0.63029 - Test loss: 0.57619\n",
      "Epoch 15142 - lr: 0.01000 - Train loss: 0.62674 - Test loss: 0.57544\n",
      "Epoch 15143 - lr: 0.01000 - Train loss: 0.62174 - Test loss: 0.57574\n",
      "Epoch 15144 - lr: 0.01000 - Train loss: 0.62748 - Test loss: 0.57621\n",
      "Epoch 15145 - lr: 0.01000 - Train loss: 0.62940 - Test loss: 0.57560\n",
      "Epoch 15146 - lr: 0.01000 - Train loss: 0.63163 - Test loss: 0.57594\n",
      "Epoch 15147 - lr: 0.01000 - Train loss: 0.64079 - Test loss: 0.57540\n",
      "Epoch 15148 - lr: 0.01000 - Train loss: 0.62502 - Test loss: 0.57499\n",
      "Epoch 15149 - lr: 0.01000 - Train loss: 0.61830 - Test loss: 0.57548\n",
      "Epoch 15150 - lr: 0.01000 - Train loss: 0.62320 - Test loss: 0.57577\n",
      "Epoch 15151 - lr: 0.01000 - Train loss: 0.63161 - Test loss: 0.57617\n",
      "Epoch 15152 - lr: 0.01000 - Train loss: 0.64711 - Test loss: 0.57561\n",
      "Epoch 15153 - lr: 0.01000 - Train loss: 0.62687 - Test loss: 0.57492\n",
      "Epoch 15154 - lr: 0.01000 - Train loss: 0.61960 - Test loss: 0.57536\n",
      "Epoch 15155 - lr: 0.01000 - Train loss: 0.61779 - Test loss: 0.57584\n",
      "Epoch 15156 - lr: 0.01000 - Train loss: 0.62431 - Test loss: 0.57606\n",
      "Epoch 15157 - lr: 0.01000 - Train loss: 0.63097 - Test loss: 0.57620\n",
      "Epoch 15158 - lr: 0.01000 - Train loss: 0.63289 - Test loss: 0.57572\n",
      "Epoch 15159 - lr: 0.01000 - Train loss: 0.62558 - Test loss: 0.57514\n",
      "Epoch 15160 - lr: 0.01000 - Train loss: 0.62101 - Test loss: 0.57552\n",
      "Epoch 15161 - lr: 0.01000 - Train loss: 0.62393 - Test loss: 0.57601\n",
      "Epoch 15162 - lr: 0.01000 - Train loss: 0.62526 - Test loss: 0.57547\n",
      "Epoch 15163 - lr: 0.01000 - Train loss: 0.62215 - Test loss: 0.57578\n",
      "Epoch 15164 - lr: 0.01000 - Train loss: 0.62953 - Test loss: 0.57625\n",
      "Epoch 15165 - lr: 0.01000 - Train loss: 0.62947 - Test loss: 0.57598\n",
      "Epoch 15166 - lr: 0.01000 - Train loss: 0.63080 - Test loss: 0.57632\n",
      "Epoch 15167 - lr: 0.01000 - Train loss: 0.64379 - Test loss: 0.57571\n",
      "Epoch 15168 - lr: 0.01000 - Train loss: 0.62645 - Test loss: 0.57500\n",
      "Epoch 15169 - lr: 0.01000 - Train loss: 0.61864 - Test loss: 0.57546\n",
      "Epoch 15170 - lr: 0.01000 - Train loss: 0.62082 - Test loss: 0.57582\n",
      "Epoch 15171 - lr: 0.01000 - Train loss: 0.62226 - Test loss: 0.57624\n",
      "Epoch 15172 - lr: 0.01000 - Train loss: 0.62644 - Test loss: 0.57595\n",
      "Epoch 15173 - lr: 0.01000 - Train loss: 0.63145 - Test loss: 0.57622\n",
      "Epoch 15174 - lr: 0.01000 - Train loss: 0.62886 - Test loss: 0.57589\n",
      "Epoch 15175 - lr: 0.01000 - Train loss: 0.62380 - Test loss: 0.57603\n",
      "Epoch 15176 - lr: 0.01000 - Train loss: 0.63145 - Test loss: 0.57627\n",
      "Epoch 15177 - lr: 0.01000 - Train loss: 0.63186 - Test loss: 0.57600\n",
      "Epoch 15178 - lr: 0.01000 - Train loss: 0.62524 - Test loss: 0.57556\n",
      "Epoch 15179 - lr: 0.01000 - Train loss: 0.62065 - Test loss: 0.57599\n",
      "Epoch 15180 - lr: 0.01000 - Train loss: 0.63119 - Test loss: 0.57617\n",
      "Epoch 15181 - lr: 0.01000 - Train loss: 0.63747 - Test loss: 0.57596\n",
      "Epoch 15182 - lr: 0.01000 - Train loss: 0.63640 - Test loss: 0.57539\n",
      "Epoch 15183 - lr: 0.01000 - Train loss: 0.63123 - Test loss: 0.57559\n",
      "Epoch 15184 - lr: 0.01000 - Train loss: 0.63726 - Test loss: 0.57543\n",
      "Epoch 15185 - lr: 0.01000 - Train loss: 0.63050 - Test loss: 0.57521\n",
      "Epoch 15186 - lr: 0.01000 - Train loss: 0.63104 - Test loss: 0.57537\n",
      "Epoch 15187 - lr: 0.01000 - Train loss: 0.63048 - Test loss: 0.57488\n",
      "Epoch 15188 - lr: 0.01000 - Train loss: 0.62930 - Test loss: 0.57496\n",
      "Epoch 15189 - lr: 0.01000 - Train loss: 0.62700 - Test loss: 0.57436\n",
      "Epoch 15190 - lr: 0.01000 - Train loss: 0.61829 - Test loss: 0.57494\n",
      "Epoch 15191 - lr: 0.01000 - Train loss: 0.61857 - Test loss: 0.57546\n",
      "Epoch 15192 - lr: 0.01000 - Train loss: 0.62061 - Test loss: 0.57589\n",
      "Epoch 15193 - lr: 0.01000 - Train loss: 0.63173 - Test loss: 0.57609\n",
      "Epoch 15194 - lr: 0.01000 - Train loss: 0.63341 - Test loss: 0.57584\n",
      "Epoch 15195 - lr: 0.01000 - Train loss: 0.62660 - Test loss: 0.57507\n",
      "Epoch 15196 - lr: 0.01000 - Train loss: 0.61829 - Test loss: 0.57549\n",
      "Epoch 15197 - lr: 0.01000 - Train loss: 0.62262 - Test loss: 0.57576\n",
      "Epoch 15198 - lr: 0.01000 - Train loss: 0.63108 - Test loss: 0.57618\n",
      "Epoch 15199 - lr: 0.01000 - Train loss: 0.64766 - Test loss: 0.57558\n",
      "Epoch 15200 - lr: 0.01000 - Train loss: 0.62704 - Test loss: 0.57485\n",
      "Epoch 15201 - lr: 0.01000 - Train loss: 0.61878 - Test loss: 0.57528\n",
      "Epoch 15202 - lr: 0.01000 - Train loss: 0.61994 - Test loss: 0.57565\n",
      "Epoch 15203 - lr: 0.01000 - Train loss: 0.61809 - Test loss: 0.57606\n",
      "Epoch 15204 - lr: 0.01000 - Train loss: 0.61763 - Test loss: 0.57639\n",
      "Epoch 15205 - lr: 0.01000 - Train loss: 0.62488 - Test loss: 0.57648\n",
      "Epoch 15206 - lr: 0.01000 - Train loss: 0.62981 - Test loss: 0.57638\n",
      "Epoch 15207 - lr: 0.01000 - Train loss: 0.62665 - Test loss: 0.57555\n",
      "Epoch 15208 - lr: 0.01000 - Train loss: 0.62006 - Test loss: 0.57585\n",
      "Epoch 15209 - lr: 0.01000 - Train loss: 0.61880 - Test loss: 0.57626\n",
      "Epoch 15210 - lr: 0.01000 - Train loss: 0.62515 - Test loss: 0.57660\n",
      "Epoch 15211 - lr: 0.01000 - Train loss: 0.62629 - Test loss: 0.57577\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15212 - lr: 0.01000 - Train loss: 0.62141 - Test loss: 0.57601\n",
      "Epoch 15213 - lr: 0.01000 - Train loss: 0.62672 - Test loss: 0.57642\n",
      "Epoch 15214 - lr: 0.01000 - Train loss: 0.62743 - Test loss: 0.57572\n",
      "Epoch 15215 - lr: 0.01000 - Train loss: 0.62104 - Test loss: 0.57610\n",
      "Epoch 15216 - lr: 0.01000 - Train loss: 0.62941 - Test loss: 0.57606\n",
      "Epoch 15217 - lr: 0.01000 - Train loss: 0.62594 - Test loss: 0.57534\n",
      "Epoch 15218 - lr: 0.01000 - Train loss: 0.62179 - Test loss: 0.57565\n",
      "Epoch 15219 - lr: 0.01000 - Train loss: 0.62884 - Test loss: 0.57612\n",
      "Epoch 15220 - lr: 0.01000 - Train loss: 0.63343 - Test loss: 0.57591\n",
      "Epoch 15221 - lr: 0.01000 - Train loss: 0.62601 - Test loss: 0.57516\n",
      "Epoch 15222 - lr: 0.01000 - Train loss: 0.62114 - Test loss: 0.57550\n",
      "Epoch 15223 - lr: 0.01000 - Train loss: 0.62585 - Test loss: 0.57597\n",
      "Epoch 15224 - lr: 0.01000 - Train loss: 0.62601 - Test loss: 0.57530\n",
      "Epoch 15225 - lr: 0.01000 - Train loss: 0.62130 - Test loss: 0.57561\n",
      "Epoch 15226 - lr: 0.01000 - Train loss: 0.62661 - Test loss: 0.57608\n",
      "Epoch 15227 - lr: 0.01000 - Train loss: 0.62805 - Test loss: 0.57547\n",
      "Epoch 15228 - lr: 0.01000 - Train loss: 0.62893 - Test loss: 0.57590\n",
      "Epoch 15229 - lr: 0.01000 - Train loss: 0.63058 - Test loss: 0.57567\n",
      "Epoch 15230 - lr: 0.01000 - Train loss: 0.62745 - Test loss: 0.57547\n",
      "Epoch 15231 - lr: 0.01000 - Train loss: 0.62535 - Test loss: 0.57521\n",
      "Epoch 15232 - lr: 0.01000 - Train loss: 0.62898 - Test loss: 0.57571\n",
      "Epoch 15233 - lr: 0.01000 - Train loss: 0.62993 - Test loss: 0.57550\n",
      "Epoch 15234 - lr: 0.01000 - Train loss: 0.63011 - Test loss: 0.57556\n",
      "Epoch 15235 - lr: 0.01000 - Train loss: 0.62770 - Test loss: 0.57499\n",
      "Epoch 15236 - lr: 0.01000 - Train loss: 0.62663 - Test loss: 0.57547\n",
      "Epoch 15237 - lr: 0.01000 - Train loss: 0.62756 - Test loss: 0.57492\n",
      "Epoch 15238 - lr: 0.01000 - Train loss: 0.62548 - Test loss: 0.57541\n",
      "Epoch 15239 - lr: 0.01000 - Train loss: 0.62587 - Test loss: 0.57478\n",
      "Epoch 15240 - lr: 0.01000 - Train loss: 0.62165 - Test loss: 0.57514\n",
      "Epoch 15241 - lr: 0.01000 - Train loss: 0.62864 - Test loss: 0.57565\n",
      "Epoch 15242 - lr: 0.01000 - Train loss: 0.63374 - Test loss: 0.57549\n",
      "Epoch 15243 - lr: 0.01000 - Train loss: 0.62598 - Test loss: 0.57478\n",
      "Epoch 15244 - lr: 0.01000 - Train loss: 0.62163 - Test loss: 0.57512\n",
      "Epoch 15245 - lr: 0.01000 - Train loss: 0.62858 - Test loss: 0.57563\n",
      "Epoch 15246 - lr: 0.01000 - Train loss: 0.63467 - Test loss: 0.57548\n",
      "Epoch 15247 - lr: 0.01000 - Train loss: 0.62829 - Test loss: 0.57486\n",
      "Epoch 15248 - lr: 0.01000 - Train loss: 0.62985 - Test loss: 0.57534\n",
      "Epoch 15249 - lr: 0.01000 - Train loss: 0.63547 - Test loss: 0.57488\n",
      "Epoch 15250 - lr: 0.01000 - Train loss: 0.63110 - Test loss: 0.57517\n",
      "Epoch 15251 - lr: 0.01000 - Train loss: 0.62857 - Test loss: 0.57498\n",
      "Epoch 15252 - lr: 0.01000 - Train loss: 0.62505 - Test loss: 0.57539\n",
      "Epoch 15253 - lr: 0.01000 - Train loss: 0.62641 - Test loss: 0.57467\n",
      "Epoch 15254 - lr: 0.01000 - Train loss: 0.61846 - Test loss: 0.57510\n",
      "Epoch 15255 - lr: 0.01000 - Train loss: 0.61999 - Test loss: 0.57545\n",
      "Epoch 15256 - lr: 0.01000 - Train loss: 0.61884 - Test loss: 0.57585\n",
      "Epoch 15257 - lr: 0.01000 - Train loss: 0.62722 - Test loss: 0.57621\n",
      "Epoch 15258 - lr: 0.01000 - Train loss: 0.63068 - Test loss: 0.57556\n",
      "Epoch 15259 - lr: 0.01000 - Train loss: 0.62663 - Test loss: 0.57530\n",
      "Epoch 15260 - lr: 0.01000 - Train loss: 0.62888 - Test loss: 0.57530\n",
      "Epoch 15261 - lr: 0.01000 - Train loss: 0.62572 - Test loss: 0.57465\n",
      "Epoch 15262 - lr: 0.01000 - Train loss: 0.62010 - Test loss: 0.57505\n",
      "Epoch 15263 - lr: 0.01000 - Train loss: 0.62060 - Test loss: 0.57553\n",
      "Epoch 15264 - lr: 0.01000 - Train loss: 0.62971 - Test loss: 0.57559\n",
      "Epoch 15265 - lr: 0.01000 - Train loss: 0.62618 - Test loss: 0.57494\n",
      "Epoch 15266 - lr: 0.01000 - Train loss: 0.61751 - Test loss: 0.57538\n",
      "Epoch 15267 - lr: 0.01000 - Train loss: 0.62452 - Test loss: 0.57561\n",
      "Epoch 15268 - lr: 0.01000 - Train loss: 0.62846 - Test loss: 0.57557\n",
      "Epoch 15269 - lr: 0.01000 - Train loss: 0.62487 - Test loss: 0.57496\n",
      "Epoch 15270 - lr: 0.01000 - Train loss: 0.62277 - Test loss: 0.57527\n",
      "Epoch 15271 - lr: 0.01000 - Train loss: 0.63061 - Test loss: 0.57563\n",
      "Epoch 15272 - lr: 0.01000 - Train loss: 0.63594 - Test loss: 0.57512\n",
      "Epoch 15273 - lr: 0.01000 - Train loss: 0.63005 - Test loss: 0.57527\n",
      "Epoch 15274 - lr: 0.01000 - Train loss: 0.63299 - Test loss: 0.57494\n",
      "Epoch 15275 - lr: 0.01000 - Train loss: 0.62543 - Test loss: 0.57433\n",
      "Epoch 15276 - lr: 0.01000 - Train loss: 0.62085 - Test loss: 0.57477\n",
      "Epoch 15277 - lr: 0.01000 - Train loss: 0.62573 - Test loss: 0.57532\n",
      "Epoch 15278 - lr: 0.01000 - Train loss: 0.62590 - Test loss: 0.57474\n",
      "Epoch 15279 - lr: 0.01000 - Train loss: 0.61752 - Test loss: 0.57521\n",
      "Epoch 15280 - lr: 0.01000 - Train loss: 0.62381 - Test loss: 0.57548\n",
      "Epoch 15281 - lr: 0.01000 - Train loss: 0.62956 - Test loss: 0.57558\n",
      "Epoch 15282 - lr: 0.01000 - Train loss: 0.62793 - Test loss: 0.57502\n",
      "Epoch 15283 - lr: 0.01000 - Train loss: 0.63002 - Test loss: 0.57545\n",
      "Epoch 15284 - lr: 0.01000 - Train loss: 0.64649 - Test loss: 0.57495\n",
      "Epoch 15285 - lr: 0.01000 - Train loss: 0.62723 - Test loss: 0.57440\n",
      "Epoch 15286 - lr: 0.01000 - Train loss: 0.62616 - Test loss: 0.57496\n",
      "Epoch 15287 - lr: 0.01000 - Train loss: 0.62688 - Test loss: 0.57447\n",
      "Epoch 15288 - lr: 0.01000 - Train loss: 0.62276 - Test loss: 0.57499\n",
      "Epoch 15289 - lr: 0.01000 - Train loss: 0.62411 - Test loss: 0.57462\n",
      "Epoch 15290 - lr: 0.01000 - Train loss: 0.61920 - Test loss: 0.57505\n",
      "Epoch 15291 - lr: 0.01000 - Train loss: 0.61681 - Test loss: 0.57555\n",
      "Epoch 15292 - lr: 0.01000 - Train loss: 0.62159 - Test loss: 0.57578\n",
      "Epoch 15293 - lr: 0.01000 - Train loss: 0.62834 - Test loss: 0.57616\n",
      "Epoch 15294 - lr: 0.01000 - Train loss: 0.62992 - Test loss: 0.57583\n",
      "Epoch 15295 - lr: 0.01000 - Train loss: 0.62817 - Test loss: 0.57563\n",
      "Epoch 15296 - lr: 0.01000 - Train loss: 0.62433 - Test loss: 0.57504\n",
      "Epoch 15297 - lr: 0.01000 - Train loss: 0.62316 - Test loss: 0.57532\n",
      "Epoch 15298 - lr: 0.01000 - Train loss: 0.63010 - Test loss: 0.57555\n",
      "Epoch 15299 - lr: 0.01000 - Train loss: 0.63573 - Test loss: 0.57542\n",
      "Epoch 15300 - lr: 0.01000 - Train loss: 0.62853 - Test loss: 0.57507\n",
      "Epoch 15301 - lr: 0.01000 - Train loss: 0.62271 - Test loss: 0.57527\n",
      "Epoch 15302 - lr: 0.01000 - Train loss: 0.63068 - Test loss: 0.57561\n",
      "Epoch 15303 - lr: 0.01000 - Train loss: 0.64314 - Test loss: 0.57502\n",
      "Epoch 15304 - lr: 0.01000 - Train loss: 0.62534 - Test loss: 0.57435\n",
      "Epoch 15305 - lr: 0.01000 - Train loss: 0.62038 - Test loss: 0.57477\n",
      "Epoch 15306 - lr: 0.01000 - Train loss: 0.62368 - Test loss: 0.57529\n",
      "Epoch 15307 - lr: 0.01000 - Train loss: 0.62472 - Test loss: 0.57472\n",
      "Epoch 15308 - lr: 0.01000 - Train loss: 0.62231 - Test loss: 0.57505\n",
      "Epoch 15309 - lr: 0.01000 - Train loss: 0.63041 - Test loss: 0.57546\n",
      "Epoch 15310 - lr: 0.01000 - Train loss: 0.64598 - Test loss: 0.57493\n",
      "Epoch 15311 - lr: 0.01000 - Train loss: 0.62615 - Test loss: 0.57433\n",
      "Epoch 15312 - lr: 0.01000 - Train loss: 0.61700 - Test loss: 0.57488\n",
      "Epoch 15313 - lr: 0.01000 - Train loss: 0.61845 - Test loss: 0.57530\n",
      "Epoch 15314 - lr: 0.01000 - Train loss: 0.61837 - Test loss: 0.57565\n",
      "Epoch 15315 - lr: 0.01000 - Train loss: 0.61907 - Test loss: 0.57592\n",
      "Epoch 15316 - lr: 0.01000 - Train loss: 0.61649 - Test loss: 0.57626\n",
      "Epoch 15317 - lr: 0.01000 - Train loss: 0.62782 - Test loss: 0.57633\n",
      "Epoch 15318 - lr: 0.01000 - Train loss: 0.62412 - Test loss: 0.57566\n",
      "Epoch 15319 - lr: 0.01000 - Train loss: 0.62357 - Test loss: 0.57584\n",
      "Epoch 15320 - lr: 0.01000 - Train loss: 0.62941 - Test loss: 0.57589\n",
      "Epoch 15321 - lr: 0.01000 - Train loss: 0.62945 - Test loss: 0.57532\n",
      "Epoch 15322 - lr: 0.01000 - Train loss: 0.62684 - Test loss: 0.57518\n",
      "Epoch 15323 - lr: 0.01000 - Train loss: 0.62434 - Test loss: 0.57491\n",
      "Epoch 15324 - lr: 0.01000 - Train loss: 0.62423 - Test loss: 0.57540\n",
      "Epoch 15325 - lr: 0.01000 - Train loss: 0.62454 - Test loss: 0.57483\n",
      "Epoch 15326 - lr: 0.01000 - Train loss: 0.62253 - Test loss: 0.57515\n",
      "Epoch 15327 - lr: 0.01000 - Train loss: 0.62993 - Test loss: 0.57549\n",
      "Epoch 15328 - lr: 0.01000 - Train loss: 0.63063 - Test loss: 0.57510\n",
      "Epoch 15329 - lr: 0.01000 - Train loss: 0.61714 - Test loss: 0.57552\n",
      "Epoch 15330 - lr: 0.01000 - Train loss: 0.61633 - Test loss: 0.57595\n",
      "Epoch 15331 - lr: 0.01000 - Train loss: 0.62355 - Test loss: 0.57607\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15332 - lr: 0.01000 - Train loss: 0.62948 - Test loss: 0.57608\n",
      "Epoch 15333 - lr: 0.01000 - Train loss: 0.63303 - Test loss: 0.57562\n",
      "Epoch 15334 - lr: 0.01000 - Train loss: 0.62597 - Test loss: 0.57498\n",
      "Epoch 15335 - lr: 0.01000 - Train loss: 0.61927 - Test loss: 0.57543\n",
      "Epoch 15336 - lr: 0.01000 - Train loss: 0.63009 - Test loss: 0.57571\n",
      "Epoch 15337 - lr: 0.01000 - Train loss: 0.63987 - Test loss: 0.57510\n",
      "Epoch 15338 - lr: 0.01000 - Train loss: 0.62364 - Test loss: 0.57465\n",
      "Epoch 15339 - lr: 0.01000 - Train loss: 0.61812 - Test loss: 0.57510\n",
      "Epoch 15340 - lr: 0.01000 - Train loss: 0.61780 - Test loss: 0.57551\n",
      "Epoch 15341 - lr: 0.01000 - Train loss: 0.62039 - Test loss: 0.57577\n",
      "Epoch 15342 - lr: 0.01000 - Train loss: 0.62337 - Test loss: 0.57612\n",
      "Epoch 15343 - lr: 0.01000 - Train loss: 0.62396 - Test loss: 0.57546\n",
      "Epoch 15344 - lr: 0.01000 - Train loss: 0.62357 - Test loss: 0.57566\n",
      "Epoch 15345 - lr: 0.01000 - Train loss: 0.62853 - Test loss: 0.57566\n",
      "Epoch 15346 - lr: 0.01000 - Train loss: 0.62592 - Test loss: 0.57508\n",
      "Epoch 15347 - lr: 0.01000 - Train loss: 0.62194 - Test loss: 0.57551\n",
      "Epoch 15348 - lr: 0.01000 - Train loss: 0.62370 - Test loss: 0.57511\n",
      "Epoch 15349 - lr: 0.01000 - Train loss: 0.61603 - Test loss: 0.57561\n",
      "Epoch 15350 - lr: 0.01000 - Train loss: 0.62139 - Test loss: 0.57583\n",
      "Epoch 15351 - lr: 0.01000 - Train loss: 0.62814 - Test loss: 0.57618\n",
      "Epoch 15352 - lr: 0.01000 - Train loss: 0.63873 - Test loss: 0.57550\n",
      "Epoch 15353 - lr: 0.01000 - Train loss: 0.62403 - Test loss: 0.57508\n",
      "Epoch 15354 - lr: 0.01000 - Train loss: 0.62220 - Test loss: 0.57553\n",
      "Epoch 15355 - lr: 0.01000 - Train loss: 0.62348 - Test loss: 0.57509\n",
      "Epoch 15356 - lr: 0.01000 - Train loss: 0.61658 - Test loss: 0.57553\n",
      "Epoch 15357 - lr: 0.01000 - Train loss: 0.62587 - Test loss: 0.57574\n",
      "Epoch 15358 - lr: 0.01000 - Train loss: 0.62387 - Test loss: 0.57535\n",
      "Epoch 15359 - lr: 0.01000 - Train loss: 0.62052 - Test loss: 0.57575\n",
      "Epoch 15360 - lr: 0.01000 - Train loss: 0.62627 - Test loss: 0.57554\n",
      "Epoch 15361 - lr: 0.01000 - Train loss: 0.62479 - Test loss: 0.57525\n",
      "Epoch 15362 - lr: 0.01000 - Train loss: 0.62911 - Test loss: 0.57556\n",
      "Epoch 15363 - lr: 0.01000 - Train loss: 0.64558 - Test loss: 0.57502\n",
      "Epoch 15364 - lr: 0.01000 - Train loss: 0.63192 - Test loss: 0.57469\n",
      "Epoch 15365 - lr: 0.01000 - Train loss: 0.62560 - Test loss: 0.57424\n",
      "Epoch 15366 - lr: 0.01000 - Train loss: 0.62293 - Test loss: 0.57479\n",
      "Epoch 15367 - lr: 0.01000 - Train loss: 0.62346 - Test loss: 0.57439\n",
      "Epoch 15368 - lr: 0.01000 - Train loss: 0.62021 - Test loss: 0.57480\n",
      "Epoch 15369 - lr: 0.01000 - Train loss: 0.62431 - Test loss: 0.57530\n",
      "Epoch 15370 - lr: 0.01000 - Train loss: 0.62605 - Test loss: 0.57479\n",
      "Epoch 15371 - lr: 0.01000 - Train loss: 0.62637 - Test loss: 0.57524\n",
      "Epoch 15372 - lr: 0.01000 - Train loss: 0.63314 - Test loss: 0.57511\n",
      "Epoch 15373 - lr: 0.01000 - Train loss: 0.63473 - Test loss: 0.57493\n",
      "Epoch 15374 - lr: 0.01000 - Train loss: 0.64694 - Test loss: 0.57442\n",
      "Epoch 15375 - lr: 0.01000 - Train loss: 0.63423 - Test loss: 0.57424\n",
      "Epoch 15376 - lr: 0.01000 - Train loss: 0.63083 - Test loss: 0.57415\n",
      "Epoch 15377 - lr: 0.01000 - Train loss: 0.62354 - Test loss: 0.57373\n",
      "Epoch 15378 - lr: 0.01000 - Train loss: 0.62141 - Test loss: 0.57416\n",
      "Epoch 15379 - lr: 0.01000 - Train loss: 0.62905 - Test loss: 0.57469\n",
      "Epoch 15380 - lr: 0.01000 - Train loss: 0.64708 - Test loss: 0.57423\n",
      "Epoch 15381 - lr: 0.01000 - Train loss: 0.63310 - Test loss: 0.57397\n",
      "Epoch 15382 - lr: 0.01000 - Train loss: 0.62631 - Test loss: 0.57352\n",
      "Epoch 15383 - lr: 0.01000 - Train loss: 0.62433 - Test loss: 0.57412\n",
      "Epoch 15384 - lr: 0.01000 - Train loss: 0.62454 - Test loss: 0.57362\n",
      "Epoch 15385 - lr: 0.01000 - Train loss: 0.62131 - Test loss: 0.57405\n",
      "Epoch 15386 - lr: 0.01000 - Train loss: 0.62903 - Test loss: 0.57459\n",
      "Epoch 15387 - lr: 0.01000 - Train loss: 0.64473 - Test loss: 0.57411\n",
      "Epoch 15388 - lr: 0.01000 - Train loss: 0.62542 - Test loss: 0.57357\n",
      "Epoch 15389 - lr: 0.01000 - Train loss: 0.61626 - Test loss: 0.57417\n",
      "Epoch 15390 - lr: 0.01000 - Train loss: 0.62122 - Test loss: 0.57454\n",
      "Epoch 15391 - lr: 0.01000 - Train loss: 0.62869 - Test loss: 0.57501\n",
      "Epoch 15392 - lr: 0.01000 - Train loss: 0.63878 - Test loss: 0.57443\n",
      "Epoch 15393 - lr: 0.01000 - Train loss: 0.62367 - Test loss: 0.57406\n",
      "Epoch 15394 - lr: 0.01000 - Train loss: 0.61693 - Test loss: 0.57460\n",
      "Epoch 15395 - lr: 0.01000 - Train loss: 0.61681 - Test loss: 0.57508\n",
      "Epoch 15396 - lr: 0.01000 - Train loss: 0.61599 - Test loss: 0.57547\n",
      "Epoch 15397 - lr: 0.01000 - Train loss: 0.62562 - Test loss: 0.57556\n",
      "Epoch 15398 - lr: 0.01000 - Train loss: 0.62412 - Test loss: 0.57510\n",
      "Epoch 15399 - lr: 0.01000 - Train loss: 0.62316 - Test loss: 0.57548\n",
      "Epoch 15400 - lr: 0.01000 - Train loss: 0.62384 - Test loss: 0.57480\n",
      "Epoch 15401 - lr: 0.01000 - Train loss: 0.62400 - Test loss: 0.57502\n",
      "Epoch 15402 - lr: 0.01000 - Train loss: 0.62656 - Test loss: 0.57490\n",
      "Epoch 15403 - lr: 0.01000 - Train loss: 0.62344 - Test loss: 0.57453\n",
      "Epoch 15404 - lr: 0.01000 - Train loss: 0.61660 - Test loss: 0.57504\n",
      "Epoch 15405 - lr: 0.01000 - Train loss: 0.61593 - Test loss: 0.57548\n",
      "Epoch 15406 - lr: 0.01000 - Train loss: 0.62042 - Test loss: 0.57566\n",
      "Epoch 15407 - lr: 0.01000 - Train loss: 0.62488 - Test loss: 0.57597\n",
      "Epoch 15408 - lr: 0.01000 - Train loss: 0.62782 - Test loss: 0.57528\n",
      "Epoch 15409 - lr: 0.01000 - Train loss: 0.62899 - Test loss: 0.57533\n",
      "Epoch 15410 - lr: 0.01000 - Train loss: 0.63494 - Test loss: 0.57512\n",
      "Epoch 15411 - lr: 0.01000 - Train loss: 0.64956 - Test loss: 0.57457\n",
      "Epoch 15412 - lr: 0.01000 - Train loss: 0.63129 - Test loss: 0.57438\n",
      "Epoch 15413 - lr: 0.01000 - Train loss: 0.62388 - Test loss: 0.57381\n",
      "Epoch 15414 - lr: 0.01000 - Train loss: 0.62227 - Test loss: 0.57420\n",
      "Epoch 15415 - lr: 0.01000 - Train loss: 0.62929 - Test loss: 0.57452\n",
      "Epoch 15416 - lr: 0.01000 - Train loss: 0.63195 - Test loss: 0.57441\n",
      "Epoch 15417 - lr: 0.01000 - Train loss: 0.62430 - Test loss: 0.57379\n",
      "Epoch 15418 - lr: 0.01000 - Train loss: 0.62165 - Test loss: 0.57417\n",
      "Epoch 15419 - lr: 0.01000 - Train loss: 0.62950 - Test loss: 0.57461\n",
      "Epoch 15420 - lr: 0.01000 - Train loss: 0.64524 - Test loss: 0.57411\n",
      "Epoch 15421 - lr: 0.01000 - Train loss: 0.62655 - Test loss: 0.57359\n",
      "Epoch 15422 - lr: 0.01000 - Train loss: 0.62705 - Test loss: 0.57416\n",
      "Epoch 15423 - lr: 0.01000 - Train loss: 0.63499 - Test loss: 0.57414\n",
      "Epoch 15424 - lr: 0.01000 - Train loss: 0.62684 - Test loss: 0.57392\n",
      "Epoch 15425 - lr: 0.01000 - Train loss: 0.62223 - Test loss: 0.57419\n",
      "Epoch 15426 - lr: 0.01000 - Train loss: 0.62993 - Test loss: 0.57452\n",
      "Epoch 15427 - lr: 0.01000 - Train loss: 0.62763 - Test loss: 0.57423\n",
      "Epoch 15428 - lr: 0.01000 - Train loss: 0.62232 - Test loss: 0.57444\n",
      "Epoch 15429 - lr: 0.01000 - Train loss: 0.63003 - Test loss: 0.57472\n",
      "Epoch 15430 - lr: 0.01000 - Train loss: 0.62680 - Test loss: 0.57443\n",
      "Epoch 15431 - lr: 0.01000 - Train loss: 0.62217 - Test loss: 0.57460\n",
      "Epoch 15432 - lr: 0.01000 - Train loss: 0.63025 - Test loss: 0.57489\n",
      "Epoch 15433 - lr: 0.01000 - Train loss: 0.63918 - Test loss: 0.57426\n",
      "Epoch 15434 - lr: 0.01000 - Train loss: 0.62348 - Test loss: 0.57380\n",
      "Epoch 15435 - lr: 0.01000 - Train loss: 0.61699 - Test loss: 0.57428\n",
      "Epoch 15436 - lr: 0.01000 - Train loss: 0.62146 - Test loss: 0.57457\n",
      "Epoch 15437 - lr: 0.01000 - Train loss: 0.62984 - Test loss: 0.57498\n",
      "Epoch 15438 - lr: 0.01000 - Train loss: 0.65305 - Test loss: 0.57445\n",
      "Epoch 15439 - lr: 0.01000 - Train loss: 0.62756 - Test loss: 0.57416\n",
      "Epoch 15440 - lr: 0.01000 - Train loss: 0.62459 - Test loss: 0.57459\n",
      "Epoch 15441 - lr: 0.01000 - Train loss: 0.62550 - Test loss: 0.57381\n",
      "Epoch 15442 - lr: 0.01000 - Train loss: 0.61839 - Test loss: 0.57421\n",
      "Epoch 15443 - lr: 0.01000 - Train loss: 0.61630 - Test loss: 0.57469\n",
      "Epoch 15444 - lr: 0.01000 - Train loss: 0.61941 - Test loss: 0.57494\n",
      "Epoch 15445 - lr: 0.01000 - Train loss: 0.62006 - Test loss: 0.57529\n",
      "Epoch 15446 - lr: 0.01000 - Train loss: 0.62778 - Test loss: 0.57510\n",
      "Epoch 15447 - lr: 0.01000 - Train loss: 0.62432 - Test loss: 0.57433\n",
      "Epoch 15448 - lr: 0.01000 - Train loss: 0.62118 - Test loss: 0.57460\n",
      "Epoch 15449 - lr: 0.01000 - Train loss: 0.62950 - Test loss: 0.57503\n",
      "Epoch 15450 - lr: 0.01000 - Train loss: 0.65036 - Test loss: 0.57447\n",
      "Epoch 15451 - lr: 0.01000 - Train loss: 0.63594 - Test loss: 0.57430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15452 - lr: 0.01000 - Train loss: 0.63676 - Test loss: 0.57375\n",
      "Epoch 15453 - lr: 0.01000 - Train loss: 0.62614 - Test loss: 0.57367\n",
      "Epoch 15454 - lr: 0.01000 - Train loss: 0.62443 - Test loss: 0.57352\n",
      "Epoch 15455 - lr: 0.01000 - Train loss: 0.62955 - Test loss: 0.57409\n",
      "Epoch 15456 - lr: 0.01000 - Train loss: 0.64235 - Test loss: 0.57361\n",
      "Epoch 15457 - lr: 0.01000 - Train loss: 0.62558 - Test loss: 0.57294\n",
      "Epoch 15458 - lr: 0.01000 - Train loss: 0.61683 - Test loss: 0.57359\n",
      "Epoch 15459 - lr: 0.01000 - Train loss: 0.61743 - Test loss: 0.57416\n",
      "Epoch 15460 - lr: 0.01000 - Train loss: 0.62319 - Test loss: 0.57464\n",
      "Epoch 15461 - lr: 0.01000 - Train loss: 0.62491 - Test loss: 0.57389\n",
      "Epoch 15462 - lr: 0.01000 - Train loss: 0.61756 - Test loss: 0.57431\n",
      "Epoch 15463 - lr: 0.01000 - Train loss: 0.61736 - Test loss: 0.57469\n",
      "Epoch 15464 - lr: 0.01000 - Train loss: 0.61900 - Test loss: 0.57495\n",
      "Epoch 15465 - lr: 0.01000 - Train loss: 0.61845 - Test loss: 0.57529\n",
      "Epoch 15466 - lr: 0.01000 - Train loss: 0.63017 - Test loss: 0.57555\n",
      "Epoch 15467 - lr: 0.01000 - Train loss: 0.65697 - Test loss: 0.57494\n",
      "Epoch 15468 - lr: 0.01000 - Train loss: 0.62869 - Test loss: 0.57443\n",
      "Epoch 15469 - lr: 0.01000 - Train loss: 0.61838 - Test loss: 0.57469\n",
      "Epoch 15470 - lr: 0.01000 - Train loss: 0.61620 - Test loss: 0.57507\n",
      "Epoch 15471 - lr: 0.01000 - Train loss: 0.61959 - Test loss: 0.57524\n",
      "Epoch 15472 - lr: 0.01000 - Train loss: 0.62196 - Test loss: 0.57554\n",
      "Epoch 15473 - lr: 0.01000 - Train loss: 0.62354 - Test loss: 0.57484\n",
      "Epoch 15474 - lr: 0.01000 - Train loss: 0.62024 - Test loss: 0.57503\n",
      "Epoch 15475 - lr: 0.01000 - Train loss: 0.62662 - Test loss: 0.57541\n",
      "Epoch 15476 - lr: 0.01000 - Train loss: 0.63435 - Test loss: 0.57494\n",
      "Epoch 15477 - lr: 0.01000 - Train loss: 0.63028 - Test loss: 0.57431\n",
      "Epoch 15478 - lr: 0.01000 - Train loss: 0.62333 - Test loss: 0.57389\n",
      "Epoch 15479 - lr: 0.01000 - Train loss: 0.61626 - Test loss: 0.57440\n",
      "Epoch 15480 - lr: 0.01000 - Train loss: 0.61603 - Test loss: 0.57483\n",
      "Epoch 15481 - lr: 0.01000 - Train loss: 0.62439 - Test loss: 0.57497\n",
      "Epoch 15482 - lr: 0.01000 - Train loss: 0.62492 - Test loss: 0.57464\n",
      "Epoch 15483 - lr: 0.01000 - Train loss: 0.62934 - Test loss: 0.57479\n",
      "Epoch 15484 - lr: 0.01000 - Train loss: 0.63550 - Test loss: 0.57454\n",
      "Epoch 15485 - lr: 0.01000 - Train loss: 0.62880 - Test loss: 0.57411\n",
      "Epoch 15486 - lr: 0.01000 - Train loss: 0.61693 - Test loss: 0.57446\n",
      "Epoch 15487 - lr: 0.01000 - Train loss: 0.62030 - Test loss: 0.57470\n",
      "Epoch 15488 - lr: 0.01000 - Train loss: 0.62715 - Test loss: 0.57511\n",
      "Epoch 15489 - lr: 0.01000 - Train loss: 0.63596 - Test loss: 0.57486\n",
      "Epoch 15490 - lr: 0.01000 - Train loss: 0.64029 - Test loss: 0.57414\n",
      "Epoch 15491 - lr: 0.01000 - Train loss: 0.62381 - Test loss: 0.57351\n",
      "Epoch 15492 - lr: 0.01000 - Train loss: 0.61885 - Test loss: 0.57392\n",
      "Epoch 15493 - lr: 0.01000 - Train loss: 0.62017 - Test loss: 0.57442\n",
      "Epoch 15494 - lr: 0.01000 - Train loss: 0.62552 - Test loss: 0.57421\n",
      "Epoch 15495 - lr: 0.01000 - Train loss: 0.62635 - Test loss: 0.57410\n",
      "Epoch 15496 - lr: 0.01000 - Train loss: 0.62304 - Test loss: 0.57370\n",
      "Epoch 15497 - lr: 0.01000 - Train loss: 0.61593 - Test loss: 0.57420\n",
      "Epoch 15498 - lr: 0.01000 - Train loss: 0.62306 - Test loss: 0.57445\n",
      "Epoch 15499 - lr: 0.01000 - Train loss: 0.62731 - Test loss: 0.57441\n",
      "Epoch 15500 - lr: 0.01000 - Train loss: 0.62412 - Test loss: 0.57371\n",
      "Epoch 15501 - lr: 0.01000 - Train loss: 0.61893 - Test loss: 0.57410\n",
      "Epoch 15502 - lr: 0.01000 - Train loss: 0.62117 - Test loss: 0.57459\n",
      "Epoch 15503 - lr: 0.01000 - Train loss: 0.62293 - Test loss: 0.57412\n",
      "Epoch 15504 - lr: 0.01000 - Train loss: 0.61571 - Test loss: 0.57457\n",
      "Epoch 15505 - lr: 0.01000 - Train loss: 0.62433 - Test loss: 0.57476\n",
      "Epoch 15506 - lr: 0.01000 - Train loss: 0.62401 - Test loss: 0.57441\n",
      "Epoch 15507 - lr: 0.01000 - Train loss: 0.62891 - Test loss: 0.57480\n",
      "Epoch 15508 - lr: 0.01000 - Train loss: 0.64784 - Test loss: 0.57422\n",
      "Epoch 15509 - lr: 0.01000 - Train loss: 0.63283 - Test loss: 0.57385\n",
      "Epoch 15510 - lr: 0.01000 - Train loss: 0.62493 - Test loss: 0.57325\n",
      "Epoch 15511 - lr: 0.01000 - Train loss: 0.61563 - Test loss: 0.57384\n",
      "Epoch 15512 - lr: 0.01000 - Train loss: 0.61845 - Test loss: 0.57423\n",
      "Epoch 15513 - lr: 0.01000 - Train loss: 0.61793 - Test loss: 0.57468\n",
      "Epoch 15514 - lr: 0.01000 - Train loss: 0.62935 - Test loss: 0.57499\n",
      "Epoch 15515 - lr: 0.01000 - Train loss: 0.65172 - Test loss: 0.57438\n",
      "Epoch 15516 - lr: 0.01000 - Train loss: 0.63102 - Test loss: 0.57411\n",
      "Epoch 15517 - lr: 0.01000 - Train loss: 0.62388 - Test loss: 0.57342\n",
      "Epoch 15518 - lr: 0.01000 - Train loss: 0.61843 - Test loss: 0.57384\n",
      "Epoch 15519 - lr: 0.01000 - Train loss: 0.61880 - Test loss: 0.57434\n",
      "Epoch 15520 - lr: 0.01000 - Train loss: 0.62871 - Test loss: 0.57445\n",
      "Epoch 15521 - lr: 0.01000 - Train loss: 0.62905 - Test loss: 0.57388\n",
      "Epoch 15522 - lr: 0.01000 - Train loss: 0.62412 - Test loss: 0.57363\n",
      "Epoch 15523 - lr: 0.01000 - Train loss: 0.62927 - Test loss: 0.57403\n",
      "Epoch 15524 - lr: 0.01000 - Train loss: 0.63072 - Test loss: 0.57362\n",
      "Epoch 15525 - lr: 0.01000 - Train loss: 0.62214 - Test loss: 0.57408\n",
      "Epoch 15526 - lr: 0.01000 - Train loss: 0.62354 - Test loss: 0.57345\n",
      "Epoch 15527 - lr: 0.01000 - Train loss: 0.61928 - Test loss: 0.57383\n",
      "Epoch 15528 - lr: 0.01000 - Train loss: 0.62435 - Test loss: 0.57434\n",
      "Epoch 15529 - lr: 0.01000 - Train loss: 0.62447 - Test loss: 0.57361\n",
      "Epoch 15530 - lr: 0.01000 - Train loss: 0.61773 - Test loss: 0.57399\n",
      "Epoch 15531 - lr: 0.01000 - Train loss: 0.61556 - Test loss: 0.57446\n",
      "Epoch 15532 - lr: 0.01000 - Train loss: 0.61674 - Test loss: 0.57476\n",
      "Epoch 15533 - lr: 0.01000 - Train loss: 0.61885 - Test loss: 0.57493\n",
      "Epoch 15534 - lr: 0.01000 - Train loss: 0.62038 - Test loss: 0.57522\n",
      "Epoch 15535 - lr: 0.01000 - Train loss: 0.62348 - Test loss: 0.57468\n",
      "Epoch 15536 - lr: 0.01000 - Train loss: 0.62501 - Test loss: 0.57502\n",
      "Epoch 15537 - lr: 0.01000 - Train loss: 0.62914 - Test loss: 0.57435\n",
      "Epoch 15538 - lr: 0.01000 - Train loss: 0.62305 - Test loss: 0.57393\n",
      "Epoch 15539 - lr: 0.01000 - Train loss: 0.62264 - Test loss: 0.57439\n",
      "Epoch 15540 - lr: 0.01000 - Train loss: 0.62335 - Test loss: 0.57368\n",
      "Epoch 15541 - lr: 0.01000 - Train loss: 0.62154 - Test loss: 0.57398\n",
      "Epoch 15542 - lr: 0.01000 - Train loss: 0.62833 - Test loss: 0.57419\n",
      "Epoch 15543 - lr: 0.01000 - Train loss: 0.63447 - Test loss: 0.57400\n",
      "Epoch 15544 - lr: 0.01000 - Train loss: 0.63722 - Test loss: 0.57341\n",
      "Epoch 15545 - lr: 0.01000 - Train loss: 0.62283 - Test loss: 0.57312\n",
      "Epoch 15546 - lr: 0.01000 - Train loss: 0.62197 - Test loss: 0.57370\n",
      "Epoch 15547 - lr: 0.01000 - Train loss: 0.62302 - Test loss: 0.57314\n",
      "Epoch 15548 - lr: 0.01000 - Train loss: 0.62022 - Test loss: 0.57353\n",
      "Epoch 15549 - lr: 0.01000 - Train loss: 0.62829 - Test loss: 0.57402\n",
      "Epoch 15550 - lr: 0.01000 - Train loss: 0.64543 - Test loss: 0.57348\n",
      "Epoch 15551 - lr: 0.01000 - Train loss: 0.62690 - Test loss: 0.57294\n",
      "Epoch 15552 - lr: 0.01000 - Train loss: 0.62846 - Test loss: 0.57330\n",
      "Epoch 15553 - lr: 0.01000 - Train loss: 0.63444 - Test loss: 0.57328\n",
      "Epoch 15554 - lr: 0.01000 - Train loss: 0.62787 - Test loss: 0.57299\n",
      "Epoch 15555 - lr: 0.01000 - Train loss: 0.61632 - Test loss: 0.57344\n",
      "Epoch 15556 - lr: 0.01000 - Train loss: 0.61883 - Test loss: 0.57378\n",
      "Epoch 15557 - lr: 0.01000 - Train loss: 0.62243 - Test loss: 0.57422\n",
      "Epoch 15558 - lr: 0.01000 - Train loss: 0.62369 - Test loss: 0.57343\n",
      "Epoch 15559 - lr: 0.01000 - Train loss: 0.61917 - Test loss: 0.57374\n",
      "Epoch 15560 - lr: 0.01000 - Train loss: 0.62499 - Test loss: 0.57422\n",
      "Epoch 15561 - lr: 0.01000 - Train loss: 0.62888 - Test loss: 0.57362\n",
      "Epoch 15562 - lr: 0.01000 - Train loss: 0.62295 - Test loss: 0.57328\n",
      "Epoch 15563 - lr: 0.01000 - Train loss: 0.62436 - Test loss: 0.57380\n",
      "Epoch 15564 - lr: 0.01000 - Train loss: 0.62537 - Test loss: 0.57318\n",
      "Epoch 15565 - lr: 0.01000 - Train loss: 0.62325 - Test loss: 0.57369\n",
      "Epoch 15566 - lr: 0.01000 - Train loss: 0.62363 - Test loss: 0.57299\n",
      "Epoch 15567 - lr: 0.01000 - Train loss: 0.61993 - Test loss: 0.57336\n",
      "Epoch 15568 - lr: 0.01000 - Train loss: 0.62801 - Test loss: 0.57385\n",
      "Epoch 15569 - lr: 0.01000 - Train loss: 0.64448 - Test loss: 0.57328\n",
      "Epoch 15570 - lr: 0.01000 - Train loss: 0.62460 - Test loss: 0.57265\n",
      "Epoch 15571 - lr: 0.01000 - Train loss: 0.61656 - Test loss: 0.57324\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15572 - lr: 0.01000 - Train loss: 0.62517 - Test loss: 0.57377\n",
      "Epoch 15573 - lr: 0.01000 - Train loss: 0.63057 - Test loss: 0.57331\n",
      "Epoch 15574 - lr: 0.01000 - Train loss: 0.62330 - Test loss: 0.57263\n",
      "Epoch 15575 - lr: 0.01000 - Train loss: 0.61792 - Test loss: 0.57309\n",
      "Epoch 15576 - lr: 0.01000 - Train loss: 0.61890 - Test loss: 0.57363\n",
      "Epoch 15577 - lr: 0.01000 - Train loss: 0.62559 - Test loss: 0.57352\n",
      "Epoch 15578 - lr: 0.01000 - Train loss: 0.62201 - Test loss: 0.57306\n",
      "Epoch 15579 - lr: 0.01000 - Train loss: 0.61680 - Test loss: 0.57349\n",
      "Epoch 15580 - lr: 0.01000 - Train loss: 0.61460 - Test loss: 0.57398\n",
      "Epoch 15581 - lr: 0.01000 - Train loss: 0.62466 - Test loss: 0.57414\n",
      "Epoch 15582 - lr: 0.01000 - Train loss: 0.62195 - Test loss: 0.57361\n",
      "Epoch 15583 - lr: 0.01000 - Train loss: 0.61524 - Test loss: 0.57402\n",
      "Epoch 15584 - lr: 0.01000 - Train loss: 0.62247 - Test loss: 0.57420\n",
      "Epoch 15585 - lr: 0.01000 - Train loss: 0.62542 - Test loss: 0.57400\n",
      "Epoch 15586 - lr: 0.01000 - Train loss: 0.62173 - Test loss: 0.57348\n",
      "Epoch 15587 - lr: 0.01000 - Train loss: 0.61700 - Test loss: 0.57385\n",
      "Epoch 15588 - lr: 0.01000 - Train loss: 0.61424 - Test loss: 0.57433\n",
      "Epoch 15589 - lr: 0.01000 - Train loss: 0.62103 - Test loss: 0.57447\n",
      "Epoch 15590 - lr: 0.01000 - Train loss: 0.62803 - Test loss: 0.57455\n",
      "Epoch 15591 - lr: 0.01000 - Train loss: 0.62547 - Test loss: 0.57418\n",
      "Epoch 15592 - lr: 0.01000 - Train loss: 0.62323 - Test loss: 0.57448\n",
      "Epoch 15593 - lr: 0.01000 - Train loss: 0.62417 - Test loss: 0.57372\n",
      "Epoch 15594 - lr: 0.01000 - Train loss: 0.61791 - Test loss: 0.57412\n",
      "Epoch 15595 - lr: 0.01000 - Train loss: 0.62715 - Test loss: 0.57411\n",
      "Epoch 15596 - lr: 0.01000 - Train loss: 0.62721 - Test loss: 0.57350\n",
      "Epoch 15597 - lr: 0.01000 - Train loss: 0.62430 - Test loss: 0.57332\n",
      "Epoch 15598 - lr: 0.01000 - Train loss: 0.62313 - Test loss: 0.57312\n",
      "Epoch 15599 - lr: 0.01000 - Train loss: 0.62751 - Test loss: 0.57342\n",
      "Epoch 15600 - lr: 0.01000 - Train loss: 0.62706 - Test loss: 0.57325\n",
      "Epoch 15601 - lr: 0.01000 - Train loss: 0.62542 - Test loss: 0.57318\n",
      "Epoch 15602 - lr: 0.01000 - Train loss: 0.62153 - Test loss: 0.57269\n",
      "Epoch 15603 - lr: 0.01000 - Train loss: 0.62020 - Test loss: 0.57309\n",
      "Epoch 15604 - lr: 0.01000 - Train loss: 0.62762 - Test loss: 0.57345\n",
      "Epoch 15605 - lr: 0.01000 - Train loss: 0.63233 - Test loss: 0.57295\n",
      "Epoch 15606 - lr: 0.01000 - Train loss: 0.62774 - Test loss: 0.57326\n",
      "Epoch 15607 - lr: 0.01000 - Train loss: 0.63002 - Test loss: 0.57284\n",
      "Epoch 15608 - lr: 0.01000 - Train loss: 0.62330 - Test loss: 0.57333\n",
      "Epoch 15609 - lr: 0.01000 - Train loss: 0.62438 - Test loss: 0.57274\n",
      "Epoch 15610 - lr: 0.01000 - Train loss: 0.62210 - Test loss: 0.57325\n",
      "Epoch 15611 - lr: 0.01000 - Train loss: 0.62238 - Test loss: 0.57262\n",
      "Epoch 15612 - lr: 0.01000 - Train loss: 0.62043 - Test loss: 0.57298\n",
      "Epoch 15613 - lr: 0.01000 - Train loss: 0.62752 - Test loss: 0.57327\n",
      "Epoch 15614 - lr: 0.01000 - Train loss: 0.62820 - Test loss: 0.57308\n",
      "Epoch 15615 - lr: 0.01000 - Train loss: 0.62154 - Test loss: 0.57266\n",
      "Epoch 15616 - lr: 0.01000 - Train loss: 0.61393 - Test loss: 0.57323\n",
      "Epoch 15617 - lr: 0.01000 - Train loss: 0.62092 - Test loss: 0.57348\n",
      "Epoch 15618 - lr: 0.01000 - Train loss: 0.62714 - Test loss: 0.57357\n",
      "Epoch 15619 - lr: 0.01000 - Train loss: 0.63189 - Test loss: 0.57321\n",
      "Epoch 15620 - lr: 0.01000 - Train loss: 0.63300 - Test loss: 0.57296\n",
      "Epoch 15621 - lr: 0.01000 - Train loss: 0.62762 - Test loss: 0.57258\n",
      "Epoch 15622 - lr: 0.01000 - Train loss: 0.61399 - Test loss: 0.57309\n",
      "Epoch 15623 - lr: 0.01000 - Train loss: 0.62280 - Test loss: 0.57331\n",
      "Epoch 15624 - lr: 0.01000 - Train loss: 0.62284 - Test loss: 0.57300\n",
      "Epoch 15625 - lr: 0.01000 - Train loss: 0.62780 - Test loss: 0.57333\n",
      "Epoch 15626 - lr: 0.01000 - Train loss: 0.64179 - Test loss: 0.57271\n",
      "Epoch 15627 - lr: 0.01000 - Train loss: 0.62263 - Test loss: 0.57205\n",
      "Epoch 15628 - lr: 0.01000 - Train loss: 0.61826 - Test loss: 0.57251\n",
      "Epoch 15629 - lr: 0.01000 - Train loss: 0.62414 - Test loss: 0.57308\n",
      "Epoch 15630 - lr: 0.01000 - Train loss: 0.63079 - Test loss: 0.57271\n",
      "Epoch 15631 - lr: 0.01000 - Train loss: 0.62290 - Test loss: 0.57208\n",
      "Epoch 15632 - lr: 0.01000 - Train loss: 0.61517 - Test loss: 0.57261\n",
      "Epoch 15633 - lr: 0.01000 - Train loss: 0.61795 - Test loss: 0.57299\n",
      "Epoch 15634 - lr: 0.01000 - Train loss: 0.62215 - Test loss: 0.57345\n",
      "Epoch 15635 - lr: 0.01000 - Train loss: 0.62237 - Test loss: 0.57269\n",
      "Epoch 15636 - lr: 0.01000 - Train loss: 0.61977 - Test loss: 0.57298\n",
      "Epoch 15637 - lr: 0.01000 - Train loss: 0.62763 - Test loss: 0.57331\n",
      "Epoch 15638 - lr: 0.01000 - Train loss: 0.64169 - Test loss: 0.57267\n",
      "Epoch 15639 - lr: 0.01000 - Train loss: 0.62247 - Test loss: 0.57201\n",
      "Epoch 15640 - lr: 0.01000 - Train loss: 0.61751 - Test loss: 0.57247\n",
      "Epoch 15641 - lr: 0.01000 - Train loss: 0.62074 - Test loss: 0.57303\n",
      "Epoch 15642 - lr: 0.01000 - Train loss: 0.62158 - Test loss: 0.57240\n",
      "Epoch 15643 - lr: 0.01000 - Train loss: 0.62072 - Test loss: 0.57274\n",
      "Epoch 15644 - lr: 0.01000 - Train loss: 0.62610 - Test loss: 0.57285\n",
      "Epoch 15645 - lr: 0.01000 - Train loss: 0.62302 - Test loss: 0.57226\n",
      "Epoch 15646 - lr: 0.01000 - Train loss: 0.61446 - Test loss: 0.57284\n",
      "Epoch 15647 - lr: 0.01000 - Train loss: 0.61486 - Test loss: 0.57334\n",
      "Epoch 15648 - lr: 0.01000 - Train loss: 0.61853 - Test loss: 0.57371\n",
      "Epoch 15649 - lr: 0.01000 - Train loss: 0.62260 - Test loss: 0.57327\n",
      "Epoch 15650 - lr: 0.01000 - Train loss: 0.62734 - Test loss: 0.57349\n",
      "Epoch 15651 - lr: 0.01000 - Train loss: 0.64223 - Test loss: 0.57280\n",
      "Epoch 15652 - lr: 0.01000 - Train loss: 0.62343 - Test loss: 0.57217\n",
      "Epoch 15653 - lr: 0.01000 - Train loss: 0.61896 - Test loss: 0.57272\n",
      "Epoch 15654 - lr: 0.01000 - Train loss: 0.62108 - Test loss: 0.57239\n",
      "Epoch 15655 - lr: 0.01000 - Train loss: 0.61518 - Test loss: 0.57294\n",
      "Epoch 15656 - lr: 0.01000 - Train loss: 0.62316 - Test loss: 0.57338\n",
      "Epoch 15657 - lr: 0.01000 - Train loss: 0.62850 - Test loss: 0.57283\n",
      "Epoch 15658 - lr: 0.01000 - Train loss: 0.62095 - Test loss: 0.57221\n",
      "Epoch 15659 - lr: 0.01000 - Train loss: 0.62049 - Test loss: 0.57256\n",
      "Epoch 15660 - lr: 0.01000 - Train loss: 0.62557 - Test loss: 0.57267\n",
      "Epoch 15661 - lr: 0.01000 - Train loss: 0.62232 - Test loss: 0.57209\n",
      "Epoch 15662 - lr: 0.01000 - Train loss: 0.61302 - Test loss: 0.57271\n",
      "Epoch 15663 - lr: 0.01000 - Train loss: 0.62188 - Test loss: 0.57297\n",
      "Epoch 15664 - lr: 0.01000 - Train loss: 0.62253 - Test loss: 0.57271\n",
      "Epoch 15665 - lr: 0.01000 - Train loss: 0.62582 - Test loss: 0.57281\n",
      "Epoch 15666 - lr: 0.01000 - Train loss: 0.62704 - Test loss: 0.57234\n",
      "Epoch 15667 - lr: 0.01000 - Train loss: 0.62047 - Test loss: 0.57199\n",
      "Epoch 15668 - lr: 0.01000 - Train loss: 0.61273 - Test loss: 0.57265\n",
      "Epoch 15669 - lr: 0.01000 - Train loss: 0.62325 - Test loss: 0.57292\n",
      "Epoch 15670 - lr: 0.01000 - Train loss: 0.62039 - Test loss: 0.57247\n",
      "Epoch 15671 - lr: 0.01000 - Train loss: 0.61284 - Test loss: 0.57302\n",
      "Epoch 15672 - lr: 0.01000 - Train loss: 0.62549 - Test loss: 0.57325\n",
      "Epoch 15673 - lr: 0.01000 - Train loss: 0.62115 - Test loss: 0.57261\n",
      "Epoch 15674 - lr: 0.01000 - Train loss: 0.61418 - Test loss: 0.57306\n",
      "Epoch 15675 - lr: 0.01000 - Train loss: 0.61870 - Test loss: 0.57331\n",
      "Epoch 15676 - lr: 0.01000 - Train loss: 0.62605 - Test loss: 0.57360\n",
      "Epoch 15677 - lr: 0.01000 - Train loss: 0.65872 - Test loss: 0.57306\n",
      "Epoch 15678 - lr: 0.01000 - Train loss: 0.63766 - Test loss: 0.57238\n",
      "Epoch 15679 - lr: 0.01000 - Train loss: 0.62036 - Test loss: 0.57187\n",
      "Epoch 15680 - lr: 0.01000 - Train loss: 0.61896 - Test loss: 0.57232\n",
      "Epoch 15681 - lr: 0.01000 - Train loss: 0.62627 - Test loss: 0.57271\n",
      "Epoch 15682 - lr: 0.01000 - Train loss: 0.63438 - Test loss: 0.57217\n",
      "Epoch 15683 - lr: 0.01000 - Train loss: 0.62145 - Test loss: 0.57196\n",
      "Epoch 15684 - lr: 0.01000 - Train loss: 0.62606 - Test loss: 0.57246\n",
      "Epoch 15685 - lr: 0.01000 - Train loss: 0.64537 - Test loss: 0.57198\n",
      "Epoch 15686 - lr: 0.01000 - Train loss: 0.63129 - Test loss: 0.57193\n",
      "Epoch 15687 - lr: 0.01000 - Train loss: 0.62725 - Test loss: 0.57184\n",
      "Epoch 15688 - lr: 0.01000 - Train loss: 0.62031 - Test loss: 0.57149\n",
      "Epoch 15689 - lr: 0.01000 - Train loss: 0.61324 - Test loss: 0.57213\n",
      "Epoch 15690 - lr: 0.01000 - Train loss: 0.62193 - Test loss: 0.57244\n",
      "Epoch 15691 - lr: 0.01000 - Train loss: 0.62110 - Test loss: 0.57214\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15692 - lr: 0.01000 - Train loss: 0.62418 - Test loss: 0.57265\n",
      "Epoch 15693 - lr: 0.01000 - Train loss: 0.62669 - Test loss: 0.57243\n",
      "Epoch 15694 - lr: 0.01000 - Train loss: 0.62098 - Test loss: 0.57205\n",
      "Epoch 15695 - lr: 0.01000 - Train loss: 0.62218 - Test loss: 0.57257\n",
      "Epoch 15696 - lr: 0.01000 - Train loss: 0.62471 - Test loss: 0.57198\n",
      "Epoch 15697 - lr: 0.01000 - Train loss: 0.62615 - Test loss: 0.57221\n",
      "Epoch 15698 - lr: 0.01000 - Train loss: 0.63161 - Test loss: 0.57211\n",
      "Epoch 15699 - lr: 0.01000 - Train loss: 0.62361 - Test loss: 0.57187\n",
      "Epoch 15700 - lr: 0.01000 - Train loss: 0.62066 - Test loss: 0.57212\n",
      "Epoch 15701 - lr: 0.01000 - Train loss: 0.62366 - Test loss: 0.57204\n",
      "Epoch 15702 - lr: 0.01000 - Train loss: 0.62002 - Test loss: 0.57159\n",
      "Epoch 15703 - lr: 0.01000 - Train loss: 0.61603 - Test loss: 0.57205\n",
      "Epoch 15704 - lr: 0.01000 - Train loss: 0.61671 - Test loss: 0.57257\n",
      "Epoch 15705 - lr: 0.01000 - Train loss: 0.62425 - Test loss: 0.57245\n",
      "Epoch 15706 - lr: 0.01000 - Train loss: 0.62039 - Test loss: 0.57178\n",
      "Epoch 15707 - lr: 0.01000 - Train loss: 0.62002 - Test loss: 0.57212\n",
      "Epoch 15708 - lr: 0.01000 - Train loss: 0.62411 - Test loss: 0.57213\n",
      "Epoch 15709 - lr: 0.01000 - Train loss: 0.62029 - Test loss: 0.57154\n",
      "Epoch 15710 - lr: 0.01000 - Train loss: 0.61960 - Test loss: 0.57193\n",
      "Epoch 15711 - lr: 0.01000 - Train loss: 0.62456 - Test loss: 0.57205\n",
      "Epoch 15712 - lr: 0.01000 - Train loss: 0.62110 - Test loss: 0.57149\n",
      "Epoch 15713 - lr: 0.01000 - Train loss: 0.61269 - Test loss: 0.57210\n",
      "Epoch 15714 - lr: 0.01000 - Train loss: 0.62215 - Test loss: 0.57239\n",
      "Epoch 15715 - lr: 0.01000 - Train loss: 0.61977 - Test loss: 0.57198\n",
      "Epoch 15716 - lr: 0.01000 - Train loss: 0.61219 - Test loss: 0.57256\n",
      "Epoch 15717 - lr: 0.01000 - Train loss: 0.61699 - Test loss: 0.57282\n",
      "Epoch 15718 - lr: 0.01000 - Train loss: 0.62266 - Test loss: 0.57318\n",
      "Epoch 15719 - lr: 0.01000 - Train loss: 0.63085 - Test loss: 0.57287\n",
      "Epoch 15720 - lr: 0.01000 - Train loss: 0.63331 - Test loss: 0.57213\n",
      "Epoch 15721 - lr: 0.01000 - Train loss: 0.62189 - Test loss: 0.57189\n",
      "Epoch 15722 - lr: 0.01000 - Train loss: 0.62327 - Test loss: 0.57187\n",
      "Epoch 15723 - lr: 0.01000 - Train loss: 0.61943 - Test loss: 0.57142\n",
      "Epoch 15724 - lr: 0.01000 - Train loss: 0.61769 - Test loss: 0.57186\n",
      "Epoch 15725 - lr: 0.01000 - Train loss: 0.62536 - Test loss: 0.57231\n",
      "Epoch 15726 - lr: 0.01000 - Train loss: 0.65053 - Test loss: 0.57182\n",
      "Epoch 15727 - lr: 0.01000 - Train loss: 0.64210 - Test loss: 0.57127\n",
      "Epoch 15728 - lr: 0.01000 - Train loss: 0.62600 - Test loss: 0.57091\n",
      "Epoch 15729 - lr: 0.01000 - Train loss: 0.61955 - Test loss: 0.57074\n",
      "Epoch 15730 - lr: 0.01000 - Train loss: 0.61328 - Test loss: 0.57149\n",
      "Epoch 15731 - lr: 0.01000 - Train loss: 0.61636 - Test loss: 0.57208\n",
      "Epoch 15732 - lr: 0.01000 - Train loss: 0.62295 - Test loss: 0.57195\n",
      "Epoch 15733 - lr: 0.01000 - Train loss: 0.61930 - Test loss: 0.57149\n",
      "Epoch 15734 - lr: 0.01000 - Train loss: 0.61374 - Test loss: 0.57200\n",
      "Epoch 15735 - lr: 0.01000 - Train loss: 0.61382 - Test loss: 0.57242\n",
      "Epoch 15736 - lr: 0.01000 - Train loss: 0.61387 - Test loss: 0.57275\n",
      "Epoch 15737 - lr: 0.01000 - Train loss: 0.61389 - Test loss: 0.57301\n",
      "Epoch 15738 - lr: 0.01000 - Train loss: 0.61388 - Test loss: 0.57323\n",
      "Epoch 15739 - lr: 0.01000 - Train loss: 0.61402 - Test loss: 0.57339\n",
      "Epoch 15740 - lr: 0.01000 - Train loss: 0.61312 - Test loss: 0.57358\n",
      "Epoch 15741 - lr: 0.01000 - Train loss: 0.61911 - Test loss: 0.57354\n",
      "Epoch 15742 - lr: 0.01000 - Train loss: 0.62483 - Test loss: 0.57335\n",
      "Epoch 15743 - lr: 0.01000 - Train loss: 0.63016 - Test loss: 0.57287\n",
      "Epoch 15744 - lr: 0.01000 - Train loss: 0.62660 - Test loss: 0.57227\n",
      "Epoch 15745 - lr: 0.01000 - Train loss: 0.61460 - Test loss: 0.57267\n",
      "Epoch 15746 - lr: 0.01000 - Train loss: 0.62555 - Test loss: 0.57288\n",
      "Epoch 15747 - lr: 0.01000 - Train loss: 0.65125 - Test loss: 0.57224\n",
      "Epoch 15748 - lr: 0.01000 - Train loss: 0.64206 - Test loss: 0.57157\n",
      "Epoch 15749 - lr: 0.01000 - Train loss: 0.62662 - Test loss: 0.57116\n",
      "Epoch 15750 - lr: 0.01000 - Train loss: 0.61923 - Test loss: 0.57073\n",
      "Epoch 15751 - lr: 0.01000 - Train loss: 0.61757 - Test loss: 0.57124\n",
      "Epoch 15752 - lr: 0.01000 - Train loss: 0.62528 - Test loss: 0.57170\n",
      "Epoch 15753 - lr: 0.01000 - Train loss: 0.63398 - Test loss: 0.57116\n",
      "Epoch 15754 - lr: 0.01000 - Train loss: 0.61920 - Test loss: 0.57087\n",
      "Epoch 15755 - lr: 0.01000 - Train loss: 0.61387 - Test loss: 0.57154\n",
      "Epoch 15756 - lr: 0.01000 - Train loss: 0.62438 - Test loss: 0.57204\n",
      "Epoch 15757 - lr: 0.01000 - Train loss: 0.63971 - Test loss: 0.57139\n",
      "Epoch 15758 - lr: 0.01000 - Train loss: 0.62042 - Test loss: 0.57073\n",
      "Epoch 15759 - lr: 0.01000 - Train loss: 0.61435 - Test loss: 0.57128\n",
      "Epoch 15760 - lr: 0.01000 - Train loss: 0.61268 - Test loss: 0.57188\n",
      "Epoch 15761 - lr: 0.01000 - Train loss: 0.61521 - Test loss: 0.57233\n",
      "Epoch 15762 - lr: 0.01000 - Train loss: 0.62446 - Test loss: 0.57226\n",
      "Epoch 15763 - lr: 0.01000 - Train loss: 0.62314 - Test loss: 0.57155\n",
      "Epoch 15764 - lr: 0.01000 - Train loss: 0.62531 - Test loss: 0.57185\n",
      "Epoch 15765 - lr: 0.01000 - Train loss: 0.63634 - Test loss: 0.57121\n",
      "Epoch 15766 - lr: 0.01000 - Train loss: 0.61934 - Test loss: 0.57062\n",
      "Epoch 15767 - lr: 0.01000 - Train loss: 0.61714 - Test loss: 0.57110\n",
      "Epoch 15768 - lr: 0.01000 - Train loss: 0.62515 - Test loss: 0.57156\n",
      "Epoch 15769 - lr: 0.01000 - Train loss: 0.63931 - Test loss: 0.57099\n",
      "Epoch 15770 - lr: 0.01000 - Train loss: 0.62014 - Test loss: 0.57036\n",
      "Epoch 15771 - lr: 0.01000 - Train loss: 0.61524 - Test loss: 0.57091\n",
      "Epoch 15772 - lr: 0.01000 - Train loss: 0.61957 - Test loss: 0.57154\n",
      "Epoch 15773 - lr: 0.01000 - Train loss: 0.61990 - Test loss: 0.57084\n",
      "Epoch 15774 - lr: 0.01000 - Train loss: 0.61708 - Test loss: 0.57123\n",
      "Epoch 15775 - lr: 0.01000 - Train loss: 0.62508 - Test loss: 0.57163\n",
      "Epoch 15776 - lr: 0.01000 - Train loss: 0.63866 - Test loss: 0.57101\n",
      "Epoch 15777 - lr: 0.01000 - Train loss: 0.61992 - Test loss: 0.57034\n",
      "Epoch 15778 - lr: 0.01000 - Train loss: 0.61594 - Test loss: 0.57086\n",
      "Epoch 15779 - lr: 0.01000 - Train loss: 0.62339 - Test loss: 0.57146\n",
      "Epoch 15780 - lr: 0.01000 - Train loss: 0.62425 - Test loss: 0.57113\n",
      "Epoch 15781 - lr: 0.01000 - Train loss: 0.61208 - Test loss: 0.57158\n",
      "Epoch 15782 - lr: 0.01000 - Train loss: 0.61808 - Test loss: 0.57180\n",
      "Epoch 15783 - lr: 0.01000 - Train loss: 0.62427 - Test loss: 0.57183\n",
      "Epoch 15784 - lr: 0.01000 - Train loss: 0.62472 - Test loss: 0.57120\n",
      "Epoch 15785 - lr: 0.01000 - Train loss: 0.61961 - Test loss: 0.57093\n",
      "Epoch 15786 - lr: 0.01000 - Train loss: 0.62452 - Test loss: 0.57144\n",
      "Epoch 15787 - lr: 0.01000 - Train loss: 0.64609 - Test loss: 0.57091\n",
      "Epoch 15788 - lr: 0.01000 - Train loss: 0.62570 - Test loss: 0.57077\n",
      "Epoch 15789 - lr: 0.01000 - Train loss: 0.61851 - Test loss: 0.57036\n",
      "Epoch 15790 - lr: 0.01000 - Train loss: 0.61349 - Test loss: 0.57091\n",
      "Epoch 15791 - lr: 0.01000 - Train loss: 0.61141 - Test loss: 0.57152\n",
      "Epoch 15792 - lr: 0.01000 - Train loss: 0.61185 - Test loss: 0.57192\n",
      "Epoch 15793 - lr: 0.01000 - Train loss: 0.61830 - Test loss: 0.57203\n",
      "Epoch 15794 - lr: 0.01000 - Train loss: 0.62331 - Test loss: 0.57189\n",
      "Epoch 15795 - lr: 0.01000 - Train loss: 0.61954 - Test loss: 0.57103\n",
      "Epoch 15796 - lr: 0.01000 - Train loss: 0.61718 - Test loss: 0.57135\n",
      "Epoch 15797 - lr: 0.01000 - Train loss: 0.62445 - Test loss: 0.57158\n",
      "Epoch 15798 - lr: 0.01000 - Train loss: 0.62820 - Test loss: 0.57139\n",
      "Epoch 15799 - lr: 0.01000 - Train loss: 0.62236 - Test loss: 0.57073\n",
      "Epoch 15800 - lr: 0.01000 - Train loss: 0.62463 - Test loss: 0.57118\n",
      "Epoch 15801 - lr: 0.01000 - Train loss: 0.64396 - Test loss: 0.57065\n",
      "Epoch 15802 - lr: 0.01000 - Train loss: 0.63068 - Test loss: 0.57054\n",
      "Epoch 15803 - lr: 0.01000 - Train loss: 0.63463 - Test loss: 0.57004\n",
      "Epoch 15804 - lr: 0.01000 - Train loss: 0.61867 - Test loss: 0.56962\n",
      "Epoch 15805 - lr: 0.01000 - Train loss: 0.61338 - Test loss: 0.57028\n",
      "Epoch 15806 - lr: 0.01000 - Train loss: 0.61248 - Test loss: 0.57095\n",
      "Epoch 15807 - lr: 0.01000 - Train loss: 0.62116 - Test loss: 0.57146\n",
      "Epoch 15808 - lr: 0.01000 - Train loss: 0.62607 - Test loss: 0.57087\n",
      "Epoch 15809 - lr: 0.01000 - Train loss: 0.61881 - Test loss: 0.57020\n",
      "Epoch 15810 - lr: 0.01000 - Train loss: 0.61419 - Test loss: 0.57069\n",
      "Epoch 15811 - lr: 0.01000 - Train loss: 0.61768 - Test loss: 0.57126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15812 - lr: 0.01000 - Train loss: 0.61922 - Test loss: 0.57048\n",
      "Epoch 15813 - lr: 0.01000 - Train loss: 0.61376 - Test loss: 0.57092\n",
      "Epoch 15814 - lr: 0.01000 - Train loss: 0.61515 - Test loss: 0.57144\n",
      "Epoch 15815 - lr: 0.01000 - Train loss: 0.61962 - Test loss: 0.57110\n",
      "Epoch 15816 - lr: 0.01000 - Train loss: 0.62413 - Test loss: 0.57128\n",
      "Epoch 15817 - lr: 0.01000 - Train loss: 0.63003 - Test loss: 0.57111\n",
      "Epoch 15818 - lr: 0.01000 - Train loss: 0.62833 - Test loss: 0.57053\n",
      "Epoch 15819 - lr: 0.01000 - Train loss: 0.62466 - Test loss: 0.57093\n",
      "Epoch 15820 - lr: 0.01000 - Train loss: 0.63967 - Test loss: 0.57033\n",
      "Epoch 15821 - lr: 0.01000 - Train loss: 0.61980 - Test loss: 0.56964\n",
      "Epoch 15822 - lr: 0.01000 - Train loss: 0.61265 - Test loss: 0.57025\n",
      "Epoch 15823 - lr: 0.01000 - Train loss: 0.61081 - Test loss: 0.57091\n",
      "Epoch 15824 - lr: 0.01000 - Train loss: 0.61120 - Test loss: 0.57135\n",
      "Epoch 15825 - lr: 0.01000 - Train loss: 0.61675 - Test loss: 0.57149\n",
      "Epoch 15826 - lr: 0.01000 - Train loss: 0.62433 - Test loss: 0.57157\n",
      "Epoch 15827 - lr: 0.01000 - Train loss: 0.62870 - Test loss: 0.57129\n",
      "Epoch 15828 - lr: 0.01000 - Train loss: 0.62670 - Test loss: 0.57070\n",
      "Epoch 15829 - lr: 0.01000 - Train loss: 0.61975 - Test loss: 0.56986\n",
      "Epoch 15830 - lr: 0.01000 - Train loss: 0.61039 - Test loss: 0.57053\n",
      "Epoch 15831 - lr: 0.01000 - Train loss: 0.61690 - Test loss: 0.57086\n",
      "Epoch 15832 - lr: 0.01000 - Train loss: 0.62327 - Test loss: 0.57097\n",
      "Epoch 15833 - lr: 0.01000 - Train loss: 0.62106 - Test loss: 0.57028\n",
      "Epoch 15834 - lr: 0.01000 - Train loss: 0.62137 - Test loss: 0.57085\n",
      "Epoch 15835 - lr: 0.01000 - Train loss: 0.63030 - Test loss: 0.57071\n",
      "Epoch 15836 - lr: 0.01000 - Train loss: 0.63997 - Test loss: 0.57008\n",
      "Epoch 15837 - lr: 0.01000 - Train loss: 0.61968 - Test loss: 0.56941\n",
      "Epoch 15838 - lr: 0.01000 - Train loss: 0.61078 - Test loss: 0.57010\n",
      "Epoch 15839 - lr: 0.01000 - Train loss: 0.61488 - Test loss: 0.57052\n",
      "Epoch 15840 - lr: 0.01000 - Train loss: 0.62334 - Test loss: 0.57102\n",
      "Epoch 15841 - lr: 0.01000 - Train loss: 0.63948 - Test loss: 0.57033\n",
      "Epoch 15842 - lr: 0.01000 - Train loss: 0.61958 - Test loss: 0.56953\n",
      "Epoch 15843 - lr: 0.01000 - Train loss: 0.61197 - Test loss: 0.57012\n",
      "Epoch 15844 - lr: 0.01000 - Train loss: 0.61018 - Test loss: 0.57074\n",
      "Epoch 15845 - lr: 0.01000 - Train loss: 0.61424 - Test loss: 0.57102\n",
      "Epoch 15846 - lr: 0.01000 - Train loss: 0.62112 - Test loss: 0.57143\n",
      "Epoch 15847 - lr: 0.01000 - Train loss: 0.63051 - Test loss: 0.57107\n",
      "Epoch 15848 - lr: 0.01000 - Train loss: 0.64053 - Test loss: 0.57032\n",
      "Epoch 15849 - lr: 0.01000 - Train loss: 0.61983 - Test loss: 0.56958\n",
      "Epoch 15850 - lr: 0.01000 - Train loss: 0.61024 - Test loss: 0.57026\n",
      "Epoch 15851 - lr: 0.01000 - Train loss: 0.61128 - Test loss: 0.57075\n",
      "Epoch 15852 - lr: 0.01000 - Train loss: 0.61175 - Test loss: 0.57109\n",
      "Epoch 15853 - lr: 0.01000 - Train loss: 0.61011 - Test loss: 0.57145\n",
      "Epoch 15854 - lr: 0.01000 - Train loss: 0.61755 - Test loss: 0.57148\n",
      "Epoch 15855 - lr: 0.01000 - Train loss: 0.62055 - Test loss: 0.57115\n",
      "Epoch 15856 - lr: 0.01000 - Train loss: 0.61709 - Test loss: 0.57053\n",
      "Epoch 15857 - lr: 0.01000 - Train loss: 0.61148 - Test loss: 0.57095\n",
      "Epoch 15858 - lr: 0.01000 - Train loss: 0.60974 - Test loss: 0.57139\n",
      "Epoch 15859 - lr: 0.01000 - Train loss: 0.61800 - Test loss: 0.57147\n",
      "Epoch 15860 - lr: 0.01000 - Train loss: 0.61855 - Test loss: 0.57104\n",
      "Epoch 15861 - lr: 0.01000 - Train loss: 0.62338 - Test loss: 0.57119\n",
      "Epoch 15862 - lr: 0.01000 - Train loss: 0.62762 - Test loss: 0.57096\n",
      "Epoch 15863 - lr: 0.01000 - Train loss: 0.62353 - Test loss: 0.57032\n",
      "Epoch 15864 - lr: 0.01000 - Train loss: 0.61718 - Test loss: 0.57000\n",
      "Epoch 15865 - lr: 0.01000 - Train loss: 0.61664 - Test loss: 0.57062\n",
      "Epoch 15866 - lr: 0.01000 - Train loss: 0.61848 - Test loss: 0.56983\n",
      "Epoch 15867 - lr: 0.01000 - Train loss: 0.60977 - Test loss: 0.57044\n",
      "Epoch 15868 - lr: 0.01000 - Train loss: 0.61536 - Test loss: 0.57075\n",
      "Epoch 15869 - lr: 0.01000 - Train loss: 0.62313 - Test loss: 0.57096\n",
      "Epoch 15870 - lr: 0.01000 - Train loss: 0.62483 - Test loss: 0.57071\n",
      "Epoch 15871 - lr: 0.01000 - Train loss: 0.61799 - Test loss: 0.56988\n",
      "Epoch 15872 - lr: 0.01000 - Train loss: 0.61065 - Test loss: 0.57042\n",
      "Epoch 15873 - lr: 0.01000 - Train loss: 0.61015 - Test loss: 0.57090\n",
      "Epoch 15874 - lr: 0.01000 - Train loss: 0.61339 - Test loss: 0.57114\n",
      "Epoch 15875 - lr: 0.01000 - Train loss: 0.62004 - Test loss: 0.57151\n",
      "Epoch 15876 - lr: 0.01000 - Train loss: 0.62943 - Test loss: 0.57115\n",
      "Epoch 15877 - lr: 0.01000 - Train loss: 0.64761 - Test loss: 0.57047\n",
      "Epoch 15878 - lr: 0.01000 - Train loss: 0.62057 - Test loss: 0.57016\n",
      "Epoch 15879 - lr: 0.01000 - Train loss: 0.61371 - Test loss: 0.57048\n",
      "Epoch 15880 - lr: 0.01000 - Train loss: 0.62192 - Test loss: 0.57097\n",
      "Epoch 15881 - lr: 0.01000 - Train loss: 0.63113 - Test loss: 0.57025\n",
      "Epoch 15882 - lr: 0.01000 - Train loss: 0.61673 - Test loss: 0.56986\n",
      "Epoch 15883 - lr: 0.01000 - Train loss: 0.61473 - Test loss: 0.57047\n",
      "Epoch 15884 - lr: 0.01000 - Train loss: 0.61653 - Test loss: 0.56991\n",
      "Epoch 15885 - lr: 0.01000 - Train loss: 0.61185 - Test loss: 0.57035\n",
      "Epoch 15886 - lr: 0.01000 - Train loss: 0.61330 - Test loss: 0.57088\n",
      "Epoch 15887 - lr: 0.01000 - Train loss: 0.61787 - Test loss: 0.57052\n",
      "Epoch 15888 - lr: 0.01000 - Train loss: 0.62250 - Test loss: 0.57069\n",
      "Epoch 15889 - lr: 0.01000 - Train loss: 0.62896 - Test loss: 0.57043\n",
      "Epoch 15890 - lr: 0.01000 - Train loss: 0.63106 - Test loss: 0.56979\n",
      "Epoch 15891 - lr: 0.01000 - Train loss: 0.61621 - Test loss: 0.56946\n",
      "Epoch 15892 - lr: 0.01000 - Train loss: 0.61028 - Test loss: 0.57015\n",
      "Epoch 15893 - lr: 0.01000 - Train loss: 0.61954 - Test loss: 0.57070\n",
      "Epoch 15894 - lr: 0.01000 - Train loss: 0.62691 - Test loss: 0.57026\n",
      "Epoch 15895 - lr: 0.01000 - Train loss: 0.61899 - Test loss: 0.56951\n",
      "Epoch 15896 - lr: 0.01000 - Train loss: 0.61303 - Test loss: 0.57013\n",
      "Epoch 15897 - lr: 0.01000 - Train loss: 0.61751 - Test loss: 0.56991\n",
      "Epoch 15898 - lr: 0.01000 - Train loss: 0.62234 - Test loss: 0.57019\n",
      "Epoch 15899 - lr: 0.01000 - Train loss: 0.62905 - Test loss: 0.57004\n",
      "Epoch 15900 - lr: 0.01000 - Train loss: 0.63928 - Test loss: 0.56945\n",
      "Epoch 15901 - lr: 0.01000 - Train loss: 0.61855 - Test loss: 0.56881\n",
      "Epoch 15902 - lr: 0.01000 - Train loss: 0.60968 - Test loss: 0.56957\n",
      "Epoch 15903 - lr: 0.01000 - Train loss: 0.61525 - Test loss: 0.57020\n",
      "Epoch 15904 - lr: 0.01000 - Train loss: 0.61773 - Test loss: 0.56938\n",
      "Epoch 15905 - lr: 0.01000 - Train loss: 0.60881 - Test loss: 0.57002\n",
      "Epoch 15906 - lr: 0.01000 - Train loss: 0.60858 - Test loss: 0.57054\n",
      "Epoch 15907 - lr: 0.01000 - Train loss: 0.61408 - Test loss: 0.57070\n",
      "Epoch 15908 - lr: 0.01000 - Train loss: 0.62287 - Test loss: 0.57090\n",
      "Epoch 15909 - lr: 0.01000 - Train loss: 0.63508 - Test loss: 0.57010\n",
      "Epoch 15910 - lr: 0.01000 - Train loss: 0.61853 - Test loss: 0.56915\n",
      "Epoch 15911 - lr: 0.01000 - Train loss: 0.61188 - Test loss: 0.56986\n",
      "Epoch 15912 - lr: 0.01000 - Train loss: 0.61948 - Test loss: 0.56989\n",
      "Epoch 15913 - lr: 0.01000 - Train loss: 0.61694 - Test loss: 0.56920\n",
      "Epoch 15914 - lr: 0.01000 - Train loss: 0.60829 - Test loss: 0.56989\n",
      "Epoch 15915 - lr: 0.01000 - Train loss: 0.61302 - Test loss: 0.57026\n",
      "Epoch 15916 - lr: 0.01000 - Train loss: 0.62196 - Test loss: 0.57069\n",
      "Epoch 15917 - lr: 0.01000 - Train loss: 0.64661 - Test loss: 0.57007\n",
      "Epoch 15918 - lr: 0.01000 - Train loss: 0.61987 - Test loss: 0.56984\n",
      "Epoch 15919 - lr: 0.01000 - Train loss: 0.61961 - Test loss: 0.57037\n",
      "Epoch 15920 - lr: 0.01000 - Train loss: 0.62900 - Test loss: 0.57016\n",
      "Epoch 15921 - lr: 0.01000 - Train loss: 0.64012 - Test loss: 0.56951\n",
      "Epoch 15922 - lr: 0.01000 - Train loss: 0.61944 - Test loss: 0.56889\n",
      "Epoch 15923 - lr: 0.01000 - Train loss: 0.62083 - Test loss: 0.56961\n",
      "Epoch 15924 - lr: 0.01000 - Train loss: 0.61974 - Test loss: 0.56944\n",
      "Epoch 15925 - lr: 0.01000 - Train loss: 0.60947 - Test loss: 0.56990\n",
      "Epoch 15926 - lr: 0.01000 - Train loss: 0.60890 - Test loss: 0.57034\n",
      "Epoch 15927 - lr: 0.01000 - Train loss: 0.61110 - Test loss: 0.57057\n",
      "Epoch 15928 - lr: 0.01000 - Train loss: 0.61386 - Test loss: 0.57093\n",
      "Epoch 15929 - lr: 0.01000 - Train loss: 0.61614 - Test loss: 0.57005\n",
      "Epoch 15930 - lr: 0.01000 - Train loss: 0.60978 - Test loss: 0.57041\n",
      "Epoch 15931 - lr: 0.01000 - Train loss: 0.60808 - Test loss: 0.57085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15932 - lr: 0.01000 - Train loss: 0.60928 - Test loss: 0.57108\n",
      "Epoch 15933 - lr: 0.01000 - Train loss: 0.60913 - Test loss: 0.57127\n",
      "Epoch 15934 - lr: 0.01000 - Train loss: 0.60981 - Test loss: 0.57139\n",
      "Epoch 15935 - lr: 0.01000 - Train loss: 0.60768 - Test loss: 0.57162\n",
      "Epoch 15936 - lr: 0.01000 - Train loss: 0.61382 - Test loss: 0.57154\n",
      "Epoch 15937 - lr: 0.01000 - Train loss: 0.62219 - Test loss: 0.57150\n",
      "Epoch 15938 - lr: 0.01000 - Train loss: 0.61911 - Test loss: 0.57101\n",
      "Epoch 15939 - lr: 0.01000 - Train loss: 0.61497 - Test loss: 0.57130\n",
      "Epoch 15940 - lr: 0.01000 - Train loss: 0.61763 - Test loss: 0.57014\n",
      "Epoch 15941 - lr: 0.01000 - Train loss: 0.60852 - Test loss: 0.57065\n",
      "Epoch 15942 - lr: 0.01000 - Train loss: 0.61484 - Test loss: 0.57108\n",
      "Epoch 15943 - lr: 0.01000 - Train loss: 0.61736 - Test loss: 0.56998\n",
      "Epoch 15944 - lr: 0.01000 - Train loss: 0.60795 - Test loss: 0.57052\n",
      "Epoch 15945 - lr: 0.01000 - Train loss: 0.60954 - Test loss: 0.57098\n",
      "Epoch 15946 - lr: 0.01000 - Train loss: 0.62179 - Test loss: 0.57122\n",
      "Epoch 15947 - lr: 0.01000 - Train loss: 0.65449 - Test loss: 0.57056\n",
      "Epoch 15948 - lr: 0.01000 - Train loss: 0.62468 - Test loss: 0.57029\n",
      "Epoch 15949 - lr: 0.01000 - Train loss: 0.61754 - Test loss: 0.56930\n",
      "Epoch 15950 - lr: 0.01000 - Train loss: 0.60754 - Test loss: 0.56997\n",
      "Epoch 15951 - lr: 0.01000 - Train loss: 0.60730 - Test loss: 0.57052\n",
      "Epoch 15952 - lr: 0.01000 - Train loss: 0.61199 - Test loss: 0.57071\n",
      "Epoch 15953 - lr: 0.01000 - Train loss: 0.62110 - Test loss: 0.57104\n",
      "Epoch 15954 - lr: 0.01000 - Train loss: 0.64816 - Test loss: 0.57034\n",
      "Epoch 15955 - lr: 0.01000 - Train loss: 0.61875 - Test loss: 0.57004\n",
      "Epoch 15956 - lr: 0.01000 - Train loss: 0.61522 - Test loss: 0.57052\n",
      "Epoch 15957 - lr: 0.01000 - Train loss: 0.61797 - Test loss: 0.56943\n",
      "Epoch 15958 - lr: 0.01000 - Train loss: 0.61345 - Test loss: 0.57005\n",
      "Epoch 15959 - lr: 0.01000 - Train loss: 0.61616 - Test loss: 0.56928\n",
      "Epoch 15960 - lr: 0.01000 - Train loss: 0.60758 - Test loss: 0.56995\n",
      "Epoch 15961 - lr: 0.01000 - Train loss: 0.60935 - Test loss: 0.57050\n",
      "Epoch 15962 - lr: 0.01000 - Train loss: 0.62155 - Test loss: 0.57075\n",
      "Epoch 15963 - lr: 0.01000 - Train loss: 0.63803 - Test loss: 0.56998\n",
      "Epoch 15964 - lr: 0.01000 - Train loss: 0.61682 - Test loss: 0.56916\n",
      "Epoch 15965 - lr: 0.01000 - Train loss: 0.60705 - Test loss: 0.56983\n",
      "Epoch 15966 - lr: 0.01000 - Train loss: 0.60846 - Test loss: 0.57029\n",
      "Epoch 15967 - lr: 0.01000 - Train loss: 0.60694 - Test loss: 0.57072\n",
      "Epoch 15968 - lr: 0.01000 - Train loss: 0.61195 - Test loss: 0.57083\n",
      "Epoch 15969 - lr: 0.01000 - Train loss: 0.62135 - Test loss: 0.57107\n",
      "Epoch 15970 - lr: 0.01000 - Train loss: 0.65031 - Test loss: 0.57038\n",
      "Epoch 15971 - lr: 0.01000 - Train loss: 0.61957 - Test loss: 0.57007\n",
      "Epoch 15972 - lr: 0.01000 - Train loss: 0.62079 - Test loss: 0.57025\n",
      "Epoch 15973 - lr: 0.01000 - Train loss: 0.62693 - Test loss: 0.56995\n",
      "Epoch 15974 - lr: 0.01000 - Train loss: 0.62523 - Test loss: 0.56981\n",
      "Epoch 15975 - lr: 0.01000 - Train loss: 0.61693 - Test loss: 0.56900\n",
      "Epoch 15976 - lr: 0.01000 - Train loss: 0.60724 - Test loss: 0.56966\n",
      "Epoch 15977 - lr: 0.01000 - Train loss: 0.60805 - Test loss: 0.57022\n",
      "Epoch 15978 - lr: 0.01000 - Train loss: 0.61756 - Test loss: 0.57066\n",
      "Epoch 15979 - lr: 0.01000 - Train loss: 0.62308 - Test loss: 0.56998\n",
      "Epoch 15980 - lr: 0.01000 - Train loss: 0.61725 - Test loss: 0.56896\n",
      "Epoch 15981 - lr: 0.01000 - Train loss: 0.61609 - Test loss: 0.56965\n",
      "Epoch 15982 - lr: 0.01000 - Train loss: 0.61648 - Test loss: 0.56887\n",
      "Epoch 15983 - lr: 0.01000 - Train loss: 0.60687 - Test loss: 0.56952\n",
      "Epoch 15984 - lr: 0.01000 - Train loss: 0.60895 - Test loss: 0.56995\n",
      "Epoch 15985 - lr: 0.01000 - Train loss: 0.61011 - Test loss: 0.57043\n",
      "Epoch 15986 - lr: 0.01000 - Train loss: 0.61695 - Test loss: 0.57016\n",
      "Epoch 15987 - lr: 0.01000 - Train loss: 0.61378 - Test loss: 0.56955\n",
      "Epoch 15988 - lr: 0.01000 - Train loss: 0.60702 - Test loss: 0.57006\n",
      "Epoch 15989 - lr: 0.01000 - Train loss: 0.60858 - Test loss: 0.57040\n",
      "Epoch 15990 - lr: 0.01000 - Train loss: 0.60830 - Test loss: 0.57081\n",
      "Epoch 15991 - lr: 0.01000 - Train loss: 0.62084 - Test loss: 0.57105\n",
      "Epoch 15992 - lr: 0.01000 - Train loss: 0.65356 - Test loss: 0.57038\n",
      "Epoch 15993 - lr: 0.01000 - Train loss: 0.62624 - Test loss: 0.57014\n",
      "Epoch 15994 - lr: 0.01000 - Train loss: 0.62427 - Test loss: 0.56995\n",
      "Epoch 15995 - lr: 0.01000 - Train loss: 0.61664 - Test loss: 0.56897\n",
      "Epoch 15996 - lr: 0.01000 - Train loss: 0.60664 - Test loss: 0.56964\n",
      "Epoch 15997 - lr: 0.01000 - Train loss: 0.60771 - Test loss: 0.57021\n",
      "Epoch 15998 - lr: 0.01000 - Train loss: 0.61904 - Test loss: 0.57064\n",
      "Epoch 15999 - lr: 0.01000 - Train loss: 0.62377 - Test loss: 0.56994\n",
      "Epoch 16000 - lr: 0.01000 - Train loss: 0.62088 - Test loss: 0.57025\n",
      "Epoch 16001 - lr: 0.01000 - Train loss: 0.64067 - Test loss: 0.56959\n",
      "Epoch 16002 - lr: 0.01000 - Train loss: 0.62301 - Test loss: 0.56914\n",
      "Epoch 16003 - lr: 0.01000 - Train loss: 0.61773 - Test loss: 0.56823\n",
      "Epoch 16004 - lr: 0.01000 - Train loss: 0.62009 - Test loss: 0.56900\n",
      "Epoch 16005 - lr: 0.01000 - Train loss: 0.63576 - Test loss: 0.56860\n",
      "Epoch 16006 - lr: 0.01000 - Train loss: 0.61734 - Test loss: 0.56779\n",
      "Epoch 16007 - lr: 0.01000 - Train loss: 0.61610 - Test loss: 0.56871\n",
      "Epoch 16008 - lr: 0.01000 - Train loss: 0.61621 - Test loss: 0.56815\n",
      "Epoch 16009 - lr: 0.01000 - Train loss: 0.60786 - Test loss: 0.56895\n",
      "Epoch 16010 - lr: 0.01000 - Train loss: 0.62034 - Test loss: 0.56955\n",
      "Epoch 16011 - lr: 0.01000 - Train loss: 0.64093 - Test loss: 0.56901\n",
      "Epoch 16012 - lr: 0.01000 - Train loss: 0.62355 - Test loss: 0.56869\n",
      "Epoch 16013 - lr: 0.01000 - Train loss: 0.61812 - Test loss: 0.56777\n",
      "Epoch 16014 - lr: 0.01000 - Train loss: 0.62006 - Test loss: 0.56851\n",
      "Epoch 16015 - lr: 0.01000 - Train loss: 0.61756 - Test loss: 0.56855\n",
      "Epoch 16016 - lr: 0.01000 - Train loss: 0.60892 - Test loss: 0.56921\n",
      "Epoch 16017 - lr: 0.01000 - Train loss: 0.61823 - Test loss: 0.56934\n",
      "Epoch 16018 - lr: 0.01000 - Train loss: 0.61812 - Test loss: 0.56827\n",
      "Epoch 16019 - lr: 0.01000 - Train loss: 0.62007 - Test loss: 0.56888\n",
      "Epoch 16020 - lr: 0.01000 - Train loss: 0.61788 - Test loss: 0.56878\n",
      "Epoch 16021 - lr: 0.01000 - Train loss: 0.60628 - Test loss: 0.56938\n",
      "Epoch 16022 - lr: 0.01000 - Train loss: 0.60680 - Test loss: 0.56991\n",
      "Epoch 16023 - lr: 0.01000 - Train loss: 0.61507 - Test loss: 0.57035\n",
      "Epoch 16024 - lr: 0.01000 - Train loss: 0.61708 - Test loss: 0.56912\n",
      "Epoch 16025 - lr: 0.01000 - Train loss: 0.61233 - Test loss: 0.56971\n",
      "Epoch 16026 - lr: 0.01000 - Train loss: 0.61678 - Test loss: 0.56873\n",
      "Epoch 16027 - lr: 0.01000 - Train loss: 0.61938 - Test loss: 0.56942\n",
      "Epoch 16028 - lr: 0.01000 - Train loss: 0.63796 - Test loss: 0.56892\n",
      "Epoch 16029 - lr: 0.01000 - Train loss: 0.61596 - Test loss: 0.56824\n",
      "Epoch 16030 - lr: 0.01000 - Train loss: 0.60819 - Test loss: 0.56903\n",
      "Epoch 16031 - lr: 0.01000 - Train loss: 0.61863 - Test loss: 0.56933\n",
      "Epoch 16032 - lr: 0.01000 - Train loss: 0.61618 - Test loss: 0.56844\n",
      "Epoch 16033 - lr: 0.01000 - Train loss: 0.60815 - Test loss: 0.56917\n",
      "Epoch 16034 - lr: 0.01000 - Train loss: 0.61826 - Test loss: 0.56941\n",
      "Epoch 16035 - lr: 0.01000 - Train loss: 0.61683 - Test loss: 0.56841\n",
      "Epoch 16036 - lr: 0.01000 - Train loss: 0.61536 - Test loss: 0.56919\n",
      "Epoch 16037 - lr: 0.01000 - Train loss: 0.61559 - Test loss: 0.56848\n",
      "Epoch 16038 - lr: 0.01000 - Train loss: 0.60655 - Test loss: 0.56921\n",
      "Epoch 16039 - lr: 0.01000 - Train loss: 0.61754 - Test loss: 0.56985\n",
      "Epoch 16040 - lr: 0.01000 - Train loss: 0.62232 - Test loss: 0.56968\n",
      "Epoch 16041 - lr: 0.01000 - Train loss: 0.61766 - Test loss: 0.56850\n",
      "Epoch 16042 - lr: 0.01000 - Train loss: 0.61922 - Test loss: 0.56901\n",
      "Epoch 16043 - lr: 0.01000 - Train loss: 0.62629 - Test loss: 0.56908\n",
      "Epoch 16044 - lr: 0.01000 - Train loss: 0.62805 - Test loss: 0.56857\n",
      "Epoch 16045 - lr: 0.01000 - Train loss: 0.61325 - Test loss: 0.56812\n",
      "Epoch 16046 - lr: 0.01000 - Train loss: 0.60686 - Test loss: 0.56893\n",
      "Epoch 16047 - lr: 0.01000 - Train loss: 0.61972 - Test loss: 0.56955\n",
      "Epoch 16048 - lr: 0.01000 - Train loss: 0.64477 - Test loss: 0.56907\n",
      "Epoch 16049 - lr: 0.01000 - Train loss: 0.62480 - Test loss: 0.56904\n",
      "Epoch 16050 - lr: 0.01000 - Train loss: 0.62007 - Test loss: 0.56857\n",
      "Epoch 16051 - lr: 0.01000 - Train loss: 0.61310 - Test loss: 0.56813\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16052 - lr: 0.01000 - Train loss: 0.60709 - Test loss: 0.56894\n",
      "Epoch 16053 - lr: 0.01000 - Train loss: 0.61973 - Test loss: 0.56945\n",
      "Epoch 16054 - lr: 0.01000 - Train loss: 0.62022 - Test loss: 0.56904\n",
      "Epoch 16055 - lr: 0.01000 - Train loss: 0.61540 - Test loss: 0.56964\n",
      "Epoch 16056 - lr: 0.01000 - Train loss: 0.61570 - Test loss: 0.56879\n",
      "Epoch 16057 - lr: 0.01000 - Train loss: 0.60851 - Test loss: 0.56942\n",
      "Epoch 16058 - lr: 0.01000 - Train loss: 0.61409 - Test loss: 0.56925\n",
      "Epoch 16059 - lr: 0.01000 - Train loss: 0.61489 - Test loss: 0.56919\n",
      "Epoch 16060 - lr: 0.01000 - Train loss: 0.61234 - Test loss: 0.56876\n",
      "Epoch 16061 - lr: 0.01000 - Train loss: 0.60624 - Test loss: 0.56946\n",
      "Epoch 16062 - lr: 0.01000 - Train loss: 0.61893 - Test loss: 0.57004\n",
      "Epoch 16063 - lr: 0.01000 - Train loss: 0.64377 - Test loss: 0.56946\n",
      "Epoch 16064 - lr: 0.01000 - Train loss: 0.62660 - Test loss: 0.56937\n",
      "Epoch 16065 - lr: 0.01000 - Train loss: 0.63798 - Test loss: 0.56884\n",
      "Epoch 16066 - lr: 0.01000 - Train loss: 0.61546 - Test loss: 0.56813\n",
      "Epoch 16067 - lr: 0.01000 - Train loss: 0.60858 - Test loss: 0.56895\n",
      "Epoch 16068 - lr: 0.01000 - Train loss: 0.61277 - Test loss: 0.56881\n",
      "Epoch 16069 - lr: 0.01000 - Train loss: 0.61951 - Test loss: 0.56944\n",
      "Epoch 16070 - lr: 0.01000 - Train loss: 0.64088 - Test loss: 0.56896\n",
      "Epoch 16071 - lr: 0.01000 - Train loss: 0.62204 - Test loss: 0.56863\n",
      "Epoch 16072 - lr: 0.01000 - Train loss: 0.61766 - Test loss: 0.56770\n",
      "Epoch 16073 - lr: 0.01000 - Train loss: 0.61452 - Test loss: 0.56802\n",
      "Epoch 16074 - lr: 0.01000 - Train loss: 0.61270 - Test loss: 0.56779\n",
      "Epoch 16075 - lr: 0.01000 - Train loss: 0.60884 - Test loss: 0.56871\n",
      "Epoch 16076 - lr: 0.01000 - Train loss: 0.61191 - Test loss: 0.56850\n",
      "Epoch 16077 - lr: 0.01000 - Train loss: 0.60997 - Test loss: 0.56927\n",
      "Epoch 16078 - lr: 0.01000 - Train loss: 0.61395 - Test loss: 0.56858\n",
      "Epoch 16079 - lr: 0.01000 - Train loss: 0.61383 - Test loss: 0.56936\n",
      "Epoch 16080 - lr: 0.01000 - Train loss: 0.61640 - Test loss: 0.56838\n",
      "Epoch 16081 - lr: 0.01000 - Train loss: 0.61760 - Test loss: 0.56918\n",
      "Epoch 16082 - lr: 0.01000 - Train loss: 0.62173 - Test loss: 0.56885\n",
      "Epoch 16083 - lr: 0.01000 - Train loss: 0.61903 - Test loss: 0.56937\n",
      "Epoch 16084 - lr: 0.01000 - Train loss: 0.61763 - Test loss: 0.56912\n",
      "Epoch 16085 - lr: 0.01000 - Train loss: 0.60725 - Test loss: 0.56970\n",
      "Epoch 16086 - lr: 0.01000 - Train loss: 0.61588 - Test loss: 0.56971\n",
      "Epoch 16087 - lr: 0.01000 - Train loss: 0.61666 - Test loss: 0.56863\n",
      "Epoch 16088 - lr: 0.01000 - Train loss: 0.61651 - Test loss: 0.56898\n",
      "Epoch 16089 - lr: 0.01000 - Train loss: 0.61715 - Test loss: 0.56803\n",
      "Epoch 16090 - lr: 0.01000 - Train loss: 0.61584 - Test loss: 0.56849\n",
      "Epoch 16091 - lr: 0.01000 - Train loss: 0.61691 - Test loss: 0.56771\n",
      "Epoch 16092 - lr: 0.01000 - Train loss: 0.61404 - Test loss: 0.56810\n",
      "Epoch 16093 - lr: 0.01000 - Train loss: 0.61257 - Test loss: 0.56784\n",
      "Epoch 16094 - lr: 0.01000 - Train loss: 0.61049 - Test loss: 0.56881\n",
      "Epoch 16095 - lr: 0.01000 - Train loss: 0.61552 - Test loss: 0.56809\n",
      "Epoch 16096 - lr: 0.01000 - Train loss: 0.61742 - Test loss: 0.56872\n",
      "Epoch 16097 - lr: 0.01000 - Train loss: 0.61714 - Test loss: 0.56839\n",
      "Epoch 16098 - lr: 0.01000 - Train loss: 0.61348 - Test loss: 0.56858\n",
      "Epoch 16099 - lr: 0.01000 - Train loss: 0.61112 - Test loss: 0.56842\n",
      "Epoch 16100 - lr: 0.01000 - Train loss: 0.60815 - Test loss: 0.56926\n",
      "Epoch 16101 - lr: 0.01000 - Train loss: 0.61112 - Test loss: 0.56896\n",
      "Epoch 16102 - lr: 0.01000 - Train loss: 0.60773 - Test loss: 0.56968\n",
      "Epoch 16103 - lr: 0.01000 - Train loss: 0.61127 - Test loss: 0.56940\n",
      "Epoch 16104 - lr: 0.01000 - Train loss: 0.61507 - Test loss: 0.57008\n",
      "Epoch 16105 - lr: 0.01000 - Train loss: 0.62260 - Test loss: 0.56973\n",
      "Epoch 16106 - lr: 0.01000 - Train loss: 0.61517 - Test loss: 0.56872\n",
      "Epoch 16107 - lr: 0.01000 - Train loss: 0.61345 - Test loss: 0.56953\n",
      "Epoch 16108 - lr: 0.01000 - Train loss: 0.61446 - Test loss: 0.56871\n",
      "Epoch 16109 - lr: 0.01000 - Train loss: 0.60782 - Test loss: 0.56949\n",
      "Epoch 16110 - lr: 0.01000 - Train loss: 0.61084 - Test loss: 0.56919\n",
      "Epoch 16111 - lr: 0.01000 - Train loss: 0.60797 - Test loss: 0.56989\n",
      "Epoch 16112 - lr: 0.01000 - Train loss: 0.61088 - Test loss: 0.56944\n",
      "Epoch 16113 - lr: 0.01000 - Train loss: 0.60518 - Test loss: 0.57008\n",
      "Epoch 16114 - lr: 0.01000 - Train loss: 0.61830 - Test loss: 0.57052\n",
      "Epoch 16115 - lr: 0.01000 - Train loss: 0.63539 - Test loss: 0.56985\n",
      "Epoch 16116 - lr: 0.01000 - Train loss: 0.61448 - Test loss: 0.56888\n",
      "Epoch 16117 - lr: 0.01000 - Train loss: 0.60934 - Test loss: 0.56967\n",
      "Epoch 16118 - lr: 0.01000 - Train loss: 0.61401 - Test loss: 0.56889\n",
      "Epoch 16119 - lr: 0.01000 - Train loss: 0.61739 - Test loss: 0.56966\n",
      "Epoch 16120 - lr: 0.01000 - Train loss: 0.63609 - Test loss: 0.56925\n",
      "Epoch 16121 - lr: 0.01000 - Train loss: 0.61380 - Test loss: 0.56855\n",
      "Epoch 16122 - lr: 0.01000 - Train loss: 0.60621 - Test loss: 0.56939\n",
      "Epoch 16123 - lr: 0.01000 - Train loss: 0.61366 - Test loss: 0.56952\n",
      "Epoch 16124 - lr: 0.01000 - Train loss: 0.61259 - Test loss: 0.56888\n",
      "Epoch 16125 - lr: 0.01000 - Train loss: 0.61285 - Test loss: 0.56970\n",
      "Epoch 16126 - lr: 0.01000 - Train loss: 0.61413 - Test loss: 0.56886\n",
      "Epoch 16127 - lr: 0.01000 - Train loss: 0.60894 - Test loss: 0.56965\n",
      "Epoch 16128 - lr: 0.01000 - Train loss: 0.61344 - Test loss: 0.56892\n",
      "Epoch 16129 - lr: 0.01000 - Train loss: 0.61661 - Test loss: 0.56971\n",
      "Epoch 16130 - lr: 0.01000 - Train loss: 0.62884 - Test loss: 0.56926\n",
      "Epoch 16131 - lr: 0.01000 - Train loss: 0.61481 - Test loss: 0.56842\n",
      "Epoch 16132 - lr: 0.01000 - Train loss: 0.61568 - Test loss: 0.56899\n",
      "Epoch 16133 - lr: 0.01000 - Train loss: 0.61438 - Test loss: 0.56828\n",
      "Epoch 16134 - lr: 0.01000 - Train loss: 0.61432 - Test loss: 0.56923\n",
      "Epoch 16135 - lr: 0.01000 - Train loss: 0.62148 - Test loss: 0.56915\n",
      "Epoch 16136 - lr: 0.01000 - Train loss: 0.61488 - Test loss: 0.56827\n",
      "Epoch 16137 - lr: 0.01000 - Train loss: 0.61680 - Test loss: 0.56915\n",
      "Epoch 16138 - lr: 0.01000 - Train loss: 0.62993 - Test loss: 0.56882\n",
      "Epoch 16139 - lr: 0.01000 - Train loss: 0.61549 - Test loss: 0.56799\n",
      "Epoch 16140 - lr: 0.01000 - Train loss: 0.61242 - Test loss: 0.56838\n",
      "Epoch 16141 - lr: 0.01000 - Train loss: 0.61048 - Test loss: 0.56824\n",
      "Epoch 16142 - lr: 0.01000 - Train loss: 0.60655 - Test loss: 0.56919\n",
      "Epoch 16143 - lr: 0.01000 - Train loss: 0.61017 - Test loss: 0.56911\n",
      "Epoch 16144 - lr: 0.01000 - Train loss: 0.61537 - Test loss: 0.56989\n",
      "Epoch 16145 - lr: 0.01000 - Train loss: 0.61532 - Test loss: 0.56982\n",
      "Epoch 16146 - lr: 0.01000 - Train loss: 0.61731 - Test loss: 0.57021\n",
      "Epoch 16147 - lr: 0.01000 - Train loss: 0.61481 - Test loss: 0.56997\n",
      "Epoch 16148 - lr: 0.01000 - Train loss: 0.60424 - Test loss: 0.57047\n",
      "Epoch 16149 - lr: 0.01000 - Train loss: 0.61767 - Test loss: 0.57090\n",
      "Epoch 16150 - lr: 0.01000 - Train loss: 0.65096 - Test loss: 0.57038\n",
      "Epoch 16151 - lr: 0.01000 - Train loss: 0.62178 - Test loss: 0.56999\n",
      "Epoch 16152 - lr: 0.01000 - Train loss: 0.61430 - Test loss: 0.56899\n",
      "Epoch 16153 - lr: 0.01000 - Train loss: 0.61362 - Test loss: 0.56983\n",
      "Epoch 16154 - lr: 0.01000 - Train loss: 0.61784 - Test loss: 0.56945\n",
      "Epoch 16155 - lr: 0.01000 - Train loss: 0.61169 - Test loss: 0.56886\n",
      "Epoch 16156 - lr: 0.01000 - Train loss: 0.61198 - Test loss: 0.56974\n",
      "Epoch 16157 - lr: 0.01000 - Train loss: 0.61433 - Test loss: 0.56884\n",
      "Epoch 16158 - lr: 0.01000 - Train loss: 0.61534 - Test loss: 0.56969\n",
      "Epoch 16159 - lr: 0.01000 - Train loss: 0.61497 - Test loss: 0.56959\n",
      "Epoch 16160 - lr: 0.01000 - Train loss: 0.60468 - Test loss: 0.57020\n",
      "Epoch 16161 - lr: 0.01000 - Train loss: 0.61597 - Test loss: 0.57043\n",
      "Epoch 16162 - lr: 0.01000 - Train loss: 0.61313 - Test loss: 0.56956\n",
      "Epoch 16163 - lr: 0.01000 - Train loss: 0.60683 - Test loss: 0.57022\n",
      "Epoch 16164 - lr: 0.01000 - Train loss: 0.61005 - Test loss: 0.56970\n",
      "Epoch 16165 - lr: 0.01000 - Train loss: 0.60460 - Test loss: 0.57035\n",
      "Epoch 16166 - lr: 0.01000 - Train loss: 0.61503 - Test loss: 0.57052\n",
      "Epoch 16167 - lr: 0.01000 - Train loss: 0.61499 - Test loss: 0.56935\n",
      "Epoch 16168 - lr: 0.01000 - Train loss: 0.61626 - Test loss: 0.56989\n",
      "Epoch 16169 - lr: 0.01000 - Train loss: 0.62408 - Test loss: 0.56998\n",
      "Epoch 16170 - lr: 0.01000 - Train loss: 0.63622 - Test loss: 0.56952\n",
      "Epoch 16171 - lr: 0.01000 - Train loss: 0.61313 - Test loss: 0.56890\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16172 - lr: 0.01000 - Train loss: 0.61140 - Test loss: 0.56977\n",
      "Epoch 16173 - lr: 0.01000 - Train loss: 0.61465 - Test loss: 0.56881\n",
      "Epoch 16174 - lr: 0.01000 - Train loss: 0.61649 - Test loss: 0.56952\n",
      "Epoch 16175 - lr: 0.01000 - Train loss: 0.61525 - Test loss: 0.56943\n",
      "Epoch 16176 - lr: 0.01000 - Train loss: 0.60609 - Test loss: 0.57011\n",
      "Epoch 16177 - lr: 0.01000 - Train loss: 0.60937 - Test loss: 0.56975\n",
      "Epoch 16178 - lr: 0.01000 - Train loss: 0.60912 - Test loss: 0.57043\n",
      "Epoch 16179 - lr: 0.01000 - Train loss: 0.61495 - Test loss: 0.56931\n",
      "Epoch 16180 - lr: 0.01000 - Train loss: 0.61126 - Test loss: 0.56943\n",
      "Epoch 16181 - lr: 0.01000 - Train loss: 0.60907 - Test loss: 0.56925\n",
      "Epoch 16182 - lr: 0.01000 - Train loss: 0.60741 - Test loss: 0.57008\n",
      "Epoch 16183 - lr: 0.01000 - Train loss: 0.61224 - Test loss: 0.56933\n",
      "Epoch 16184 - lr: 0.01000 - Train loss: 0.61606 - Test loss: 0.57011\n",
      "Epoch 16185 - lr: 0.01000 - Train loss: 0.63894 - Test loss: 0.56976\n",
      "Epoch 16186 - lr: 0.01000 - Train loss: 0.62020 - Test loss: 0.56955\n",
      "Epoch 16187 - lr: 0.01000 - Train loss: 0.61485 - Test loss: 0.56859\n",
      "Epoch 16188 - lr: 0.01000 - Train loss: 0.61275 - Test loss: 0.56905\n",
      "Epoch 16189 - lr: 0.01000 - Train loss: 0.61401 - Test loss: 0.56838\n",
      "Epoch 16190 - lr: 0.01000 - Train loss: 0.61111 - Test loss: 0.56878\n",
      "Epoch 16191 - lr: 0.01000 - Train loss: 0.60922 - Test loss: 0.56867\n",
      "Epoch 16192 - lr: 0.01000 - Train loss: 0.60623 - Test loss: 0.56963\n",
      "Epoch 16193 - lr: 0.01000 - Train loss: 0.60963 - Test loss: 0.56927\n",
      "Epoch 16194 - lr: 0.01000 - Train loss: 0.60648 - Test loss: 0.57009\n",
      "Epoch 16195 - lr: 0.01000 - Train loss: 0.61035 - Test loss: 0.56953\n",
      "Epoch 16196 - lr: 0.01000 - Train loss: 0.60956 - Test loss: 0.57034\n",
      "Epoch 16197 - lr: 0.01000 - Train loss: 0.61464 - Test loss: 0.56929\n",
      "Epoch 16198 - lr: 0.01000 - Train loss: 0.61082 - Test loss: 0.56950\n",
      "Epoch 16199 - lr: 0.01000 - Train loss: 0.60867 - Test loss: 0.56932\n",
      "Epoch 16200 - lr: 0.01000 - Train loss: 0.60503 - Test loss: 0.57018\n",
      "Epoch 16201 - lr: 0.01000 - Train loss: 0.60871 - Test loss: 0.57002\n",
      "Epoch 16202 - lr: 0.01000 - Train loss: 0.61368 - Test loss: 0.57077\n",
      "Epoch 16203 - lr: 0.01000 - Train loss: 0.61791 - Test loss: 0.57072\n",
      "Epoch 16204 - lr: 0.01000 - Train loss: 0.61379 - Test loss: 0.56963\n",
      "Epoch 16205 - lr: 0.01000 - Train loss: 0.61182 - Test loss: 0.56989\n",
      "Epoch 16206 - lr: 0.01000 - Train loss: 0.61229 - Test loss: 0.56921\n",
      "Epoch 16207 - lr: 0.01000 - Train loss: 0.61490 - Test loss: 0.56990\n",
      "Epoch 16208 - lr: 0.01000 - Train loss: 0.62103 - Test loss: 0.56992\n",
      "Epoch 16209 - lr: 0.01000 - Train loss: 0.61398 - Test loss: 0.56950\n",
      "Epoch 16210 - lr: 0.01000 - Train loss: 0.61221 - Test loss: 0.56982\n",
      "Epoch 16211 - lr: 0.01000 - Train loss: 0.61326 - Test loss: 0.56903\n",
      "Epoch 16212 - lr: 0.01000 - Train loss: 0.61136 - Test loss: 0.56943\n",
      "Epoch 16213 - lr: 0.01000 - Train loss: 0.61151 - Test loss: 0.56892\n",
      "Epoch 16214 - lr: 0.01000 - Train loss: 0.61515 - Test loss: 0.56978\n",
      "Epoch 16215 - lr: 0.01000 - Train loss: 0.61476 - Test loss: 0.56976\n",
      "Epoch 16216 - lr: 0.01000 - Train loss: 0.60744 - Test loss: 0.57051\n",
      "Epoch 16217 - lr: 0.01000 - Train loss: 0.61288 - Test loss: 0.56960\n",
      "Epoch 16218 - lr: 0.01000 - Train loss: 0.61277 - Test loss: 0.56999\n",
      "Epoch 16219 - lr: 0.01000 - Train loss: 0.61388 - Test loss: 0.56910\n",
      "Epoch 16220 - lr: 0.01000 - Train loss: 0.60981 - Test loss: 0.56939\n",
      "Epoch 16221 - lr: 0.01000 - Train loss: 0.60796 - Test loss: 0.56945\n",
      "Epoch 16222 - lr: 0.01000 - Train loss: 0.61212 - Test loss: 0.57038\n",
      "Epoch 16223 - lr: 0.01000 - Train loss: 0.61910 - Test loss: 0.57023\n",
      "Epoch 16224 - lr: 0.01000 - Train loss: 0.61356 - Test loss: 0.56924\n",
      "Epoch 16225 - lr: 0.01000 - Train loss: 0.61280 - Test loss: 0.56976\n",
      "Epoch 16226 - lr: 0.01000 - Train loss: 0.61356 - Test loss: 0.56895\n",
      "Epoch 16227 - lr: 0.01000 - Train loss: 0.61092 - Test loss: 0.56940\n",
      "Epoch 16228 - lr: 0.01000 - Train loss: 0.61104 - Test loss: 0.56895\n",
      "Epoch 16229 - lr: 0.01000 - Train loss: 0.61468 - Test loss: 0.56984\n",
      "Epoch 16230 - lr: 0.01000 - Train loss: 0.61362 - Test loss: 0.56989\n",
      "Epoch 16231 - lr: 0.01000 - Train loss: 0.60455 - Test loss: 0.57063\n",
      "Epoch 16232 - lr: 0.01000 - Train loss: 0.60775 - Test loss: 0.57035\n",
      "Epoch 16233 - lr: 0.01000 - Train loss: 0.60798 - Test loss: 0.57106\n",
      "Epoch 16234 - lr: 0.01000 - Train loss: 0.61347 - Test loss: 0.56997\n",
      "Epoch 16235 - lr: 0.01000 - Train loss: 0.60971 - Test loss: 0.57012\n",
      "Epoch 16236 - lr: 0.01000 - Train loss: 0.60749 - Test loss: 0.56997\n",
      "Epoch 16237 - lr: 0.01000 - Train loss: 0.60526 - Test loss: 0.57081\n",
      "Epoch 16238 - lr: 0.01000 - Train loss: 0.60898 - Test loss: 0.57026\n",
      "Epoch 16239 - lr: 0.01000 - Train loss: 0.60685 - Test loss: 0.57104\n",
      "Epoch 16240 - lr: 0.01000 - Train loss: 0.61233 - Test loss: 0.57011\n",
      "Epoch 16241 - lr: 0.01000 - Train loss: 0.61206 - Test loss: 0.57049\n",
      "Epoch 16242 - lr: 0.01000 - Train loss: 0.61305 - Test loss: 0.56960\n",
      "Epoch 16243 - lr: 0.01000 - Train loss: 0.60999 - Test loss: 0.56995\n",
      "Epoch 16244 - lr: 0.01000 - Train loss: 0.60874 - Test loss: 0.56964\n",
      "Epoch 16245 - lr: 0.01000 - Train loss: 0.60795 - Test loss: 0.57060\n",
      "Epoch 16246 - lr: 0.01000 - Train loss: 0.61287 - Test loss: 0.56972\n",
      "Epoch 16247 - lr: 0.01000 - Train loss: 0.60993 - Test loss: 0.57005\n",
      "Epoch 16248 - lr: 0.01000 - Train loss: 0.60881 - Test loss: 0.56970\n",
      "Epoch 16249 - lr: 0.01000 - Train loss: 0.60886 - Test loss: 0.57066\n",
      "Epoch 16250 - lr: 0.01000 - Train loss: 0.61221 - Test loss: 0.56981\n",
      "Epoch 16251 - lr: 0.01000 - Train loss: 0.61382 - Test loss: 0.57052\n",
      "Epoch 16252 - lr: 0.01000 - Train loss: 0.61892 - Test loss: 0.57073\n",
      "Epoch 16253 - lr: 0.01000 - Train loss: 0.61040 - Test loss: 0.57004\n",
      "Epoch 16254 - lr: 0.01000 - Train loss: 0.60320 - Test loss: 0.57085\n",
      "Epoch 16255 - lr: 0.01000 - Train loss: 0.60814 - Test loss: 0.57082\n",
      "Epoch 16256 - lr: 0.01000 - Train loss: 0.61197 - Test loss: 0.57110\n",
      "Epoch 16257 - lr: 0.01000 - Train loss: 0.61256 - Test loss: 0.57009\n",
      "Epoch 16258 - lr: 0.01000 - Train loss: 0.61123 - Test loss: 0.57049\n",
      "Epoch 16259 - lr: 0.01000 - Train loss: 0.61229 - Test loss: 0.56969\n",
      "Epoch 16260 - lr: 0.01000 - Train loss: 0.60945 - Test loss: 0.57007\n",
      "Epoch 16261 - lr: 0.01000 - Train loss: 0.60804 - Test loss: 0.56980\n",
      "Epoch 16262 - lr: 0.01000 - Train loss: 0.60625 - Test loss: 0.57077\n",
      "Epoch 16263 - lr: 0.01000 - Train loss: 0.61142 - Test loss: 0.57001\n",
      "Epoch 16264 - lr: 0.01000 - Train loss: 0.61149 - Test loss: 0.57050\n",
      "Epoch 16265 - lr: 0.01000 - Train loss: 0.61205 - Test loss: 0.56971\n",
      "Epoch 16266 - lr: 0.01000 - Train loss: 0.61099 - Test loss: 0.57024\n",
      "Epoch 16267 - lr: 0.01000 - Train loss: 0.61192 - Test loss: 0.56954\n",
      "Epoch 16268 - lr: 0.01000 - Train loss: 0.60944 - Test loss: 0.57001\n",
      "Epoch 16269 - lr: 0.01000 - Train loss: 0.60871 - Test loss: 0.56969\n",
      "Epoch 16270 - lr: 0.01000 - Train loss: 0.61084 - Test loss: 0.57069\n",
      "Epoch 16271 - lr: 0.01000 - Train loss: 0.61945 - Test loss: 0.57078\n",
      "Epoch 16272 - lr: 0.01000 - Train loss: 0.61886 - Test loss: 0.57072\n",
      "Epoch 16273 - lr: 0.01000 - Train loss: 0.61079 - Test loss: 0.57026\n",
      "Epoch 16274 - lr: 0.01000 - Train loss: 0.61334 - Test loss: 0.57104\n",
      "Epoch 16275 - lr: 0.01000 - Train loss: 0.63033 - Test loss: 0.57068\n",
      "Epoch 16276 - lr: 0.01000 - Train loss: 0.61104 - Test loss: 0.56984\n",
      "Epoch 16277 - lr: 0.01000 - Train loss: 0.61260 - Test loss: 0.57076\n",
      "Epoch 16278 - lr: 0.01000 - Train loss: 0.62025 - Test loss: 0.57050\n",
      "Epoch 16279 - lr: 0.01000 - Train loss: 0.60646 - Test loss: 0.57038\n",
      "Epoch 16280 - lr: 0.01000 - Train loss: 0.60874 - Test loss: 0.57125\n",
      "Epoch 16281 - lr: 0.01000 - Train loss: 0.61081 - Test loss: 0.57038\n",
      "Epoch 16282 - lr: 0.01000 - Train loss: 0.61095 - Test loss: 0.57121\n",
      "Epoch 16283 - lr: 0.01000 - Train loss: 0.62097 - Test loss: 0.57131\n",
      "Epoch 16284 - lr: 0.01000 - Train loss: 0.63155 - Test loss: 0.57085\n",
      "Epoch 16285 - lr: 0.01000 - Train loss: 0.61019 - Test loss: 0.57008\n",
      "Epoch 16286 - lr: 0.01000 - Train loss: 0.60594 - Test loss: 0.57098\n",
      "Epoch 16287 - lr: 0.01000 - Train loss: 0.61157 - Test loss: 0.57014\n",
      "Epoch 16288 - lr: 0.01000 - Train loss: 0.60898 - Test loss: 0.57045\n",
      "Epoch 16289 - lr: 0.01000 - Train loss: 0.60793 - Test loss: 0.57009\n",
      "Epoch 16290 - lr: 0.01000 - Train loss: 0.60842 - Test loss: 0.57105\n",
      "Epoch 16291 - lr: 0.01000 - Train loss: 0.61093 - Test loss: 0.57023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16292 - lr: 0.01000 - Train loss: 0.61277 - Test loss: 0.57109\n",
      "Epoch 16293 - lr: 0.01000 - Train loss: 0.62882 - Test loss: 0.57077\n",
      "Epoch 16294 - lr: 0.01000 - Train loss: 0.61187 - Test loss: 0.56986\n",
      "Epoch 16295 - lr: 0.01000 - Train loss: 0.61029 - Test loss: 0.57038\n",
      "Epoch 16296 - lr: 0.01000 - Train loss: 0.61165 - Test loss: 0.56968\n",
      "Epoch 16297 - lr: 0.01000 - Train loss: 0.60714 - Test loss: 0.57001\n",
      "Epoch 16298 - lr: 0.01000 - Train loss: 0.60910 - Test loss: 0.57047\n",
      "Epoch 16299 - lr: 0.01000 - Train loss: 0.60900 - Test loss: 0.57001\n",
      "Epoch 16300 - lr: 0.01000 - Train loss: 0.61293 - Test loss: 0.57092\n",
      "Epoch 16301 - lr: 0.01000 - Train loss: 0.62579 - Test loss: 0.57066\n",
      "Epoch 16302 - lr: 0.01000 - Train loss: 0.61159 - Test loss: 0.56984\n",
      "Epoch 16303 - lr: 0.01000 - Train loss: 0.60691 - Test loss: 0.57014\n",
      "Epoch 16304 - lr: 0.01000 - Train loss: 0.60955 - Test loss: 0.57064\n",
      "Epoch 16305 - lr: 0.01000 - Train loss: 0.61054 - Test loss: 0.57001\n",
      "Epoch 16306 - lr: 0.01000 - Train loss: 0.60945 - Test loss: 0.57052\n",
      "Epoch 16307 - lr: 0.01000 - Train loss: 0.61049 - Test loss: 0.56994\n",
      "Epoch 16308 - lr: 0.01000 - Train loss: 0.60915 - Test loss: 0.57046\n",
      "Epoch 16309 - lr: 0.01000 - Train loss: 0.60987 - Test loss: 0.56996\n",
      "Epoch 16310 - lr: 0.01000 - Train loss: 0.61082 - Test loss: 0.57063\n",
      "Epoch 16311 - lr: 0.01000 - Train loss: 0.61102 - Test loss: 0.56995\n",
      "Epoch 16312 - lr: 0.01000 - Train loss: 0.61095 - Test loss: 0.57064\n",
      "Epoch 16313 - lr: 0.01000 - Train loss: 0.61055 - Test loss: 0.57000\n",
      "Epoch 16314 - lr: 0.01000 - Train loss: 0.61246 - Test loss: 0.57088\n",
      "Epoch 16315 - lr: 0.01000 - Train loss: 0.61112 - Test loss: 0.57111\n",
      "Epoch 16316 - lr: 0.01000 - Train loss: 0.61235 - Test loss: 0.57162\n",
      "Epoch 16317 - lr: 0.01000 - Train loss: 0.61793 - Test loss: 0.57152\n",
      "Epoch 16318 - lr: 0.01000 - Train loss: 0.60903 - Test loss: 0.57085\n",
      "Epoch 16319 - lr: 0.01000 - Train loss: 0.60320 - Test loss: 0.57167\n",
      "Epoch 16320 - lr: 0.01000 - Train loss: 0.60688 - Test loss: 0.57118\n",
      "Epoch 16321 - lr: 0.01000 - Train loss: 0.60488 - Test loss: 0.57198\n",
      "Epoch 16322 - lr: 0.01000 - Train loss: 0.61053 - Test loss: 0.57107\n",
      "Epoch 16323 - lr: 0.01000 - Train loss: 0.60913 - Test loss: 0.57139\n",
      "Epoch 16324 - lr: 0.01000 - Train loss: 0.61012 - Test loss: 0.57067\n",
      "Epoch 16325 - lr: 0.01000 - Train loss: 0.60940 - Test loss: 0.57114\n",
      "Epoch 16326 - lr: 0.01000 - Train loss: 0.61068 - Test loss: 0.57043\n",
      "Epoch 16327 - lr: 0.01000 - Train loss: 0.60715 - Test loss: 0.57079\n",
      "Epoch 16328 - lr: 0.01000 - Train loss: 0.60518 - Test loss: 0.57088\n",
      "Epoch 16329 - lr: 0.01000 - Train loss: 0.60762 - Test loss: 0.57183\n",
      "Epoch 16330 - lr: 0.01000 - Train loss: 0.60958 - Test loss: 0.57104\n",
      "Epoch 16331 - lr: 0.01000 - Train loss: 0.60965 - Test loss: 0.57191\n",
      "Epoch 16332 - lr: 0.01000 - Train loss: 0.61921 - Test loss: 0.57197\n",
      "Epoch 16333 - lr: 0.01000 - Train loss: 0.61011 - Test loss: 0.57193\n",
      "Epoch 16334 - lr: 0.01000 - Train loss: 0.61072 - Test loss: 0.57258\n",
      "Epoch 16335 - lr: 0.01000 - Train loss: 0.61014 - Test loss: 0.57245\n",
      "Epoch 16336 - lr: 0.01000 - Train loss: 0.61116 - Test loss: 0.57298\n",
      "Epoch 16337 - lr: 0.01000 - Train loss: 0.61814 - Test loss: 0.57238\n",
      "Epoch 16338 - lr: 0.01000 - Train loss: 0.60665 - Test loss: 0.57219\n",
      "Epoch 16339 - lr: 0.01000 - Train loss: 0.60837 - Test loss: 0.57226\n",
      "Epoch 16340 - lr: 0.01000 - Train loss: 0.60811 - Test loss: 0.57154\n",
      "Epoch 16341 - lr: 0.01000 - Train loss: 0.61198 - Test loss: 0.57234\n",
      "Epoch 16342 - lr: 0.01000 - Train loss: 0.63727 - Test loss: 0.57203\n",
      "Epoch 16343 - lr: 0.01000 - Train loss: 0.61991 - Test loss: 0.57207\n",
      "Epoch 16344 - lr: 0.01000 - Train loss: 0.62950 - Test loss: 0.57167\n",
      "Epoch 16345 - lr: 0.01000 - Train loss: 0.61023 - Test loss: 0.57078\n",
      "Epoch 16346 - lr: 0.01000 - Train loss: 0.61241 - Test loss: 0.57165\n",
      "Epoch 16347 - lr: 0.01000 - Train loss: 0.62836 - Test loss: 0.57139\n",
      "Epoch 16348 - lr: 0.01000 - Train loss: 0.61093 - Test loss: 0.57051\n",
      "Epoch 16349 - lr: 0.01000 - Train loss: 0.60888 - Test loss: 0.57103\n",
      "Epoch 16350 - lr: 0.01000 - Train loss: 0.61042 - Test loss: 0.57040\n",
      "Epoch 16351 - lr: 0.01000 - Train loss: 0.60568 - Test loss: 0.57070\n",
      "Epoch 16352 - lr: 0.01000 - Train loss: 0.61010 - Test loss: 0.57135\n",
      "Epoch 16353 - lr: 0.01000 - Train loss: 0.61088 - Test loss: 0.57056\n",
      "Epoch 16354 - lr: 0.01000 - Train loss: 0.60725 - Test loss: 0.57097\n",
      "Epoch 16355 - lr: 0.01000 - Train loss: 0.60588 - Test loss: 0.57080\n",
      "Epoch 16356 - lr: 0.01000 - Train loss: 0.60540 - Test loss: 0.57181\n",
      "Epoch 16357 - lr: 0.01000 - Train loss: 0.61091 - Test loss: 0.57095\n",
      "Epoch 16358 - lr: 0.01000 - Train loss: 0.60498 - Test loss: 0.57108\n",
      "Epoch 16359 - lr: 0.01000 - Train loss: 0.61196 - Test loss: 0.57200\n",
      "Epoch 16360 - lr: 0.01000 - Train loss: 0.62911 - Test loss: 0.57172\n",
      "Epoch 16361 - lr: 0.01000 - Train loss: 0.60989 - Test loss: 0.57088\n",
      "Epoch 16362 - lr: 0.01000 - Train loss: 0.61190 - Test loss: 0.57174\n",
      "Epoch 16363 - lr: 0.01000 - Train loss: 0.61366 - Test loss: 0.57166\n",
      "Epoch 16364 - lr: 0.01000 - Train loss: 0.61213 - Test loss: 0.57242\n",
      "Epoch 16365 - lr: 0.01000 - Train loss: 0.63750 - Test loss: 0.57214\n",
      "Epoch 16366 - lr: 0.01000 - Train loss: 0.61956 - Test loss: 0.57219\n",
      "Epoch 16367 - lr: 0.01000 - Train loss: 0.62358 - Test loss: 0.57178\n",
      "Epoch 16368 - lr: 0.01000 - Train loss: 0.61016 - Test loss: 0.57095\n",
      "Epoch 16369 - lr: 0.01000 - Train loss: 0.60543 - Test loss: 0.57119\n",
      "Epoch 16370 - lr: 0.01000 - Train loss: 0.60958 - Test loss: 0.57178\n",
      "Epoch 16371 - lr: 0.01000 - Train loss: 0.61095 - Test loss: 0.57093\n",
      "Epoch 16372 - lr: 0.01000 - Train loss: 0.60477 - Test loss: 0.57111\n",
      "Epoch 16373 - lr: 0.01000 - Train loss: 0.61189 - Test loss: 0.57207\n",
      "Epoch 16374 - lr: 0.01000 - Train loss: 0.62982 - Test loss: 0.57183\n",
      "Epoch 16375 - lr: 0.01000 - Train loss: 0.60936 - Test loss: 0.57105\n",
      "Epoch 16376 - lr: 0.01000 - Train loss: 0.61129 - Test loss: 0.57203\n",
      "Epoch 16377 - lr: 0.01000 - Train loss: 0.62794 - Test loss: 0.57179\n",
      "Epoch 16378 - lr: 0.01000 - Train loss: 0.61068 - Test loss: 0.57090\n",
      "Epoch 16379 - lr: 0.01000 - Train loss: 0.60644 - Test loss: 0.57129\n",
      "Epoch 16380 - lr: 0.01000 - Train loss: 0.60456 - Test loss: 0.57129\n",
      "Epoch 16381 - lr: 0.01000 - Train loss: 0.60400 - Test loss: 0.57228\n",
      "Epoch 16382 - lr: 0.01000 - Train loss: 0.61008 - Test loss: 0.57146\n",
      "Epoch 16383 - lr: 0.01000 - Train loss: 0.60471 - Test loss: 0.57159\n",
      "Epoch 16384 - lr: 0.01000 - Train loss: 0.61167 - Test loss: 0.57241\n",
      "Epoch 16385 - lr: 0.01000 - Train loss: 0.61063 - Test loss: 0.57238\n",
      "Epoch 16386 - lr: 0.01000 - Train loss: 0.60336 - Test loss: 0.57310\n",
      "Epoch 16387 - lr: 0.01000 - Train loss: 0.60934 - Test loss: 0.57216\n",
      "Epoch 16388 - lr: 0.01000 - Train loss: 0.60721 - Test loss: 0.57239\n",
      "Epoch 16389 - lr: 0.01000 - Train loss: 0.60739 - Test loss: 0.57183\n",
      "Epoch 16390 - lr: 0.01000 - Train loss: 0.61167 - Test loss: 0.57266\n",
      "Epoch 16391 - lr: 0.01000 - Train loss: 0.62400 - Test loss: 0.57234\n",
      "Epoch 16392 - lr: 0.01000 - Train loss: 0.61015 - Test loss: 0.57147\n",
      "Epoch 16393 - lr: 0.01000 - Train loss: 0.60409 - Test loss: 0.57158\n",
      "Epoch 16394 - lr: 0.01000 - Train loss: 0.60934 - Test loss: 0.57263\n",
      "Epoch 16395 - lr: 0.01000 - Train loss: 0.61634 - Test loss: 0.57286\n",
      "Epoch 16396 - lr: 0.01000 - Train loss: 0.60876 - Test loss: 0.57197\n",
      "Epoch 16397 - lr: 0.01000 - Train loss: 0.60898 - Test loss: 0.57287\n",
      "Epoch 16398 - lr: 0.01000 - Train loss: 0.61922 - Test loss: 0.57305\n",
      "Epoch 16399 - lr: 0.01000 - Train loss: 0.63088 - Test loss: 0.57262\n",
      "Epoch 16400 - lr: 0.01000 - Train loss: 0.60804 - Test loss: 0.57188\n",
      "Epoch 16401 - lr: 0.01000 - Train loss: 0.60412 - Test loss: 0.57278\n",
      "Epoch 16402 - lr: 0.01000 - Train loss: 0.61030 - Test loss: 0.57186\n",
      "Epoch 16403 - lr: 0.01000 - Train loss: 0.60392 - Test loss: 0.57183\n",
      "Epoch 16404 - lr: 0.01000 - Train loss: 0.60610 - Test loss: 0.57283\n",
      "Epoch 16405 - lr: 0.01000 - Train loss: 0.60992 - Test loss: 0.57191\n",
      "Epoch 16406 - lr: 0.01000 - Train loss: 0.60878 - Test loss: 0.57242\n",
      "Epoch 16407 - lr: 0.01000 - Train loss: 0.61037 - Test loss: 0.57157\n",
      "Epoch 16408 - lr: 0.01000 - Train loss: 0.60376 - Test loss: 0.57166\n",
      "Epoch 16409 - lr: 0.01000 - Train loss: 0.60756 - Test loss: 0.57273\n",
      "Epoch 16410 - lr: 0.01000 - Train loss: 0.60911 - Test loss: 0.57236\n",
      "Epoch 16411 - lr: 0.01000 - Train loss: 0.61009 - Test loss: 0.57290\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16412 - lr: 0.01000 - Train loss: 0.60758 - Test loss: 0.57230\n",
      "Epoch 16413 - lr: 0.01000 - Train loss: 0.60388 - Test loss: 0.57316\n",
      "Epoch 16414 - lr: 0.01000 - Train loss: 0.61002 - Test loss: 0.57220\n",
      "Epoch 16415 - lr: 0.01000 - Train loss: 0.60361 - Test loss: 0.57215\n",
      "Epoch 16416 - lr: 0.01000 - Train loss: 0.60616 - Test loss: 0.57314\n",
      "Epoch 16417 - lr: 0.01000 - Train loss: 0.60893 - Test loss: 0.57227\n",
      "Epoch 16418 - lr: 0.01000 - Train loss: 0.61119 - Test loss: 0.57309\n",
      "Epoch 16419 - lr: 0.01000 - Train loss: 0.62999 - Test loss: 0.57282\n",
      "Epoch 16420 - lr: 0.01000 - Train loss: 0.60776 - Test loss: 0.57209\n",
      "Epoch 16421 - lr: 0.01000 - Train loss: 0.60538 - Test loss: 0.57305\n",
      "Epoch 16422 - lr: 0.01000 - Train loss: 0.60993 - Test loss: 0.57213\n",
      "Epoch 16423 - lr: 0.01000 - Train loss: 0.60521 - Test loss: 0.57239\n",
      "Epoch 16424 - lr: 0.01000 - Train loss: 0.60358 - Test loss: 0.57247\n",
      "Epoch 16425 - lr: 0.01000 - Train loss: 0.60964 - Test loss: 0.57341\n",
      "Epoch 16426 - lr: 0.01000 - Train loss: 0.61762 - Test loss: 0.57307\n",
      "Epoch 16427 - lr: 0.01000 - Train loss: 0.60356 - Test loss: 0.57283\n",
      "Epoch 16428 - lr: 0.01000 - Train loss: 0.60412 - Test loss: 0.57369\n",
      "Epoch 16429 - lr: 0.01000 - Train loss: 0.61002 - Test loss: 0.57265\n",
      "Epoch 16430 - lr: 0.01000 - Train loss: 0.60330 - Test loss: 0.57256\n",
      "Epoch 16431 - lr: 0.01000 - Train loss: 0.60504 - Test loss: 0.57353\n",
      "Epoch 16432 - lr: 0.01000 - Train loss: 0.60974 - Test loss: 0.57255\n",
      "Epoch 16433 - lr: 0.01000 - Train loss: 0.60482 - Test loss: 0.57277\n",
      "Epoch 16434 - lr: 0.01000 - Train loss: 0.60375 - Test loss: 0.57290\n",
      "Epoch 16435 - lr: 0.01000 - Train loss: 0.61054 - Test loss: 0.57362\n",
      "Epoch 16436 - lr: 0.01000 - Train loss: 0.61122 - Test loss: 0.57373\n",
      "Epoch 16437 - lr: 0.01000 - Train loss: 0.60511 - Test loss: 0.57312\n",
      "Epoch 16438 - lr: 0.01000 - Train loss: 0.60721 - Test loss: 0.57398\n",
      "Epoch 16439 - lr: 0.01000 - Train loss: 0.61184 - Test loss: 0.57364\n",
      "Epoch 16440 - lr: 0.01000 - Train loss: 0.60685 - Test loss: 0.57289\n",
      "Epoch 16441 - lr: 0.01000 - Train loss: 0.61010 - Test loss: 0.57357\n",
      "Epoch 16442 - lr: 0.01000 - Train loss: 0.61840 - Test loss: 0.57376\n",
      "Epoch 16443 - lr: 0.01000 - Train loss: 0.63260 - Test loss: 0.57342\n",
      "Epoch 16444 - lr: 0.01000 - Train loss: 0.61028 - Test loss: 0.57306\n",
      "Epoch 16445 - lr: 0.01000 - Train loss: 0.60318 - Test loss: 0.57291\n",
      "Epoch 16446 - lr: 0.01000 - Train loss: 0.60379 - Test loss: 0.57383\n",
      "Epoch 16447 - lr: 0.01000 - Train loss: 0.60968 - Test loss: 0.57283\n",
      "Epoch 16448 - lr: 0.01000 - Train loss: 0.60298 - Test loss: 0.57275\n",
      "Epoch 16449 - lr: 0.01000 - Train loss: 0.60367 - Test loss: 0.57374\n",
      "Epoch 16450 - lr: 0.01000 - Train loss: 0.60953 - Test loss: 0.57280\n",
      "Epoch 16451 - lr: 0.01000 - Train loss: 0.60288 - Test loss: 0.57274\n",
      "Epoch 16452 - lr: 0.01000 - Train loss: 0.60363 - Test loss: 0.57375\n",
      "Epoch 16453 - lr: 0.01000 - Train loss: 0.60942 - Test loss: 0.57283\n",
      "Epoch 16454 - lr: 0.01000 - Train loss: 0.60277 - Test loss: 0.57280\n",
      "Epoch 16455 - lr: 0.01000 - Train loss: 0.60387 - Test loss: 0.57382\n",
      "Epoch 16456 - lr: 0.01000 - Train loss: 0.60936 - Test loss: 0.57289\n",
      "Epoch 16457 - lr: 0.01000 - Train loss: 0.60265 - Test loss: 0.57292\n",
      "Epoch 16458 - lr: 0.01000 - Train loss: 0.60644 - Test loss: 0.57394\n",
      "Epoch 16459 - lr: 0.01000 - Train loss: 0.60795 - Test loss: 0.57354\n",
      "Epoch 16460 - lr: 0.01000 - Train loss: 0.60933 - Test loss: 0.57408\n",
      "Epoch 16461 - lr: 0.01000 - Train loss: 0.60958 - Test loss: 0.57372\n",
      "Epoch 16462 - lr: 0.01000 - Train loss: 0.60274 - Test loss: 0.57360\n",
      "Epoch 16463 - lr: 0.01000 - Train loss: 0.60783 - Test loss: 0.57448\n",
      "Epoch 16464 - lr: 0.01000 - Train loss: 0.61401 - Test loss: 0.57458\n",
      "Epoch 16465 - lr: 0.01000 - Train loss: 0.60921 - Test loss: 0.57341\n",
      "Epoch 16466 - lr: 0.01000 - Train loss: 0.60380 - Test loss: 0.57355\n",
      "Epoch 16467 - lr: 0.01000 - Train loss: 0.60454 - Test loss: 0.57379\n",
      "Epoch 16468 - lr: 0.01000 - Train loss: 0.60257 - Test loss: 0.57365\n",
      "Epoch 16469 - lr: 0.01000 - Train loss: 0.60239 - Test loss: 0.57453\n",
      "Epoch 16470 - lr: 0.01000 - Train loss: 0.60866 - Test loss: 0.57357\n",
      "Epoch 16471 - lr: 0.01000 - Train loss: 0.60234 - Test loss: 0.57352\n",
      "Epoch 16472 - lr: 0.01000 - Train loss: 0.60615 - Test loss: 0.57449\n",
      "Epoch 16473 - lr: 0.01000 - Train loss: 0.60826 - Test loss: 0.57407\n",
      "Epoch 16474 - lr: 0.01000 - Train loss: 0.60610 - Test loss: 0.57432\n",
      "Epoch 16475 - lr: 0.01000 - Train loss: 0.60760 - Test loss: 0.57351\n",
      "Epoch 16476 - lr: 0.01000 - Train loss: 0.60358 - Test loss: 0.57373\n",
      "Epoch 16477 - lr: 0.01000 - Train loss: 0.60368 - Test loss: 0.57396\n",
      "Epoch 16478 - lr: 0.01000 - Train loss: 0.60341 - Test loss: 0.57411\n",
      "Epoch 16479 - lr: 0.01000 - Train loss: 0.60500 - Test loss: 0.57438\n",
      "Epoch 16480 - lr: 0.01000 - Train loss: 0.60467 - Test loss: 0.57386\n",
      "Epoch 16481 - lr: 0.01000 - Train loss: 0.60898 - Test loss: 0.57476\n",
      "Epoch 16482 - lr: 0.01000 - Train loss: 0.62896 - Test loss: 0.57447\n",
      "Epoch 16483 - lr: 0.01000 - Train loss: 0.60609 - Test loss: 0.57376\n",
      "Epoch 16484 - lr: 0.01000 - Train loss: 0.60212 - Test loss: 0.57468\n",
      "Epoch 16485 - lr: 0.01000 - Train loss: 0.60826 - Test loss: 0.57378\n",
      "Epoch 16486 - lr: 0.01000 - Train loss: 0.60193 - Test loss: 0.57377\n",
      "Epoch 16487 - lr: 0.01000 - Train loss: 0.60511 - Test loss: 0.57478\n",
      "Epoch 16488 - lr: 0.01000 - Train loss: 0.60595 - Test loss: 0.57411\n",
      "Epoch 16489 - lr: 0.01000 - Train loss: 0.60160 - Test loss: 0.57497\n",
      "Epoch 16490 - lr: 0.01000 - Train loss: 0.60782 - Test loss: 0.57406\n",
      "Epoch 16491 - lr: 0.01000 - Train loss: 0.60206 - Test loss: 0.57411\n",
      "Epoch 16492 - lr: 0.01000 - Train loss: 0.60929 - Test loss: 0.57501\n",
      "Epoch 16493 - lr: 0.01000 - Train loss: 0.63468 - Test loss: 0.57479\n",
      "Epoch 16494 - lr: 0.01000 - Train loss: 0.61553 - Test loss: 0.57477\n",
      "Epoch 16495 - lr: 0.01000 - Train loss: 0.61274 - Test loss: 0.57461\n",
      "Epoch 16496 - lr: 0.01000 - Train loss: 0.60840 - Test loss: 0.57366\n",
      "Epoch 16497 - lr: 0.01000 - Train loss: 0.60177 - Test loss: 0.57366\n",
      "Epoch 16498 - lr: 0.01000 - Train loss: 0.60244 - Test loss: 0.57472\n",
      "Epoch 16499 - lr: 0.01000 - Train loss: 0.60825 - Test loss: 0.57385\n",
      "Epoch 16500 - lr: 0.01000 - Train loss: 0.60167 - Test loss: 0.57383\n",
      "Epoch 16501 - lr: 0.01000 - Train loss: 0.60230 - Test loss: 0.57486\n",
      "Epoch 16502 - lr: 0.01000 - Train loss: 0.60813 - Test loss: 0.57398\n",
      "Epoch 16503 - lr: 0.01000 - Train loss: 0.60156 - Test loss: 0.57396\n",
      "Epoch 16504 - lr: 0.01000 - Train loss: 0.60239 - Test loss: 0.57499\n",
      "Epoch 16505 - lr: 0.01000 - Train loss: 0.60808 - Test loss: 0.57410\n",
      "Epoch 16506 - lr: 0.01000 - Train loss: 0.60143 - Test loss: 0.57410\n",
      "Epoch 16507 - lr: 0.01000 - Train loss: 0.60317 - Test loss: 0.57513\n",
      "Epoch 16508 - lr: 0.01000 - Train loss: 0.60787 - Test loss: 0.57422\n",
      "Epoch 16509 - lr: 0.01000 - Train loss: 0.60257 - Test loss: 0.57445\n",
      "Epoch 16510 - lr: 0.01000 - Train loss: 0.60369 - Test loss: 0.57480\n",
      "Epoch 16511 - lr: 0.01000 - Train loss: 0.60211 - Test loss: 0.57457\n",
      "Epoch 16512 - lr: 0.01000 - Train loss: 0.60104 - Test loss: 0.57551\n",
      "Epoch 16513 - lr: 0.01000 - Train loss: 0.60720 - Test loss: 0.57464\n",
      "Epoch 16514 - lr: 0.01000 - Train loss: 0.60140 - Test loss: 0.57470\n",
      "Epoch 16515 - lr: 0.01000 - Train loss: 0.60846 - Test loss: 0.57563\n",
      "Epoch 16516 - lr: 0.01000 - Train loss: 0.63091 - Test loss: 0.57539\n",
      "Epoch 16517 - lr: 0.01000 - Train loss: 0.60844 - Test loss: 0.57505\n",
      "Epoch 16518 - lr: 0.01000 - Train loss: 0.60128 - Test loss: 0.57491\n",
      "Epoch 16519 - lr: 0.01000 - Train loss: 0.60126 - Test loss: 0.57583\n",
      "Epoch 16520 - lr: 0.01000 - Train loss: 0.60739 - Test loss: 0.57489\n",
      "Epoch 16521 - lr: 0.01000 - Train loss: 0.60102 - Test loss: 0.57487\n",
      "Epoch 16522 - lr: 0.01000 - Train loss: 0.60442 - Test loss: 0.57585\n",
      "Epoch 16523 - lr: 0.01000 - Train loss: 0.60491 - Test loss: 0.57527\n",
      "Epoch 16524 - lr: 0.01000 - Train loss: 0.60108 - Test loss: 0.57611\n",
      "Epoch 16525 - lr: 0.01000 - Train loss: 0.60723 - Test loss: 0.57514\n",
      "Epoch 16526 - lr: 0.01000 - Train loss: 0.60090 - Test loss: 0.57511\n",
      "Epoch 16527 - lr: 0.01000 - Train loss: 0.60531 - Test loss: 0.57608\n",
      "Epoch 16528 - lr: 0.01000 - Train loss: 0.61312 - Test loss: 0.57600\n",
      "Epoch 16529 - lr: 0.01000 - Train loss: 0.60646 - Test loss: 0.57504\n",
      "Epoch 16530 - lr: 0.01000 - Train loss: 0.60807 - Test loss: 0.57577\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16531 - lr: 0.01000 - Train loss: 0.61068 - Test loss: 0.57599\n",
      "Epoch 16532 - lr: 0.01000 - Train loss: 0.60694 - Test loss: 0.57503\n",
      "Epoch 16533 - lr: 0.01000 - Train loss: 0.60075 - Test loss: 0.57506\n",
      "Epoch 16534 - lr: 0.01000 - Train loss: 0.60571 - Test loss: 0.57607\n",
      "Epoch 16535 - lr: 0.01000 - Train loss: 0.61578 - Test loss: 0.57629\n",
      "Epoch 16536 - lr: 0.01000 - Train loss: 0.62213 - Test loss: 0.57584\n",
      "Epoch 16537 - lr: 0.01000 - Train loss: 0.60730 - Test loss: 0.57486\n",
      "Epoch 16538 - lr: 0.01000 - Train loss: 0.60069 - Test loss: 0.57485\n",
      "Epoch 16539 - lr: 0.01000 - Train loss: 0.60089 - Test loss: 0.57590\n",
      "Epoch 16540 - lr: 0.01000 - Train loss: 0.60698 - Test loss: 0.57506\n",
      "Epoch 16541 - lr: 0.01000 - Train loss: 0.60055 - Test loss: 0.57504\n",
      "Epoch 16542 - lr: 0.01000 - Train loss: 0.60113 - Test loss: 0.57608\n",
      "Epoch 16543 - lr: 0.01000 - Train loss: 0.60701 - Test loss: 0.57520\n",
      "Epoch 16544 - lr: 0.01000 - Train loss: 0.60043 - Test loss: 0.57518\n",
      "Epoch 16545 - lr: 0.01000 - Train loss: 0.60100 - Test loss: 0.57621\n",
      "Epoch 16546 - lr: 0.01000 - Train loss: 0.60687 - Test loss: 0.57533\n",
      "Epoch 16547 - lr: 0.01000 - Train loss: 0.60030 - Test loss: 0.57532\n",
      "Epoch 16548 - lr: 0.01000 - Train loss: 0.60115 - Test loss: 0.57636\n",
      "Epoch 16549 - lr: 0.01000 - Train loss: 0.60683 - Test loss: 0.57546\n",
      "Epoch 16550 - lr: 0.01000 - Train loss: 0.60016 - Test loss: 0.57547\n",
      "Epoch 16551 - lr: 0.01000 - Train loss: 0.60217 - Test loss: 0.57651\n",
      "Epoch 16552 - lr: 0.01000 - Train loss: 0.60638 - Test loss: 0.57562\n",
      "Epoch 16553 - lr: 0.01000 - Train loss: 0.60313 - Test loss: 0.57600\n",
      "Epoch 16554 - lr: 0.01000 - Train loss: 0.60360 - Test loss: 0.57553\n",
      "Epoch 16555 - lr: 0.01000 - Train loss: 0.60688 - Test loss: 0.57633\n",
      "Epoch 16556 - lr: 0.01000 - Train loss: 0.61425 - Test loss: 0.57651\n",
      "Epoch 16557 - lr: 0.01000 - Train loss: 0.61296 - Test loss: 0.57672\n",
      "Epoch 16558 - lr: 0.01000 - Train loss: 0.60463 - Test loss: 0.57619\n",
      "Epoch 16559 - lr: 0.01000 - Train loss: 0.60731 - Test loss: 0.57702\n",
      "Epoch 16560 - lr: 0.01000 - Train loss: 0.63561 - Test loss: 0.57679\n",
      "Epoch 16561 - lr: 0.01000 - Train loss: 0.61530 - Test loss: 0.57688\n",
      "Epoch 16562 - lr: 0.01000 - Train loss: 0.62641 - Test loss: 0.57653\n",
      "Epoch 16563 - lr: 0.01000 - Train loss: 0.60442 - Test loss: 0.57578\n",
      "Epoch 16564 - lr: 0.01000 - Train loss: 0.60355 - Test loss: 0.57677\n",
      "Epoch 16565 - lr: 0.01000 - Train loss: 0.60444 - Test loss: 0.57631\n",
      "Epoch 16566 - lr: 0.01000 - Train loss: 0.60709 - Test loss: 0.57718\n",
      "Epoch 16567 - lr: 0.01000 - Train loss: 0.63644 - Test loss: 0.57696\n",
      "Epoch 16568 - lr: 0.01000 - Train loss: 0.61520 - Test loss: 0.57703\n",
      "Epoch 16569 - lr: 0.01000 - Train loss: 0.62443 - Test loss: 0.57666\n",
      "Epoch 16570 - lr: 0.01000 - Train loss: 0.60607 - Test loss: 0.57571\n",
      "Epoch 16571 - lr: 0.01000 - Train loss: 0.60310 - Test loss: 0.57619\n",
      "Epoch 16572 - lr: 0.01000 - Train loss: 0.60462 - Test loss: 0.57563\n",
      "Epoch 16573 - lr: 0.01000 - Train loss: 0.60124 - Test loss: 0.57602\n",
      "Epoch 16574 - lr: 0.01000 - Train loss: 0.59980 - Test loss: 0.57619\n",
      "Epoch 16575 - lr: 0.01000 - Train loss: 0.60689 - Test loss: 0.57719\n",
      "Epoch 16576 - lr: 0.01000 - Train loss: 0.63412 - Test loss: 0.57701\n",
      "Epoch 16577 - lr: 0.01000 - Train loss: 0.61455 - Test loss: 0.57710\n",
      "Epoch 16578 - lr: 0.01000 - Train loss: 0.60782 - Test loss: 0.57691\n",
      "Epoch 16579 - lr: 0.01000 - Train loss: 0.60618 - Test loss: 0.57775\n",
      "Epoch 16580 - lr: 0.01000 - Train loss: 0.62412 - Test loss: 0.57731\n",
      "Epoch 16581 - lr: 0.01000 - Train loss: 0.60598 - Test loss: 0.57625\n",
      "Epoch 16582 - lr: 0.01000 - Train loss: 0.60168 - Test loss: 0.57659\n",
      "Epoch 16583 - lr: 0.01000 - Train loss: 0.60059 - Test loss: 0.57636\n",
      "Epoch 16584 - lr: 0.01000 - Train loss: 0.60237 - Test loss: 0.57741\n",
      "Epoch 16585 - lr: 0.01000 - Train loss: 0.60398 - Test loss: 0.57664\n",
      "Epoch 16586 - lr: 0.01000 - Train loss: 0.60385 - Test loss: 0.57760\n",
      "Epoch 16587 - lr: 0.01000 - Train loss: 0.61341 - Test loss: 0.57761\n",
      "Epoch 16588 - lr: 0.01000 - Train loss: 0.61422 - Test loss: 0.57761\n",
      "Epoch 16589 - lr: 0.01000 - Train loss: 0.60564 - Test loss: 0.57741\n",
      "Epoch 16590 - lr: 0.01000 - Train loss: 0.59934 - Test loss: 0.57817\n",
      "Epoch 16591 - lr: 0.01000 - Train loss: 0.60591 - Test loss: 0.57702\n",
      "Epoch 16592 - lr: 0.01000 - Train loss: 0.59975 - Test loss: 0.57670\n",
      "Epoch 16593 - lr: 0.01000 - Train loss: 0.59989 - Test loss: 0.57771\n",
      "Epoch 16594 - lr: 0.01000 - Train loss: 0.60595 - Test loss: 0.57669\n",
      "Epoch 16595 - lr: 0.01000 - Train loss: 0.59962 - Test loss: 0.57648\n",
      "Epoch 16596 - lr: 0.01000 - Train loss: 0.60020 - Test loss: 0.57757\n",
      "Epoch 16597 - lr: 0.01000 - Train loss: 0.60578 - Test loss: 0.57659\n",
      "Epoch 16598 - lr: 0.01000 - Train loss: 0.59896 - Test loss: 0.57651\n",
      "Epoch 16599 - lr: 0.01000 - Train loss: 0.59981 - Test loss: 0.57761\n",
      "Epoch 16600 - lr: 0.01000 - Train loss: 0.60561 - Test loss: 0.57664\n",
      "Epoch 16601 - lr: 0.01000 - Train loss: 0.59916 - Test loss: 0.57649\n",
      "Epoch 16602 - lr: 0.01000 - Train loss: 0.59966 - Test loss: 0.57761\n",
      "Epoch 16603 - lr: 0.01000 - Train loss: 0.60544 - Test loss: 0.57666\n",
      "Epoch 16604 - lr: 0.01000 - Train loss: 0.59901 - Test loss: 0.57652\n",
      "Epoch 16605 - lr: 0.01000 - Train loss: 0.59952 - Test loss: 0.57764\n",
      "Epoch 16606 - lr: 0.01000 - Train loss: 0.60527 - Test loss: 0.57669\n",
      "Epoch 16607 - lr: 0.01000 - Train loss: 0.59884 - Test loss: 0.57656\n",
      "Epoch 16608 - lr: 0.01000 - Train loss: 0.59932 - Test loss: 0.57769\n",
      "Epoch 16609 - lr: 0.01000 - Train loss: 0.60511 - Test loss: 0.57674\n",
      "Epoch 16610 - lr: 0.01000 - Train loss: 0.59872 - Test loss: 0.57659\n",
      "Epoch 16611 - lr: 0.01000 - Train loss: 0.59919 - Test loss: 0.57773\n",
      "Epoch 16612 - lr: 0.01000 - Train loss: 0.60495 - Test loss: 0.57677\n",
      "Epoch 16613 - lr: 0.01000 - Train loss: 0.59853 - Test loss: 0.57663\n",
      "Epoch 16614 - lr: 0.01000 - Train loss: 0.59891 - Test loss: 0.57777\n",
      "Epoch 16615 - lr: 0.01000 - Train loss: 0.60477 - Test loss: 0.57681\n",
      "Epoch 16616 - lr: 0.01000 - Train loss: 0.59846 - Test loss: 0.57666\n",
      "Epoch 16617 - lr: 0.01000 - Train loss: 0.59885 - Test loss: 0.57780\n",
      "Epoch 16618 - lr: 0.01000 - Train loss: 0.60463 - Test loss: 0.57683\n",
      "Epoch 16619 - lr: 0.01000 - Train loss: 0.59821 - Test loss: 0.57669\n",
      "Epoch 16620 - lr: 0.01000 - Train loss: 0.59844 - Test loss: 0.57784\n",
      "Epoch 16621 - lr: 0.01000 - Train loss: 0.60440 - Test loss: 0.57687\n",
      "Epoch 16622 - lr: 0.01000 - Train loss: 0.59816 - Test loss: 0.57670\n",
      "Epoch 16623 - lr: 0.01000 - Train loss: 0.59840 - Test loss: 0.57785\n",
      "Epoch 16624 - lr: 0.01000 - Train loss: 0.60428 - Test loss: 0.57687\n",
      "Epoch 16625 - lr: 0.01000 - Train loss: 0.59793 - Test loss: 0.57671\n",
      "Epoch 16626 - lr: 0.01000 - Train loss: 0.59802 - Test loss: 0.57787\n",
      "Epoch 16627 - lr: 0.01000 - Train loss: 0.60405 - Test loss: 0.57690\n",
      "Epoch 16628 - lr: 0.01000 - Train loss: 0.59780 - Test loss: 0.57672\n",
      "Epoch 16629 - lr: 0.01000 - Train loss: 0.59780 - Test loss: 0.57789\n",
      "Epoch 16630 - lr: 0.01000 - Train loss: 0.60386 - Test loss: 0.57691\n",
      "Epoch 16631 - lr: 0.01000 - Train loss: 0.59760 - Test loss: 0.57674\n",
      "Epoch 16632 - lr: 0.01000 - Train loss: 0.59749 - Test loss: 0.57792\n",
      "Epoch 16633 - lr: 0.01000 - Train loss: 0.60363 - Test loss: 0.57694\n",
      "Epoch 16634 - lr: 0.01000 - Train loss: 0.59737 - Test loss: 0.57678\n",
      "Epoch 16635 - lr: 0.01000 - Train loss: 0.59721 - Test loss: 0.57796\n",
      "Epoch 16636 - lr: 0.01000 - Train loss: 0.60340 - Test loss: 0.57698\n",
      "Epoch 16637 - lr: 0.01000 - Train loss: 0.59712 - Test loss: 0.57684\n",
      "Epoch 16638 - lr: 0.01000 - Train loss: 0.59712 - Test loss: 0.57802\n",
      "Epoch 16639 - lr: 0.01000 - Train loss: 0.60330 - Test loss: 0.57702\n",
      "Epoch 16640 - lr: 0.01000 - Train loss: 0.59697 - Test loss: 0.57688\n",
      "Epoch 16641 - lr: 0.01000 - Train loss: 0.59695 - Test loss: 0.57807\n",
      "Epoch 16642 - lr: 0.01000 - Train loss: 0.60314 - Test loss: 0.57706\n",
      "Epoch 16643 - lr: 0.01000 - Train loss: 0.59679 - Test loss: 0.57694\n",
      "Epoch 16644 - lr: 0.01000 - Train loss: 0.59700 - Test loss: 0.57814\n",
      "Epoch 16645 - lr: 0.01000 - Train loss: 0.60311 - Test loss: 0.57710\n",
      "Epoch 16646 - lr: 0.01000 - Train loss: 0.59668 - Test loss: 0.57697\n",
      "Epoch 16647 - lr: 0.01000 - Train loss: 0.59668 - Test loss: 0.57818\n",
      "Epoch 16648 - lr: 0.01000 - Train loss: 0.60286 - Test loss: 0.57717\n",
      "Epoch 16649 - lr: 0.01000 - Train loss: 0.59648 - Test loss: 0.57707\n",
      "Epoch 16650 - lr: 0.01000 - Train loss: 0.59726 - Test loss: 0.57829\n",
      "Epoch 16651 - lr: 0.01000 - Train loss: 0.60306 - Test loss: 0.57720\n",
      "Epoch 16652 - lr: 0.01000 - Train loss: 0.59637 - Test loss: 0.57711\n",
      "Epoch 16653 - lr: 0.01000 - Train loss: 0.59707 - Test loss: 0.57835\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16654 - lr: 0.01000 - Train loss: 0.60292 - Test loss: 0.57727\n",
      "Epoch 16655 - lr: 0.01000 - Train loss: 0.59626 - Test loss: 0.57718\n",
      "Epoch 16656 - lr: 0.01000 - Train loss: 0.59697 - Test loss: 0.57844\n",
      "Epoch 16657 - lr: 0.01000 - Train loss: 0.60280 - Test loss: 0.57735\n",
      "Epoch 16658 - lr: 0.01000 - Train loss: 0.59613 - Test loss: 0.57728\n",
      "Epoch 16659 - lr: 0.01000 - Train loss: 0.59720 - Test loss: 0.57855\n",
      "Epoch 16660 - lr: 0.01000 - Train loss: 0.60275 - Test loss: 0.57744\n",
      "Epoch 16661 - lr: 0.01000 - Train loss: 0.59603 - Test loss: 0.57744\n",
      "Epoch 16662 - lr: 0.01000 - Train loss: 0.59990 - Test loss: 0.57874\n",
      "Epoch 16663 - lr: 0.01000 - Train loss: 0.60198 - Test loss: 0.57813\n",
      "Epoch 16664 - lr: 0.01000 - Train loss: 0.59972 - Test loss: 0.57857\n",
      "Epoch 16665 - lr: 0.01000 - Train loss: 0.60129 - Test loss: 0.57764\n",
      "Epoch 16666 - lr: 0.01000 - Train loss: 0.59724 - Test loss: 0.57793\n",
      "Epoch 16667 - lr: 0.01000 - Train loss: 0.59735 - Test loss: 0.57823\n",
      "Epoch 16668 - lr: 0.01000 - Train loss: 0.59695 - Test loss: 0.57840\n",
      "Epoch 16669 - lr: 0.01000 - Train loss: 0.59938 - Test loss: 0.57884\n",
      "Epoch 16670 - lr: 0.01000 - Train loss: 0.60075 - Test loss: 0.57794\n",
      "Epoch 16671 - lr: 0.01000 - Train loss: 0.59841 - Test loss: 0.57840\n",
      "Epoch 16672 - lr: 0.01000 - Train loss: 0.59799 - Test loss: 0.57792\n",
      "Epoch 16673 - lr: 0.01000 - Train loss: 0.60158 - Test loss: 0.57922\n",
      "Epoch 16674 - lr: 0.01000 - Train loss: 0.60110 - Test loss: 0.57897\n",
      "Epoch 16675 - lr: 0.01000 - Train loss: 0.59311 - Test loss: 0.57994\n",
      "Epoch 16676 - lr: 0.01000 - Train loss: 0.59721 - Test loss: 0.57916\n",
      "Epoch 16677 - lr: 0.01000 - Train loss: 0.59644 - Test loss: 0.58016\n",
      "Epoch 16678 - lr: 0.01000 - Train loss: 0.60235 - Test loss: 0.57878\n",
      "Epoch 16679 - lr: 0.01000 - Train loss: 0.59564 - Test loss: 0.57869\n",
      "Epoch 16680 - lr: 0.01000 - Train loss: 0.60116 - Test loss: 0.57996\n",
      "Epoch 16681 - lr: 0.01000 - Train loss: 0.60094 - Test loss: 0.57970\n",
      "Epoch 16682 - lr: 0.01000 - Train loss: 0.60318 - Test loss: 0.58068\n",
      "Epoch 16683 - lr: 0.01000 - Train loss: 0.63656 - Test loss: 0.58015\n",
      "Epoch 16684 - lr: 0.01000 - Train loss: 0.60376 - Test loss: 0.57953\n",
      "Epoch 16685 - lr: 0.01000 - Train loss: 0.59748 - Test loss: 0.57892\n",
      "Epoch 16686 - lr: 0.01000 - Train loss: 0.59959 - Test loss: 0.58019\n",
      "Epoch 16687 - lr: 0.01000 - Train loss: 0.60517 - Test loss: 0.57974\n",
      "Epoch 16688 - lr: 0.01000 - Train loss: 0.60104 - Test loss: 0.57873\n",
      "Epoch 16689 - lr: 0.01000 - Train loss: 0.59624 - Test loss: 0.57897\n",
      "Epoch 16690 - lr: 0.01000 - Train loss: 0.59994 - Test loss: 0.57968\n",
      "Epoch 16691 - lr: 0.01000 - Train loss: 0.60188 - Test loss: 0.57860\n",
      "Epoch 16692 - lr: 0.01000 - Train loss: 0.59524 - Test loss: 0.57859\n",
      "Epoch 16693 - lr: 0.01000 - Train loss: 0.59577 - Test loss: 0.57999\n",
      "Epoch 16694 - lr: 0.01000 - Train loss: 0.60179 - Test loss: 0.57887\n",
      "Epoch 16695 - lr: 0.01000 - Train loss: 0.59513 - Test loss: 0.57884\n",
      "Epoch 16696 - lr: 0.01000 - Train loss: 0.59604 - Test loss: 0.58021\n",
      "Epoch 16697 - lr: 0.01000 - Train loss: 0.60177 - Test loss: 0.57905\n",
      "Epoch 16698 - lr: 0.01000 - Train loss: 0.59503 - Test loss: 0.57906\n",
      "Epoch 16699 - lr: 0.01000 - Train loss: 0.59816 - Test loss: 0.58046\n",
      "Epoch 16700 - lr: 0.01000 - Train loss: 0.59906 - Test loss: 0.57947\n",
      "Epoch 16701 - lr: 0.01000 - Train loss: 0.59388 - Test loss: 0.58068\n",
      "Epoch 16702 - lr: 0.01000 - Train loss: 0.60002 - Test loss: 0.57966\n",
      "Epoch 16703 - lr: 0.01000 - Train loss: 0.59778 - Test loss: 0.58011\n",
      "Epoch 16704 - lr: 0.01000 - Train loss: 0.59782 - Test loss: 0.57948\n",
      "Epoch 16705 - lr: 0.01000 - Train loss: 0.60211 - Test loss: 0.58078\n",
      "Epoch 16706 - lr: 0.01000 - Train loss: 0.62087 - Test loss: 0.58035\n",
      "Epoch 16707 - lr: 0.01000 - Train loss: 0.59961 - Test loss: 0.57935\n",
      "Epoch 16708 - lr: 0.01000 - Train loss: 0.59986 - Test loss: 0.58081\n",
      "Epoch 16709 - lr: 0.01000 - Train loss: 0.60946 - Test loss: 0.58101\n",
      "Epoch 16710 - lr: 0.01000 - Train loss: 0.60099 - Test loss: 0.58063\n",
      "Epoch 16711 - lr: 0.01000 - Train loss: 0.59266 - Test loss: 0.58169\n",
      "Epoch 16712 - lr: 0.01000 - Train loss: 0.59776 - Test loss: 0.58070\n",
      "Epoch 16713 - lr: 0.01000 - Train loss: 0.60199 - Test loss: 0.58183\n",
      "Epoch 16714 - lr: 0.01000 - Train loss: 0.63166 - Test loss: 0.58134\n",
      "Epoch 16715 - lr: 0.01000 - Train loss: 0.60935 - Test loss: 0.58130\n",
      "Epoch 16716 - lr: 0.01000 - Train loss: 0.60568 - Test loss: 0.58136\n",
      "Epoch 16717 - lr: 0.01000 - Train loss: 0.60174 - Test loss: 0.58010\n",
      "Epoch 16718 - lr: 0.01000 - Train loss: 0.59483 - Test loss: 0.57999\n",
      "Epoch 16719 - lr: 0.01000 - Train loss: 0.59437 - Test loss: 0.58139\n",
      "Epoch 16720 - lr: 0.01000 - Train loss: 0.60094 - Test loss: 0.58027\n",
      "Epoch 16721 - lr: 0.01000 - Train loss: 0.59459 - Test loss: 0.58022\n",
      "Epoch 16722 - lr: 0.01000 - Train loss: 0.59629 - Test loss: 0.58164\n",
      "Epoch 16723 - lr: 0.01000 - Train loss: 0.60124 - Test loss: 0.58035\n",
      "Epoch 16724 - lr: 0.01000 - Train loss: 0.59563 - Test loss: 0.58066\n",
      "Epoch 16725 - lr: 0.01000 - Train loss: 0.59704 - Test loss: 0.58117\n",
      "Epoch 16726 - lr: 0.01000 - Train loss: 0.59646 - Test loss: 0.58063\n",
      "Epoch 16727 - lr: 0.01000 - Train loss: 0.59952 - Test loss: 0.58204\n",
      "Epoch 16728 - lr: 0.01000 - Train loss: 0.60767 - Test loss: 0.58214\n",
      "Epoch 16729 - lr: 0.01000 - Train loss: 0.60128 - Test loss: 0.58146\n",
      "Epoch 16730 - lr: 0.01000 - Train loss: 0.59499 - Test loss: 0.58147\n",
      "Epoch 16731 - lr: 0.01000 - Train loss: 0.60179 - Test loss: 0.58248\n",
      "Epoch 16732 - lr: 0.01000 - Train loss: 0.60022 - Test loss: 0.58218\n",
      "Epoch 16733 - lr: 0.01000 - Train loss: 0.60133 - Test loss: 0.58291\n",
      "Epoch 16734 - lr: 0.01000 - Train loss: 0.60493 - Test loss: 0.58236\n",
      "Epoch 16735 - lr: 0.01000 - Train loss: 0.60100 - Test loss: 0.58109\n",
      "Epoch 16736 - lr: 0.01000 - Train loss: 0.59447 - Test loss: 0.58095\n",
      "Epoch 16737 - lr: 0.01000 - Train loss: 0.59343 - Test loss: 0.58238\n",
      "Epoch 16738 - lr: 0.01000 - Train loss: 0.60006 - Test loss: 0.58129\n",
      "Epoch 16739 - lr: 0.01000 - Train loss: 0.59436 - Test loss: 0.58139\n",
      "Epoch 16740 - lr: 0.01000 - Train loss: 0.60157 - Test loss: 0.58278\n",
      "Epoch 16741 - lr: 0.01000 - Train loss: 0.63110 - Test loss: 0.58242\n",
      "Epoch 16742 - lr: 0.01000 - Train loss: 0.60917 - Test loss: 0.58252\n",
      "Epoch 16743 - lr: 0.01000 - Train loss: 0.60585 - Test loss: 0.58201\n",
      "Epoch 16744 - lr: 0.01000 - Train loss: 0.59764 - Test loss: 0.58254\n",
      "Epoch 16745 - lr: 0.01000 - Train loss: 0.59928 - Test loss: 0.58156\n",
      "Epoch 16746 - lr: 0.01000 - Train loss: 0.59557 - Test loss: 0.58196\n",
      "Epoch 16747 - lr: 0.01000 - Train loss: 0.59433 - Test loss: 0.58208\n",
      "Epoch 16748 - lr: 0.01000 - Train loss: 0.60170 - Test loss: 0.58334\n",
      "Epoch 16749 - lr: 0.01000 - Train loss: 0.62501 - Test loss: 0.58285\n",
      "Epoch 16750 - lr: 0.01000 - Train loss: 0.60380 - Test loss: 0.58251\n",
      "Epoch 16751 - lr: 0.01000 - Train loss: 0.59986 - Test loss: 0.58158\n",
      "Epoch 16752 - lr: 0.01000 - Train loss: 0.59387 - Test loss: 0.58172\n",
      "Epoch 16753 - lr: 0.01000 - Train loss: 0.59829 - Test loss: 0.58330\n",
      "Epoch 16754 - lr: 0.01000 - Train loss: 0.60701 - Test loss: 0.58318\n",
      "Epoch 16755 - lr: 0.01000 - Train loss: 0.59801 - Test loss: 0.58218\n",
      "Epoch 16756 - lr: 0.01000 - Train loss: 0.59392 - Test loss: 0.58358\n",
      "Epoch 16757 - lr: 0.01000 - Train loss: 0.60052 - Test loss: 0.58230\n",
      "Epoch 16758 - lr: 0.01000 - Train loss: 0.59432 - Test loss: 0.58207\n",
      "Epoch 16759 - lr: 0.01000 - Train loss: 0.59391 - Test loss: 0.58357\n",
      "Epoch 16760 - lr: 0.01000 - Train loss: 0.60042 - Test loss: 0.58235\n",
      "Epoch 16761 - lr: 0.01000 - Train loss: 0.59425 - Test loss: 0.58216\n",
      "Epoch 16762 - lr: 0.01000 - Train loss: 0.59400 - Test loss: 0.58371\n",
      "Epoch 16763 - lr: 0.01000 - Train loss: 0.60037 - Test loss: 0.58249\n",
      "Epoch 16764 - lr: 0.01000 - Train loss: 0.59415 - Test loss: 0.58232\n",
      "Epoch 16765 - lr: 0.01000 - Train loss: 0.59393 - Test loss: 0.58388\n",
      "Epoch 16766 - lr: 0.01000 - Train loss: 0.60026 - Test loss: 0.58267\n",
      "Epoch 16767 - lr: 0.01000 - Train loss: 0.59400 - Test loss: 0.58251\n",
      "Epoch 16768 - lr: 0.01000 - Train loss: 0.59367 - Test loss: 0.58408\n",
      "Epoch 16769 - lr: 0.01000 - Train loss: 0.60006 - Test loss: 0.58289\n",
      "Epoch 16770 - lr: 0.01000 - Train loss: 0.59381 - Test loss: 0.58275\n",
      "Epoch 16771 - lr: 0.01000 - Train loss: 0.59330 - Test loss: 0.58430\n",
      "Epoch 16772 - lr: 0.01000 - Train loss: 0.59979 - Test loss: 0.58313\n",
      "Epoch 16773 - lr: 0.01000 - Train loss: 0.59351 - Test loss: 0.58303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16774 - lr: 0.01000 - Train loss: 0.59294 - Test loss: 0.58457\n",
      "Epoch 16775 - lr: 0.01000 - Train loss: 0.59948 - Test loss: 0.58342\n",
      "Epoch 16776 - lr: 0.01000 - Train loss: 0.59319 - Test loss: 0.58338\n",
      "Epoch 16777 - lr: 0.01000 - Train loss: 0.59371 - Test loss: 0.58491\n",
      "Epoch 16778 - lr: 0.01000 - Train loss: 0.59989 - Test loss: 0.58363\n",
      "Epoch 16779 - lr: 0.01000 - Train loss: 0.59332 - Test loss: 0.58352\n",
      "Epoch 16780 - lr: 0.01000 - Train loss: 0.59249 - Test loss: 0.58505\n",
      "Epoch 16781 - lr: 0.01000 - Train loss: 0.59903 - Test loss: 0.58392\n",
      "Epoch 16782 - lr: 0.01000 - Train loss: 0.59294 - Test loss: 0.58398\n",
      "Epoch 16783 - lr: 0.01000 - Train loss: 0.59742 - Test loss: 0.58554\n",
      "Epoch 16784 - lr: 0.01000 - Train loss: 0.60670 - Test loss: 0.58543\n",
      "Epoch 16785 - lr: 0.01000 - Train loss: 0.60295 - Test loss: 0.58504\n",
      "Epoch 16786 - lr: 0.01000 - Train loss: 0.59900 - Test loss: 0.58399\n",
      "Epoch 16787 - lr: 0.01000 - Train loss: 0.59281 - Test loss: 0.58410\n",
      "Epoch 16788 - lr: 0.01000 - Train loss: 0.59577 - Test loss: 0.58570\n",
      "Epoch 16789 - lr: 0.01000 - Train loss: 0.59732 - Test loss: 0.58452\n",
      "Epoch 16790 - lr: 0.01000 - Train loss: 0.59502 - Test loss: 0.58600\n",
      "Epoch 16791 - lr: 0.01000 - Train loss: 0.59892 - Test loss: 0.58459\n",
      "Epoch 16792 - lr: 0.01000 - Train loss: 0.59819 - Test loss: 0.58555\n",
      "Epoch 16793 - lr: 0.01000 - Train loss: 0.59912 - Test loss: 0.58431\n",
      "Epoch 16794 - lr: 0.01000 - Train loss: 0.59523 - Test loss: 0.58502\n",
      "Epoch 16795 - lr: 0.01000 - Train loss: 0.59509 - Test loss: 0.58454\n",
      "Epoch 16796 - lr: 0.01000 - Train loss: 0.59947 - Test loss: 0.58612\n",
      "Epoch 16797 - lr: 0.01000 - Train loss: 0.62027 - Test loss: 0.58568\n",
      "Epoch 16798 - lr: 0.01000 - Train loss: 0.59666 - Test loss: 0.58483\n",
      "Epoch 16799 - lr: 0.01000 - Train loss: 0.59367 - Test loss: 0.58641\n",
      "Epoch 16800 - lr: 0.01000 - Train loss: 0.59948 - Test loss: 0.58503\n",
      "Epoch 16801 - lr: 0.01000 - Train loss: 0.59253 - Test loss: 0.58500\n",
      "Epoch 16802 - lr: 0.01000 - Train loss: 0.59305 - Test loss: 0.58658\n",
      "Epoch 16803 - lr: 0.01000 - Train loss: 0.59927 - Test loss: 0.58525\n",
      "Epoch 16804 - lr: 0.01000 - Train loss: 0.59269 - Test loss: 0.58513\n",
      "Epoch 16805 - lr: 0.01000 - Train loss: 0.59172 - Test loss: 0.58671\n",
      "Epoch 16806 - lr: 0.01000 - Train loss: 0.59831 - Test loss: 0.58555\n",
      "Epoch 16807 - lr: 0.01000 - Train loss: 0.59232 - Test loss: 0.58565\n",
      "Epoch 16808 - lr: 0.01000 - Train loss: 0.59785 - Test loss: 0.58724\n",
      "Epoch 16809 - lr: 0.01000 - Train loss: 0.59763 - Test loss: 0.58694\n",
      "Epoch 16810 - lr: 0.01000 - Train loss: 0.59938 - Test loss: 0.58819\n",
      "Epoch 16811 - lr: 0.01000 - Train loss: 0.62940 - Test loss: 0.58753\n",
      "Epoch 16812 - lr: 0.01000 - Train loss: 0.60688 - Test loss: 0.58744\n",
      "Epoch 16813 - lr: 0.01000 - Train loss: 0.60467 - Test loss: 0.58755\n",
      "Epoch 16814 - lr: 0.01000 - Train loss: 0.59792 - Test loss: 0.58618\n",
      "Epoch 16815 - lr: 0.01000 - Train loss: 0.59998 - Test loss: 0.58766\n",
      "Epoch 16816 - lr: 0.01000 - Train loss: 0.63175 - Test loss: 0.58730\n",
      "Epoch 16817 - lr: 0.01000 - Train loss: 0.60489 - Test loss: 0.58720\n",
      "Epoch 16818 - lr: 0.01000 - Train loss: 0.59796 - Test loss: 0.58599\n",
      "Epoch 16819 - lr: 0.01000 - Train loss: 0.59966 - Test loss: 0.58747\n",
      "Epoch 16820 - lr: 0.01000 - Train loss: 0.60459 - Test loss: 0.58691\n",
      "Epoch 16821 - lr: 0.01000 - Train loss: 0.59377 - Test loss: 0.58724\n",
      "Epoch 16822 - lr: 0.01000 - Train loss: 0.59255 - Test loss: 0.58724\n",
      "Epoch 16823 - lr: 0.01000 - Train loss: 0.59995 - Test loss: 0.58847\n",
      "Epoch 16824 - lr: 0.01000 - Train loss: 0.61972 - Test loss: 0.58780\n",
      "Epoch 16825 - lr: 0.01000 - Train loss: 0.59638 - Test loss: 0.58672\n",
      "Epoch 16826 - lr: 0.01000 - Train loss: 0.59247 - Test loss: 0.58831\n",
      "Epoch 16827 - lr: 0.01000 - Train loss: 0.59904 - Test loss: 0.58690\n",
      "Epoch 16828 - lr: 0.01000 - Train loss: 0.59305 - Test loss: 0.58662\n",
      "Epoch 16829 - lr: 0.01000 - Train loss: 0.59427 - Test loss: 0.58834\n",
      "Epoch 16830 - lr: 0.01000 - Train loss: 0.59815 - Test loss: 0.58694\n",
      "Epoch 16831 - lr: 0.01000 - Train loss: 0.59727 - Test loss: 0.58801\n",
      "Epoch 16832 - lr: 0.01000 - Train loss: 0.59866 - Test loss: 0.58673\n",
      "Epoch 16833 - lr: 0.01000 - Train loss: 0.59228 - Test loss: 0.58716\n",
      "Epoch 16834 - lr: 0.01000 - Train loss: 0.59836 - Test loss: 0.58844\n",
      "Epoch 16835 - lr: 0.01000 - Train loss: 0.59892 - Test loss: 0.58784\n",
      "Epoch 16836 - lr: 0.01000 - Train loss: 0.59183 - Test loss: 0.58779\n",
      "Epoch 16837 - lr: 0.01000 - Train loss: 0.59563 - Test loss: 0.58934\n",
      "Epoch 16838 - lr: 0.01000 - Train loss: 0.59928 - Test loss: 0.58856\n",
      "Epoch 16839 - lr: 0.01000 - Train loss: 0.59210 - Test loss: 0.58819\n",
      "Epoch 16840 - lr: 0.01000 - Train loss: 0.59124 - Test loss: 0.58962\n",
      "Epoch 16841 - lr: 0.01000 - Train loss: 0.59818 - Test loss: 0.58817\n",
      "Epoch 16842 - lr: 0.01000 - Train loss: 0.59191 - Test loss: 0.58797\n",
      "Epoch 16843 - lr: 0.01000 - Train loss: 0.59140 - Test loss: 0.58958\n",
      "Epoch 16844 - lr: 0.01000 - Train loss: 0.59821 - Test loss: 0.58821\n",
      "Epoch 16845 - lr: 0.01000 - Train loss: 0.59210 - Test loss: 0.58802\n",
      "Epoch 16846 - lr: 0.01000 - Train loss: 0.59186 - Test loss: 0.58970\n",
      "Epoch 16847 - lr: 0.01000 - Train loss: 0.59835 - Test loss: 0.58831\n",
      "Epoch 16848 - lr: 0.01000 - Train loss: 0.59226 - Test loss: 0.58811\n",
      "Epoch 16849 - lr: 0.01000 - Train loss: 0.59278 - Test loss: 0.58986\n",
      "Epoch 16850 - lr: 0.01000 - Train loss: 0.59833 - Test loss: 0.58842\n",
      "Epoch 16851 - lr: 0.01000 - Train loss: 0.59126 - Test loss: 0.58854\n",
      "Epoch 16852 - lr: 0.01000 - Train loss: 0.59449 - Test loss: 0.59026\n",
      "Epoch 16853 - lr: 0.01000 - Train loss: 0.59544 - Test loss: 0.58908\n",
      "Epoch 16854 - lr: 0.01000 - Train loss: 0.59113 - Test loss: 0.59061\n",
      "Epoch 16855 - lr: 0.01000 - Train loss: 0.59788 - Test loss: 0.58916\n",
      "Epoch 16856 - lr: 0.01000 - Train loss: 0.59163 - Test loss: 0.58897\n",
      "Epoch 16857 - lr: 0.01000 - Train loss: 0.59097 - Test loss: 0.59063\n",
      "Epoch 16858 - lr: 0.01000 - Train loss: 0.59769 - Test loss: 0.58928\n",
      "Epoch 16859 - lr: 0.01000 - Train loss: 0.59146 - Test loss: 0.58915\n",
      "Epoch 16860 - lr: 0.01000 - Train loss: 0.59081 - Test loss: 0.59084\n",
      "Epoch 16861 - lr: 0.01000 - Train loss: 0.59751 - Test loss: 0.58951\n",
      "Epoch 16862 - lr: 0.01000 - Train loss: 0.59125 - Test loss: 0.58942\n",
      "Epoch 16863 - lr: 0.01000 - Train loss: 0.59062 - Test loss: 0.59110\n",
      "Epoch 16864 - lr: 0.01000 - Train loss: 0.59731 - Test loss: 0.58979\n",
      "Epoch 16865 - lr: 0.01000 - Train loss: 0.59101 - Test loss: 0.58973\n",
      "Epoch 16866 - lr: 0.01000 - Train loss: 0.59067 - Test loss: 0.59141\n",
      "Epoch 16867 - lr: 0.01000 - Train loss: 0.59731 - Test loss: 0.59006\n",
      "Epoch 16868 - lr: 0.01000 - Train loss: 0.59098 - Test loss: 0.58998\n",
      "Epoch 16869 - lr: 0.01000 - Train loss: 0.59029 - Test loss: 0.59166\n",
      "Epoch 16870 - lr: 0.01000 - Train loss: 0.59697 - Test loss: 0.59035\n",
      "Epoch 16871 - lr: 0.01000 - Train loss: 0.59066 - Test loss: 0.59037\n",
      "Epoch 16872 - lr: 0.01000 - Train loss: 0.59175 - Test loss: 0.59205\n",
      "Epoch 16873 - lr: 0.01000 - Train loss: 0.59757 - Test loss: 0.59055\n",
      "Epoch 16874 - lr: 0.01000 - Train loss: 0.59056 - Test loss: 0.59059\n",
      "Epoch 16875 - lr: 0.01000 - Train loss: 0.59159 - Test loss: 0.59229\n",
      "Epoch 16876 - lr: 0.01000 - Train loss: 0.59746 - Test loss: 0.59080\n",
      "Epoch 16877 - lr: 0.01000 - Train loss: 0.59048 - Test loss: 0.59084\n",
      "Epoch 16878 - lr: 0.01000 - Train loss: 0.59105 - Test loss: 0.59253\n",
      "Epoch 16879 - lr: 0.01000 - Train loss: 0.59727 - Test loss: 0.59108\n",
      "Epoch 16880 - lr: 0.01000 - Train loss: 0.59059 - Test loss: 0.59103\n",
      "Epoch 16881 - lr: 0.01000 - Train loss: 0.58955 - Test loss: 0.59272\n",
      "Epoch 16882 - lr: 0.01000 - Train loss: 0.59614 - Test loss: 0.59146\n",
      "Epoch 16883 - lr: 0.01000 - Train loss: 0.59054 - Test loss: 0.59170\n",
      "Epoch 16884 - lr: 0.01000 - Train loss: 0.59777 - Test loss: 0.59322\n",
      "Epoch 16885 - lr: 0.01000 - Train loss: 0.61805 - Test loss: 0.59264\n",
      "Epoch 16886 - lr: 0.01000 - Train loss: 0.59434 - Test loss: 0.59171\n",
      "Epoch 16887 - lr: 0.01000 - Train loss: 0.59056 - Test loss: 0.59340\n",
      "Epoch 16888 - lr: 0.01000 - Train loss: 0.59700 - Test loss: 0.59194\n",
      "Epoch 16889 - lr: 0.01000 - Train loss: 0.59043 - Test loss: 0.59185\n",
      "Epoch 16890 - lr: 0.01000 - Train loss: 0.58921 - Test loss: 0.59354\n",
      "Epoch 16891 - lr: 0.01000 - Train loss: 0.59583 - Test loss: 0.59228\n",
      "Epoch 16892 - lr: 0.01000 - Train loss: 0.59052 - Test loss: 0.59256\n",
      "Epoch 16893 - lr: 0.01000 - Train loss: 0.59717 - Test loss: 0.59388\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16894 - lr: 0.01000 - Train loss: 0.60320 - Test loss: 0.59400\n",
      "Epoch 16895 - lr: 0.01000 - Train loss: 0.59547 - Test loss: 0.59311\n",
      "Epoch 16896 - lr: 0.01000 - Train loss: 0.59718 - Test loss: 0.59430\n",
      "Epoch 16897 - lr: 0.01000 - Train loss: 0.60549 - Test loss: 0.59439\n",
      "Epoch 16898 - lr: 0.01000 - Train loss: 0.62046 - Test loss: 0.59384\n",
      "Epoch 16899 - lr: 0.01000 - Train loss: 0.59856 - Test loss: 0.59340\n",
      "Epoch 16900 - lr: 0.01000 - Train loss: 0.59321 - Test loss: 0.59275\n",
      "Epoch 16901 - lr: 0.01000 - Train loss: 0.59756 - Test loss: 0.59434\n",
      "Epoch 16902 - lr: 0.01000 - Train loss: 0.61323 - Test loss: 0.59374\n",
      "Epoch 16903 - lr: 0.01000 - Train loss: 0.59691 - Test loss: 0.59246\n",
      "Epoch 16904 - lr: 0.01000 - Train loss: 0.58985 - Test loss: 0.59279\n",
      "Epoch 16905 - lr: 0.01000 - Train loss: 0.59371 - Test loss: 0.59465\n",
      "Epoch 16906 - lr: 0.01000 - Train loss: 0.59676 - Test loss: 0.59391\n",
      "Epoch 16907 - lr: 0.01000 - Train loss: 0.59026 - Test loss: 0.59399\n",
      "Epoch 16908 - lr: 0.01000 - Train loss: 0.59764 - Test loss: 0.59536\n",
      "Epoch 16909 - lr: 0.01000 - Train loss: 0.62079 - Test loss: 0.59472\n",
      "Epoch 16910 - lr: 0.01000 - Train loss: 0.59941 - Test loss: 0.59435\n",
      "Epoch 16911 - lr: 0.01000 - Train loss: 0.59540 - Test loss: 0.59338\n",
      "Epoch 16912 - lr: 0.01000 - Train loss: 0.58999 - Test loss: 0.59379\n",
      "Epoch 16913 - lr: 0.01000 - Train loss: 0.59739 - Test loss: 0.59540\n",
      "Epoch 16914 - lr: 0.01000 - Train loss: 0.61718 - Test loss: 0.59483\n",
      "Epoch 16915 - lr: 0.01000 - Train loss: 0.59405 - Test loss: 0.59380\n",
      "Epoch 16916 - lr: 0.01000 - Train loss: 0.59053 - Test loss: 0.59561\n",
      "Epoch 16917 - lr: 0.01000 - Train loss: 0.59680 - Test loss: 0.59407\n",
      "Epoch 16918 - lr: 0.01000 - Train loss: 0.59048 - Test loss: 0.59392\n",
      "Epoch 16919 - lr: 0.01000 - Train loss: 0.59113 - Test loss: 0.59579\n",
      "Epoch 16920 - lr: 0.01000 - Train loss: 0.59661 - Test loss: 0.59424\n",
      "Epoch 16921 - lr: 0.01000 - Train loss: 0.58950 - Test loss: 0.59446\n",
      "Epoch 16922 - lr: 0.01000 - Train loss: 0.59424 - Test loss: 0.59628\n",
      "Epoch 16923 - lr: 0.01000 - Train loss: 0.60534 - Test loss: 0.59637\n",
      "Epoch 16924 - lr: 0.01000 - Train loss: 0.62847 - Test loss: 0.59596\n",
      "Epoch 16925 - lr: 0.01000 - Train loss: 0.60276 - Test loss: 0.59594\n",
      "Epoch 16926 - lr: 0.01000 - Train loss: 0.59391 - Test loss: 0.59488\n",
      "Epoch 16927 - lr: 0.01000 - Train loss: 0.59045 - Test loss: 0.59668\n",
      "Epoch 16928 - lr: 0.01000 - Train loss: 0.59668 - Test loss: 0.59508\n",
      "Epoch 16929 - lr: 0.01000 - Train loss: 0.59027 - Test loss: 0.59492\n",
      "Epoch 16930 - lr: 0.01000 - Train loss: 0.59113 - Test loss: 0.59680\n",
      "Epoch 16931 - lr: 0.01000 - Train loss: 0.59634 - Test loss: 0.59523\n",
      "Epoch 16932 - lr: 0.01000 - Train loss: 0.58959 - Test loss: 0.59561\n",
      "Epoch 16933 - lr: 0.01000 - Train loss: 0.59693 - Test loss: 0.59717\n",
      "Epoch 16934 - lr: 0.01000 - Train loss: 0.61020 - Test loss: 0.59647\n",
      "Epoch 16935 - lr: 0.01000 - Train loss: 0.59613 - Test loss: 0.59526\n",
      "Epoch 16936 - lr: 0.01000 - Train loss: 0.59076 - Test loss: 0.59518\n",
      "Epoch 16937 - lr: 0.01000 - Train loss: 0.59430 - Test loss: 0.59723\n",
      "Epoch 16938 - lr: 0.01000 - Train loss: 0.60163 - Test loss: 0.59742\n",
      "Epoch 16939 - lr: 0.01000 - Train loss: 0.59517 - Test loss: 0.59596\n",
      "Epoch 16940 - lr: 0.01000 - Train loss: 0.59632 - Test loss: 0.59748\n",
      "Epoch 16941 - lr: 0.01000 - Train loss: 0.60240 - Test loss: 0.59770\n",
      "Epoch 16942 - lr: 0.01000 - Train loss: 0.59423 - Test loss: 0.59678\n",
      "Epoch 16943 - lr: 0.01000 - Train loss: 0.59711 - Test loss: 0.59835\n",
      "Epoch 16944 - lr: 0.01000 - Train loss: 0.63547 - Test loss: 0.59792\n",
      "Epoch 16945 - lr: 0.01000 - Train loss: 0.59457 - Test loss: 0.59657\n",
      "Epoch 16946 - lr: 0.01000 - Train loss: 0.59656 - Test loss: 0.59844\n",
      "Epoch 16947 - lr: 0.01000 - Train loss: 0.63546 - Test loss: 0.59811\n",
      "Epoch 16948 - lr: 0.01000 - Train loss: 0.59469 - Test loss: 0.59680\n",
      "Epoch 16949 - lr: 0.01000 - Train loss: 0.59680 - Test loss: 0.59863\n",
      "Epoch 16950 - lr: 0.01000 - Train loss: 0.62891 - Test loss: 0.59829\n",
      "Epoch 16951 - lr: 0.01000 - Train loss: 0.59960 - Test loss: 0.59808\n",
      "Epoch 16952 - lr: 0.01000 - Train loss: 0.59570 - Test loss: 0.59694\n",
      "Epoch 16953 - lr: 0.01000 - Train loss: 0.59070 - Test loss: 0.59683\n",
      "Epoch 16954 - lr: 0.01000 - Train loss: 0.59529 - Test loss: 0.59890\n",
      "Epoch 16955 - lr: 0.01000 - Train loss: 0.61086 - Test loss: 0.59831\n",
      "Epoch 16956 - lr: 0.01000 - Train loss: 0.59595 - Test loss: 0.59708\n",
      "Epoch 16957 - lr: 0.01000 - Train loss: 0.59100 - Test loss: 0.59698\n",
      "Epoch 16958 - lr: 0.01000 - Train loss: 0.59614 - Test loss: 0.59904\n",
      "Epoch 16959 - lr: 0.01000 - Train loss: 0.62339 - Test loss: 0.59874\n",
      "Epoch 16960 - lr: 0.01000 - Train loss: 0.60243 - Test loss: 0.59889\n",
      "Epoch 16961 - lr: 0.01000 - Train loss: 0.59337 - Test loss: 0.59809\n",
      "Epoch 16962 - lr: 0.01000 - Train loss: 0.59434 - Test loss: 0.60000\n",
      "Epoch 16963 - lr: 0.01000 - Train loss: 0.59432 - Test loss: 0.59961\n",
      "Epoch 16964 - lr: 0.01000 - Train loss: 0.59460 - Test loss: 0.60108\n",
      "Epoch 16965 - lr: 0.01000 - Train loss: 0.59980 - Test loss: 0.60003\n",
      "Epoch 16966 - lr: 0.01000 - Train loss: 0.59450 - Test loss: 0.60078\n",
      "Epoch 16967 - lr: 0.01000 - Train loss: 0.59594 - Test loss: 0.59897\n",
      "Epoch 16968 - lr: 0.01000 - Train loss: 0.58905 - Test loss: 0.59935\n",
      "Epoch 16969 - lr: 0.01000 - Train loss: 0.59583 - Test loss: 0.60074\n",
      "Epoch 16970 - lr: 0.01000 - Train loss: 0.60470 - Test loss: 0.60092\n",
      "Epoch 16971 - lr: 0.01000 - Train loss: 0.63031 - Test loss: 0.60053\n",
      "Epoch 16972 - lr: 0.01000 - Train loss: 0.59568 - Test loss: 0.59992\n",
      "Epoch 16973 - lr: 0.01000 - Train loss: 0.58879 - Test loss: 0.60009\n",
      "Epoch 16974 - lr: 0.01000 - Train loss: 0.59661 - Test loss: 0.60170\n",
      "Epoch 16975 - lr: 0.01000 - Train loss: 0.64349 - Test loss: 0.60129\n",
      "Epoch 16976 - lr: 0.01000 - Train loss: 0.59513 - Test loss: 0.59993\n",
      "Epoch 16977 - lr: 0.01000 - Train loss: 0.58980 - Test loss: 0.59975\n",
      "Epoch 16978 - lr: 0.01000 - Train loss: 0.59342 - Test loss: 0.60177\n",
      "Epoch 16979 - lr: 0.01000 - Train loss: 0.60299 - Test loss: 0.60193\n",
      "Epoch 16980 - lr: 0.01000 - Train loss: 0.60476 - Test loss: 0.60211\n",
      "Epoch 16981 - lr: 0.01000 - Train loss: 0.63214 - Test loss: 0.60171\n",
      "Epoch 16982 - lr: 0.01000 - Train loss: 0.59297 - Test loss: 0.60063\n",
      "Epoch 16983 - lr: 0.01000 - Train loss: 0.59092 - Test loss: 0.60251\n",
      "Epoch 16984 - lr: 0.01000 - Train loss: 0.59472 - Test loss: 0.60080\n",
      "Epoch 16985 - lr: 0.01000 - Train loss: 0.59468 - Test loss: 0.60215\n",
      "Epoch 16986 - lr: 0.01000 - Train loss: 0.59280 - Test loss: 0.60101\n",
      "Epoch 16987 - lr: 0.01000 - Train loss: 0.59078 - Test loss: 0.60286\n",
      "Epoch 16988 - lr: 0.01000 - Train loss: 0.59467 - Test loss: 0.60115\n",
      "Epoch 16989 - lr: 0.01000 - Train loss: 0.59412 - Test loss: 0.60244\n",
      "Epoch 16990 - lr: 0.01000 - Train loss: 0.59442 - Test loss: 0.60098\n",
      "Epoch 16991 - lr: 0.01000 - Train loss: 0.59434 - Test loss: 0.60246\n",
      "Epoch 16992 - lr: 0.01000 - Train loss: 0.59286 - Test loss: 0.60129\n",
      "Epoch 16993 - lr: 0.01000 - Train loss: 0.59179 - Test loss: 0.60324\n",
      "Epoch 16994 - lr: 0.01000 - Train loss: 0.59394 - Test loss: 0.60227\n",
      "Epoch 16995 - lr: 0.01000 - Train loss: 0.59363 - Test loss: 0.60330\n",
      "Epoch 16996 - lr: 0.01000 - Train loss: 0.59528 - Test loss: 0.60163\n",
      "Epoch 16997 - lr: 0.01000 - Train loss: 0.58793 - Test loss: 0.60195\n",
      "Epoch 16998 - lr: 0.01000 - Train loss: 0.59413 - Test loss: 0.60387\n",
      "Epoch 16999 - lr: 0.01000 - Train loss: 0.60841 - Test loss: 0.60308\n",
      "Epoch 17000 - lr: 0.01000 - Train loss: 0.59472 - Test loss: 0.60184\n",
      "Epoch 17001 - lr: 0.01000 - Train loss: 0.59041 - Test loss: 0.60166\n",
      "Epoch 17002 - lr: 0.01000 - Train loss: 0.59547 - Test loss: 0.60362\n",
      "Epoch 17003 - lr: 0.01000 - Train loss: 0.61333 - Test loss: 0.60316\n",
      "Epoch 17004 - lr: 0.01000 - Train loss: 0.59469 - Test loss: 0.60191\n",
      "Epoch 17005 - lr: 0.01000 - Train loss: 0.58844 - Test loss: 0.60272\n",
      "Epoch 17006 - lr: 0.01000 - Train loss: 0.59082 - Test loss: 0.60367\n",
      "Epoch 17007 - lr: 0.01000 - Train loss: 0.59298 - Test loss: 0.60266\n",
      "Epoch 17008 - lr: 0.01000 - Train loss: 0.58770 - Test loss: 0.60310\n",
      "Epoch 17009 - lr: 0.01000 - Train loss: 0.59545 - Test loss: 0.60487\n",
      "Epoch 17010 - lr: 0.01000 - Train loss: 0.62824 - Test loss: 0.60447\n",
      "Epoch 17011 - lr: 0.01000 - Train loss: 0.59545 - Test loss: 0.60400\n",
      "Epoch 17012 - lr: 0.01000 - Train loss: 0.58855 - Test loss: 0.60377\n",
      "Epoch 17013 - lr: 0.01000 - Train loss: 0.59103 - Test loss: 0.60563\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17014 - lr: 0.01000 - Train loss: 0.59222 - Test loss: 0.60439\n",
      "Epoch 17015 - lr: 0.01000 - Train loss: 0.59225 - Test loss: 0.60615\n",
      "Epoch 17016 - lr: 0.01000 - Train loss: 0.60415 - Test loss: 0.60612\n",
      "Epoch 17017 - lr: 0.01000 - Train loss: 0.63725 - Test loss: 0.60575\n",
      "Epoch 17018 - lr: 0.01000 - Train loss: 0.59497 - Test loss: 0.60422\n",
      "Epoch 17019 - lr: 0.01000 - Train loss: 0.58836 - Test loss: 0.60425\n",
      "Epoch 17020 - lr: 0.01000 - Train loss: 0.59140 - Test loss: 0.60631\n",
      "Epoch 17021 - lr: 0.01000 - Train loss: 0.59606 - Test loss: 0.60568\n",
      "Epoch 17022 - lr: 0.01000 - Train loss: 0.59083 - Test loss: 0.60492\n",
      "Epoch 17023 - lr: 0.01000 - Train loss: 0.59386 - Test loss: 0.60634\n",
      "Epoch 17024 - lr: 0.01000 - Train loss: 0.59285 - Test loss: 0.60550\n",
      "Epoch 17025 - lr: 0.01000 - Train loss: 0.59434 - Test loss: 0.60687\n",
      "Epoch 17026 - lr: 0.01000 - Train loss: 0.60211 - Test loss: 0.60679\n",
      "Epoch 17027 - lr: 0.01000 - Train loss: 0.60381 - Test loss: 0.60704\n",
      "Epoch 17028 - lr: 0.01000 - Train loss: 0.63519 - Test loss: 0.60678\n",
      "Epoch 17029 - lr: 0.01000 - Train loss: 0.59435 - Test loss: 0.60534\n",
      "Epoch 17030 - lr: 0.01000 - Train loss: 0.58760 - Test loss: 0.60599\n",
      "Epoch 17031 - lr: 0.01000 - Train loss: 0.59247 - Test loss: 0.60719\n",
      "Epoch 17032 - lr: 0.01000 - Train loss: 0.59446 - Test loss: 0.60563\n",
      "Epoch 17033 - lr: 0.01000 - Train loss: 0.58760 - Test loss: 0.60575\n",
      "Epoch 17034 - lr: 0.01000 - Train loss: 0.59024 - Test loss: 0.60777\n",
      "Epoch 17035 - lr: 0.01000 - Train loss: 0.59178 - Test loss: 0.60647\n",
      "Epoch 17036 - lr: 0.01000 - Train loss: 0.59033 - Test loss: 0.60831\n",
      "Epoch 17037 - lr: 0.01000 - Train loss: 0.59162 - Test loss: 0.60698\n",
      "Epoch 17038 - lr: 0.01000 - Train loss: 0.58958 - Test loss: 0.60872\n",
      "Epoch 17039 - lr: 0.01000 - Train loss: 0.59326 - Test loss: 0.60699\n",
      "Epoch 17040 - lr: 0.01000 - Train loss: 0.59368 - Test loss: 0.60839\n",
      "Epoch 17041 - lr: 0.01000 - Train loss: 0.59604 - Test loss: 0.60791\n",
      "Epoch 17042 - lr: 0.01000 - Train loss: 0.59165 - Test loss: 0.60701\n",
      "Epoch 17043 - lr: 0.01000 - Train loss: 0.58803 - Test loss: 0.60774\n",
      "Epoch 17044 - lr: 0.01000 - Train loss: 0.58686 - Test loss: 0.60795\n",
      "Epoch 17045 - lr: 0.01000 - Train loss: 0.59482 - Test loss: 0.60952\n",
      "Epoch 17046 - lr: 0.01000 - Train loss: 0.63537 - Test loss: 0.60910\n",
      "Epoch 17047 - lr: 0.01000 - Train loss: 0.59406 - Test loss: 0.60758\n",
      "Epoch 17048 - lr: 0.01000 - Train loss: 0.58665 - Test loss: 0.60787\n",
      "Epoch 17049 - lr: 0.01000 - Train loss: 0.59010 - Test loss: 0.60983\n",
      "Epoch 17050 - lr: 0.01000 - Train loss: 0.59134 - Test loss: 0.60869\n",
      "Epoch 17051 - lr: 0.01000 - Train loss: 0.59204 - Test loss: 0.61043\n",
      "Epoch 17052 - lr: 0.01000 - Train loss: 0.59305 - Test loss: 0.61012\n",
      "Epoch 17053 - lr: 0.01000 - Train loss: 0.59119 - Test loss: 0.61064\n",
      "Epoch 17054 - lr: 0.01000 - Train loss: 0.59381 - Test loss: 0.60890\n",
      "Epoch 17055 - lr: 0.01000 - Train loss: 0.58964 - Test loss: 0.60846\n",
      "Epoch 17056 - lr: 0.01000 - Train loss: 0.59309 - Test loss: 0.61010\n",
      "Epoch 17057 - lr: 0.01000 - Train loss: 0.59525 - Test loss: 0.60973\n",
      "Epoch 17058 - lr: 0.01000 - Train loss: 0.59047 - Test loss: 0.60899\n",
      "Epoch 17059 - lr: 0.01000 - Train loss: 0.58991 - Test loss: 0.61009\n",
      "Epoch 17060 - lr: 0.01000 - Train loss: 0.59259 - Test loss: 0.60897\n",
      "Epoch 17061 - lr: 0.01000 - Train loss: 0.58789 - Test loss: 0.60893\n",
      "Epoch 17062 - lr: 0.01000 - Train loss: 0.59314 - Test loss: 0.61098\n",
      "Epoch 17063 - lr: 0.01000 - Train loss: 0.61854 - Test loss: 0.61058\n",
      "Epoch 17064 - lr: 0.01000 - Train loss: 0.59349 - Test loss: 0.61018\n",
      "Epoch 17065 - lr: 0.01000 - Train loss: 0.58616 - Test loss: 0.61031\n",
      "Epoch 17066 - lr: 0.01000 - Train loss: 0.59096 - Test loss: 0.61206\n",
      "Epoch 17067 - lr: 0.01000 - Train loss: 0.60273 - Test loss: 0.61212\n",
      "Epoch 17068 - lr: 0.01000 - Train loss: 0.63257 - Test loss: 0.61176\n",
      "Epoch 17069 - lr: 0.01000 - Train loss: 0.59275 - Test loss: 0.61035\n",
      "Epoch 17070 - lr: 0.01000 - Train loss: 0.58995 - Test loss: 0.61154\n",
      "Epoch 17071 - lr: 0.01000 - Train loss: 0.59269 - Test loss: 0.61036\n",
      "Epoch 17072 - lr: 0.01000 - Train loss: 0.58878 - Test loss: 0.61018\n",
      "Epoch 17073 - lr: 0.01000 - Train loss: 0.59284 - Test loss: 0.61194\n",
      "Epoch 17074 - lr: 0.01000 - Train loss: 0.60163 - Test loss: 0.61223\n",
      "Epoch 17075 - lr: 0.01000 - Train loss: 0.59834 - Test loss: 0.61180\n",
      "Epoch 17076 - lr: 0.01000 - Train loss: 0.58726 - Test loss: 0.61226\n",
      "Epoch 17077 - lr: 0.01000 - Train loss: 0.58589 - Test loss: 0.61224\n",
      "Epoch 17078 - lr: 0.01000 - Train loss: 0.59361 - Test loss: 0.61376\n",
      "Epoch 17079 - lr: 0.01000 - Train loss: 0.63786 - Test loss: 0.61336\n",
      "Epoch 17080 - lr: 0.01000 - Train loss: 0.59311 - Test loss: 0.61186\n",
      "Epoch 17081 - lr: 0.01000 - Train loss: 0.58907 - Test loss: 0.61155\n",
      "Epoch 17082 - lr: 0.01000 - Train loss: 0.59116 - Test loss: 0.61307\n",
      "Epoch 17083 - lr: 0.01000 - Train loss: 0.59274 - Test loss: 0.61177\n",
      "Epoch 17084 - lr: 0.01000 - Train loss: 0.58551 - Test loss: 0.61235\n",
      "Epoch 17085 - lr: 0.01000 - Train loss: 0.59336 - Test loss: 0.61412\n",
      "Epoch 17086 - lr: 0.01000 - Train loss: 0.62528 - Test loss: 0.61376\n",
      "Epoch 17087 - lr: 0.01000 - Train loss: 0.59205 - Test loss: 0.61323\n",
      "Epoch 17088 - lr: 0.01000 - Train loss: 0.58749 - Test loss: 0.61390\n",
      "Epoch 17089 - lr: 0.01000 - Train loss: 0.58701 - Test loss: 0.61340\n",
      "Epoch 17090 - lr: 0.01000 - Train loss: 0.59162 - Test loss: 0.61514\n",
      "Epoch 17091 - lr: 0.01000 - Train loss: 0.60150 - Test loss: 0.61438\n",
      "Epoch 17092 - lr: 0.01000 - Train loss: 0.58809 - Test loss: 0.61373\n",
      "Epoch 17093 - lr: 0.01000 - Train loss: 0.59334 - Test loss: 0.61534\n",
      "Epoch 17094 - lr: 0.01000 - Train loss: 0.61707 - Test loss: 0.61486\n",
      "Epoch 17095 - lr: 0.01000 - Train loss: 0.59044 - Test loss: 0.61417\n",
      "Epoch 17096 - lr: 0.01000 - Train loss: 0.59326 - Test loss: 0.61589\n",
      "Epoch 17097 - lr: 0.01000 - Train loss: 0.63406 - Test loss: 0.61557\n",
      "Epoch 17098 - lr: 0.01000 - Train loss: 0.59276 - Test loss: 0.61409\n",
      "Epoch 17099 - lr: 0.01000 - Train loss: 0.58615 - Test loss: 0.61417\n",
      "Epoch 17100 - lr: 0.01000 - Train loss: 0.59019 - Test loss: 0.61614\n",
      "Epoch 17101 - lr: 0.01000 - Train loss: 0.60118 - Test loss: 0.61638\n",
      "Epoch 17102 - lr: 0.01000 - Train loss: 0.60400 - Test loss: 0.61572\n",
      "Epoch 17103 - lr: 0.01000 - Train loss: 0.59095 - Test loss: 0.61468\n",
      "Epoch 17104 - lr: 0.01000 - Train loss: 0.58538 - Test loss: 0.61485\n",
      "Epoch 17105 - lr: 0.01000 - Train loss: 0.58841 - Test loss: 0.61675\n",
      "Epoch 17106 - lr: 0.01000 - Train loss: 0.58977 - Test loss: 0.61559\n",
      "Epoch 17107 - lr: 0.01000 - Train loss: 0.58831 - Test loss: 0.61729\n",
      "Epoch 17108 - lr: 0.01000 - Train loss: 0.58983 - Test loss: 0.61599\n",
      "Epoch 17109 - lr: 0.01000 - Train loss: 0.58818 - Test loss: 0.61765\n",
      "Epoch 17110 - lr: 0.01000 - Train loss: 0.58989 - Test loss: 0.61629\n",
      "Epoch 17111 - lr: 0.01000 - Train loss: 0.58856 - Test loss: 0.61795\n",
      "Epoch 17112 - lr: 0.01000 - Train loss: 0.59038 - Test loss: 0.61692\n",
      "Epoch 17113 - lr: 0.01000 - Train loss: 0.59294 - Test loss: 0.61820\n",
      "Epoch 17114 - lr: 0.01000 - Train loss: 0.61040 - Test loss: 0.61750\n",
      "Epoch 17115 - lr: 0.01000 - Train loss: 0.59222 - Test loss: 0.61609\n",
      "Epoch 17116 - lr: 0.01000 - Train loss: 0.58491 - Test loss: 0.61635\n",
      "Epoch 17117 - lr: 0.01000 - Train loss: 0.58784 - Test loss: 0.61824\n",
      "Epoch 17118 - lr: 0.01000 - Train loss: 0.58964 - Test loss: 0.61700\n",
      "Epoch 17119 - lr: 0.01000 - Train loss: 0.58901 - Test loss: 0.61871\n",
      "Epoch 17120 - lr: 0.01000 - Train loss: 0.59820 - Test loss: 0.61844\n",
      "Epoch 17121 - lr: 0.01000 - Train loss: 0.59077 - Test loss: 0.61707\n",
      "Epoch 17122 - lr: 0.01000 - Train loss: 0.59195 - Test loss: 0.61856\n",
      "Epoch 17123 - lr: 0.01000 - Train loss: 0.59414 - Test loss: 0.61876\n",
      "Epoch 17124 - lr: 0.01000 - Train loss: 0.59050 - Test loss: 0.61750\n",
      "Epoch 17125 - lr: 0.01000 - Train loss: 0.58485 - Test loss: 0.61753\n",
      "Epoch 17126 - lr: 0.01000 - Train loss: 0.58766 - Test loss: 0.61933\n",
      "Epoch 17127 - lr: 0.01000 - Train loss: 0.58916 - Test loss: 0.61816\n",
      "Epoch 17128 - lr: 0.01000 - Train loss: 0.58758 - Test loss: 0.61981\n",
      "Epoch 17129 - lr: 0.01000 - Train loss: 0.58922 - Test loss: 0.61853\n",
      "Epoch 17130 - lr: 0.01000 - Train loss: 0.58774 - Test loss: 0.62014\n",
      "Epoch 17131 - lr: 0.01000 - Train loss: 0.58908 - Test loss: 0.61902\n",
      "Epoch 17132 - lr: 0.01000 - Train loss: 0.58936 - Test loss: 0.62056\n",
      "Epoch 17133 - lr: 0.01000 - Train loss: 0.59586 - Test loss: 0.62056\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17134 - lr: 0.01000 - Train loss: 0.59187 - Test loss: 0.61884\n",
      "Epoch 17135 - lr: 0.01000 - Train loss: 0.58759 - Test loss: 0.61841\n",
      "Epoch 17136 - lr: 0.01000 - Train loss: 0.58939 - Test loss: 0.61976\n",
      "Epoch 17137 - lr: 0.01000 - Train loss: 0.59141 - Test loss: 0.61846\n",
      "Epoch 17138 - lr: 0.01000 - Train loss: 0.58465 - Test loss: 0.61866\n",
      "Epoch 17139 - lr: 0.01000 - Train loss: 0.58833 - Test loss: 0.62055\n",
      "Epoch 17140 - lr: 0.01000 - Train loss: 0.59761 - Test loss: 0.62048\n",
      "Epoch 17141 - lr: 0.01000 - Train loss: 0.58944 - Test loss: 0.61934\n",
      "Epoch 17142 - lr: 0.01000 - Train loss: 0.59155 - Test loss: 0.62105\n",
      "Epoch 17143 - lr: 0.01000 - Train loss: 0.63563 - Test loss: 0.62093\n",
      "Epoch 17144 - lr: 0.01000 - Train loss: 0.59122 - Test loss: 0.61956\n",
      "Epoch 17145 - lr: 0.01000 - Train loss: 0.58751 - Test loss: 0.61927\n",
      "Epoch 17146 - lr: 0.01000 - Train loss: 0.58700 - Test loss: 0.62049\n",
      "Epoch 17147 - lr: 0.01000 - Train loss: 0.58974 - Test loss: 0.61961\n",
      "Epoch 17148 - lr: 0.01000 - Train loss: 0.58531 - Test loss: 0.61966\n",
      "Epoch 17149 - lr: 0.01000 - Train loss: 0.59103 - Test loss: 0.62153\n",
      "Epoch 17150 - lr: 0.01000 - Train loss: 0.62214 - Test loss: 0.62137\n",
      "Epoch 17151 - lr: 0.01000 - Train loss: 0.59147 - Test loss: 0.62114\n",
      "Epoch 17152 - lr: 0.01000 - Train loss: 0.58462 - Test loss: 0.62097\n",
      "Epoch 17153 - lr: 0.01000 - Train loss: 0.58880 - Test loss: 0.62264\n",
      "Epoch 17154 - lr: 0.01000 - Train loss: 0.59510 - Test loss: 0.62281\n",
      "Epoch 17155 - lr: 0.01000 - Train loss: 0.59098 - Test loss: 0.62125\n",
      "Epoch 17156 - lr: 0.01000 - Train loss: 0.58721 - Test loss: 0.62085\n",
      "Epoch 17157 - lr: 0.01000 - Train loss: 0.58666 - Test loss: 0.62201\n",
      "Epoch 17158 - lr: 0.01000 - Train loss: 0.58940 - Test loss: 0.62111\n",
      "Epoch 17159 - lr: 0.01000 - Train loss: 0.58496 - Test loss: 0.62116\n",
      "Epoch 17160 - lr: 0.01000 - Train loss: 0.59070 - Test loss: 0.62299\n",
      "Epoch 17161 - lr: 0.01000 - Train loss: 0.62193 - Test loss: 0.62285\n",
      "Epoch 17162 - lr: 0.01000 - Train loss: 0.59052 - Test loss: 0.62257\n",
      "Epoch 17163 - lr: 0.01000 - Train loss: 0.58311 - Test loss: 0.62274\n",
      "Epoch 17164 - lr: 0.01000 - Train loss: 0.58933 - Test loss: 0.62427\n",
      "Epoch 17165 - lr: 0.01000 - Train loss: 0.59900 - Test loss: 0.62365\n",
      "Epoch 17166 - lr: 0.01000 - Train loss: 0.58559 - Test loss: 0.62306\n",
      "Epoch 17167 - lr: 0.01000 - Train loss: 0.59119 - Test loss: 0.62451\n",
      "Epoch 17168 - lr: 0.01000 - Train loss: 0.62486 - Test loss: 0.62427\n",
      "Epoch 17169 - lr: 0.01000 - Train loss: 0.58779 - Test loss: 0.62351\n",
      "Epoch 17170 - lr: 0.01000 - Train loss: 0.58715 - Test loss: 0.62514\n",
      "Epoch 17171 - lr: 0.01000 - Train loss: 0.59239 - Test loss: 0.62467\n",
      "Epoch 17172 - lr: 0.01000 - Train loss: 0.58793 - Test loss: 0.62369\n",
      "Epoch 17173 - lr: 0.01000 - Train loss: 0.58320 - Test loss: 0.62418\n",
      "Epoch 17174 - lr: 0.01000 - Train loss: 0.58800 - Test loss: 0.62513\n",
      "Epoch 17175 - lr: 0.01000 - Train loss: 0.59036 - Test loss: 0.62371\n",
      "Epoch 17176 - lr: 0.01000 - Train loss: 0.58548 - Test loss: 0.62354\n",
      "Epoch 17177 - lr: 0.01000 - Train loss: 0.58978 - Test loss: 0.62506\n",
      "Epoch 17178 - lr: 0.01000 - Train loss: 0.59919 - Test loss: 0.62546\n",
      "Epoch 17179 - lr: 0.01000 - Train loss: 0.61210 - Test loss: 0.62519\n",
      "Epoch 17180 - lr: 0.01000 - Train loss: 0.58808 - Test loss: 0.62432\n",
      "Epoch 17181 - lr: 0.01000 - Train loss: 0.58997 - Test loss: 0.62600\n",
      "Epoch 17182 - lr: 0.01000 - Train loss: 0.62369 - Test loss: 0.62582\n",
      "Epoch 17183 - lr: 0.01000 - Train loss: 0.58756 - Test loss: 0.62521\n",
      "Epoch 17184 - lr: 0.01000 - Train loss: 0.58981 - Test loss: 0.62674\n",
      "Epoch 17185 - lr: 0.01000 - Train loss: 0.62276 - Test loss: 0.62643\n",
      "Epoch 17186 - lr: 0.01000 - Train loss: 0.58778 - Test loss: 0.62581\n",
      "Epoch 17187 - lr: 0.01000 - Train loss: 0.59074 - Test loss: 0.62719\n",
      "Epoch 17188 - lr: 0.01000 - Train loss: 0.63215 - Test loss: 0.62703\n",
      "Epoch 17189 - lr: 0.01000 - Train loss: 0.59004 - Test loss: 0.62562\n",
      "Epoch 17190 - lr: 0.01000 - Train loss: 0.58531 - Test loss: 0.62540\n",
      "Epoch 17191 - lr: 0.01000 - Train loss: 0.58863 - Test loss: 0.62679\n",
      "Epoch 17192 - lr: 0.01000 - Train loss: 0.58720 - Test loss: 0.62606\n",
      "Epoch 17193 - lr: 0.01000 - Train loss: 0.58681 - Test loss: 0.62769\n",
      "Epoch 17194 - lr: 0.01000 - Train loss: 0.59561 - Test loss: 0.62758\n",
      "Epoch 17195 - lr: 0.01000 - Train loss: 0.58958 - Test loss: 0.62623\n",
      "Epoch 17196 - lr: 0.01000 - Train loss: 0.58205 - Test loss: 0.62667\n",
      "Epoch 17197 - lr: 0.01000 - Train loss: 0.59025 - Test loss: 0.62812\n",
      "Epoch 17198 - lr: 0.01000 - Train loss: 0.62722 - Test loss: 0.62797\n",
      "Epoch 17199 - lr: 0.01000 - Train loss: 0.58874 - Test loss: 0.62682\n",
      "Epoch 17200 - lr: 0.01000 - Train loss: 0.58657 - Test loss: 0.62788\n",
      "Epoch 17201 - lr: 0.01000 - Train loss: 0.58924 - Test loss: 0.62671\n",
      "Epoch 17202 - lr: 0.01000 - Train loss: 0.58623 - Test loss: 0.62640\n",
      "Epoch 17203 - lr: 0.01000 - Train loss: 0.58272 - Test loss: 0.62730\n",
      "Epoch 17204 - lr: 0.01000 - Train loss: 0.58223 - Test loss: 0.62782\n",
      "Epoch 17205 - lr: 0.01000 - Train loss: 0.58655 - Test loss: 0.62867\n",
      "Epoch 17206 - lr: 0.01000 - Train loss: 0.58915 - Test loss: 0.62738\n",
      "Epoch 17207 - lr: 0.01000 - Train loss: 0.58597 - Test loss: 0.62705\n",
      "Epoch 17208 - lr: 0.01000 - Train loss: 0.58284 - Test loss: 0.62798\n",
      "Epoch 17209 - lr: 0.01000 - Train loss: 0.58157 - Test loss: 0.62828\n",
      "Epoch 17210 - lr: 0.01000 - Train loss: 0.58830 - Test loss: 0.62976\n",
      "Epoch 17211 - lr: 0.01000 - Train loss: 0.60192 - Test loss: 0.62931\n",
      "Epoch 17212 - lr: 0.01000 - Train loss: 0.58816 - Test loss: 0.62823\n",
      "Epoch 17213 - lr: 0.01000 - Train loss: 0.58460 - Test loss: 0.62805\n",
      "Epoch 17214 - lr: 0.01000 - Train loss: 0.58733 - Test loss: 0.62938\n",
      "Epoch 17215 - lr: 0.01000 - Train loss: 0.58847 - Test loss: 0.62837\n",
      "Epoch 17216 - lr: 0.01000 - Train loss: 0.58246 - Test loss: 0.62914\n",
      "Epoch 17217 - lr: 0.01000 - Train loss: 0.58147 - Test loss: 0.62951\n",
      "Epoch 17218 - lr: 0.01000 - Train loss: 0.58934 - Test loss: 0.63070\n",
      "Epoch 17219 - lr: 0.01000 - Train loss: 0.59710 - Test loss: 0.63032\n",
      "Epoch 17220 - lr: 0.01000 - Train loss: 0.58342 - Test loss: 0.62986\n",
      "Epoch 17221 - lr: 0.01000 - Train loss: 0.58930 - Test loss: 0.63124\n",
      "Epoch 17222 - lr: 0.01000 - Train loss: 0.62643 - Test loss: 0.63114\n",
      "Epoch 17223 - lr: 0.01000 - Train loss: 0.58821 - Test loss: 0.62997\n",
      "Epoch 17224 - lr: 0.01000 - Train loss: 0.58392 - Test loss: 0.63080\n",
      "Epoch 17225 - lr: 0.01000 - Train loss: 0.58609 - Test loss: 0.63008\n",
      "Epoch 17226 - lr: 0.01000 - Train loss: 0.58124 - Test loss: 0.63059\n",
      "Epoch 17227 - lr: 0.01000 - Train loss: 0.58811 - Test loss: 0.63171\n",
      "Epoch 17228 - lr: 0.01000 - Train loss: 0.59085 - Test loss: 0.63152\n",
      "Epoch 17229 - lr: 0.01000 - Train loss: 0.58647 - Test loss: 0.63063\n",
      "Epoch 17230 - lr: 0.01000 - Train loss: 0.58088 - Test loss: 0.63093\n",
      "Epoch 17231 - lr: 0.01000 - Train loss: 0.58672 - Test loss: 0.63244\n",
      "Epoch 17232 - lr: 0.01000 - Train loss: 0.58886 - Test loss: 0.63269\n",
      "Epoch 17233 - lr: 0.01000 - Train loss: 0.58127 - Test loss: 0.63232\n",
      "Epoch 17234 - lr: 0.01000 - Train loss: 0.58450 - Test loss: 0.63351\n",
      "Epoch 17235 - lr: 0.01000 - Train loss: 0.58598 - Test loss: 0.63247\n",
      "Epoch 17236 - lr: 0.01000 - Train loss: 0.58439 - Test loss: 0.63371\n",
      "Epoch 17237 - lr: 0.01000 - Train loss: 0.58593 - Test loss: 0.63270\n",
      "Epoch 17238 - lr: 0.01000 - Train loss: 0.58430 - Test loss: 0.63395\n",
      "Epoch 17239 - lr: 0.01000 - Train loss: 0.58587 - Test loss: 0.63294\n",
      "Epoch 17240 - lr: 0.01000 - Train loss: 0.58421 - Test loss: 0.63418\n",
      "Epoch 17241 - lr: 0.01000 - Train loss: 0.58584 - Test loss: 0.63315\n",
      "Epoch 17242 - lr: 0.01000 - Train loss: 0.58425 - Test loss: 0.63439\n",
      "Epoch 17243 - lr: 0.01000 - Train loss: 0.58566 - Test loss: 0.63345\n",
      "Epoch 17244 - lr: 0.01000 - Train loss: 0.58433 - Test loss: 0.63465\n",
      "Epoch 17245 - lr: 0.01000 - Train loss: 0.58575 - Test loss: 0.63379\n",
      "Epoch 17246 - lr: 0.01000 - Train loss: 0.58735 - Test loss: 0.63493\n",
      "Epoch 17247 - lr: 0.01000 - Train loss: 0.61059 - Test loss: 0.63442\n",
      "Epoch 17248 - lr: 0.01000 - Train loss: 0.58636 - Test loss: 0.63337\n",
      "Epoch 17249 - lr: 0.01000 - Train loss: 0.58865 - Test loss: 0.63468\n",
      "Epoch 17250 - lr: 0.01000 - Train loss: 0.63636 - Test loss: 0.63481\n",
      "Epoch 17251 - lr: 0.01000 - Train loss: 0.58690 - Test loss: 0.63359\n",
      "Epoch 17252 - lr: 0.01000 - Train loss: 0.58250 - Test loss: 0.63335\n",
      "Epoch 17253 - lr: 0.01000 - Train loss: 0.58848 - Test loss: 0.63475\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17254 - lr: 0.01000 - Train loss: 0.62186 - Test loss: 0.63472\n",
      "Epoch 17255 - lr: 0.01000 - Train loss: 0.58540 - Test loss: 0.63402\n",
      "Epoch 17256 - lr: 0.01000 - Train loss: 0.58504 - Test loss: 0.63540\n",
      "Epoch 17257 - lr: 0.01000 - Train loss: 0.59544 - Test loss: 0.63544\n",
      "Epoch 17258 - lr: 0.01000 - Train loss: 0.59057 - Test loss: 0.63520\n",
      "Epoch 17259 - lr: 0.01000 - Train loss: 0.58643 - Test loss: 0.63412\n",
      "Epoch 17260 - lr: 0.01000 - Train loss: 0.58190 - Test loss: 0.63399\n",
      "Epoch 17261 - lr: 0.01000 - Train loss: 0.58797 - Test loss: 0.63544\n",
      "Epoch 17262 - lr: 0.01000 - Train loss: 0.62427 - Test loss: 0.63548\n",
      "Epoch 17263 - lr: 0.01000 - Train loss: 0.58654 - Test loss: 0.63450\n",
      "Epoch 17264 - lr: 0.01000 - Train loss: 0.58650 - Test loss: 0.63556\n",
      "Epoch 17265 - lr: 0.01000 - Train loss: 0.58525 - Test loss: 0.63488\n",
      "Epoch 17266 - lr: 0.01000 - Train loss: 0.58574 - Test loss: 0.63625\n",
      "Epoch 17267 - lr: 0.01000 - Train loss: 0.58766 - Test loss: 0.63656\n",
      "Epoch 17268 - lr: 0.01000 - Train loss: 0.58008 - Test loss: 0.63630\n",
      "Epoch 17269 - lr: 0.01000 - Train loss: 0.58676 - Test loss: 0.63734\n",
      "Epoch 17270 - lr: 0.01000 - Train loss: 0.60696 - Test loss: 0.63682\n",
      "Epoch 17271 - lr: 0.01000 - Train loss: 0.58759 - Test loss: 0.63550\n",
      "Epoch 17272 - lr: 0.01000 - Train loss: 0.58008 - Test loss: 0.63562\n",
      "Epoch 17273 - lr: 0.01000 - Train loss: 0.58390 - Test loss: 0.63705\n",
      "Epoch 17274 - lr: 0.01000 - Train loss: 0.58662 - Test loss: 0.63665\n",
      "Epoch 17275 - lr: 0.01000 - Train loss: 0.58151 - Test loss: 0.63696\n",
      "Epoch 17276 - lr: 0.01000 - Train loss: 0.58057 - Test loss: 0.63664\n",
      "Epoch 17277 - lr: 0.01000 - Train loss: 0.58418 - Test loss: 0.63787\n",
      "Epoch 17278 - lr: 0.01000 - Train loss: 0.59099 - Test loss: 0.63763\n",
      "Epoch 17279 - lr: 0.01000 - Train loss: 0.58676 - Test loss: 0.63635\n",
      "Epoch 17280 - lr: 0.01000 - Train loss: 0.58385 - Test loss: 0.63589\n",
      "Epoch 17281 - lr: 0.01000 - Train loss: 0.58095 - Test loss: 0.63666\n",
      "Epoch 17282 - lr: 0.01000 - Train loss: 0.57989 - Test loss: 0.63673\n",
      "Epoch 17283 - lr: 0.01000 - Train loss: 0.58363 - Test loss: 0.63807\n",
      "Epoch 17284 - lr: 0.01000 - Train loss: 0.58596 - Test loss: 0.63766\n",
      "Epoch 17285 - lr: 0.01000 - Train loss: 0.58326 - Test loss: 0.63814\n",
      "Epoch 17286 - lr: 0.01000 - Train loss: 0.58612 - Test loss: 0.63700\n",
      "Epoch 17287 - lr: 0.01000 - Train loss: 0.58265 - Test loss: 0.63668\n",
      "Epoch 17288 - lr: 0.01000 - Train loss: 0.58475 - Test loss: 0.63774\n",
      "Epoch 17289 - lr: 0.01000 - Train loss: 0.58694 - Test loss: 0.63673\n",
      "Epoch 17290 - lr: 0.01000 - Train loss: 0.58152 - Test loss: 0.63670\n",
      "Epoch 17291 - lr: 0.01000 - Train loss: 0.58710 - Test loss: 0.63806\n",
      "Epoch 17292 - lr: 0.01000 - Train loss: 0.59541 - Test loss: 0.63805\n",
      "Epoch 17293 - lr: 0.01000 - Train loss: 0.58222 - Test loss: 0.63760\n",
      "Epoch 17294 - lr: 0.01000 - Train loss: 0.58614 - Test loss: 0.63862\n",
      "Epoch 17295 - lr: 0.01000 - Train loss: 0.58725 - Test loss: 0.63852\n",
      "Epoch 17296 - lr: 0.01000 - Train loss: 0.58021 - Test loss: 0.63830\n",
      "Epoch 17297 - lr: 0.01000 - Train loss: 0.58477 - Test loss: 0.63955\n",
      "Epoch 17298 - lr: 0.01000 - Train loss: 0.58941 - Test loss: 0.63993\n",
      "Epoch 17299 - lr: 0.01000 - Train loss: 0.58566 - Test loss: 0.63865\n",
      "Epoch 17300 - lr: 0.01000 - Train loss: 0.58151 - Test loss: 0.63833\n",
      "Epoch 17301 - lr: 0.01000 - Train loss: 0.58703 - Test loss: 0.63952\n",
      "Epoch 17302 - lr: 0.01000 - Train loss: 0.59847 - Test loss: 0.63938\n",
      "Epoch 17303 - lr: 0.01000 - Train loss: 0.58524 - Test loss: 0.63844\n",
      "Epoch 17304 - lr: 0.01000 - Train loss: 0.58125 - Test loss: 0.63832\n",
      "Epoch 17305 - lr: 0.01000 - Train loss: 0.58678 - Test loss: 0.63960\n",
      "Epoch 17306 - lr: 0.01000 - Train loss: 0.59355 - Test loss: 0.63962\n",
      "Epoch 17307 - lr: 0.01000 - Train loss: 0.57946 - Test loss: 0.63948\n",
      "Epoch 17308 - lr: 0.01000 - Train loss: 0.58320 - Test loss: 0.64069\n",
      "Epoch 17309 - lr: 0.01000 - Train loss: 0.58711 - Test loss: 0.64042\n",
      "Epoch 17310 - lr: 0.01000 - Train loss: 0.58018 - Test loss: 0.64002\n",
      "Epoch 17311 - lr: 0.01000 - Train loss: 0.58518 - Test loss: 0.64117\n",
      "Epoch 17312 - lr: 0.01000 - Train loss: 0.59480 - Test loss: 0.64089\n",
      "Epoch 17313 - lr: 0.01000 - Train loss: 0.58138 - Test loss: 0.64028\n",
      "Epoch 17314 - lr: 0.01000 - Train loss: 0.58709 - Test loss: 0.64128\n",
      "Epoch 17315 - lr: 0.01000 - Train loss: 0.61217 - Test loss: 0.64114\n",
      "Epoch 17316 - lr: 0.01000 - Train loss: 0.58371 - Test loss: 0.64063\n",
      "Epoch 17317 - lr: 0.01000 - Train loss: 0.58452 - Test loss: 0.64177\n",
      "Epoch 17318 - lr: 0.01000 - Train loss: 0.58495 - Test loss: 0.64206\n",
      "Epoch 17319 - lr: 0.01000 - Train loss: 0.58752 - Test loss: 0.64263\n",
      "Epoch 17320 - lr: 0.01000 - Train loss: 0.62159 - Test loss: 0.64234\n",
      "Epoch 17321 - lr: 0.01000 - Train loss: 0.58483 - Test loss: 0.64125\n",
      "Epoch 17322 - lr: 0.01000 - Train loss: 0.58710 - Test loss: 0.64221\n",
      "Epoch 17323 - lr: 0.01000 - Train loss: 0.62239 - Test loss: 0.64219\n",
      "Epoch 17324 - lr: 0.01000 - Train loss: 0.58523 - Test loss: 0.64115\n",
      "Epoch 17325 - lr: 0.01000 - Train loss: 0.58448 - Test loss: 0.64194\n",
      "Epoch 17326 - lr: 0.01000 - Train loss: 0.58593 - Test loss: 0.64090\n",
      "Epoch 17327 - lr: 0.01000 - Train loss: 0.57844 - Test loss: 0.64128\n",
      "Epoch 17328 - lr: 0.01000 - Train loss: 0.58640 - Test loss: 0.64229\n",
      "Epoch 17329 - lr: 0.01000 - Train loss: 0.58655 - Test loss: 0.64252\n",
      "Epoch 17330 - lr: 0.01000 - Train loss: 0.58524 - Test loss: 0.64340\n",
      "Epoch 17331 - lr: 0.01000 - Train loss: 0.60263 - Test loss: 0.64294\n",
      "Epoch 17332 - lr: 0.01000 - Train loss: 0.58607 - Test loss: 0.64158\n",
      "Epoch 17333 - lr: 0.01000 - Train loss: 0.58300 - Test loss: 0.64105\n",
      "Epoch 17334 - lr: 0.01000 - Train loss: 0.57834 - Test loss: 0.64161\n",
      "Epoch 17335 - lr: 0.01000 - Train loss: 0.58505 - Test loss: 0.64258\n",
      "Epoch 17336 - lr: 0.01000 - Train loss: 0.58337 - Test loss: 0.64223\n",
      "Epoch 17337 - lr: 0.01000 - Train loss: 0.58495 - Test loss: 0.64334\n",
      "Epoch 17338 - lr: 0.01000 - Train loss: 0.59919 - Test loss: 0.64308\n",
      "Epoch 17339 - lr: 0.01000 - Train loss: 0.58509 - Test loss: 0.64194\n",
      "Epoch 17340 - lr: 0.01000 - Train loss: 0.58246 - Test loss: 0.64152\n",
      "Epoch 17341 - lr: 0.01000 - Train loss: 0.57896 - Test loss: 0.64220\n",
      "Epoch 17342 - lr: 0.01000 - Train loss: 0.57845 - Test loss: 0.64265\n",
      "Epoch 17343 - lr: 0.01000 - Train loss: 0.58332 - Test loss: 0.64336\n",
      "Epoch 17344 - lr: 0.01000 - Train loss: 0.58577 - Test loss: 0.64224\n",
      "Epoch 17345 - lr: 0.01000 - Train loss: 0.58238 - Test loss: 0.64185\n",
      "Epoch 17346 - lr: 0.01000 - Train loss: 0.57865 - Test loss: 0.64254\n",
      "Epoch 17347 - lr: 0.01000 - Train loss: 0.57921 - Test loss: 0.64312\n",
      "Epoch 17348 - lr: 0.01000 - Train loss: 0.57793 - Test loss: 0.64331\n",
      "Epoch 17349 - lr: 0.01000 - Train loss: 0.58518 - Test loss: 0.64442\n",
      "Epoch 17350 - lr: 0.01000 - Train loss: 0.60728 - Test loss: 0.64425\n",
      "Epoch 17351 - lr: 0.01000 - Train loss: 0.58503 - Test loss: 0.64324\n",
      "Epoch 17352 - lr: 0.01000 - Train loss: 0.58017 - Test loss: 0.64381\n",
      "Epoch 17353 - lr: 0.01000 - Train loss: 0.58169 - Test loss: 0.64332\n",
      "Epoch 17354 - lr: 0.01000 - Train loss: 0.58133 - Test loss: 0.64402\n",
      "Epoch 17355 - lr: 0.01000 - Train loss: 0.58421 - Test loss: 0.64319\n",
      "Epoch 17356 - lr: 0.01000 - Train loss: 0.58086 - Test loss: 0.64300\n",
      "Epoch 17357 - lr: 0.01000 - Train loss: 0.58332 - Test loss: 0.64402\n",
      "Epoch 17358 - lr: 0.01000 - Train loss: 0.58540 - Test loss: 0.64316\n",
      "Epoch 17359 - lr: 0.01000 - Train loss: 0.57975 - Test loss: 0.64316\n",
      "Epoch 17360 - lr: 0.01000 - Train loss: 0.58567 - Test loss: 0.64441\n",
      "Epoch 17361 - lr: 0.01000 - Train loss: 0.59764 - Test loss: 0.64449\n",
      "Epoch 17362 - lr: 0.01000 - Train loss: 0.58404 - Test loss: 0.64362\n",
      "Epoch 17363 - lr: 0.01000 - Train loss: 0.58084 - Test loss: 0.64343\n",
      "Epoch 17364 - lr: 0.01000 - Train loss: 0.58236 - Test loss: 0.64440\n",
      "Epoch 17365 - lr: 0.01000 - Train loss: 0.58494 - Test loss: 0.64357\n",
      "Epoch 17366 - lr: 0.01000 - Train loss: 0.58241 - Test loss: 0.64325\n",
      "Epoch 17367 - lr: 0.01000 - Train loss: 0.57733 - Test loss: 0.64384\n",
      "Epoch 17368 - lr: 0.01000 - Train loss: 0.58459 - Test loss: 0.64519\n",
      "Epoch 17369 - lr: 0.01000 - Train loss: 0.59744 - Test loss: 0.64524\n",
      "Epoch 17370 - lr: 0.01000 - Train loss: 0.58385 - Test loss: 0.64434\n",
      "Epoch 17371 - lr: 0.01000 - Train loss: 0.58052 - Test loss: 0.64416\n",
      "Epoch 17372 - lr: 0.01000 - Train loss: 0.58290 - Test loss: 0.64515\n",
      "Epoch 17373 - lr: 0.01000 - Train loss: 0.58507 - Test loss: 0.64432\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17374 - lr: 0.01000 - Train loss: 0.58038 - Test loss: 0.64421\n",
      "Epoch 17375 - lr: 0.01000 - Train loss: 0.58296 - Test loss: 0.64526\n",
      "Epoch 17376 - lr: 0.01000 - Train loss: 0.58499 - Test loss: 0.64448\n",
      "Epoch 17377 - lr: 0.01000 - Train loss: 0.57944 - Test loss: 0.64450\n",
      "Epoch 17378 - lr: 0.01000 - Train loss: 0.58517 - Test loss: 0.64573\n",
      "Epoch 17379 - lr: 0.01000 - Train loss: 0.58715 - Test loss: 0.64619\n",
      "Epoch 17380 - lr: 0.01000 - Train loss: 0.58555 - Test loss: 0.64693\n",
      "Epoch 17381 - lr: 0.01000 - Train loss: 0.59058 - Test loss: 0.64691\n",
      "Epoch 17382 - lr: 0.01000 - Train loss: 0.57783 - Test loss: 0.64692\n",
      "Epoch 17383 - lr: 0.01000 - Train loss: 0.58228 - Test loss: 0.64736\n",
      "Epoch 17384 - lr: 0.01000 - Train loss: 0.58491 - Test loss: 0.64613\n",
      "Epoch 17385 - lr: 0.01000 - Train loss: 0.58212 - Test loss: 0.64557\n",
      "Epoch 17386 - lr: 0.01000 - Train loss: 0.57702 - Test loss: 0.64603\n",
      "Epoch 17387 - lr: 0.01000 - Train loss: 0.58542 - Test loss: 0.64717\n",
      "Epoch 17388 - lr: 0.01000 - Train loss: 0.61583 - Test loss: 0.64731\n",
      "Epoch 17389 - lr: 0.01000 - Train loss: 0.58228 - Test loss: 0.64688\n",
      "Epoch 17390 - lr: 0.01000 - Train loss: 0.58226 - Test loss: 0.64789\n",
      "Epoch 17391 - lr: 0.01000 - Train loss: 0.59317 - Test loss: 0.64809\n",
      "Epoch 17392 - lr: 0.01000 - Train loss: 0.59476 - Test loss: 0.64832\n",
      "Epoch 17393 - lr: 0.01000 - Train loss: 0.61726 - Test loss: 0.64816\n",
      "Epoch 17394 - lr: 0.01000 - Train loss: 0.58244 - Test loss: 0.64747\n",
      "Epoch 17395 - lr: 0.01000 - Train loss: 0.58248 - Test loss: 0.64839\n",
      "Epoch 17396 - lr: 0.01000 - Train loss: 0.59472 - Test loss: 0.64875\n",
      "Epoch 17397 - lr: 0.01000 - Train loss: 0.61986 - Test loss: 0.64859\n",
      "Epoch 17398 - lr: 0.01000 - Train loss: 0.58363 - Test loss: 0.64759\n",
      "Epoch 17399 - lr: 0.01000 - Train loss: 0.58472 - Test loss: 0.64828\n",
      "Epoch 17400 - lr: 0.01000 - Train loss: 0.59458 - Test loss: 0.64873\n",
      "Epoch 17401 - lr: 0.01000 - Train loss: 0.61740 - Test loss: 0.64862\n",
      "Epoch 17402 - lr: 0.01000 - Train loss: 0.58247 - Test loss: 0.64791\n",
      "Epoch 17403 - lr: 0.01000 - Train loss: 0.58298 - Test loss: 0.64883\n",
      "Epoch 17404 - lr: 0.01000 - Train loss: 0.58444 - Test loss: 0.64928\n",
      "Epoch 17405 - lr: 0.01000 - Train loss: 0.57805 - Test loss: 0.64911\n",
      "Epoch 17406 - lr: 0.01000 - Train loss: 0.57873 - Test loss: 0.64914\n",
      "Epoch 17407 - lr: 0.01000 - Train loss: 0.57760 - Test loss: 0.64880\n",
      "Epoch 17408 - lr: 0.01000 - Train loss: 0.58123 - Test loss: 0.64968\n",
      "Epoch 17409 - lr: 0.01000 - Train loss: 0.58355 - Test loss: 0.64929\n",
      "Epoch 17410 - lr: 0.01000 - Train loss: 0.58183 - Test loss: 0.64954\n",
      "Epoch 17411 - lr: 0.01000 - Train loss: 0.58459 - Test loss: 0.64825\n",
      "Epoch 17412 - lr: 0.01000 - Train loss: 0.58215 - Test loss: 0.64756\n",
      "Epoch 17413 - lr: 0.01000 - Train loss: 0.57680 - Test loss: 0.64784\n",
      "Epoch 17414 - lr: 0.01000 - Train loss: 0.58237 - Test loss: 0.64905\n",
      "Epoch 17415 - lr: 0.01000 - Train loss: 0.59340 - Test loss: 0.64962\n",
      "Epoch 17416 - lr: 0.01000 - Train loss: 0.58426 - Test loss: 0.64985\n",
      "Epoch 17417 - lr: 0.01000 - Train loss: 0.58146 - Test loss: 0.65048\n",
      "Epoch 17418 - lr: 0.01000 - Train loss: 0.58630 - Test loss: 0.65012\n",
      "Epoch 17419 - lr: 0.01000 - Train loss: 0.58124 - Test loss: 0.64912\n",
      "Epoch 17420 - lr: 0.01000 - Train loss: 0.57925 - Test loss: 0.64944\n",
      "Epoch 17421 - lr: 0.01000 - Train loss: 0.58110 - Test loss: 0.64876\n",
      "Epoch 17422 - lr: 0.01000 - Train loss: 0.57879 - Test loss: 0.64920\n",
      "Epoch 17423 - lr: 0.01000 - Train loss: 0.57963 - Test loss: 0.64881\n",
      "Epoch 17424 - lr: 0.01000 - Train loss: 0.58434 - Test loss: 0.64962\n",
      "Epoch 17425 - lr: 0.01000 - Train loss: 0.59439 - Test loss: 0.65010\n",
      "Epoch 17426 - lr: 0.01000 - Train loss: 0.61874 - Test loss: 0.65010\n",
      "Epoch 17427 - lr: 0.01000 - Train loss: 0.58284 - Test loss: 0.64928\n",
      "Epoch 17428 - lr: 0.01000 - Train loss: 0.58526 - Test loss: 0.65008\n",
      "Epoch 17429 - lr: 0.01000 - Train loss: 0.61965 - Test loss: 0.65021\n",
      "Epoch 17430 - lr: 0.01000 - Train loss: 0.58339 - Test loss: 0.64931\n",
      "Epoch 17431 - lr: 0.01000 - Train loss: 0.58295 - Test loss: 0.64993\n",
      "Epoch 17432 - lr: 0.01000 - Train loss: 0.58389 - Test loss: 0.64910\n",
      "Epoch 17433 - lr: 0.01000 - Train loss: 0.57832 - Test loss: 0.64954\n",
      "Epoch 17434 - lr: 0.01000 - Train loss: 0.57823 - Test loss: 0.64937\n",
      "Epoch 17435 - lr: 0.01000 - Train loss: 0.58464 - Test loss: 0.65038\n",
      "Epoch 17436 - lr: 0.01000 - Train loss: 0.61964 - Test loss: 0.65058\n",
      "Epoch 17437 - lr: 0.01000 - Train loss: 0.58345 - Test loss: 0.64968\n",
      "Epoch 17438 - lr: 0.01000 - Train loss: 0.58174 - Test loss: 0.65026\n",
      "Epoch 17439 - lr: 0.01000 - Train loss: 0.58423 - Test loss: 0.64925\n",
      "Epoch 17440 - lr: 0.01000 - Train loss: 0.58141 - Test loss: 0.64877\n",
      "Epoch 17441 - lr: 0.01000 - Train loss: 0.57629 - Test loss: 0.64920\n",
      "Epoch 17442 - lr: 0.01000 - Train loss: 0.58385 - Test loss: 0.65039\n",
      "Epoch 17443 - lr: 0.01000 - Train loss: 0.60223 - Test loss: 0.65048\n",
      "Epoch 17444 - lr: 0.01000 - Train loss: 0.58422 - Test loss: 0.64940\n",
      "Epoch 17445 - lr: 0.01000 - Train loss: 0.58094 - Test loss: 0.64898\n",
      "Epoch 17446 - lr: 0.01000 - Train loss: 0.57648 - Test loss: 0.64954\n",
      "Epoch 17447 - lr: 0.01000 - Train loss: 0.58294 - Test loss: 0.65043\n",
      "Epoch 17448 - lr: 0.01000 - Train loss: 0.58309 - Test loss: 0.64987\n",
      "Epoch 17449 - lr: 0.01000 - Train loss: 0.58251 - Test loss: 0.65059\n",
      "Epoch 17450 - lr: 0.01000 - Train loss: 0.58403 - Test loss: 0.64980\n",
      "Epoch 17451 - lr: 0.01000 - Train loss: 0.57629 - Test loss: 0.65003\n",
      "Epoch 17452 - lr: 0.01000 - Train loss: 0.58280 - Test loss: 0.65115\n",
      "Epoch 17453 - lr: 0.01000 - Train loss: 0.58317 - Test loss: 0.65176\n",
      "Epoch 17454 - lr: 0.01000 - Train loss: 0.58097 - Test loss: 0.65243\n",
      "Epoch 17455 - lr: 0.01000 - Train loss: 0.58365 - Test loss: 0.65209\n",
      "Epoch 17456 - lr: 0.01000 - Train loss: 0.57755 - Test loss: 0.65204\n",
      "Epoch 17457 - lr: 0.01000 - Train loss: 0.57662 - Test loss: 0.65200\n",
      "Epoch 17458 - lr: 0.01000 - Train loss: 0.58424 - Test loss: 0.65250\n",
      "Epoch 17459 - lr: 0.01000 - Train loss: 0.59451 - Test loss: 0.65281\n",
      "Epoch 17460 - lr: 0.01000 - Train loss: 0.62803 - Test loss: 0.65286\n",
      "Epoch 17461 - lr: 0.01000 - Train loss: 0.58398 - Test loss: 0.65143\n",
      "Epoch 17462 - lr: 0.01000 - Train loss: 0.58166 - Test loss: 0.65067\n",
      "Epoch 17463 - lr: 0.01000 - Train loss: 0.57657 - Test loss: 0.65080\n",
      "Epoch 17464 - lr: 0.01000 - Train loss: 0.58167 - Test loss: 0.65195\n",
      "Epoch 17465 - lr: 0.01000 - Train loss: 0.59402 - Test loss: 0.65246\n",
      "Epoch 17466 - lr: 0.01000 - Train loss: 0.61083 - Test loss: 0.65236\n",
      "Epoch 17467 - lr: 0.01000 - Train loss: 0.58187 - Test loss: 0.65171\n",
      "Epoch 17468 - lr: 0.01000 - Train loss: 0.58328 - Test loss: 0.65253\n",
      "Epoch 17469 - lr: 0.01000 - Train loss: 0.59795 - Test loss: 0.65244\n",
      "Epoch 17470 - lr: 0.01000 - Train loss: 0.58328 - Test loss: 0.65126\n",
      "Epoch 17471 - lr: 0.01000 - Train loss: 0.58125 - Test loss: 0.65067\n",
      "Epoch 17472 - lr: 0.01000 - Train loss: 0.57608 - Test loss: 0.65098\n",
      "Epoch 17473 - lr: 0.01000 - Train loss: 0.58238 - Test loss: 0.65214\n",
      "Epoch 17474 - lr: 0.01000 - Train loss: 0.58440 - Test loss: 0.65287\n",
      "Epoch 17475 - lr: 0.01000 - Train loss: 0.57677 - Test loss: 0.65252\n",
      "Epoch 17476 - lr: 0.01000 - Train loss: 0.58119 - Test loss: 0.65332\n",
      "Epoch 17477 - lr: 0.01000 - Train loss: 0.58928 - Test loss: 0.65335\n",
      "Epoch 17478 - lr: 0.01000 - Train loss: 0.58406 - Test loss: 0.65199\n",
      "Epoch 17479 - lr: 0.01000 - Train loss: 0.58126 - Test loss: 0.65129\n",
      "Epoch 17480 - lr: 0.01000 - Train loss: 0.57603 - Test loss: 0.65153\n",
      "Epoch 17481 - lr: 0.01000 - Train loss: 0.58211 - Test loss: 0.65265\n",
      "Epoch 17482 - lr: 0.01000 - Train loss: 0.58715 - Test loss: 0.65335\n",
      "Epoch 17483 - lr: 0.01000 - Train loss: 0.58310 - Test loss: 0.65212\n",
      "Epoch 17484 - lr: 0.01000 - Train loss: 0.58074 - Test loss: 0.65152\n",
      "Epoch 17485 - lr: 0.01000 - Train loss: 0.57606 - Test loss: 0.65192\n",
      "Epoch 17486 - lr: 0.01000 - Train loss: 0.58336 - Test loss: 0.65273\n",
      "Epoch 17487 - lr: 0.01000 - Train loss: 0.58471 - Test loss: 0.65285\n",
      "Epoch 17488 - lr: 0.01000 - Train loss: 0.57834 - Test loss: 0.65241\n",
      "Epoch 17489 - lr: 0.01000 - Train loss: 0.58461 - Test loss: 0.65321\n",
      "Epoch 17490 - lr: 0.01000 - Train loss: 0.60726 - Test loss: 0.65326\n",
      "Epoch 17491 - lr: 0.01000 - Train loss: 0.58303 - Test loss: 0.65240\n",
      "Epoch 17492 - lr: 0.01000 - Train loss: 0.58008 - Test loss: 0.65287\n",
      "Epoch 17493 - lr: 0.01000 - Train loss: 0.58299 - Test loss: 0.65197\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17494 - lr: 0.01000 - Train loss: 0.58112 - Test loss: 0.65147\n",
      "Epoch 17495 - lr: 0.01000 - Train loss: 0.57612 - Test loss: 0.65178\n",
      "Epoch 17496 - lr: 0.01000 - Train loss: 0.58170 - Test loss: 0.65297\n",
      "Epoch 17497 - lr: 0.01000 - Train loss: 0.59329 - Test loss: 0.65367\n",
      "Epoch 17498 - lr: 0.01000 - Train loss: 0.59763 - Test loss: 0.65353\n",
      "Epoch 17499 - lr: 0.01000 - Train loss: 0.58295 - Test loss: 0.65240\n",
      "Epoch 17500 - lr: 0.01000 - Train loss: 0.58105 - Test loss: 0.65182\n",
      "Epoch 17501 - lr: 0.01000 - Train loss: 0.57603 - Test loss: 0.65209\n",
      "Epoch 17502 - lr: 0.01000 - Train loss: 0.58172 - Test loss: 0.65324\n",
      "Epoch 17503 - lr: 0.01000 - Train loss: 0.59273 - Test loss: 0.65395\n",
      "Epoch 17504 - lr: 0.01000 - Train loss: 0.58253 - Test loss: 0.65441\n",
      "Epoch 17505 - lr: 0.01000 - Train loss: 0.58205 - Test loss: 0.65492\n",
      "Epoch 17506 - lr: 0.01000 - Train loss: 0.58575 - Test loss: 0.65528\n",
      "Epoch 17507 - lr: 0.01000 - Train loss: 0.58152 - Test loss: 0.65400\n",
      "Epoch 17508 - lr: 0.01000 - Train loss: 0.57585 - Test loss: 0.65387\n",
      "Epoch 17509 - lr: 0.01000 - Train loss: 0.58287 - Test loss: 0.65467\n",
      "Epoch 17510 - lr: 0.01000 - Train loss: 0.59774 - Test loss: 0.65455\n",
      "Epoch 17511 - lr: 0.01000 - Train loss: 0.58305 - Test loss: 0.65330\n",
      "Epoch 17512 - lr: 0.01000 - Train loss: 0.58111 - Test loss: 0.65263\n",
      "Epoch 17513 - lr: 0.01000 - Train loss: 0.57603 - Test loss: 0.65282\n",
      "Epoch 17514 - lr: 0.01000 - Train loss: 0.58150 - Test loss: 0.65393\n",
      "Epoch 17515 - lr: 0.01000 - Train loss: 0.59341 - Test loss: 0.65458\n",
      "Epoch 17516 - lr: 0.01000 - Train loss: 0.60467 - Test loss: 0.65443\n",
      "Epoch 17517 - lr: 0.01000 - Train loss: 0.58373 - Test loss: 0.65330\n",
      "Epoch 17518 - lr: 0.01000 - Train loss: 0.57657 - Test loss: 0.65319\n",
      "Epoch 17519 - lr: 0.01000 - Train loss: 0.58243 - Test loss: 0.65419\n",
      "Epoch 17520 - lr: 0.01000 - Train loss: 0.58648 - Test loss: 0.65458\n",
      "Epoch 17521 - lr: 0.01000 - Train loss: 0.58309 - Test loss: 0.65491\n",
      "Epoch 17522 - lr: 0.01000 - Train loss: 0.58129 - Test loss: 0.65457\n",
      "Epoch 17523 - lr: 0.01000 - Train loss: 0.58351 - Test loss: 0.65520\n",
      "Epoch 17524 - lr: 0.01000 - Train loss: 0.61296 - Test loss: 0.65517\n",
      "Epoch 17525 - lr: 0.01000 - Train loss: 0.58148 - Test loss: 0.65448\n",
      "Epoch 17526 - lr: 0.01000 - Train loss: 0.58276 - Test loss: 0.65516\n",
      "Epoch 17527 - lr: 0.01000 - Train loss: 0.59640 - Test loss: 0.65505\n",
      "Epoch 17528 - lr: 0.01000 - Train loss: 0.58254 - Test loss: 0.65387\n",
      "Epoch 17529 - lr: 0.01000 - Train loss: 0.58021 - Test loss: 0.65330\n",
      "Epoch 17530 - lr: 0.01000 - Train loss: 0.57598 - Test loss: 0.65372\n",
      "Epoch 17531 - lr: 0.01000 - Train loss: 0.58000 - Test loss: 0.65434\n",
      "Epoch 17532 - lr: 0.01000 - Train loss: 0.58286 - Test loss: 0.65346\n",
      "Epoch 17533 - lr: 0.01000 - Train loss: 0.58116 - Test loss: 0.65293\n",
      "Epoch 17534 - lr: 0.01000 - Train loss: 0.57678 - Test loss: 0.65309\n",
      "Epoch 17535 - lr: 0.01000 - Train loss: 0.58340 - Test loss: 0.65422\n",
      "Epoch 17536 - lr: 0.01000 - Train loss: 0.60407 - Test loss: 0.65449\n",
      "Epoch 17537 - lr: 0.01000 - Train loss: 0.58357 - Test loss: 0.65354\n",
      "Epoch 17538 - lr: 0.01000 - Train loss: 0.57780 - Test loss: 0.65338\n",
      "Epoch 17539 - lr: 0.01000 - Train loss: 0.58370 - Test loss: 0.65431\n",
      "Epoch 17540 - lr: 0.01000 - Train loss: 0.58299 - Test loss: 0.65519\n",
      "Epoch 17541 - lr: 0.01000 - Train loss: 0.57785 - Test loss: 0.65530\n",
      "Epoch 17542 - lr: 0.01000 - Train loss: 0.57869 - Test loss: 0.65475\n",
      "Epoch 17543 - lr: 0.01000 - Train loss: 0.58292 - Test loss: 0.65532\n",
      "Epoch 17544 - lr: 0.01000 - Train loss: 0.58161 - Test loss: 0.65523\n",
      "Epoch 17545 - lr: 0.01000 - Train loss: 0.58410 - Test loss: 0.65572\n",
      "Epoch 17546 - lr: 0.01000 - Train loss: 0.58449 - Test loss: 0.65610\n",
      "Epoch 17547 - lr: 0.01000 - Train loss: 0.58426 - Test loss: 0.65654\n",
      "Epoch 17548 - lr: 0.01000 - Train loss: 0.62880 - Test loss: 0.65674\n",
      "Epoch 17549 - lr: 0.01000 - Train loss: 0.58315 - Test loss: 0.65529\n",
      "Epoch 17550 - lr: 0.01000 - Train loss: 0.58126 - Test loss: 0.65444\n",
      "Epoch 17551 - lr: 0.01000 - Train loss: 0.57688 - Test loss: 0.65437\n",
      "Epoch 17552 - lr: 0.01000 - Train loss: 0.58367 - Test loss: 0.65536\n",
      "Epoch 17553 - lr: 0.01000 - Train loss: 0.61069 - Test loss: 0.65561\n",
      "Epoch 17554 - lr: 0.01000 - Train loss: 0.58193 - Test loss: 0.65499\n",
      "Epoch 17555 - lr: 0.01000 - Train loss: 0.58411 - Test loss: 0.65565\n",
      "Epoch 17556 - lr: 0.01000 - Train loss: 0.59646 - Test loss: 0.65573\n",
      "Epoch 17557 - lr: 0.01000 - Train loss: 0.58227 - Test loss: 0.65468\n",
      "Epoch 17558 - lr: 0.01000 - Train loss: 0.58017 - Test loss: 0.65416\n",
      "Epoch 17559 - lr: 0.01000 - Train loss: 0.57538 - Test loss: 0.65456\n",
      "Epoch 17560 - lr: 0.01000 - Train loss: 0.58399 - Test loss: 0.65547\n",
      "Epoch 17561 - lr: 0.01000 - Train loss: 0.59223 - Test loss: 0.65572\n",
      "Epoch 17562 - lr: 0.01000 - Train loss: 0.57889 - Test loss: 0.65513\n",
      "Epoch 17563 - lr: 0.01000 - Train loss: 0.58074 - Test loss: 0.65567\n",
      "Epoch 17564 - lr: 0.01000 - Train loss: 0.58321 - Test loss: 0.65474\n",
      "Epoch 17565 - lr: 0.01000 - Train loss: 0.58102 - Test loss: 0.65416\n",
      "Epoch 17566 - lr: 0.01000 - Train loss: 0.57693 - Test loss: 0.65425\n",
      "Epoch 17567 - lr: 0.01000 - Train loss: 0.58376 - Test loss: 0.65531\n",
      "Epoch 17568 - lr: 0.01000 - Train loss: 0.60289 - Test loss: 0.65562\n",
      "Epoch 17569 - lr: 0.01000 - Train loss: 0.58336 - Test loss: 0.65464\n",
      "Epoch 17570 - lr: 0.01000 - Train loss: 0.57981 - Test loss: 0.65423\n",
      "Epoch 17571 - lr: 0.01000 - Train loss: 0.57554 - Test loss: 0.65475\n",
      "Epoch 17572 - lr: 0.01000 - Train loss: 0.58118 - Test loss: 0.65552\n",
      "Epoch 17573 - lr: 0.01000 - Train loss: 0.58335 - Test loss: 0.65474\n",
      "Epoch 17574 - lr: 0.01000 - Train loss: 0.57993 - Test loss: 0.65435\n",
      "Epoch 17575 - lr: 0.01000 - Train loss: 0.57535 - Test loss: 0.65485\n",
      "Epoch 17576 - lr: 0.01000 - Train loss: 0.58317 - Test loss: 0.65574\n",
      "Epoch 17577 - lr: 0.01000 - Train loss: 0.59162 - Test loss: 0.65629\n",
      "Epoch 17578 - lr: 0.01000 - Train loss: 0.58484 - Test loss: 0.65633\n",
      "Epoch 17579 - lr: 0.01000 - Train loss: 0.57948 - Test loss: 0.65562\n",
      "Epoch 17580 - lr: 0.01000 - Train loss: 0.57746 - Test loss: 0.65599\n",
      "Epoch 17581 - lr: 0.01000 - Train loss: 0.57868 - Test loss: 0.65562\n",
      "Epoch 17582 - lr: 0.01000 - Train loss: 0.58071 - Test loss: 0.65620\n",
      "Epoch 17583 - lr: 0.01000 - Train loss: 0.58313 - Test loss: 0.65531\n",
      "Epoch 17584 - lr: 0.01000 - Train loss: 0.58081 - Test loss: 0.65476\n",
      "Epoch 17585 - lr: 0.01000 - Train loss: 0.57652 - Test loss: 0.65489\n",
      "Epoch 17586 - lr: 0.01000 - Train loss: 0.58330 - Test loss: 0.65596\n",
      "Epoch 17587 - lr: 0.01000 - Train loss: 0.60020 - Test loss: 0.65625\n",
      "Epoch 17588 - lr: 0.01000 - Train loss: 0.58287 - Test loss: 0.65525\n",
      "Epoch 17589 - lr: 0.01000 - Train loss: 0.58098 - Test loss: 0.65469\n",
      "Epoch 17590 - lr: 0.01000 - Train loss: 0.57749 - Test loss: 0.65472\n",
      "Epoch 17591 - lr: 0.01000 - Train loss: 0.58291 - Test loss: 0.65569\n",
      "Epoch 17592 - lr: 0.01000 - Train loss: 0.58966 - Test loss: 0.65629\n",
      "Epoch 17593 - lr: 0.01000 - Train loss: 0.58328 - Test loss: 0.65541\n",
      "Epoch 17594 - lr: 0.01000 - Train loss: 0.57762 - Test loss: 0.65522\n",
      "Epoch 17595 - lr: 0.01000 - Train loss: 0.58292 - Test loss: 0.65606\n",
      "Epoch 17596 - lr: 0.01000 - Train loss: 0.58937 - Test loss: 0.65657\n",
      "Epoch 17597 - lr: 0.01000 - Train loss: 0.58329 - Test loss: 0.65559\n",
      "Epoch 17598 - lr: 0.01000 - Train loss: 0.57911 - Test loss: 0.65520\n",
      "Epoch 17599 - lr: 0.01000 - Train loss: 0.57699 - Test loss: 0.65577\n",
      "Epoch 17600 - lr: 0.01000 - Train loss: 0.57743 - Test loss: 0.65571\n",
      "Epoch 17601 - lr: 0.01000 - Train loss: 0.58363 - Test loss: 0.65656\n",
      "Epoch 17602 - lr: 0.01000 - Train loss: 0.58394 - Test loss: 0.65725\n",
      "Epoch 17603 - lr: 0.01000 - Train loss: 0.58337 - Test loss: 0.65780\n",
      "Epoch 17604 - lr: 0.01000 - Train loss: 0.60812 - Test loss: 0.65778\n",
      "Epoch 17605 - lr: 0.01000 - Train loss: 0.58274 - Test loss: 0.65678\n",
      "Epoch 17606 - lr: 0.01000 - Train loss: 0.57741 - Test loss: 0.65698\n",
      "Epoch 17607 - lr: 0.01000 - Train loss: 0.57888 - Test loss: 0.65649\n",
      "Epoch 17608 - lr: 0.01000 - Train loss: 0.57921 - Test loss: 0.65696\n",
      "Epoch 17609 - lr: 0.01000 - Train loss: 0.58215 - Test loss: 0.65613\n",
      "Epoch 17610 - lr: 0.01000 - Train loss: 0.58038 - Test loss: 0.65561\n",
      "Epoch 17611 - lr: 0.01000 - Train loss: 0.57547 - Test loss: 0.65585\n",
      "Epoch 17612 - lr: 0.01000 - Train loss: 0.58146 - Test loss: 0.65692\n",
      "Epoch 17613 - lr: 0.01000 - Train loss: 0.59138 - Test loss: 0.65771\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17614 - lr: 0.01000 - Train loss: 0.58640 - Test loss: 0.65772\n",
      "Epoch 17615 - lr: 0.01000 - Train loss: 0.58185 - Test loss: 0.65662\n",
      "Epoch 17616 - lr: 0.01000 - Train loss: 0.57945 - Test loss: 0.65608\n",
      "Epoch 17617 - lr: 0.01000 - Train loss: 0.57587 - Test loss: 0.65650\n",
      "Epoch 17618 - lr: 0.01000 - Train loss: 0.57597 - Test loss: 0.65691\n",
      "Epoch 17619 - lr: 0.01000 - Train loss: 0.57609 - Test loss: 0.65724\n",
      "Epoch 17620 - lr: 0.01000 - Train loss: 0.57587 - Test loss: 0.65749\n",
      "Epoch 17621 - lr: 0.01000 - Train loss: 0.57750 - Test loss: 0.65779\n",
      "Epoch 17622 - lr: 0.01000 - Train loss: 0.57893 - Test loss: 0.65724\n",
      "Epoch 17623 - lr: 0.01000 - Train loss: 0.57924 - Test loss: 0.65765\n",
      "Epoch 17624 - lr: 0.01000 - Train loss: 0.58216 - Test loss: 0.65677\n",
      "Epoch 17625 - lr: 0.01000 - Train loss: 0.58038 - Test loss: 0.65620\n",
      "Epoch 17626 - lr: 0.01000 - Train loss: 0.57548 - Test loss: 0.65639\n",
      "Epoch 17627 - lr: 0.01000 - Train loss: 0.58135 - Test loss: 0.65742\n",
      "Epoch 17628 - lr: 0.01000 - Train loss: 0.59197 - Test loss: 0.65819\n",
      "Epoch 17629 - lr: 0.01000 - Train loss: 0.59257 - Test loss: 0.65860\n",
      "Epoch 17630 - lr: 0.01000 - Train loss: 0.58243 - Test loss: 0.65893\n",
      "Epoch 17631 - lr: 0.01000 - Train loss: 0.58014 - Test loss: 0.65926\n",
      "Epoch 17632 - lr: 0.01000 - Train loss: 0.58299 - Test loss: 0.65898\n",
      "Epoch 17633 - lr: 0.01000 - Train loss: 0.57542 - Test loss: 0.65867\n",
      "Epoch 17634 - lr: 0.01000 - Train loss: 0.58358 - Test loss: 0.65897\n",
      "Epoch 17635 - lr: 0.01000 - Train loss: 0.59018 - Test loss: 0.65940\n",
      "Epoch 17636 - lr: 0.01000 - Train loss: 0.58232 - Test loss: 0.65832\n",
      "Epoch 17637 - lr: 0.01000 - Train loss: 0.58207 - Test loss: 0.65851\n",
      "Epoch 17638 - lr: 0.01000 - Train loss: 0.58216 - Test loss: 0.65776\n",
      "Epoch 17639 - lr: 0.01000 - Train loss: 0.58170 - Test loss: 0.65809\n",
      "Epoch 17640 - lr: 0.01000 - Train loss: 0.58297 - Test loss: 0.65727\n",
      "Epoch 17641 - lr: 0.01000 - Train loss: 0.57502 - Test loss: 0.65736\n",
      "Epoch 17642 - lr: 0.01000 - Train loss: 0.58398 - Test loss: 0.65806\n",
      "Epoch 17643 - lr: 0.01000 - Train loss: 0.60507 - Test loss: 0.65819\n",
      "Epoch 17644 - lr: 0.01000 - Train loss: 0.58317 - Test loss: 0.65714\n",
      "Epoch 17645 - lr: 0.01000 - Train loss: 0.57621 - Test loss: 0.65695\n",
      "Epoch 17646 - lr: 0.01000 - Train loss: 0.58291 - Test loss: 0.65781\n",
      "Epoch 17647 - lr: 0.01000 - Train loss: 0.60262 - Test loss: 0.65800\n",
      "Epoch 17648 - lr: 0.01000 - Train loss: 0.58310 - Test loss: 0.65691\n",
      "Epoch 17649 - lr: 0.01000 - Train loss: 0.57976 - Test loss: 0.65636\n",
      "Epoch 17650 - lr: 0.01000 - Train loss: 0.57487 - Test loss: 0.65671\n",
      "Epoch 17651 - lr: 0.01000 - Train loss: 0.58367 - Test loss: 0.65762\n",
      "Epoch 17652 - lr: 0.01000 - Train loss: 0.60678 - Test loss: 0.65792\n",
      "Epoch 17653 - lr: 0.01000 - Train loss: 0.58278 - Test loss: 0.65708\n",
      "Epoch 17654 - lr: 0.01000 - Train loss: 0.57505 - Test loss: 0.65725\n",
      "Epoch 17655 - lr: 0.01000 - Train loss: 0.58298 - Test loss: 0.65793\n",
      "Epoch 17656 - lr: 0.01000 - Train loss: 0.59186 - Test loss: 0.65841\n",
      "Epoch 17657 - lr: 0.01000 - Train loss: 0.59261 - Test loss: 0.65867\n",
      "Epoch 17658 - lr: 0.01000 - Train loss: 0.58186 - Test loss: 0.65911\n",
      "Epoch 17659 - lr: 0.01000 - Train loss: 0.58208 - Test loss: 0.65943\n",
      "Epoch 17660 - lr: 0.01000 - Train loss: 0.58645 - Test loss: 0.65952\n",
      "Epoch 17661 - lr: 0.01000 - Train loss: 0.58154 - Test loss: 0.65946\n",
      "Epoch 17662 - lr: 0.01000 - Train loss: 0.58350 - Test loss: 0.65821\n",
      "Epoch 17663 - lr: 0.01000 - Train loss: 0.57816 - Test loss: 0.65760\n",
      "Epoch 17664 - lr: 0.01000 - Train loss: 0.58122 - Test loss: 0.65809\n",
      "Epoch 17665 - lr: 0.01000 - Train loss: 0.58319 - Test loss: 0.65725\n",
      "Epoch 17666 - lr: 0.01000 - Train loss: 0.57797 - Test loss: 0.65691\n",
      "Epoch 17667 - lr: 0.01000 - Train loss: 0.58102 - Test loss: 0.65757\n",
      "Epoch 17668 - lr: 0.01000 - Train loss: 0.58306 - Test loss: 0.65686\n",
      "Epoch 17669 - lr: 0.01000 - Train loss: 0.57891 - Test loss: 0.65651\n",
      "Epoch 17670 - lr: 0.01000 - Train loss: 0.57646 - Test loss: 0.65708\n",
      "Epoch 17671 - lr: 0.01000 - Train loss: 0.57611 - Test loss: 0.65718\n",
      "Epoch 17672 - lr: 0.01000 - Train loss: 0.58277 - Test loss: 0.65810\n",
      "Epoch 17673 - lr: 0.01000 - Train loss: 0.59797 - Test loss: 0.65834\n",
      "Epoch 17674 - lr: 0.01000 - Train loss: 0.58224 - Test loss: 0.65728\n",
      "Epoch 17675 - lr: 0.01000 - Train loss: 0.58083 - Test loss: 0.65663\n",
      "Epoch 17676 - lr: 0.01000 - Train loss: 0.57783 - Test loss: 0.65652\n",
      "Epoch 17677 - lr: 0.01000 - Train loss: 0.58035 - Test loss: 0.65729\n",
      "Epoch 17678 - lr: 0.01000 - Train loss: 0.58263 - Test loss: 0.65666\n",
      "Epoch 17679 - lr: 0.01000 - Train loss: 0.58056 - Test loss: 0.65622\n",
      "Epoch 17680 - lr: 0.01000 - Train loss: 0.57720 - Test loss: 0.65630\n",
      "Epoch 17681 - lr: 0.01000 - Train loss: 0.58228 - Test loss: 0.65723\n",
      "Epoch 17682 - lr: 0.01000 - Train loss: 0.58105 - Test loss: 0.65759\n",
      "Epoch 17683 - lr: 0.01000 - Train loss: 0.58261 - Test loss: 0.65817\n",
      "Epoch 17684 - lr: 0.01000 - Train loss: 0.58354 - Test loss: 0.65842\n",
      "Epoch 17685 - lr: 0.01000 - Train loss: 0.57644 - Test loss: 0.65809\n",
      "Epoch 17686 - lr: 0.01000 - Train loss: 0.58332 - Test loss: 0.65881\n",
      "Epoch 17687 - lr: 0.01000 - Train loss: 0.60841 - Test loss: 0.65900\n",
      "Epoch 17688 - lr: 0.01000 - Train loss: 0.58246 - Test loss: 0.65812\n",
      "Epoch 17689 - lr: 0.01000 - Train loss: 0.57655 - Test loss: 0.65831\n",
      "Epoch 17690 - lr: 0.01000 - Train loss: 0.57649 - Test loss: 0.65814\n",
      "Epoch 17691 - lr: 0.01000 - Train loss: 0.58351 - Test loss: 0.65889\n",
      "Epoch 17692 - lr: 0.01000 - Train loss: 0.61065 - Test loss: 0.65913\n",
      "Epoch 17693 - lr: 0.01000 - Train loss: 0.58212 - Test loss: 0.65834\n",
      "Epoch 17694 - lr: 0.01000 - Train loss: 0.57949 - Test loss: 0.65864\n",
      "Epoch 17695 - lr: 0.01000 - Train loss: 0.58220 - Test loss: 0.65775\n",
      "Epoch 17696 - lr: 0.01000 - Train loss: 0.58081 - Test loss: 0.65712\n",
      "Epoch 17697 - lr: 0.01000 - Train loss: 0.57796 - Test loss: 0.65698\n",
      "Epoch 17698 - lr: 0.01000 - Train loss: 0.57930 - Test loss: 0.65770\n",
      "Epoch 17699 - lr: 0.01000 - Train loss: 0.58198 - Test loss: 0.65711\n",
      "Epoch 17700 - lr: 0.01000 - Train loss: 0.58070 - Test loss: 0.65667\n",
      "Epoch 17701 - lr: 0.01000 - Train loss: 0.57796 - Test loss: 0.65665\n",
      "Epoch 17702 - lr: 0.01000 - Train loss: 0.57881 - Test loss: 0.65745\n",
      "Epoch 17703 - lr: 0.01000 - Train loss: 0.58159 - Test loss: 0.65698\n",
      "Epoch 17704 - lr: 0.01000 - Train loss: 0.58023 - Test loss: 0.65665\n",
      "Epoch 17705 - lr: 0.01000 - Train loss: 0.57636 - Test loss: 0.65686\n",
      "Epoch 17706 - lr: 0.01000 - Train loss: 0.58314 - Test loss: 0.65788\n",
      "Epoch 17707 - lr: 0.01000 - Train loss: 0.58549 - Test loss: 0.65868\n",
      "Epoch 17708 - lr: 0.01000 - Train loss: 0.58238 - Test loss: 0.65911\n",
      "Epoch 17709 - lr: 0.01000 - Train loss: 0.58055 - Test loss: 0.65906\n",
      "Epoch 17710 - lr: 0.01000 - Train loss: 0.58294 - Test loss: 0.65960\n",
      "Epoch 17711 - lr: 0.01000 - Train loss: 0.60057 - Test loss: 0.65965\n",
      "Epoch 17712 - lr: 0.01000 - Train loss: 0.58260 - Test loss: 0.65844\n",
      "Epoch 17713 - lr: 0.01000 - Train loss: 0.58071 - Test loss: 0.65768\n",
      "Epoch 17714 - lr: 0.01000 - Train loss: 0.57772 - Test loss: 0.65749\n",
      "Epoch 17715 - lr: 0.01000 - Train loss: 0.58005 - Test loss: 0.65819\n",
      "Epoch 17716 - lr: 0.01000 - Train loss: 0.58239 - Test loss: 0.65754\n",
      "Epoch 17717 - lr: 0.01000 - Train loss: 0.58061 - Test loss: 0.65706\n",
      "Epoch 17718 - lr: 0.01000 - Train loss: 0.57780 - Test loss: 0.65703\n",
      "Epoch 17719 - lr: 0.01000 - Train loss: 0.57916 - Test loss: 0.65782\n",
      "Epoch 17720 - lr: 0.01000 - Train loss: 0.58180 - Test loss: 0.65731\n",
      "Epoch 17721 - lr: 0.01000 - Train loss: 0.58056 - Test loss: 0.65692\n",
      "Epoch 17722 - lr: 0.01000 - Train loss: 0.57781 - Test loss: 0.65695\n",
      "Epoch 17723 - lr: 0.01000 - Train loss: 0.57886 - Test loss: 0.65776\n",
      "Epoch 17724 - lr: 0.01000 - Train loss: 0.58157 - Test loss: 0.65731\n",
      "Epoch 17725 - lr: 0.01000 - Train loss: 0.58029 - Test loss: 0.65698\n",
      "Epoch 17726 - lr: 0.01000 - Train loss: 0.57688 - Test loss: 0.65712\n",
      "Epoch 17727 - lr: 0.01000 - Train loss: 0.58222 - Test loss: 0.65806\n",
      "Epoch 17728 - lr: 0.01000 - Train loss: 0.58165 - Test loss: 0.65852\n",
      "Epoch 17729 - lr: 0.01000 - Train loss: 0.57756 - Test loss: 0.65890\n",
      "Epoch 17730 - lr: 0.01000 - Train loss: 0.58002 - Test loss: 0.65837\n",
      "Epoch 17731 - lr: 0.01000 - Train loss: 0.57501 - Test loss: 0.65850\n",
      "Epoch 17732 - lr: 0.01000 - Train loss: 0.58119 - Test loss: 0.65939\n",
      "Epoch 17733 - lr: 0.01000 - Train loss: 0.59277 - Test loss: 0.66011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17734 - lr: 0.01000 - Train loss: 0.58890 - Test loss: 0.66018\n",
      "Epoch 17735 - lr: 0.01000 - Train loss: 0.57479 - Test loss: 0.65995\n",
      "Epoch 17736 - lr: 0.01000 - Train loss: 0.58386 - Test loss: 0.66044\n",
      "Epoch 17737 - lr: 0.01000 - Train loss: 0.61072 - Test loss: 0.66053\n",
      "Epoch 17738 - lr: 0.01000 - Train loss: 0.58238 - Test loss: 0.65955\n",
      "Epoch 17739 - lr: 0.01000 - Train loss: 0.57655 - Test loss: 0.65964\n",
      "Epoch 17740 - lr: 0.01000 - Train loss: 0.57705 - Test loss: 0.65930\n",
      "Epoch 17741 - lr: 0.01000 - Train loss: 0.58338 - Test loss: 0.65989\n",
      "Epoch 17742 - lr: 0.01000 - Train loss: 0.58200 - Test loss: 0.66062\n",
      "Epoch 17743 - lr: 0.01000 - Train loss: 0.58036 - Test loss: 0.66102\n",
      "Epoch 17744 - lr: 0.01000 - Train loss: 0.58702 - Test loss: 0.66104\n",
      "Epoch 17745 - lr: 0.01000 - Train loss: 0.58210 - Test loss: 0.65970\n",
      "Epoch 17746 - lr: 0.01000 - Train loss: 0.58048 - Test loss: 0.65883\n",
      "Epoch 17747 - lr: 0.01000 - Train loss: 0.57654 - Test loss: 0.65864\n",
      "Epoch 17748 - lr: 0.01000 - Train loss: 0.58338 - Test loss: 0.65940\n",
      "Epoch 17749 - lr: 0.01000 - Train loss: 0.59242 - Test loss: 0.65973\n",
      "Epoch 17750 - lr: 0.01000 - Train loss: 0.57924 - Test loss: 0.65903\n",
      "Epoch 17751 - lr: 0.01000 - Train loss: 0.57516 - Test loss: 0.65928\n",
      "Epoch 17752 - lr: 0.01000 - Train loss: 0.57748 - Test loss: 0.65968\n",
      "Epoch 17753 - lr: 0.01000 - Train loss: 0.58000 - Test loss: 0.65907\n",
      "Epoch 17754 - lr: 0.01000 - Train loss: 0.57482 - Test loss: 0.65915\n",
      "Epoch 17755 - lr: 0.01000 - Train loss: 0.58120 - Test loss: 0.65999\n",
      "Epoch 17756 - lr: 0.01000 - Train loss: 0.59171 - Test loss: 0.66069\n",
      "Epoch 17757 - lr: 0.01000 - Train loss: 0.59304 - Test loss: 0.66090\n",
      "Epoch 17758 - lr: 0.01000 - Train loss: 0.59169 - Test loss: 0.66072\n",
      "Epoch 17759 - lr: 0.01000 - Train loss: 0.57824 - Test loss: 0.65987\n",
      "Epoch 17760 - lr: 0.01000 - Train loss: 0.57957 - Test loss: 0.66012\n",
      "Epoch 17761 - lr: 0.01000 - Train loss: 0.58221 - Test loss: 0.65915\n",
      "Epoch 17762 - lr: 0.01000 - Train loss: 0.58075 - Test loss: 0.65841\n",
      "Epoch 17763 - lr: 0.01000 - Train loss: 0.57818 - Test loss: 0.65815\n",
      "Epoch 17764 - lr: 0.01000 - Train loss: 0.57722 - Test loss: 0.65875\n",
      "Epoch 17765 - lr: 0.01000 - Train loss: 0.57970 - Test loss: 0.65840\n",
      "Epoch 17766 - lr: 0.01000 - Train loss: 0.57468 - Test loss: 0.65867\n",
      "Epoch 17767 - lr: 0.01000 - Train loss: 0.58166 - Test loss: 0.65961\n",
      "Epoch 17768 - lr: 0.01000 - Train loss: 0.58424 - Test loss: 0.66053\n",
      "Epoch 17769 - lr: 0.01000 - Train loss: 0.57875 - Test loss: 0.65977\n",
      "Epoch 17770 - lr: 0.01000 - Train loss: 0.57719 - Test loss: 0.66001\n",
      "Epoch 17771 - lr: 0.01000 - Train loss: 0.57939 - Test loss: 0.65943\n",
      "Epoch 17772 - lr: 0.01000 - Train loss: 0.57472 - Test loss: 0.65963\n",
      "Epoch 17773 - lr: 0.01000 - Train loss: 0.58191 - Test loss: 0.66018\n",
      "Epoch 17774 - lr: 0.01000 - Train loss: 0.58105 - Test loss: 0.65985\n",
      "Epoch 17775 - lr: 0.01000 - Train loss: 0.58358 - Test loss: 0.66036\n",
      "Epoch 17776 - lr: 0.01000 - Train loss: 0.60360 - Test loss: 0.66051\n",
      "Epoch 17777 - lr: 0.01000 - Train loss: 0.58280 - Test loss: 0.65939\n",
      "Epoch 17778 - lr: 0.01000 - Train loss: 0.57924 - Test loss: 0.65878\n",
      "Epoch 17779 - lr: 0.01000 - Train loss: 0.57446 - Test loss: 0.65906\n",
      "Epoch 17780 - lr: 0.01000 - Train loss: 0.58331 - Test loss: 0.65982\n",
      "Epoch 17781 - lr: 0.01000 - Train loss: 0.58697 - Test loss: 0.66035\n",
      "Epoch 17782 - lr: 0.01000 - Train loss: 0.57759 - Test loss: 0.66046\n",
      "Epoch 17783 - lr: 0.01000 - Train loss: 0.58020 - Test loss: 0.65970\n",
      "Epoch 17784 - lr: 0.01000 - Train loss: 0.57528 - Test loss: 0.65961\n",
      "Epoch 17785 - lr: 0.01000 - Train loss: 0.58152 - Test loss: 0.66038\n",
      "Epoch 17786 - lr: 0.01000 - Train loss: 0.58490 - Test loss: 0.66118\n",
      "Epoch 17787 - lr: 0.01000 - Train loss: 0.58032 - Test loss: 0.66014\n",
      "Epoch 17788 - lr: 0.01000 - Train loss: 0.57525 - Test loss: 0.65995\n",
      "Epoch 17789 - lr: 0.01000 - Train loss: 0.58132 - Test loss: 0.66066\n",
      "Epoch 17790 - lr: 0.01000 - Train loss: 0.58814 - Test loss: 0.66136\n",
      "Epoch 17791 - lr: 0.01000 - Train loss: 0.58270 - Test loss: 0.66005\n",
      "Epoch 17792 - lr: 0.01000 - Train loss: 0.58052 - Test loss: 0.65919\n",
      "Epoch 17793 - lr: 0.01000 - Train loss: 0.57729 - Test loss: 0.65892\n",
      "Epoch 17794 - lr: 0.01000 - Train loss: 0.58097 - Test loss: 0.65955\n",
      "Epoch 17795 - lr: 0.01000 - Train loss: 0.58268 - Test loss: 0.65896\n",
      "Epoch 17796 - lr: 0.01000 - Train loss: 0.57678 - Test loss: 0.65879\n",
      "Epoch 17797 - lr: 0.01000 - Train loss: 0.58255 - Test loss: 0.65954\n",
      "Epoch 17798 - lr: 0.01000 - Train loss: 0.59051 - Test loss: 0.66016\n",
      "Epoch 17799 - lr: 0.01000 - Train loss: 0.58096 - Test loss: 0.65972\n",
      "Epoch 17800 - lr: 0.01000 - Train loss: 0.58343 - Test loss: 0.66023\n",
      "Epoch 17801 - lr: 0.01000 - Train loss: 0.59816 - Test loss: 0.66042\n",
      "Epoch 17802 - lr: 0.01000 - Train loss: 0.58187 - Test loss: 0.65933\n",
      "Epoch 17803 - lr: 0.01000 - Train loss: 0.58059 - Test loss: 0.65862\n",
      "Epoch 17804 - lr: 0.01000 - Train loss: 0.57821 - Test loss: 0.65838\n",
      "Epoch 17805 - lr: 0.01000 - Train loss: 0.57631 - Test loss: 0.65895\n",
      "Epoch 17806 - lr: 0.01000 - Train loss: 0.57708 - Test loss: 0.65892\n",
      "Epoch 17807 - lr: 0.01000 - Train loss: 0.58188 - Test loss: 0.65964\n",
      "Epoch 17808 - lr: 0.01000 - Train loss: 0.58045 - Test loss: 0.65963\n",
      "Epoch 17809 - lr: 0.01000 - Train loss: 0.58228 - Test loss: 0.66029\n",
      "Epoch 17810 - lr: 0.01000 - Train loss: 0.58794 - Test loss: 0.66072\n",
      "Epoch 17811 - lr: 0.01000 - Train loss: 0.57525 - Test loss: 0.66067\n",
      "Epoch 17812 - lr: 0.01000 - Train loss: 0.57662 - Test loss: 0.66084\n",
      "Epoch 17813 - lr: 0.01000 - Train loss: 0.57751 - Test loss: 0.66037\n",
      "Epoch 17814 - lr: 0.01000 - Train loss: 0.58184 - Test loss: 0.66079\n",
      "Epoch 17815 - lr: 0.01000 - Train loss: 0.58115 - Test loss: 0.66038\n",
      "Epoch 17816 - lr: 0.01000 - Train loss: 0.58347 - Test loss: 0.66080\n",
      "Epoch 17817 - lr: 0.01000 - Train loss: 0.59188 - Test loss: 0.66099\n",
      "Epoch 17818 - lr: 0.01000 - Train loss: 0.57859 - Test loss: 0.66021\n",
      "Epoch 17819 - lr: 0.01000 - Train loss: 0.57671 - Test loss: 0.66044\n",
      "Epoch 17820 - lr: 0.01000 - Train loss: 0.57833 - Test loss: 0.65998\n",
      "Epoch 17821 - lr: 0.01000 - Train loss: 0.57744 - Test loss: 0.66034\n",
      "Epoch 17822 - lr: 0.01000 - Train loss: 0.58019 - Test loss: 0.65972\n",
      "Epoch 17823 - lr: 0.01000 - Train loss: 0.57587 - Test loss: 0.65963\n",
      "Epoch 17824 - lr: 0.01000 - Train loss: 0.58302 - Test loss: 0.66041\n",
      "Epoch 17825 - lr: 0.01000 - Train loss: 0.59897 - Test loss: 0.66069\n",
      "Epoch 17826 - lr: 0.01000 - Train loss: 0.58194 - Test loss: 0.65964\n",
      "Epoch 17827 - lr: 0.01000 - Train loss: 0.58058 - Test loss: 0.65896\n",
      "Epoch 17828 - lr: 0.01000 - Train loss: 0.57836 - Test loss: 0.65869\n",
      "Epoch 17829 - lr: 0.01000 - Train loss: 0.57541 - Test loss: 0.65923\n",
      "Epoch 17830 - lr: 0.01000 - Train loss: 0.57437 - Test loss: 0.65963\n",
      "Epoch 17831 - lr: 0.01000 - Train loss: 0.58317 - Test loss: 0.66045\n",
      "Epoch 17832 - lr: 0.01000 - Train loss: 0.59684 - Test loss: 0.66075\n",
      "Epoch 17833 - lr: 0.01000 - Train loss: 0.58147 - Test loss: 0.65976\n",
      "Epoch 17834 - lr: 0.01000 - Train loss: 0.58010 - Test loss: 0.65914\n",
      "Epoch 17835 - lr: 0.01000 - Train loss: 0.57663 - Test loss: 0.65906\n",
      "Epoch 17836 - lr: 0.01000 - Train loss: 0.58218 - Test loss: 0.65982\n",
      "Epoch 17837 - lr: 0.01000 - Train loss: 0.58275 - Test loss: 0.66024\n",
      "Epoch 17838 - lr: 0.01000 - Train loss: 0.57483 - Test loss: 0.66018\n",
      "Epoch 17839 - lr: 0.01000 - Train loss: 0.58111 - Test loss: 0.66093\n",
      "Epoch 17840 - lr: 0.01000 - Train loss: 0.59276 - Test loss: 0.66159\n",
      "Epoch 17841 - lr: 0.01000 - Train loss: 0.58674 - Test loss: 0.66171\n",
      "Epoch 17842 - lr: 0.01000 - Train loss: 0.57814 - Test loss: 0.66162\n",
      "Epoch 17843 - lr: 0.01000 - Train loss: 0.58109 - Test loss: 0.66062\n",
      "Epoch 17844 - lr: 0.01000 - Train loss: 0.57871 - Test loss: 0.66001\n",
      "Epoch 17845 - lr: 0.01000 - Train loss: 0.57513 - Test loss: 0.66028\n",
      "Epoch 17846 - lr: 0.01000 - Train loss: 0.57511 - Test loss: 0.66058\n",
      "Epoch 17847 - lr: 0.01000 - Train loss: 0.57579 - Test loss: 0.66087\n",
      "Epoch 17848 - lr: 0.01000 - Train loss: 0.57469 - Test loss: 0.66088\n",
      "Epoch 17849 - lr: 0.01000 - Train loss: 0.58096 - Test loss: 0.66157\n",
      "Epoch 17850 - lr: 0.01000 - Train loss: 0.59334 - Test loss: 0.66211\n",
      "Epoch 17851 - lr: 0.01000 - Train loss: 0.60213 - Test loss: 0.66191\n",
      "Epoch 17852 - lr: 0.01000 - Train loss: 0.58253 - Test loss: 0.66060\n",
      "Epoch 17853 - lr: 0.01000 - Train loss: 0.58021 - Test loss: 0.65976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17854 - lr: 0.01000 - Train loss: 0.57688 - Test loss: 0.65952\n",
      "Epoch 17855 - lr: 0.01000 - Train loss: 0.58154 - Test loss: 0.66017\n",
      "Epoch 17856 - lr: 0.01000 - Train loss: 0.58135 - Test loss: 0.65994\n",
      "Epoch 17857 - lr: 0.01000 - Train loss: 0.58108 - Test loss: 0.66041\n",
      "Epoch 17858 - lr: 0.01000 - Train loss: 0.58255 - Test loss: 0.65981\n",
      "Epoch 17859 - lr: 0.01000 - Train loss: 0.57563 - Test loss: 0.65970\n",
      "Epoch 17860 - lr: 0.01000 - Train loss: 0.58279 - Test loss: 0.66050\n",
      "Epoch 17861 - lr: 0.01000 - Train loss: 0.59411 - Test loss: 0.66085\n",
      "Epoch 17862 - lr: 0.01000 - Train loss: 0.58044 - Test loss: 0.66000\n",
      "Epoch 17863 - lr: 0.01000 - Train loss: 0.57727 - Test loss: 0.65970\n",
      "Epoch 17864 - lr: 0.01000 - Train loss: 0.58018 - Test loss: 0.66028\n",
      "Epoch 17865 - lr: 0.01000 - Train loss: 0.58229 - Test loss: 0.65960\n",
      "Epoch 17866 - lr: 0.01000 - Train loss: 0.58000 - Test loss: 0.65907\n",
      "Epoch 17867 - lr: 0.01000 - Train loss: 0.57670 - Test loss: 0.65905\n",
      "Epoch 17868 - lr: 0.01000 - Train loss: 0.58150 - Test loss: 0.65983\n",
      "Epoch 17869 - lr: 0.01000 - Train loss: 0.58128 - Test loss: 0.65971\n",
      "Epoch 17870 - lr: 0.01000 - Train loss: 0.58073 - Test loss: 0.66024\n",
      "Epoch 17871 - lr: 0.01000 - Train loss: 0.58250 - Test loss: 0.65964\n",
      "Epoch 17872 - lr: 0.01000 - Train loss: 0.57825 - Test loss: 0.65930\n",
      "Epoch 17873 - lr: 0.01000 - Train loss: 0.57552 - Test loss: 0.65979\n",
      "Epoch 17874 - lr: 0.01000 - Train loss: 0.57451 - Test loss: 0.66006\n",
      "Epoch 17875 - lr: 0.01000 - Train loss: 0.58134 - Test loss: 0.66091\n",
      "Epoch 17876 - lr: 0.01000 - Train loss: 0.59010 - Test loss: 0.66172\n",
      "Epoch 17877 - lr: 0.01000 - Train loss: 0.58178 - Test loss: 0.66096\n",
      "Epoch 17878 - lr: 0.01000 - Train loss: 0.57892 - Test loss: 0.66113\n",
      "Epoch 17879 - lr: 0.01000 - Train loss: 0.58159 - Test loss: 0.66026\n",
      "Epoch 17880 - lr: 0.01000 - Train loss: 0.58037 - Test loss: 0.65958\n",
      "Epoch 17881 - lr: 0.01000 - Train loss: 0.57796 - Test loss: 0.65932\n",
      "Epoch 17882 - lr: 0.01000 - Train loss: 0.57621 - Test loss: 0.65986\n",
      "Epoch 17883 - lr: 0.01000 - Train loss: 0.57732 - Test loss: 0.65978\n",
      "Epoch 17884 - lr: 0.01000 - Train loss: 0.57980 - Test loss: 0.66040\n",
      "Epoch 17885 - lr: 0.01000 - Train loss: 0.58200 - Test loss: 0.65975\n",
      "Epoch 17886 - lr: 0.01000 - Train loss: 0.58030 - Test loss: 0.65922\n",
      "Epoch 17887 - lr: 0.01000 - Train loss: 0.57802 - Test loss: 0.65906\n",
      "Epoch 17888 - lr: 0.01000 - Train loss: 0.57565 - Test loss: 0.65966\n",
      "Epoch 17889 - lr: 0.01000 - Train loss: 0.57509 - Test loss: 0.65990\n",
      "Epoch 17890 - lr: 0.01000 - Train loss: 0.58181 - Test loss: 0.66079\n",
      "Epoch 17891 - lr: 0.01000 - Train loss: 0.58268 - Test loss: 0.66178\n",
      "Epoch 17892 - lr: 0.01000 - Train loss: 0.57438 - Test loss: 0.66161\n",
      "Epoch 17893 - lr: 0.01000 - Train loss: 0.58302 - Test loss: 0.66214\n",
      "Epoch 17894 - lr: 0.01000 - Train loss: 0.60172 - Test loss: 0.66221\n",
      "Epoch 17895 - lr: 0.01000 - Train loss: 0.58231 - Test loss: 0.66098\n",
      "Epoch 17896 - lr: 0.01000 - Train loss: 0.58021 - Test loss: 0.66015\n",
      "Epoch 17897 - lr: 0.01000 - Train loss: 0.57733 - Test loss: 0.65987\n",
      "Epoch 17898 - lr: 0.01000 - Train loss: 0.57880 - Test loss: 0.66043\n",
      "Epoch 17899 - lr: 0.01000 - Train loss: 0.58140 - Test loss: 0.65982\n",
      "Epoch 17900 - lr: 0.01000 - Train loss: 0.58028 - Test loss: 0.65932\n",
      "Epoch 17901 - lr: 0.01000 - Train loss: 0.57810 - Test loss: 0.65917\n",
      "Epoch 17902 - lr: 0.01000 - Train loss: 0.57518 - Test loss: 0.65976\n",
      "Epoch 17903 - lr: 0.01000 - Train loss: 0.57416 - Test loss: 0.66021\n",
      "Epoch 17904 - lr: 0.01000 - Train loss: 0.58299 - Test loss: 0.66103\n",
      "Epoch 17905 - lr: 0.01000 - Train loss: 0.58833 - Test loss: 0.66157\n",
      "Epoch 17906 - lr: 0.01000 - Train loss: 0.57434 - Test loss: 0.66152\n",
      "Epoch 17907 - lr: 0.01000 - Train loss: 0.58303 - Test loss: 0.66199\n",
      "Epoch 17908 - lr: 0.01000 - Train loss: 0.58607 - Test loss: 0.66269\n",
      "Epoch 17909 - lr: 0.01000 - Train loss: 0.58138 - Test loss: 0.66146\n",
      "Epoch 17910 - lr: 0.01000 - Train loss: 0.57976 - Test loss: 0.66067\n",
      "Epoch 17911 - lr: 0.01000 - Train loss: 0.57542 - Test loss: 0.66054\n",
      "Epoch 17912 - lr: 0.01000 - Train loss: 0.58258 - Test loss: 0.66127\n",
      "Epoch 17913 - lr: 0.01000 - Train loss: 0.59148 - Test loss: 0.66165\n",
      "Epoch 17914 - lr: 0.01000 - Train loss: 0.57797 - Test loss: 0.66102\n",
      "Epoch 17915 - lr: 0.01000 - Train loss: 0.57738 - Test loss: 0.66134\n",
      "Epoch 17916 - lr: 0.01000 - Train loss: 0.58022 - Test loss: 0.66070\n",
      "Epoch 17917 - lr: 0.01000 - Train loss: 0.57692 - Test loss: 0.66045\n",
      "Epoch 17918 - lr: 0.01000 - Train loss: 0.58086 - Test loss: 0.66103\n",
      "Epoch 17919 - lr: 0.01000 - Train loss: 0.58239 - Test loss: 0.66047\n",
      "Epoch 17920 - lr: 0.01000 - Train loss: 0.57593 - Test loss: 0.66032\n",
      "Epoch 17921 - lr: 0.01000 - Train loss: 0.58287 - Test loss: 0.66106\n",
      "Epoch 17922 - lr: 0.01000 - Train loss: 0.58204 - Test loss: 0.66196\n",
      "Epoch 17923 - lr: 0.01000 - Train loss: 0.58057 - Test loss: 0.66241\n",
      "Epoch 17924 - lr: 0.01000 - Train loss: 0.59083 - Test loss: 0.66272\n",
      "Epoch 17925 - lr: 0.01000 - Train loss: 0.58025 - Test loss: 0.66218\n",
      "Epoch 17926 - lr: 0.01000 - Train loss: 0.58139 - Test loss: 0.66251\n",
      "Epoch 17927 - lr: 0.01000 - Train loss: 0.58436 - Test loss: 0.66313\n",
      "Epoch 17928 - lr: 0.01000 - Train loss: 0.57953 - Test loss: 0.66202\n",
      "Epoch 17929 - lr: 0.01000 - Train loss: 0.57416 - Test loss: 0.66187\n",
      "Epoch 17930 - lr: 0.01000 - Train loss: 0.58291 - Test loss: 0.66239\n",
      "Epoch 17931 - lr: 0.01000 - Train loss: 0.60350 - Test loss: 0.66248\n",
      "Epoch 17932 - lr: 0.01000 - Train loss: 0.58250 - Test loss: 0.66128\n",
      "Epoch 17933 - lr: 0.01000 - Train loss: 0.57954 - Test loss: 0.66051\n",
      "Epoch 17934 - lr: 0.01000 - Train loss: 0.57502 - Test loss: 0.66048\n",
      "Epoch 17935 - lr: 0.01000 - Train loss: 0.58203 - Test loss: 0.66126\n",
      "Epoch 17936 - lr: 0.01000 - Train loss: 0.58371 - Test loss: 0.66201\n",
      "Epoch 17937 - lr: 0.01000 - Train loss: 0.58352 - Test loss: 0.66237\n",
      "Epoch 17938 - lr: 0.01000 - Train loss: 0.60677 - Test loss: 0.66249\n",
      "Epoch 17939 - lr: 0.01000 - Train loss: 0.58263 - Test loss: 0.66137\n",
      "Epoch 17940 - lr: 0.01000 - Train loss: 0.57688 - Test loss: 0.66087\n",
      "Epoch 17941 - lr: 0.01000 - Train loss: 0.58118 - Test loss: 0.66134\n",
      "Epoch 17942 - lr: 0.01000 - Train loss: 0.58215 - Test loss: 0.66081\n",
      "Epoch 17943 - lr: 0.01000 - Train loss: 0.57415 - Test loss: 0.66093\n",
      "Epoch 17944 - lr: 0.01000 - Train loss: 0.58270 - Test loss: 0.66154\n",
      "Epoch 17945 - lr: 0.01000 - Train loss: 0.59298 - Test loss: 0.66221\n",
      "Epoch 17946 - lr: 0.01000 - Train loss: 0.59355 - Test loss: 0.66218\n",
      "Epoch 17947 - lr: 0.01000 - Train loss: 0.58003 - Test loss: 0.66120\n",
      "Epoch 17948 - lr: 0.01000 - Train loss: 0.57607 - Test loss: 0.66090\n",
      "Epoch 17949 - lr: 0.01000 - Train loss: 0.58308 - Test loss: 0.66150\n",
      "Epoch 17950 - lr: 0.01000 - Train loss: 0.58488 - Test loss: 0.66211\n",
      "Epoch 17951 - lr: 0.01000 - Train loss: 0.58252 - Test loss: 0.66232\n",
      "Epoch 17952 - lr: 0.01000 - Train loss: 0.58589 - Test loss: 0.66249\n",
      "Epoch 17953 - lr: 0.01000 - Train loss: 0.58102 - Test loss: 0.66138\n",
      "Epoch 17954 - lr: 0.01000 - Train loss: 0.57930 - Test loss: 0.66069\n",
      "Epoch 17955 - lr: 0.01000 - Train loss: 0.57440 - Test loss: 0.66073\n",
      "Epoch 17956 - lr: 0.01000 - Train loss: 0.58129 - Test loss: 0.66152\n",
      "Epoch 17957 - lr: 0.01000 - Train loss: 0.58839 - Test loss: 0.66235\n",
      "Epoch 17958 - lr: 0.01000 - Train loss: 0.58239 - Test loss: 0.66122\n",
      "Epoch 17959 - lr: 0.01000 - Train loss: 0.58007 - Test loss: 0.66045\n",
      "Epoch 17960 - lr: 0.01000 - Train loss: 0.57716 - Test loss: 0.66019\n",
      "Epoch 17961 - lr: 0.01000 - Train loss: 0.57902 - Test loss: 0.66075\n",
      "Epoch 17962 - lr: 0.01000 - Train loss: 0.58145 - Test loss: 0.66015\n",
      "Epoch 17963 - lr: 0.01000 - Train loss: 0.58039 - Test loss: 0.65962\n",
      "Epoch 17964 - lr: 0.01000 - Train loss: 0.57884 - Test loss: 0.65939\n",
      "Epoch 17965 - lr: 0.01000 - Train loss: 0.57414 - Test loss: 0.65978\n",
      "Epoch 17966 - lr: 0.01000 - Train loss: 0.58184 - Test loss: 0.66075\n",
      "Epoch 17967 - lr: 0.01000 - Train loss: 0.58286 - Test loss: 0.66185\n",
      "Epoch 17968 - lr: 0.01000 - Train loss: 0.57462 - Test loss: 0.66167\n",
      "Epoch 17969 - lr: 0.01000 - Train loss: 0.58115 - Test loss: 0.66231\n",
      "Epoch 17970 - lr: 0.01000 - Train loss: 0.59148 - Test loss: 0.66295\n",
      "Epoch 17971 - lr: 0.01000 - Train loss: 0.58848 - Test loss: 0.66293\n",
      "Epoch 17972 - lr: 0.01000 - Train loss: 0.58225 - Test loss: 0.66163\n",
      "Epoch 17973 - lr: 0.01000 - Train loss: 0.58028 - Test loss: 0.66073\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17974 - lr: 0.01000 - Train loss: 0.57800 - Test loss: 0.66032\n",
      "Epoch 17975 - lr: 0.01000 - Train loss: 0.57531 - Test loss: 0.66072\n",
      "Epoch 17976 - lr: 0.01000 - Train loss: 0.57440 - Test loss: 0.66092\n",
      "Epoch 17977 - lr: 0.01000 - Train loss: 0.58131 - Test loss: 0.66172\n",
      "Epoch 17978 - lr: 0.01000 - Train loss: 0.58861 - Test loss: 0.66256\n",
      "Epoch 17979 - lr: 0.01000 - Train loss: 0.58246 - Test loss: 0.66143\n",
      "Epoch 17980 - lr: 0.01000 - Train loss: 0.57973 - Test loss: 0.66068\n",
      "Epoch 17981 - lr: 0.01000 - Train loss: 0.57599 - Test loss: 0.66054\n",
      "Epoch 17982 - lr: 0.01000 - Train loss: 0.58275 - Test loss: 0.66123\n",
      "Epoch 17983 - lr: 0.01000 - Train loss: 0.58609 - Test loss: 0.66216\n",
      "Epoch 17984 - lr: 0.01000 - Train loss: 0.58118 - Test loss: 0.66115\n",
      "Epoch 17985 - lr: 0.01000 - Train loss: 0.57995 - Test loss: 0.66048\n",
      "Epoch 17986 - lr: 0.01000 - Train loss: 0.57705 - Test loss: 0.66028\n",
      "Epoch 17987 - lr: 0.01000 - Train loss: 0.57903 - Test loss: 0.66086\n",
      "Epoch 17988 - lr: 0.01000 - Train loss: 0.58140 - Test loss: 0.66029\n",
      "Epoch 17989 - lr: 0.01000 - Train loss: 0.58036 - Test loss: 0.65978\n",
      "Epoch 17990 - lr: 0.01000 - Train loss: 0.57890 - Test loss: 0.65954\n",
      "Epoch 17991 - lr: 0.01000 - Train loss: 0.57433 - Test loss: 0.65989\n",
      "Epoch 17992 - lr: 0.01000 - Train loss: 0.58152 - Test loss: 0.66087\n",
      "Epoch 17993 - lr: 0.01000 - Train loss: 0.58845 - Test loss: 0.66190\n",
      "Epoch 17994 - lr: 0.01000 - Train loss: 0.58217 - Test loss: 0.66096\n",
      "Epoch 17995 - lr: 0.01000 - Train loss: 0.57988 - Test loss: 0.66033\n",
      "Epoch 17996 - lr: 0.01000 - Train loss: 0.57712 - Test loss: 0.66016\n",
      "Epoch 17997 - lr: 0.01000 - Train loss: 0.57825 - Test loss: 0.66076\n",
      "Epoch 17998 - lr: 0.01000 - Train loss: 0.58092 - Test loss: 0.66028\n",
      "Epoch 17999 - lr: 0.01000 - Train loss: 0.57994 - Test loss: 0.65986\n",
      "Epoch 18000 - lr: 0.01000 - Train loss: 0.57769 - Test loss: 0.65976\n",
      "Epoch 18001 - lr: 0.01000 - Train loss: 0.57553 - Test loss: 0.66037\n",
      "Epoch 18002 - lr: 0.01000 - Train loss: 0.57540 - Test loss: 0.66058\n",
      "Epoch 18003 - lr: 0.01000 - Train loss: 0.58269 - Test loss: 0.66142\n",
      "Epoch 18004 - lr: 0.01000 - Train loss: 0.58520 - Test loss: 0.66218\n",
      "Epoch 18005 - lr: 0.01000 - Train loss: 0.58109 - Test loss: 0.66244\n",
      "Epoch 18006 - lr: 0.01000 - Train loss: 0.58240 - Test loss: 0.66171\n",
      "Epoch 18007 - lr: 0.01000 - Train loss: 0.57520 - Test loss: 0.66148\n",
      "Epoch 18008 - lr: 0.01000 - Train loss: 0.58250 - Test loss: 0.66214\n",
      "Epoch 18009 - lr: 0.01000 - Train loss: 0.59025 - Test loss: 0.66251\n",
      "Epoch 18010 - lr: 0.01000 - Train loss: 0.57614 - Test loss: 0.66203\n",
      "Epoch 18011 - lr: 0.01000 - Train loss: 0.58308 - Test loss: 0.66251\n",
      "Epoch 18012 - lr: 0.01000 - Train loss: 0.58295 - Test loss: 0.66316\n",
      "Epoch 18013 - lr: 0.01000 - Train loss: 0.58258 - Test loss: 0.66344\n",
      "Epoch 18014 - lr: 0.01000 - Train loss: 0.59642 - Test loss: 0.66343\n",
      "Epoch 18015 - lr: 0.01000 - Train loss: 0.58115 - Test loss: 0.66221\n",
      "Epoch 18016 - lr: 0.01000 - Train loss: 0.57981 - Test loss: 0.66139\n",
      "Epoch 18017 - lr: 0.01000 - Train loss: 0.57646 - Test loss: 0.66111\n",
      "Epoch 18018 - lr: 0.01000 - Train loss: 0.58135 - Test loss: 0.66168\n",
      "Epoch 18019 - lr: 0.01000 - Train loss: 0.58134 - Test loss: 0.66142\n",
      "Epoch 18020 - lr: 0.01000 - Train loss: 0.57875 - Test loss: 0.66176\n",
      "Epoch 18021 - lr: 0.01000 - Train loss: 0.58126 - Test loss: 0.66108\n",
      "Epoch 18022 - lr: 0.01000 - Train loss: 0.58025 - Test loss: 0.66049\n",
      "Epoch 18023 - lr: 0.01000 - Train loss: 0.57861 - Test loss: 0.66020\n",
      "Epoch 18024 - lr: 0.01000 - Train loss: 0.57389 - Test loss: 0.66057\n",
      "Epoch 18025 - lr: 0.01000 - Train loss: 0.58225 - Test loss: 0.66147\n",
      "Epoch 18026 - lr: 0.01000 - Train loss: 0.58195 - Test loss: 0.66247\n",
      "Epoch 18027 - lr: 0.01000 - Train loss: 0.58094 - Test loss: 0.66295\n",
      "Epoch 18028 - lr: 0.01000 - Train loss: 0.59329 - Test loss: 0.66348\n",
      "Epoch 18029 - lr: 0.01000 - Train loss: 0.59686 - Test loss: 0.66331\n",
      "Epoch 18030 - lr: 0.01000 - Train loss: 0.58115 - Test loss: 0.66209\n",
      "Epoch 18031 - lr: 0.01000 - Train loss: 0.57999 - Test loss: 0.66127\n",
      "Epoch 18032 - lr: 0.01000 - Train loss: 0.57749 - Test loss: 0.66092\n",
      "Epoch 18033 - lr: 0.01000 - Train loss: 0.57645 - Test loss: 0.66136\n",
      "Epoch 18034 - lr: 0.01000 - Train loss: 0.57882 - Test loss: 0.66104\n",
      "Epoch 18035 - lr: 0.01000 - Train loss: 0.57392 - Test loss: 0.66129\n",
      "Epoch 18036 - lr: 0.01000 - Train loss: 0.58236 - Test loss: 0.66210\n",
      "Epoch 18037 - lr: 0.01000 - Train loss: 0.58607 - Test loss: 0.66273\n",
      "Epoch 18038 - lr: 0.01000 - Train loss: 0.57842 - Test loss: 0.66285\n",
      "Epoch 18039 - lr: 0.01000 - Train loss: 0.58115 - Test loss: 0.66198\n",
      "Epoch 18040 - lr: 0.01000 - Train loss: 0.58000 - Test loss: 0.66128\n",
      "Epoch 18041 - lr: 0.01000 - Train loss: 0.57755 - Test loss: 0.66097\n",
      "Epoch 18042 - lr: 0.01000 - Train loss: 0.57605 - Test loss: 0.66143\n",
      "Epoch 18043 - lr: 0.01000 - Train loss: 0.57772 - Test loss: 0.66124\n",
      "Epoch 18044 - lr: 0.01000 - Train loss: 0.57594 - Test loss: 0.66170\n",
      "Epoch 18045 - lr: 0.01000 - Train loss: 0.57720 - Test loss: 0.66154\n",
      "Epoch 18046 - lr: 0.01000 - Train loss: 0.57843 - Test loss: 0.66204\n",
      "Epoch 18047 - lr: 0.01000 - Train loss: 0.58104 - Test loss: 0.66142\n",
      "Epoch 18048 - lr: 0.01000 - Train loss: 0.58003 - Test loss: 0.66086\n",
      "Epoch 18049 - lr: 0.01000 - Train loss: 0.57809 - Test loss: 0.66062\n",
      "Epoch 18050 - lr: 0.01000 - Train loss: 0.57406 - Test loss: 0.66109\n",
      "Epoch 18051 - lr: 0.01000 - Train loss: 0.57891 - Test loss: 0.66175\n",
      "Epoch 18052 - lr: 0.01000 - Train loss: 0.58125 - Test loss: 0.66120\n",
      "Epoch 18053 - lr: 0.01000 - Train loss: 0.58019 - Test loss: 0.66069\n",
      "Epoch 18054 - lr: 0.01000 - Train loss: 0.57872 - Test loss: 0.66043\n",
      "Epoch 18055 - lr: 0.01000 - Train loss: 0.57414 - Test loss: 0.66076\n",
      "Epoch 18056 - lr: 0.01000 - Train loss: 0.58137 - Test loss: 0.66170\n",
      "Epoch 18057 - lr: 0.01000 - Train loss: 0.59202 - Test loss: 0.66263\n",
      "Epoch 18058 - lr: 0.01000 - Train loss: 0.59262 - Test loss: 0.66315\n",
      "Epoch 18059 - lr: 0.01000 - Train loss: 0.58136 - Test loss: 0.66372\n",
      "Epoch 18060 - lr: 0.01000 - Train loss: 0.58238 - Test loss: 0.66377\n",
      "Epoch 18061 - lr: 0.01000 - Train loss: 0.58400 - Test loss: 0.66380\n",
      "Epoch 18062 - lr: 0.01000 - Train loss: 0.57846 - Test loss: 0.66289\n",
      "Epoch 18063 - lr: 0.01000 - Train loss: 0.57451 - Test loss: 0.66291\n",
      "Epoch 18064 - lr: 0.01000 - Train loss: 0.57561 - Test loss: 0.66309\n",
      "Epoch 18065 - lr: 0.01000 - Train loss: 0.57563 - Test loss: 0.66282\n",
      "Epoch 18066 - lr: 0.01000 - Train loss: 0.58314 - Test loss: 0.66333\n",
      "Epoch 18067 - lr: 0.01000 - Train loss: 0.59791 - Test loss: 0.66350\n",
      "Epoch 18068 - lr: 0.01000 - Train loss: 0.58125 - Test loss: 0.66237\n",
      "Epoch 18069 - lr: 0.01000 - Train loss: 0.58016 - Test loss: 0.66158\n",
      "Epoch 18070 - lr: 0.01000 - Train loss: 0.57834 - Test loss: 0.66116\n",
      "Epoch 18071 - lr: 0.01000 - Train loss: 0.57374 - Test loss: 0.66147\n",
      "Epoch 18072 - lr: 0.01000 - Train loss: 0.58261 - Test loss: 0.66221\n",
      "Epoch 18073 - lr: 0.01000 - Train loss: 0.58926 - Test loss: 0.66312\n",
      "Epoch 18074 - lr: 0.01000 - Train loss: 0.58238 - Test loss: 0.66219\n",
      "Epoch 18075 - lr: 0.01000 - Train loss: 0.57758 - Test loss: 0.66169\n",
      "Epoch 18076 - lr: 0.01000 - Train loss: 0.57598 - Test loss: 0.66206\n",
      "Epoch 18077 - lr: 0.01000 - Train loss: 0.57761 - Test loss: 0.66180\n",
      "Epoch 18078 - lr: 0.01000 - Train loss: 0.57613 - Test loss: 0.66220\n",
      "Epoch 18079 - lr: 0.01000 - Train loss: 0.57799 - Test loss: 0.66191\n",
      "Epoch 18080 - lr: 0.01000 - Train loss: 0.57484 - Test loss: 0.66225\n",
      "Epoch 18081 - lr: 0.01000 - Train loss: 0.57382 - Test loss: 0.66251\n",
      "Epoch 18082 - lr: 0.01000 - Train loss: 0.58302 - Test loss: 0.66313\n",
      "Epoch 18083 - lr: 0.01000 - Train loss: 0.58478 - Test loss: 0.66373\n",
      "Epoch 18084 - lr: 0.01000 - Train loss: 0.58221 - Test loss: 0.66388\n",
      "Epoch 18085 - lr: 0.01000 - Train loss: 0.58165 - Test loss: 0.66389\n",
      "Epoch 18086 - lr: 0.01000 - Train loss: 0.57490 - Test loss: 0.66374\n",
      "Epoch 18087 - lr: 0.01000 - Train loss: 0.57407 - Test loss: 0.66371\n",
      "Epoch 18088 - lr: 0.01000 - Train loss: 0.58160 - Test loss: 0.66398\n",
      "Epoch 18089 - lr: 0.01000 - Train loss: 0.58106 - Test loss: 0.66346\n",
      "Epoch 18090 - lr: 0.01000 - Train loss: 0.58210 - Test loss: 0.66364\n",
      "Epoch 18091 - lr: 0.01000 - Train loss: 0.58178 - Test loss: 0.66371\n",
      "Epoch 18092 - lr: 0.01000 - Train loss: 0.57422 - Test loss: 0.66356\n",
      "Epoch 18093 - lr: 0.01000 - Train loss: 0.57913 - Test loss: 0.66377\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18094 - lr: 0.01000 - Train loss: 0.58156 - Test loss: 0.66280\n",
      "Epoch 18095 - lr: 0.01000 - Train loss: 0.58031 - Test loss: 0.66197\n",
      "Epoch 18096 - lr: 0.01000 - Train loss: 0.57873 - Test loss: 0.66148\n",
      "Epoch 18097 - lr: 0.01000 - Train loss: 0.57398 - Test loss: 0.66164\n",
      "Epoch 18098 - lr: 0.01000 - Train loss: 0.58123 - Test loss: 0.66247\n",
      "Epoch 18099 - lr: 0.01000 - Train loss: 0.59098 - Test loss: 0.66334\n",
      "Epoch 18100 - lr: 0.01000 - Train loss: 0.58080 - Test loss: 0.66329\n",
      "Epoch 18101 - lr: 0.01000 - Train loss: 0.57934 - Test loss: 0.66341\n",
      "Epoch 18102 - lr: 0.01000 - Train loss: 0.58157 - Test loss: 0.66250\n",
      "Epoch 18103 - lr: 0.01000 - Train loss: 0.58025 - Test loss: 0.66173\n",
      "Epoch 18104 - lr: 0.01000 - Train loss: 0.57879 - Test loss: 0.66127\n",
      "Epoch 18105 - lr: 0.01000 - Train loss: 0.57425 - Test loss: 0.66143\n",
      "Epoch 18106 - lr: 0.01000 - Train loss: 0.58142 - Test loss: 0.66227\n",
      "Epoch 18107 - lr: 0.01000 - Train loss: 0.58864 - Test loss: 0.66323\n",
      "Epoch 18108 - lr: 0.01000 - Train loss: 0.58210 - Test loss: 0.66227\n",
      "Epoch 18109 - lr: 0.01000 - Train loss: 0.57939 - Test loss: 0.66162\n",
      "Epoch 18110 - lr: 0.01000 - Train loss: 0.57594 - Test loss: 0.66150\n",
      "Epoch 18111 - lr: 0.01000 - Train loss: 0.58168 - Test loss: 0.66215\n",
      "Epoch 18112 - lr: 0.01000 - Train loss: 0.57982 - Test loss: 0.66244\n",
      "Epoch 18113 - lr: 0.01000 - Train loss: 0.58271 - Test loss: 0.66295\n",
      "Epoch 18114 - lr: 0.01000 - Train loss: 0.58959 - Test loss: 0.66336\n",
      "Epoch 18115 - lr: 0.01000 - Train loss: 0.57512 - Test loss: 0.66298\n",
      "Epoch 18116 - lr: 0.01000 - Train loss: 0.58253 - Test loss: 0.66351\n",
      "Epoch 18117 - lr: 0.01000 - Train loss: 0.59199 - Test loss: 0.66376\n",
      "Epoch 18118 - lr: 0.01000 - Train loss: 0.57856 - Test loss: 0.66293\n",
      "Epoch 18119 - lr: 0.01000 - Train loss: 0.57381 - Test loss: 0.66297\n",
      "Epoch 18120 - lr: 0.01000 - Train loss: 0.58213 - Test loss: 0.66343\n",
      "Epoch 18121 - lr: 0.01000 - Train loss: 0.58348 - Test loss: 0.66367\n",
      "Epoch 18122 - lr: 0.01000 - Train loss: 0.57745 - Test loss: 0.66300\n",
      "Epoch 18123 - lr: 0.01000 - Train loss: 0.57730 - Test loss: 0.66324\n",
      "Epoch 18124 - lr: 0.01000 - Train loss: 0.58025 - Test loss: 0.66254\n",
      "Epoch 18125 - lr: 0.01000 - Train loss: 0.57834 - Test loss: 0.66205\n",
      "Epoch 18126 - lr: 0.01000 - Train loss: 0.57365 - Test loss: 0.66228\n",
      "Epoch 18127 - lr: 0.01000 - Train loss: 0.58262 - Test loss: 0.66294\n",
      "Epoch 18128 - lr: 0.01000 - Train loss: 0.58868 - Test loss: 0.66379\n",
      "Epoch 18129 - lr: 0.01000 - Train loss: 0.58214 - Test loss: 0.66275\n",
      "Epoch 18130 - lr: 0.01000 - Train loss: 0.57935 - Test loss: 0.66204\n",
      "Epoch 18131 - lr: 0.01000 - Train loss: 0.57578 - Test loss: 0.66188\n",
      "Epoch 18132 - lr: 0.01000 - Train loss: 0.58205 - Test loss: 0.66252\n",
      "Epoch 18133 - lr: 0.01000 - Train loss: 0.58567 - Test loss: 0.66303\n",
      "Epoch 18134 - lr: 0.01000 - Train loss: 0.58049 - Test loss: 0.66220\n",
      "Epoch 18135 - lr: 0.01000 - Train loss: 0.57922 - Test loss: 0.66167\n",
      "Epoch 18136 - lr: 0.01000 - Train loss: 0.57564 - Test loss: 0.66163\n",
      "Epoch 18137 - lr: 0.01000 - Train loss: 0.58212 - Test loss: 0.66233\n",
      "Epoch 18138 - lr: 0.01000 - Train loss: 0.58844 - Test loss: 0.66297\n",
      "Epoch 18139 - lr: 0.01000 - Train loss: 0.58172 - Test loss: 0.66211\n",
      "Epoch 18140 - lr: 0.01000 - Train loss: 0.57980 - Test loss: 0.66149\n",
      "Epoch 18141 - lr: 0.01000 - Train loss: 0.57794 - Test loss: 0.66123\n",
      "Epoch 18142 - lr: 0.01000 - Train loss: 0.57377 - Test loss: 0.66165\n",
      "Epoch 18143 - lr: 0.01000 - Train loss: 0.58021 - Test loss: 0.66234\n",
      "Epoch 18144 - lr: 0.01000 - Train loss: 0.58191 - Test loss: 0.66186\n",
      "Epoch 18145 - lr: 0.01000 - Train loss: 0.57898 - Test loss: 0.66143\n",
      "Epoch 18146 - lr: 0.01000 - Train loss: 0.57500 - Test loss: 0.66153\n",
      "Epoch 18147 - lr: 0.01000 - Train loss: 0.58238 - Test loss: 0.66234\n",
      "Epoch 18148 - lr: 0.01000 - Train loss: 0.58263 - Test loss: 0.66342\n",
      "Epoch 18149 - lr: 0.01000 - Train loss: 0.57432 - Test loss: 0.66323\n",
      "Epoch 18150 - lr: 0.01000 - Train loss: 0.58115 - Test loss: 0.66384\n",
      "Epoch 18151 - lr: 0.01000 - Train loss: 0.59137 - Test loss: 0.66451\n",
      "Epoch 18152 - lr: 0.01000 - Train loss: 0.58524 - Test loss: 0.66445\n",
      "Epoch 18153 - lr: 0.01000 - Train loss: 0.58023 - Test loss: 0.66334\n",
      "Epoch 18154 - lr: 0.01000 - Train loss: 0.57812 - Test loss: 0.66270\n",
      "Epoch 18155 - lr: 0.01000 - Train loss: 0.57385 - Test loss: 0.66286\n",
      "Epoch 18156 - lr: 0.01000 - Train loss: 0.57927 - Test loss: 0.66330\n",
      "Epoch 18157 - lr: 0.01000 - Train loss: 0.58142 - Test loss: 0.66256\n",
      "Epoch 18158 - lr: 0.01000 - Train loss: 0.58011 - Test loss: 0.66189\n",
      "Epoch 18159 - lr: 0.01000 - Train loss: 0.57879 - Test loss: 0.66149\n",
      "Epoch 18160 - lr: 0.01000 - Train loss: 0.57463 - Test loss: 0.66164\n",
      "Epoch 18161 - lr: 0.01000 - Train loss: 0.58210 - Test loss: 0.66247\n",
      "Epoch 18162 - lr: 0.01000 - Train loss: 0.58162 - Test loss: 0.66358\n",
      "Epoch 18163 - lr: 0.01000 - Train loss: 0.57643 - Test loss: 0.66370\n",
      "Epoch 18164 - lr: 0.01000 - Train loss: 0.57879 - Test loss: 0.66314\n",
      "Epoch 18165 - lr: 0.01000 - Train loss: 0.57371 - Test loss: 0.66317\n",
      "Epoch 18166 - lr: 0.01000 - Train loss: 0.58172 - Test loss: 0.66381\n",
      "Epoch 18167 - lr: 0.01000 - Train loss: 0.58147 - Test loss: 0.66467\n",
      "Epoch 18168 - lr: 0.01000 - Train loss: 0.57795 - Test loss: 0.66462\n",
      "Epoch 18169 - lr: 0.01000 - Train loss: 0.58079 - Test loss: 0.66366\n",
      "Epoch 18170 - lr: 0.01000 - Train loss: 0.57951 - Test loss: 0.66288\n",
      "Epoch 18171 - lr: 0.01000 - Train loss: 0.57635 - Test loss: 0.66257\n",
      "Epoch 18172 - lr: 0.01000 - Train loss: 0.57996 - Test loss: 0.66306\n",
      "Epoch 18173 - lr: 0.01000 - Train loss: 0.58179 - Test loss: 0.66245\n",
      "Epoch 18174 - lr: 0.01000 - Train loss: 0.57932 - Test loss: 0.66191\n",
      "Epoch 18175 - lr: 0.01000 - Train loss: 0.57637 - Test loss: 0.66181\n",
      "Epoch 18176 - lr: 0.01000 - Train loss: 0.57911 - Test loss: 0.66243\n",
      "Epoch 18177 - lr: 0.01000 - Train loss: 0.58114 - Test loss: 0.66196\n",
      "Epoch 18178 - lr: 0.01000 - Train loss: 0.57998 - Test loss: 0.66148\n",
      "Epoch 18179 - lr: 0.01000 - Train loss: 0.57880 - Test loss: 0.66123\n",
      "Epoch 18180 - lr: 0.01000 - Train loss: 0.57506 - Test loss: 0.66142\n",
      "Epoch 18181 - lr: 0.01000 - Train loss: 0.58216 - Test loss: 0.66228\n",
      "Epoch 18182 - lr: 0.01000 - Train loss: 0.59209 - Test loss: 0.66325\n",
      "Epoch 18183 - lr: 0.01000 - Train loss: 0.59131 - Test loss: 0.66387\n",
      "Epoch 18184 - lr: 0.01000 - Train loss: 0.58379 - Test loss: 0.66395\n",
      "Epoch 18185 - lr: 0.01000 - Train loss: 0.57824 - Test loss: 0.66322\n",
      "Epoch 18186 - lr: 0.01000 - Train loss: 0.57370 - Test loss: 0.66332\n",
      "Epoch 18187 - lr: 0.01000 - Train loss: 0.58107 - Test loss: 0.66377\n",
      "Epoch 18188 - lr: 0.01000 - Train loss: 0.58168 - Test loss: 0.66329\n",
      "Epoch 18189 - lr: 0.01000 - Train loss: 0.57398 - Test loss: 0.66335\n",
      "Epoch 18190 - lr: 0.01000 - Train loss: 0.57719 - Test loss: 0.66370\n",
      "Epoch 18191 - lr: 0.01000 - Train loss: 0.58010 - Test loss: 0.66306\n",
      "Epoch 18192 - lr: 0.01000 - Train loss: 0.57834 - Test loss: 0.66258\n",
      "Epoch 18193 - lr: 0.01000 - Train loss: 0.57349 - Test loss: 0.66278\n",
      "Epoch 18194 - lr: 0.01000 - Train loss: 0.58221 - Test loss: 0.66350\n",
      "Epoch 18195 - lr: 0.01000 - Train loss: 0.58191 - Test loss: 0.66438\n",
      "Epoch 18196 - lr: 0.01000 - Train loss: 0.58095 - Test loss: 0.66475\n",
      "Epoch 18197 - lr: 0.01000 - Train loss: 0.59286 - Test loss: 0.66527\n",
      "Epoch 18198 - lr: 0.01000 - Train loss: 0.58491 - Test loss: 0.66539\n",
      "Epoch 18199 - lr: 0.01000 - Train loss: 0.58152 - Test loss: 0.66529\n",
      "Epoch 18200 - lr: 0.01000 - Train loss: 0.58117 - Test loss: 0.66460\n",
      "Epoch 18201 - lr: 0.01000 - Train loss: 0.58029 - Test loss: 0.66462\n",
      "Epoch 18202 - lr: 0.01000 - Train loss: 0.58210 - Test loss: 0.66368\n",
      "Epoch 18203 - lr: 0.01000 - Train loss: 0.57892 - Test loss: 0.66293\n",
      "Epoch 18204 - lr: 0.01000 - Train loss: 0.57454 - Test loss: 0.66284\n",
      "Epoch 18205 - lr: 0.01000 - Train loss: 0.58209 - Test loss: 0.66351\n",
      "Epoch 18206 - lr: 0.01000 - Train loss: 0.58252 - Test loss: 0.66433\n",
      "Epoch 18207 - lr: 0.01000 - Train loss: 0.58222 - Test loss: 0.66468\n",
      "Epoch 18208 - lr: 0.01000 - Train loss: 0.58855 - Test loss: 0.66494\n",
      "Epoch 18209 - lr: 0.01000 - Train loss: 0.57378 - Test loss: 0.66460\n",
      "Epoch 18210 - lr: 0.01000 - Train loss: 0.58181 - Test loss: 0.66502\n",
      "Epoch 18211 - lr: 0.01000 - Train loss: 0.58247 - Test loss: 0.66555\n",
      "Epoch 18212 - lr: 0.01000 - Train loss: 0.58201 - Test loss: 0.66568\n",
      "Epoch 18213 - lr: 0.01000 - Train loss: 0.58752 - Test loss: 0.66577\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18214 - lr: 0.01000 - Train loss: 0.57436 - Test loss: 0.66539\n",
      "Epoch 18215 - lr: 0.01000 - Train loss: 0.57609 - Test loss: 0.66532\n",
      "Epoch 18216 - lr: 0.01000 - Train loss: 0.57807 - Test loss: 0.66456\n",
      "Epoch 18217 - lr: 0.01000 - Train loss: 0.57475 - Test loss: 0.66456\n",
      "Epoch 18218 - lr: 0.01000 - Train loss: 0.57362 - Test loss: 0.66450\n",
      "Epoch 18219 - lr: 0.01000 - Train loss: 0.58263 - Test loss: 0.66497\n",
      "Epoch 18220 - lr: 0.01000 - Train loss: 0.59670 - Test loss: 0.66508\n",
      "Epoch 18221 - lr: 0.01000 - Train loss: 0.58080 - Test loss: 0.66392\n",
      "Epoch 18222 - lr: 0.01000 - Train loss: 0.57974 - Test loss: 0.66310\n",
      "Epoch 18223 - lr: 0.01000 - Train loss: 0.57763 - Test loss: 0.66267\n",
      "Epoch 18224 - lr: 0.01000 - Train loss: 0.57413 - Test loss: 0.66299\n",
      "Epoch 18225 - lr: 0.01000 - Train loss: 0.57461 - Test loss: 0.66340\n",
      "Epoch 18226 - lr: 0.01000 - Train loss: 0.57355 - Test loss: 0.66364\n",
      "Epoch 18227 - lr: 0.01000 - Train loss: 0.58274 - Test loss: 0.66429\n",
      "Epoch 18228 - lr: 0.01000 - Train loss: 0.59033 - Test loss: 0.66466\n",
      "Epoch 18229 - lr: 0.01000 - Train loss: 0.57622 - Test loss: 0.66411\n",
      "Epoch 18230 - lr: 0.01000 - Train loss: 0.58157 - Test loss: 0.66446\n",
      "Epoch 18231 - lr: 0.01000 - Train loss: 0.58007 - Test loss: 0.66427\n",
      "Epoch 18232 - lr: 0.01000 - Train loss: 0.58252 - Test loss: 0.66463\n",
      "Epoch 18233 - lr: 0.01000 - Train loss: 0.59005 - Test loss: 0.66490\n",
      "Epoch 18234 - lr: 0.01000 - Train loss: 0.57578 - Test loss: 0.66432\n",
      "Epoch 18235 - lr: 0.01000 - Train loss: 0.58275 - Test loss: 0.66469\n",
      "Epoch 18236 - lr: 0.01000 - Train loss: 0.58716 - Test loss: 0.66538\n",
      "Epoch 18237 - lr: 0.01000 - Train loss: 0.58131 - Test loss: 0.66416\n",
      "Epoch 18238 - lr: 0.01000 - Train loss: 0.58019 - Test loss: 0.66325\n",
      "Epoch 18239 - lr: 0.01000 - Train loss: 0.57901 - Test loss: 0.66266\n",
      "Epoch 18240 - lr: 0.01000 - Train loss: 0.57560 - Test loss: 0.66255\n",
      "Epoch 18241 - lr: 0.01000 - Train loss: 0.58162 - Test loss: 0.66319\n",
      "Epoch 18242 - lr: 0.01000 - Train loss: 0.57969 - Test loss: 0.66349\n",
      "Epoch 18243 - lr: 0.01000 - Train loss: 0.58248 - Test loss: 0.66400\n",
      "Epoch 18244 - lr: 0.01000 - Train loss: 0.58499 - Test loss: 0.66464\n",
      "Epoch 18245 - lr: 0.01000 - Train loss: 0.58061 - Test loss: 0.66477\n",
      "Epoch 18246 - lr: 0.01000 - Train loss: 0.58219 - Test loss: 0.66395\n",
      "Epoch 18247 - lr: 0.01000 - Train loss: 0.57800 - Test loss: 0.66332\n",
      "Epoch 18248 - lr: 0.01000 - Train loss: 0.57357 - Test loss: 0.66349\n",
      "Epoch 18249 - lr: 0.01000 - Train loss: 0.58077 - Test loss: 0.66400\n",
      "Epoch 18250 - lr: 0.01000 - Train loss: 0.58203 - Test loss: 0.66345\n",
      "Epoch 18251 - lr: 0.01000 - Train loss: 0.57537 - Test loss: 0.66321\n",
      "Epoch 18252 - lr: 0.01000 - Train loss: 0.58243 - Test loss: 0.66380\n",
      "Epoch 18253 - lr: 0.01000 - Train loss: 0.59178 - Test loss: 0.66459\n",
      "Epoch 18254 - lr: 0.01000 - Train loss: 0.59085 - Test loss: 0.66484\n",
      "Epoch 18255 - lr: 0.01000 - Train loss: 0.58029 - Test loss: 0.66433\n",
      "Epoch 18256 - lr: 0.01000 - Train loss: 0.58294 - Test loss: 0.66459\n",
      "Epoch 18257 - lr: 0.01000 - Train loss: 0.59076 - Test loss: 0.66484\n",
      "Epoch 18258 - lr: 0.01000 - Train loss: 0.57701 - Test loss: 0.66414\n",
      "Epoch 18259 - lr: 0.01000 - Train loss: 0.57774 - Test loss: 0.66435\n",
      "Epoch 18260 - lr: 0.01000 - Train loss: 0.58052 - Test loss: 0.66360\n",
      "Epoch 18261 - lr: 0.01000 - Train loss: 0.57959 - Test loss: 0.66294\n",
      "Epoch 18262 - lr: 0.01000 - Train loss: 0.57758 - Test loss: 0.66261\n",
      "Epoch 18263 - lr: 0.01000 - Train loss: 0.57390 - Test loss: 0.66297\n",
      "Epoch 18264 - lr: 0.01000 - Train loss: 0.57586 - Test loss: 0.66348\n",
      "Epoch 18265 - lr: 0.01000 - Train loss: 0.57797 - Test loss: 0.66322\n",
      "Epoch 18266 - lr: 0.01000 - Train loss: 0.57362 - Test loss: 0.66352\n",
      "Epoch 18267 - lr: 0.01000 - Train loss: 0.58021 - Test loss: 0.66407\n",
      "Epoch 18268 - lr: 0.01000 - Train loss: 0.58188 - Test loss: 0.66346\n",
      "Epoch 18269 - lr: 0.01000 - Train loss: 0.57896 - Test loss: 0.66291\n",
      "Epoch 18270 - lr: 0.01000 - Train loss: 0.57544 - Test loss: 0.66285\n",
      "Epoch 18271 - lr: 0.01000 - Train loss: 0.58180 - Test loss: 0.66350\n",
      "Epoch 18272 - lr: 0.01000 - Train loss: 0.58141 - Test loss: 0.66395\n",
      "Epoch 18273 - lr: 0.01000 - Train loss: 0.57392 - Test loss: 0.66406\n",
      "Epoch 18274 - lr: 0.01000 - Train loss: 0.57683 - Test loss: 0.66442\n",
      "Epoch 18275 - lr: 0.01000 - Train loss: 0.57971 - Test loss: 0.66383\n",
      "Epoch 18276 - lr: 0.01000 - Train loss: 0.57746 - Test loss: 0.66344\n",
      "Epoch 18277 - lr: 0.01000 - Train loss: 0.57439 - Test loss: 0.66375\n",
      "Epoch 18278 - lr: 0.01000 - Train loss: 0.57341 - Test loss: 0.66401\n",
      "Epoch 18279 - lr: 0.01000 - Train loss: 0.58269 - Test loss: 0.66462\n",
      "Epoch 18280 - lr: 0.01000 - Train loss: 0.58269 - Test loss: 0.66551\n",
      "Epoch 18281 - lr: 0.01000 - Train loss: 0.57458 - Test loss: 0.66508\n",
      "Epoch 18282 - lr: 0.01000 - Train loss: 0.58195 - Test loss: 0.66553\n",
      "Epoch 18283 - lr: 0.01000 - Train loss: 0.58130 - Test loss: 0.66621\n",
      "Epoch 18284 - lr: 0.01000 - Train loss: 0.58090 - Test loss: 0.66637\n",
      "Epoch 18285 - lr: 0.01000 - Train loss: 0.59353 - Test loss: 0.66670\n",
      "Epoch 18286 - lr: 0.01000 - Train loss: 0.59569 - Test loss: 0.66636\n",
      "Epoch 18287 - lr: 0.01000 - Train loss: 0.58047 - Test loss: 0.66504\n",
      "Epoch 18288 - lr: 0.01000 - Train loss: 0.57915 - Test loss: 0.66415\n",
      "Epoch 18289 - lr: 0.01000 - Train loss: 0.57567 - Test loss: 0.66380\n",
      "Epoch 18290 - lr: 0.01000 - Train loss: 0.58160 - Test loss: 0.66427\n",
      "Epoch 18291 - lr: 0.01000 - Train loss: 0.57962 - Test loss: 0.66438\n",
      "Epoch 18292 - lr: 0.01000 - Train loss: 0.58187 - Test loss: 0.66482\n",
      "Epoch 18293 - lr: 0.01000 - Train loss: 0.58118 - Test loss: 0.66567\n",
      "Epoch 18294 - lr: 0.01000 - Train loss: 0.58114 - Test loss: 0.66571\n",
      "Epoch 18295 - lr: 0.01000 - Train loss: 0.58213 - Test loss: 0.66491\n",
      "Epoch 18296 - lr: 0.01000 - Train loss: 0.57381 - Test loss: 0.66463\n",
      "Epoch 18297 - lr: 0.01000 - Train loss: 0.58130 - Test loss: 0.66515\n",
      "Epoch 18298 - lr: 0.01000 - Train loss: 0.58823 - Test loss: 0.66585\n",
      "Epoch 18299 - lr: 0.01000 - Train loss: 0.58171 - Test loss: 0.66465\n",
      "Epoch 18300 - lr: 0.01000 - Train loss: 0.57993 - Test loss: 0.66374\n",
      "Epoch 18301 - lr: 0.01000 - Train loss: 0.57858 - Test loss: 0.66319\n",
      "Epoch 18302 - lr: 0.01000 - Train loss: 0.57437 - Test loss: 0.66320\n",
      "Epoch 18303 - lr: 0.01000 - Train loss: 0.58217 - Test loss: 0.66392\n",
      "Epoch 18304 - lr: 0.01000 - Train loss: 0.58111 - Test loss: 0.66495\n",
      "Epoch 18305 - lr: 0.01000 - Train loss: 0.58186 - Test loss: 0.66518\n",
      "Epoch 18306 - lr: 0.01000 - Train loss: 0.57974 - Test loss: 0.66513\n",
      "Epoch 18307 - lr: 0.01000 - Train loss: 0.58205 - Test loss: 0.66545\n",
      "Epoch 18308 - lr: 0.01000 - Train loss: 0.58315 - Test loss: 0.66599\n",
      "Epoch 18309 - lr: 0.01000 - Train loss: 0.58329 - Test loss: 0.66611\n",
      "Epoch 18310 - lr: 0.01000 - Train loss: 0.60321 - Test loss: 0.66611\n",
      "Epoch 18311 - lr: 0.01000 - Train loss: 0.58169 - Test loss: 0.66482\n",
      "Epoch 18312 - lr: 0.01000 - Train loss: 0.57991 - Test loss: 0.66387\n",
      "Epoch 18313 - lr: 0.01000 - Train loss: 0.57861 - Test loss: 0.66328\n",
      "Epoch 18314 - lr: 0.01000 - Train loss: 0.57456 - Test loss: 0.66325\n",
      "Epoch 18315 - lr: 0.01000 - Train loss: 0.58242 - Test loss: 0.66395\n",
      "Epoch 18316 - lr: 0.01000 - Train loss: 0.58106 - Test loss: 0.66497\n",
      "Epoch 18317 - lr: 0.01000 - Train loss: 0.58310 - Test loss: 0.66531\n",
      "Epoch 18318 - lr: 0.01000 - Train loss: 0.59357 - Test loss: 0.66552\n",
      "Epoch 18319 - lr: 0.01000 - Train loss: 0.57954 - Test loss: 0.66456\n",
      "Epoch 18320 - lr: 0.01000 - Train loss: 0.57673 - Test loss: 0.66409\n",
      "Epoch 18321 - lr: 0.01000 - Train loss: 0.57730 - Test loss: 0.66440\n",
      "Epoch 18322 - lr: 0.01000 - Train loss: 0.58014 - Test loss: 0.66380\n",
      "Epoch 18323 - lr: 0.01000 - Train loss: 0.57914 - Test loss: 0.66327\n",
      "Epoch 18324 - lr: 0.01000 - Train loss: 0.57646 - Test loss: 0.66310\n",
      "Epoch 18325 - lr: 0.01000 - Train loss: 0.57742 - Test loss: 0.66362\n",
      "Epoch 18326 - lr: 0.01000 - Train loss: 0.58011 - Test loss: 0.66322\n",
      "Epoch 18327 - lr: 0.01000 - Train loss: 0.57929 - Test loss: 0.66282\n",
      "Epoch 18328 - lr: 0.01000 - Train loss: 0.57731 - Test loss: 0.66269\n",
      "Epoch 18329 - lr: 0.01000 - Train loss: 0.57394 - Test loss: 0.66317\n",
      "Epoch 18330 - lr: 0.01000 - Train loss: 0.57456 - Test loss: 0.66373\n",
      "Epoch 18331 - lr: 0.01000 - Train loss: 0.57362 - Test loss: 0.66402\n",
      "Epoch 18332 - lr: 0.01000 - Train loss: 0.58169 - Test loss: 0.66477\n",
      "Epoch 18333 - lr: 0.01000 - Train loss: 0.58831 - Test loss: 0.66566\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18334 - lr: 0.01000 - Train loss: 0.58149 - Test loss: 0.66463\n",
      "Epoch 18335 - lr: 0.01000 - Train loss: 0.57986 - Test loss: 0.66384\n",
      "Epoch 18336 - lr: 0.01000 - Train loss: 0.57871 - Test loss: 0.66334\n",
      "Epoch 18337 - lr: 0.01000 - Train loss: 0.57524 - Test loss: 0.66330\n",
      "Epoch 18338 - lr: 0.01000 - Train loss: 0.58171 - Test loss: 0.66395\n",
      "Epoch 18339 - lr: 0.01000 - Train loss: 0.58023 - Test loss: 0.66437\n",
      "Epoch 18340 - lr: 0.01000 - Train loss: 0.58059 - Test loss: 0.66472\n",
      "Epoch 18341 - lr: 0.01000 - Train loss: 0.58195 - Test loss: 0.66417\n",
      "Epoch 18342 - lr: 0.01000 - Train loss: 0.57734 - Test loss: 0.66374\n",
      "Epoch 18343 - lr: 0.01000 - Train loss: 0.57414 - Test loss: 0.66407\n",
      "Epoch 18344 - lr: 0.01000 - Train loss: 0.57353 - Test loss: 0.66441\n",
      "Epoch 18345 - lr: 0.01000 - Train loss: 0.57935 - Test loss: 0.66490\n",
      "Epoch 18346 - lr: 0.01000 - Train loss: 0.58120 - Test loss: 0.66426\n",
      "Epoch 18347 - lr: 0.01000 - Train loss: 0.57986 - Test loss: 0.66361\n",
      "Epoch 18348 - lr: 0.01000 - Train loss: 0.57886 - Test loss: 0.66318\n",
      "Epoch 18349 - lr: 0.01000 - Train loss: 0.57606 - Test loss: 0.66312\n",
      "Epoch 18350 - lr: 0.01000 - Train loss: 0.57846 - Test loss: 0.66373\n",
      "Epoch 18351 - lr: 0.01000 - Train loss: 0.58053 - Test loss: 0.66336\n",
      "Epoch 18352 - lr: 0.01000 - Train loss: 0.57981 - Test loss: 0.66292\n",
      "Epoch 18353 - lr: 0.01000 - Train loss: 0.57901 - Test loss: 0.66265\n",
      "Epoch 18354 - lr: 0.01000 - Train loss: 0.57688 - Test loss: 0.66264\n",
      "Epoch 18355 - lr: 0.01000 - Train loss: 0.57470 - Test loss: 0.66323\n",
      "Epoch 18356 - lr: 0.01000 - Train loss: 0.57429 - Test loss: 0.66356\n",
      "Epoch 18357 - lr: 0.01000 - Train loss: 0.58197 - Test loss: 0.66437\n",
      "Epoch 18358 - lr: 0.01000 - Train loss: 0.59064 - Test loss: 0.66533\n",
      "Epoch 18359 - lr: 0.01000 - Train loss: 0.58090 - Test loss: 0.66490\n",
      "Epoch 18360 - lr: 0.01000 - Train loss: 0.57900 - Test loss: 0.66510\n",
      "Epoch 18361 - lr: 0.01000 - Train loss: 0.58091 - Test loss: 0.66442\n",
      "Epoch 18362 - lr: 0.01000 - Train loss: 0.57986 - Test loss: 0.66375\n",
      "Epoch 18363 - lr: 0.01000 - Train loss: 0.57898 - Test loss: 0.66330\n",
      "Epoch 18364 - lr: 0.01000 - Train loss: 0.57670 - Test loss: 0.66318\n",
      "Epoch 18365 - lr: 0.01000 - Train loss: 0.57535 - Test loss: 0.66370\n",
      "Epoch 18366 - lr: 0.01000 - Train loss: 0.57692 - Test loss: 0.66369\n",
      "Epoch 18367 - lr: 0.01000 - Train loss: 0.57506 - Test loss: 0.66417\n",
      "Epoch 18368 - lr: 0.01000 - Train loss: 0.57576 - Test loss: 0.66421\n",
      "Epoch 18369 - lr: 0.01000 - Train loss: 0.58037 - Test loss: 0.66478\n",
      "Epoch 18370 - lr: 0.01000 - Train loss: 0.58176 - Test loss: 0.66435\n",
      "Epoch 18371 - lr: 0.01000 - Train loss: 0.57776 - Test loss: 0.66394\n",
      "Epoch 18372 - lr: 0.01000 - Train loss: 0.57318 - Test loss: 0.66420\n",
      "Epoch 18373 - lr: 0.01000 - Train loss: 0.58215 - Test loss: 0.66488\n",
      "Epoch 18374 - lr: 0.01000 - Train loss: 0.59223 - Test loss: 0.66562\n",
      "Epoch 18375 - lr: 0.01000 - Train loss: 0.59203 - Test loss: 0.66598\n",
      "Epoch 18376 - lr: 0.01000 - Train loss: 0.58835 - Test loss: 0.66612\n",
      "Epoch 18377 - lr: 0.01000 - Train loss: 0.58124 - Test loss: 0.66504\n",
      "Epoch 18378 - lr: 0.01000 - Train loss: 0.57974 - Test loss: 0.66423\n",
      "Epoch 18379 - lr: 0.01000 - Train loss: 0.57869 - Test loss: 0.66373\n",
      "Epoch 18380 - lr: 0.01000 - Train loss: 0.57558 - Test loss: 0.66364\n",
      "Epoch 18381 - lr: 0.01000 - Train loss: 0.58012 - Test loss: 0.66423\n",
      "Epoch 18382 - lr: 0.01000 - Train loss: 0.58156 - Test loss: 0.66387\n",
      "Epoch 18383 - lr: 0.01000 - Train loss: 0.57852 - Test loss: 0.66348\n",
      "Epoch 18384 - lr: 0.01000 - Train loss: 0.57492 - Test loss: 0.66356\n",
      "Epoch 18385 - lr: 0.01000 - Train loss: 0.58179 - Test loss: 0.66427\n",
      "Epoch 18386 - lr: 0.01000 - Train loss: 0.58314 - Test loss: 0.66487\n",
      "Epoch 18387 - lr: 0.01000 - Train loss: 0.57688 - Test loss: 0.66451\n",
      "Epoch 18388 - lr: 0.01000 - Train loss: 0.57557 - Test loss: 0.66486\n",
      "Epoch 18389 - lr: 0.01000 - Train loss: 0.57757 - Test loss: 0.66461\n",
      "Epoch 18390 - lr: 0.01000 - Train loss: 0.57344 - Test loss: 0.66486\n",
      "Epoch 18391 - lr: 0.01000 - Train loss: 0.57839 - Test loss: 0.66534\n",
      "Epoch 18392 - lr: 0.01000 - Train loss: 0.58059 - Test loss: 0.66474\n",
      "Epoch 18393 - lr: 0.01000 - Train loss: 0.57977 - Test loss: 0.66412\n",
      "Epoch 18394 - lr: 0.01000 - Train loss: 0.57885 - Test loss: 0.66370\n",
      "Epoch 18395 - lr: 0.01000 - Train loss: 0.57637 - Test loss: 0.66361\n",
      "Epoch 18396 - lr: 0.01000 - Train loss: 0.57634 - Test loss: 0.66414\n",
      "Epoch 18397 - lr: 0.01000 - Train loss: 0.57904 - Test loss: 0.66394\n",
      "Epoch 18398 - lr: 0.01000 - Train loss: 0.57675 - Test loss: 0.66384\n",
      "Epoch 18399 - lr: 0.01000 - Train loss: 0.57505 - Test loss: 0.66434\n",
      "Epoch 18400 - lr: 0.01000 - Train loss: 0.57600 - Test loss: 0.66438\n",
      "Epoch 18401 - lr: 0.01000 - Train loss: 0.57872 - Test loss: 0.66493\n",
      "Epoch 18402 - lr: 0.01000 - Train loss: 0.58064 - Test loss: 0.66448\n",
      "Epoch 18403 - lr: 0.01000 - Train loss: 0.57972 - Test loss: 0.66396\n",
      "Epoch 18404 - lr: 0.01000 - Train loss: 0.57886 - Test loss: 0.66362\n",
      "Epoch 18405 - lr: 0.01000 - Train loss: 0.57658 - Test loss: 0.66356\n",
      "Epoch 18406 - lr: 0.01000 - Train loss: 0.57517 - Test loss: 0.66410\n",
      "Epoch 18407 - lr: 0.01000 - Train loss: 0.57648 - Test loss: 0.66417\n",
      "Epoch 18408 - lr: 0.01000 - Train loss: 0.57616 - Test loss: 0.66470\n",
      "Epoch 18409 - lr: 0.01000 - Train loss: 0.57878 - Test loss: 0.66448\n",
      "Epoch 18410 - lr: 0.01000 - Train loss: 0.57569 - Test loss: 0.66443\n",
      "Epoch 18411 - lr: 0.01000 - Train loss: 0.57948 - Test loss: 0.66499\n",
      "Epoch 18412 - lr: 0.01000 - Train loss: 0.58109 - Test loss: 0.66459\n",
      "Epoch 18413 - lr: 0.01000 - Train loss: 0.57934 - Test loss: 0.66410\n",
      "Epoch 18414 - lr: 0.01000 - Train loss: 0.57799 - Test loss: 0.66385\n",
      "Epoch 18415 - lr: 0.01000 - Train loss: 0.57368 - Test loss: 0.66408\n",
      "Epoch 18416 - lr: 0.01000 - Train loss: 0.58121 - Test loss: 0.66488\n",
      "Epoch 18417 - lr: 0.01000 - Train loss: 0.58931 - Test loss: 0.66564\n",
      "Epoch 18418 - lr: 0.01000 - Train loss: 0.58156 - Test loss: 0.66497\n",
      "Epoch 18419 - lr: 0.01000 - Train loss: 0.57828 - Test loss: 0.66448\n",
      "Epoch 18420 - lr: 0.01000 - Train loss: 0.57420 - Test loss: 0.66452\n",
      "Epoch 18421 - lr: 0.01000 - Train loss: 0.58174 - Test loss: 0.66520\n",
      "Epoch 18422 - lr: 0.01000 - Train loss: 0.59222 - Test loss: 0.66601\n",
      "Epoch 18423 - lr: 0.01000 - Train loss: 0.59237 - Test loss: 0.66645\n",
      "Epoch 18424 - lr: 0.01000 - Train loss: 0.59245 - Test loss: 0.66681\n",
      "Epoch 18425 - lr: 0.01000 - Train loss: 0.59095 - Test loss: 0.66715\n",
      "Epoch 18426 - lr: 0.01000 - Train loss: 0.57957 - Test loss: 0.66670\n",
      "Epoch 18427 - lr: 0.01000 - Train loss: 0.58136 - Test loss: 0.66687\n",
      "Epoch 18428 - lr: 0.01000 - Train loss: 0.58959 - Test loss: 0.66744\n",
      "Epoch 18429 - lr: 0.01000 - Train loss: 0.58199 - Test loss: 0.66634\n",
      "Epoch 18430 - lr: 0.01000 - Train loss: 0.57626 - Test loss: 0.66569\n",
      "Epoch 18431 - lr: 0.01000 - Train loss: 0.57806 - Test loss: 0.66591\n",
      "Epoch 18432 - lr: 0.01000 - Train loss: 0.58040 - Test loss: 0.66522\n",
      "Epoch 18433 - lr: 0.01000 - Train loss: 0.57967 - Test loss: 0.66454\n",
      "Epoch 18434 - lr: 0.01000 - Train loss: 0.57874 - Test loss: 0.66409\n",
      "Epoch 18435 - lr: 0.01000 - Train loss: 0.57615 - Test loss: 0.66399\n",
      "Epoch 18436 - lr: 0.01000 - Train loss: 0.57695 - Test loss: 0.66452\n",
      "Epoch 18437 - lr: 0.01000 - Train loss: 0.57964 - Test loss: 0.66425\n",
      "Epoch 18438 - lr: 0.01000 - Train loss: 0.57870 - Test loss: 0.66395\n",
      "Epoch 18439 - lr: 0.01000 - Train loss: 0.57595 - Test loss: 0.66394\n",
      "Epoch 18440 - lr: 0.01000 - Train loss: 0.57781 - Test loss: 0.66454\n",
      "Epoch 18441 - lr: 0.01000 - Train loss: 0.58008 - Test loss: 0.66426\n",
      "Epoch 18442 - lr: 0.01000 - Train loss: 0.57947 - Test loss: 0.66388\n",
      "Epoch 18443 - lr: 0.01000 - Train loss: 0.57851 - Test loss: 0.66366\n",
      "Epoch 18444 - lr: 0.01000 - Train loss: 0.57559 - Test loss: 0.66374\n",
      "Epoch 18445 - lr: 0.01000 - Train loss: 0.57908 - Test loss: 0.66441\n",
      "Epoch 18446 - lr: 0.01000 - Train loss: 0.58067 - Test loss: 0.66418\n",
      "Epoch 18447 - lr: 0.01000 - Train loss: 0.57955 - Test loss: 0.66380\n",
      "Epoch 18448 - lr: 0.01000 - Train loss: 0.57876 - Test loss: 0.66358\n",
      "Epoch 18449 - lr: 0.01000 - Train loss: 0.57659 - Test loss: 0.66360\n",
      "Epoch 18450 - lr: 0.01000 - Train loss: 0.57475 - Test loss: 0.66417\n",
      "Epoch 18451 - lr: 0.01000 - Train loss: 0.57498 - Test loss: 0.66444\n",
      "Epoch 18452 - lr: 0.01000 - Train loss: 0.58134 - Test loss: 0.66514\n",
      "Epoch 18453 - lr: 0.01000 - Train loss: 0.57946 - Test loss: 0.66545\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18454 - lr: 0.01000 - Train loss: 0.58143 - Test loss: 0.66595\n",
      "Epoch 18455 - lr: 0.01000 - Train loss: 0.59240 - Test loss: 0.66668\n",
      "Epoch 18456 - lr: 0.01000 - Train loss: 0.59173 - Test loss: 0.66712\n",
      "Epoch 18457 - lr: 0.01000 - Train loss: 0.58591 - Test loss: 0.66715\n",
      "Epoch 18458 - lr: 0.01000 - Train loss: 0.58010 - Test loss: 0.66611\n",
      "Epoch 18459 - lr: 0.01000 - Train loss: 0.57923 - Test loss: 0.66535\n",
      "Epoch 18460 - lr: 0.01000 - Train loss: 0.57752 - Test loss: 0.66491\n",
      "Epoch 18461 - lr: 0.01000 - Train loss: 0.57299 - Test loss: 0.66509\n",
      "Epoch 18462 - lr: 0.01000 - Train loss: 0.58200 - Test loss: 0.66572\n",
      "Epoch 18463 - lr: 0.01000 - Train loss: 0.59188 - Test loss: 0.66643\n",
      "Epoch 18464 - lr: 0.01000 - Train loss: 0.58769 - Test loss: 0.66667\n",
      "Epoch 18465 - lr: 0.01000 - Train loss: 0.58075 - Test loss: 0.66572\n",
      "Epoch 18466 - lr: 0.01000 - Train loss: 0.57970 - Test loss: 0.66498\n",
      "Epoch 18467 - lr: 0.01000 - Train loss: 0.57890 - Test loss: 0.66448\n",
      "Epoch 18468 - lr: 0.01000 - Train loss: 0.57695 - Test loss: 0.66428\n",
      "Epoch 18469 - lr: 0.01000 - Train loss: 0.57367 - Test loss: 0.66468\n",
      "Epoch 18470 - lr: 0.01000 - Train loss: 0.57399 - Test loss: 0.66518\n",
      "Epoch 18471 - lr: 0.01000 - Train loss: 0.57309 - Test loss: 0.66556\n",
      "Epoch 18472 - lr: 0.01000 - Train loss: 0.58173 - Test loss: 0.66617\n",
      "Epoch 18473 - lr: 0.01000 - Train loss: 0.58121 - Test loss: 0.66655\n",
      "Epoch 18474 - lr: 0.01000 - Train loss: 0.57343 - Test loss: 0.66655\n",
      "Epoch 18475 - lr: 0.01000 - Train loss: 0.57733 - Test loss: 0.66683\n",
      "Epoch 18476 - lr: 0.01000 - Train loss: 0.58003 - Test loss: 0.66615\n",
      "Epoch 18477 - lr: 0.01000 - Train loss: 0.57914 - Test loss: 0.66550\n",
      "Epoch 18478 - lr: 0.01000 - Train loss: 0.57726 - Test loss: 0.66513\n",
      "Epoch 18479 - lr: 0.01000 - Train loss: 0.57313 - Test loss: 0.66538\n",
      "Epoch 18480 - lr: 0.01000 - Train loss: 0.57911 - Test loss: 0.66591\n",
      "Epoch 18481 - lr: 0.01000 - Train loss: 0.58083 - Test loss: 0.66543\n",
      "Epoch 18482 - lr: 0.01000 - Train loss: 0.57952 - Test loss: 0.66487\n",
      "Epoch 18483 - lr: 0.01000 - Train loss: 0.57860 - Test loss: 0.66452\n",
      "Epoch 18484 - lr: 0.01000 - Train loss: 0.57603 - Test loss: 0.66446\n",
      "Epoch 18485 - lr: 0.01000 - Train loss: 0.57676 - Test loss: 0.66500\n",
      "Epoch 18486 - lr: 0.01000 - Train loss: 0.57942 - Test loss: 0.66480\n",
      "Epoch 18487 - lr: 0.01000 - Train loss: 0.57837 - Test loss: 0.66456\n",
      "Epoch 18488 - lr: 0.01000 - Train loss: 0.57510 - Test loss: 0.66463\n",
      "Epoch 18489 - lr: 0.01000 - Train loss: 0.58052 - Test loss: 0.66528\n",
      "Epoch 18490 - lr: 0.01000 - Train loss: 0.58142 - Test loss: 0.66516\n",
      "Epoch 18491 - lr: 0.01000 - Train loss: 0.57457 - Test loss: 0.66511\n",
      "Epoch 18492 - lr: 0.01000 - Train loss: 0.58168 - Test loss: 0.66575\n",
      "Epoch 18493 - lr: 0.01000 - Train loss: 0.58531 - Test loss: 0.66637\n",
      "Epoch 18494 - lr: 0.01000 - Train loss: 0.57964 - Test loss: 0.66572\n",
      "Epoch 18495 - lr: 0.01000 - Train loss: 0.57862 - Test loss: 0.66525\n",
      "Epoch 18496 - lr: 0.01000 - Train loss: 0.57585 - Test loss: 0.66511\n",
      "Epoch 18497 - lr: 0.01000 - Train loss: 0.57761 - Test loss: 0.66558\n",
      "Epoch 18498 - lr: 0.01000 - Train loss: 0.57992 - Test loss: 0.66525\n",
      "Epoch 18499 - lr: 0.01000 - Train loss: 0.57930 - Test loss: 0.66483\n",
      "Epoch 18500 - lr: 0.01000 - Train loss: 0.57823 - Test loss: 0.66458\n",
      "Epoch 18501 - lr: 0.01000 - Train loss: 0.57487 - Test loss: 0.66468\n",
      "Epoch 18502 - lr: 0.01000 - Train loss: 0.58087 - Test loss: 0.66534\n",
      "Epoch 18503 - lr: 0.01000 - Train loss: 0.58073 - Test loss: 0.66543\n",
      "Epoch 18504 - lr: 0.01000 - Train loss: 0.57527 - Test loss: 0.66572\n",
      "Epoch 18505 - lr: 0.01000 - Train loss: 0.57703 - Test loss: 0.66563\n",
      "Epoch 18506 - lr: 0.01000 - Train loss: 0.57353 - Test loss: 0.66595\n",
      "Epoch 18507 - lr: 0.01000 - Train loss: 0.57437 - Test loss: 0.66637\n",
      "Epoch 18508 - lr: 0.01000 - Train loss: 0.57382 - Test loss: 0.66648\n",
      "Epoch 18509 - lr: 0.01000 - Train loss: 0.58131 - Test loss: 0.66706\n",
      "Epoch 18510 - lr: 0.01000 - Train loss: 0.59238 - Test loss: 0.66775\n",
      "Epoch 18511 - lr: 0.01000 - Train loss: 0.59152 - Test loss: 0.66814\n",
      "Epoch 18512 - lr: 0.01000 - Train loss: 0.58307 - Test loss: 0.66804\n",
      "Epoch 18513 - lr: 0.01000 - Train loss: 0.57680 - Test loss: 0.66727\n",
      "Epoch 18514 - lr: 0.01000 - Train loss: 0.57512 - Test loss: 0.66732\n",
      "Epoch 18515 - lr: 0.01000 - Train loss: 0.57678 - Test loss: 0.66690\n",
      "Epoch 18516 - lr: 0.01000 - Train loss: 0.57477 - Test loss: 0.66704\n",
      "Epoch 18517 - lr: 0.01000 - Train loss: 0.57558 - Test loss: 0.66681\n",
      "Epoch 18518 - lr: 0.01000 - Train loss: 0.58000 - Test loss: 0.66714\n",
      "Epoch 18519 - lr: 0.01000 - Train loss: 0.58146 - Test loss: 0.66657\n",
      "Epoch 18520 - lr: 0.01000 - Train loss: 0.57809 - Test loss: 0.66599\n",
      "Epoch 18521 - lr: 0.01000 - Train loss: 0.57384 - Test loss: 0.66595\n",
      "Epoch 18522 - lr: 0.01000 - Train loss: 0.58148 - Test loss: 0.66656\n",
      "Epoch 18523 - lr: 0.01000 - Train loss: 0.59220 - Test loss: 0.66734\n",
      "Epoch 18524 - lr: 0.01000 - Train loss: 0.59242 - Test loss: 0.66776\n",
      "Epoch 18525 - lr: 0.01000 - Train loss: 0.59075 - Test loss: 0.66813\n",
      "Epoch 18526 - lr: 0.01000 - Train loss: 0.57972 - Test loss: 0.66764\n",
      "Epoch 18527 - lr: 0.01000 - Train loss: 0.58216 - Test loss: 0.66777\n",
      "Epoch 18528 - lr: 0.01000 - Train loss: 0.58087 - Test loss: 0.66848\n",
      "Epoch 18529 - lr: 0.01000 - Train loss: 0.57966 - Test loss: 0.66839\n",
      "Epoch 18530 - lr: 0.01000 - Train loss: 0.58134 - Test loss: 0.66743\n",
      "Epoch 18531 - lr: 0.01000 - Train loss: 0.57934 - Test loss: 0.66653\n",
      "Epoch 18532 - lr: 0.01000 - Train loss: 0.57790 - Test loss: 0.66596\n",
      "Epoch 18533 - lr: 0.01000 - Train loss: 0.57341 - Test loss: 0.66595\n",
      "Epoch 18534 - lr: 0.01000 - Train loss: 0.58117 - Test loss: 0.66656\n",
      "Epoch 18535 - lr: 0.01000 - Train loss: 0.59241 - Test loss: 0.66731\n",
      "Epoch 18536 - lr: 0.01000 - Train loss: 0.59025 - Test loss: 0.66781\n",
      "Epoch 18537 - lr: 0.01000 - Train loss: 0.58118 - Test loss: 0.66710\n",
      "Epoch 18538 - lr: 0.01000 - Train loss: 0.57403 - Test loss: 0.66702\n",
      "Epoch 18539 - lr: 0.01000 - Train loss: 0.57294 - Test loss: 0.66707\n",
      "Epoch 18540 - lr: 0.01000 - Train loss: 0.58215 - Test loss: 0.66754\n",
      "Epoch 18541 - lr: 0.01000 - Train loss: 0.58068 - Test loss: 0.66835\n",
      "Epoch 18542 - lr: 0.01000 - Train loss: 0.58268 - Test loss: 0.66842\n",
      "Epoch 18543 - lr: 0.01000 - Train loss: 0.58634 - Test loss: 0.66898\n",
      "Epoch 18544 - lr: 0.01000 - Train loss: 0.58050 - Test loss: 0.66774\n",
      "Epoch 18545 - lr: 0.01000 - Train loss: 0.57964 - Test loss: 0.66677\n",
      "Epoch 18546 - lr: 0.01000 - Train loss: 0.57862 - Test loss: 0.66608\n",
      "Epoch 18547 - lr: 0.01000 - Train loss: 0.57589 - Test loss: 0.66579\n",
      "Epoch 18548 - lr: 0.01000 - Train loss: 0.57739 - Test loss: 0.66617\n",
      "Epoch 18549 - lr: 0.01000 - Train loss: 0.57985 - Test loss: 0.66576\n",
      "Epoch 18550 - lr: 0.01000 - Train loss: 0.57926 - Test loss: 0.66530\n",
      "Epoch 18551 - lr: 0.01000 - Train loss: 0.57822 - Test loss: 0.66501\n",
      "Epoch 18552 - lr: 0.01000 - Train loss: 0.57492 - Test loss: 0.66507\n",
      "Epoch 18553 - lr: 0.01000 - Train loss: 0.58079 - Test loss: 0.66571\n",
      "Epoch 18554 - lr: 0.01000 - Train loss: 0.58108 - Test loss: 0.66572\n",
      "Epoch 18555 - lr: 0.01000 - Train loss: 0.57313 - Test loss: 0.66591\n",
      "Epoch 18556 - lr: 0.01000 - Train loss: 0.57862 - Test loss: 0.66646\n",
      "Epoch 18557 - lr: 0.01000 - Train loss: 0.58041 - Test loss: 0.66604\n",
      "Epoch 18558 - lr: 0.01000 - Train loss: 0.57955 - Test loss: 0.66551\n",
      "Epoch 18559 - lr: 0.01000 - Train loss: 0.57889 - Test loss: 0.66514\n",
      "Epoch 18560 - lr: 0.01000 - Train loss: 0.57740 - Test loss: 0.66497\n",
      "Epoch 18561 - lr: 0.01000 - Train loss: 0.57297 - Test loss: 0.66529\n",
      "Epoch 18562 - lr: 0.01000 - Train loss: 0.58163 - Test loss: 0.66604\n",
      "Epoch 18563 - lr: 0.01000 - Train loss: 0.59074 - Test loss: 0.66687\n",
      "Epoch 18564 - lr: 0.01000 - Train loss: 0.58040 - Test loss: 0.66663\n",
      "Epoch 18565 - lr: 0.01000 - Train loss: 0.57913 - Test loss: 0.66684\n",
      "Epoch 18566 - lr: 0.01000 - Train loss: 0.58068 - Test loss: 0.66634\n",
      "Epoch 18567 - lr: 0.01000 - Train loss: 0.57943 - Test loss: 0.66575\n",
      "Epoch 18568 - lr: 0.01000 - Train loss: 0.57866 - Test loss: 0.66535\n",
      "Epoch 18569 - lr: 0.01000 - Train loss: 0.57668 - Test loss: 0.66521\n",
      "Epoch 18570 - lr: 0.01000 - Train loss: 0.57373 - Test loss: 0.66562\n",
      "Epoch 18571 - lr: 0.01000 - Train loss: 0.57304 - Test loss: 0.66610\n",
      "Epoch 18572 - lr: 0.01000 - Train loss: 0.57916 - Test loss: 0.66668\n",
      "Epoch 18573 - lr: 0.01000 - Train loss: 0.58071 - Test loss: 0.66631\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18574 - lr: 0.01000 - Train loss: 0.57939 - Test loss: 0.66579\n",
      "Epoch 18575 - lr: 0.01000 - Train loss: 0.57857 - Test loss: 0.66544\n",
      "Epoch 18576 - lr: 0.01000 - Train loss: 0.57638 - Test loss: 0.66535\n",
      "Epoch 18577 - lr: 0.01000 - Train loss: 0.57458 - Test loss: 0.66580\n",
      "Epoch 18578 - lr: 0.01000 - Train loss: 0.57499 - Test loss: 0.66599\n",
      "Epoch 18579 - lr: 0.01000 - Train loss: 0.58062 - Test loss: 0.66657\n",
      "Epoch 18580 - lr: 0.01000 - Train loss: 0.58129 - Test loss: 0.66645\n",
      "Epoch 18581 - lr: 0.01000 - Train loss: 0.57342 - Test loss: 0.66643\n",
      "Epoch 18582 - lr: 0.01000 - Train loss: 0.58112 - Test loss: 0.66705\n",
      "Epoch 18583 - lr: 0.01000 - Train loss: 0.59027 - Test loss: 0.66773\n",
      "Epoch 18584 - lr: 0.01000 - Train loss: 0.58127 - Test loss: 0.66718\n",
      "Epoch 18585 - lr: 0.01000 - Train loss: 0.57283 - Test loss: 0.66709\n",
      "Epoch 18586 - lr: 0.01000 - Train loss: 0.58196 - Test loss: 0.66757\n",
      "Epoch 18587 - lr: 0.01000 - Train loss: 0.59184 - Test loss: 0.66829\n",
      "Epoch 18588 - lr: 0.01000 - Train loss: 0.58819 - Test loss: 0.66844\n",
      "Epoch 18589 - lr: 0.01000 - Train loss: 0.58082 - Test loss: 0.66744\n",
      "Epoch 18590 - lr: 0.01000 - Train loss: 0.57943 - Test loss: 0.66663\n",
      "Epoch 18591 - lr: 0.01000 - Train loss: 0.57859 - Test loss: 0.66609\n",
      "Epoch 18592 - lr: 0.01000 - Train loss: 0.57640 - Test loss: 0.66586\n",
      "Epoch 18593 - lr: 0.01000 - Train loss: 0.57446 - Test loss: 0.66622\n",
      "Epoch 18594 - lr: 0.01000 - Train loss: 0.57472 - Test loss: 0.66635\n",
      "Epoch 18595 - lr: 0.01000 - Train loss: 0.58137 - Test loss: 0.66691\n",
      "Epoch 18596 - lr: 0.01000 - Train loss: 0.57926 - Test loss: 0.66727\n",
      "Epoch 18597 - lr: 0.01000 - Train loss: 0.58184 - Test loss: 0.66764\n",
      "Epoch 18598 - lr: 0.01000 - Train loss: 0.58987 - Test loss: 0.66842\n",
      "Epoch 18599 - lr: 0.01000 - Train loss: 0.58157 - Test loss: 0.66766\n",
      "Epoch 18600 - lr: 0.01000 - Train loss: 0.57426 - Test loss: 0.66729\n",
      "Epoch 18601 - lr: 0.01000 - Train loss: 0.58202 - Test loss: 0.66770\n",
      "Epoch 18602 - lr: 0.01000 - Train loss: 0.59052 - Test loss: 0.66843\n",
      "Epoch 18603 - lr: 0.01000 - Train loss: 0.58049 - Test loss: 0.66790\n",
      "Epoch 18604 - lr: 0.01000 - Train loss: 0.57915 - Test loss: 0.66793\n",
      "Epoch 18605 - lr: 0.01000 - Train loss: 0.58075 - Test loss: 0.66726\n",
      "Epoch 18606 - lr: 0.01000 - Train loss: 0.57941 - Test loss: 0.66654\n",
      "Epoch 18607 - lr: 0.01000 - Train loss: 0.57861 - Test loss: 0.66605\n",
      "Epoch 18608 - lr: 0.01000 - Train loss: 0.57657 - Test loss: 0.66584\n",
      "Epoch 18609 - lr: 0.01000 - Train loss: 0.57385 - Test loss: 0.66620\n",
      "Epoch 18610 - lr: 0.01000 - Train loss: 0.57283 - Test loss: 0.66658\n",
      "Epoch 18611 - lr: 0.01000 - Train loss: 0.58189 - Test loss: 0.66723\n",
      "Epoch 18612 - lr: 0.01000 - Train loss: 0.59213 - Test loss: 0.66807\n",
      "Epoch 18613 - lr: 0.01000 - Train loss: 0.59176 - Test loss: 0.66842\n",
      "Epoch 18614 - lr: 0.01000 - Train loss: 0.58358 - Test loss: 0.66845\n",
      "Epoch 18615 - lr: 0.01000 - Train loss: 0.57795 - Test loss: 0.66768\n",
      "Epoch 18616 - lr: 0.01000 - Train loss: 0.57327 - Test loss: 0.66752\n",
      "Epoch 18617 - lr: 0.01000 - Train loss: 0.58120 - Test loss: 0.66799\n",
      "Epoch 18618 - lr: 0.01000 - Train loss: 0.59246 - Test loss: 0.66865\n",
      "Epoch 18619 - lr: 0.01000 - Train loss: 0.59076 - Test loss: 0.66902\n",
      "Epoch 18620 - lr: 0.01000 - Train loss: 0.58002 - Test loss: 0.66846\n",
      "Epoch 18621 - lr: 0.01000 - Train loss: 0.58233 - Test loss: 0.66850\n",
      "Epoch 18622 - lr: 0.01000 - Train loss: 0.59126 - Test loss: 0.66905\n",
      "Epoch 18623 - lr: 0.01000 - Train loss: 0.58016 - Test loss: 0.66883\n",
      "Epoch 18624 - lr: 0.01000 - Train loss: 0.57881 - Test loss: 0.66872\n",
      "Epoch 18625 - lr: 0.01000 - Train loss: 0.58063 - Test loss: 0.66786\n",
      "Epoch 18626 - lr: 0.01000 - Train loss: 0.57963 - Test loss: 0.66700\n",
      "Epoch 18627 - lr: 0.01000 - Train loss: 0.57897 - Test loss: 0.66639\n",
      "Epoch 18628 - lr: 0.01000 - Train loss: 0.57766 - Test loss: 0.66602\n",
      "Epoch 18629 - lr: 0.01000 - Train loss: 0.57336 - Test loss: 0.66613\n",
      "Epoch 18630 - lr: 0.01000 - Train loss: 0.58144 - Test loss: 0.66680\n",
      "Epoch 18631 - lr: 0.01000 - Train loss: 0.59238 - Test loss: 0.66766\n",
      "Epoch 18632 - lr: 0.01000 - Train loss: 0.59247 - Test loss: 0.66816\n",
      "Epoch 18633 - lr: 0.01000 - Train loss: 0.59187 - Test loss: 0.66857\n",
      "Epoch 18634 - lr: 0.01000 - Train loss: 0.58722 - Test loss: 0.66864\n",
      "Epoch 18635 - lr: 0.01000 - Train loss: 0.58033 - Test loss: 0.66762\n",
      "Epoch 18636 - lr: 0.01000 - Train loss: 0.57961 - Test loss: 0.66680\n",
      "Epoch 18637 - lr: 0.01000 - Train loss: 0.57904 - Test loss: 0.66622\n",
      "Epoch 18638 - lr: 0.01000 - Train loss: 0.57797 - Test loss: 0.66586\n",
      "Epoch 18639 - lr: 0.01000 - Train loss: 0.57441 - Test loss: 0.66589\n",
      "Epoch 18640 - lr: 0.01000 - Train loss: 0.58166 - Test loss: 0.66649\n",
      "Epoch 18641 - lr: 0.01000 - Train loss: 0.58216 - Test loss: 0.66712\n",
      "Epoch 18642 - lr: 0.01000 - Train loss: 0.57457 - Test loss: 0.66698\n",
      "Epoch 18643 - lr: 0.01000 - Train loss: 0.58200 - Test loss: 0.66748\n",
      "Epoch 18644 - lr: 0.01000 - Train loss: 0.58980 - Test loss: 0.66810\n",
      "Epoch 18645 - lr: 0.01000 - Train loss: 0.58156 - Test loss: 0.66739\n",
      "Epoch 18646 - lr: 0.01000 - Train loss: 0.57722 - Test loss: 0.66685\n",
      "Epoch 18647 - lr: 0.01000 - Train loss: 0.57286 - Test loss: 0.66698\n",
      "Epoch 18648 - lr: 0.01000 - Train loss: 0.58111 - Test loss: 0.66748\n",
      "Epoch 18649 - lr: 0.01000 - Train loss: 0.58046 - Test loss: 0.66744\n",
      "Epoch 18650 - lr: 0.01000 - Train loss: 0.57864 - Test loss: 0.66761\n",
      "Epoch 18651 - lr: 0.01000 - Train loss: 0.58031 - Test loss: 0.66711\n",
      "Epoch 18652 - lr: 0.01000 - Train loss: 0.57954 - Test loss: 0.66651\n",
      "Epoch 18653 - lr: 0.01000 - Train loss: 0.57902 - Test loss: 0.66607\n",
      "Epoch 18654 - lr: 0.01000 - Train loss: 0.57804 - Test loss: 0.66580\n",
      "Epoch 18655 - lr: 0.01000 - Train loss: 0.57483 - Test loss: 0.66587\n",
      "Epoch 18656 - lr: 0.01000 - Train loss: 0.58059 - Test loss: 0.66646\n",
      "Epoch 18657 - lr: 0.01000 - Train loss: 0.58137 - Test loss: 0.66646\n",
      "Epoch 18658 - lr: 0.01000 - Train loss: 0.57513 - Test loss: 0.66635\n",
      "Epoch 18659 - lr: 0.01000 - Train loss: 0.57991 - Test loss: 0.66687\n",
      "Epoch 18660 - lr: 0.01000 - Train loss: 0.58109 - Test loss: 0.66668\n",
      "Epoch 18661 - lr: 0.01000 - Train loss: 0.57870 - Test loss: 0.66627\n",
      "Epoch 18662 - lr: 0.01000 - Train loss: 0.57699 - Test loss: 0.66612\n",
      "Epoch 18663 - lr: 0.01000 - Train loss: 0.57294 - Test loss: 0.66644\n",
      "Epoch 18664 - lr: 0.01000 - Train loss: 0.57973 - Test loss: 0.66705\n",
      "Epoch 18665 - lr: 0.01000 - Train loss: 0.58096 - Test loss: 0.66685\n",
      "Epoch 18666 - lr: 0.01000 - Train loss: 0.57895 - Test loss: 0.66642\n",
      "Epoch 18667 - lr: 0.01000 - Train loss: 0.57777 - Test loss: 0.66620\n",
      "Epoch 18668 - lr: 0.01000 - Train loss: 0.57390 - Test loss: 0.66634\n",
      "Epoch 18669 - lr: 0.01000 - Train loss: 0.58163 - Test loss: 0.66698\n",
      "Epoch 18670 - lr: 0.01000 - Train loss: 0.58660 - Test loss: 0.66773\n",
      "Epoch 18671 - lr: 0.01000 - Train loss: 0.57987 - Test loss: 0.66717\n",
      "Epoch 18672 - lr: 0.01000 - Train loss: 0.57935 - Test loss: 0.66668\n",
      "Epoch 18673 - lr: 0.01000 - Train loss: 0.57874 - Test loss: 0.66633\n",
      "Epoch 18674 - lr: 0.01000 - Train loss: 0.57734 - Test loss: 0.66616\n",
      "Epoch 18675 - lr: 0.01000 - Train loss: 0.57301 - Test loss: 0.66642\n",
      "Epoch 18676 - lr: 0.01000 - Train loss: 0.58149 - Test loss: 0.66711\n",
      "Epoch 18677 - lr: 0.01000 - Train loss: 0.58726 - Test loss: 0.66788\n",
      "Epoch 18678 - lr: 0.01000 - Train loss: 0.58007 - Test loss: 0.66732\n",
      "Epoch 18679 - lr: 0.01000 - Train loss: 0.57944 - Test loss: 0.66680\n",
      "Epoch 18680 - lr: 0.01000 - Train loss: 0.57892 - Test loss: 0.66643\n",
      "Epoch 18681 - lr: 0.01000 - Train loss: 0.57791 - Test loss: 0.66622\n",
      "Epoch 18682 - lr: 0.01000 - Train loss: 0.57451 - Test loss: 0.66631\n",
      "Epoch 18683 - lr: 0.01000 - Train loss: 0.58103 - Test loss: 0.66690\n",
      "Epoch 18684 - lr: 0.01000 - Train loss: 0.58054 - Test loss: 0.66715\n",
      "Epoch 18685 - lr: 0.01000 - Train loss: 0.57637 - Test loss: 0.66739\n",
      "Epoch 18686 - lr: 0.01000 - Train loss: 0.57899 - Test loss: 0.66722\n",
      "Epoch 18687 - lr: 0.01000 - Train loss: 0.57782 - Test loss: 0.66695\n",
      "Epoch 18688 - lr: 0.01000 - Train loss: 0.57395 - Test loss: 0.66702\n",
      "Epoch 18689 - lr: 0.01000 - Train loss: 0.58161 - Test loss: 0.66759\n",
      "Epoch 18690 - lr: 0.01000 - Train loss: 0.58538 - Test loss: 0.66827\n",
      "Epoch 18691 - lr: 0.01000 - Train loss: 0.57939 - Test loss: 0.66771\n",
      "Epoch 18692 - lr: 0.01000 - Train loss: 0.57862 - Test loss: 0.66727\n",
      "Epoch 18693 - lr: 0.01000 - Train loss: 0.57676 - Test loss: 0.66704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18694 - lr: 0.01000 - Train loss: 0.57310 - Test loss: 0.66731\n",
      "Epoch 18695 - lr: 0.01000 - Train loss: 0.57600 - Test loss: 0.66779\n",
      "Epoch 18696 - lr: 0.01000 - Train loss: 0.57869 - Test loss: 0.66761\n",
      "Epoch 18697 - lr: 0.01000 - Train loss: 0.57683 - Test loss: 0.66740\n",
      "Epoch 18698 - lr: 0.01000 - Train loss: 0.57303 - Test loss: 0.66766\n",
      "Epoch 18699 - lr: 0.01000 - Train loss: 0.57670 - Test loss: 0.66812\n",
      "Epoch 18700 - lr: 0.01000 - Train loss: 0.57931 - Test loss: 0.66786\n",
      "Epoch 18701 - lr: 0.01000 - Train loss: 0.57852 - Test loss: 0.66747\n",
      "Epoch 18702 - lr: 0.01000 - Train loss: 0.57644 - Test loss: 0.66729\n",
      "Epoch 18703 - lr: 0.01000 - Train loss: 0.57377 - Test loss: 0.66760\n",
      "Epoch 18704 - lr: 0.01000 - Train loss: 0.57275 - Test loss: 0.66794\n",
      "Epoch 18705 - lr: 0.01000 - Train loss: 0.58163 - Test loss: 0.66853\n",
      "Epoch 18706 - lr: 0.01000 - Train loss: 0.58951 - Test loss: 0.66920\n",
      "Epoch 18707 - lr: 0.01000 - Train loss: 0.58126 - Test loss: 0.66855\n",
      "Epoch 18708 - lr: 0.01000 - Train loss: 0.57822 - Test loss: 0.66796\n",
      "Epoch 18709 - lr: 0.01000 - Train loss: 0.57530 - Test loss: 0.66775\n",
      "Epoch 18710 - lr: 0.01000 - Train loss: 0.57839 - Test loss: 0.66810\n",
      "Epoch 18711 - lr: 0.01000 - Train loss: 0.58004 - Test loss: 0.66782\n",
      "Epoch 18712 - lr: 0.01000 - Train loss: 0.57935 - Test loss: 0.66735\n",
      "Epoch 18713 - lr: 0.01000 - Train loss: 0.57881 - Test loss: 0.66701\n",
      "Epoch 18714 - lr: 0.01000 - Train loss: 0.57768 - Test loss: 0.66683\n",
      "Epoch 18715 - lr: 0.01000 - Train loss: 0.57389 - Test loss: 0.66697\n",
      "Epoch 18716 - lr: 0.01000 - Train loss: 0.58134 - Test loss: 0.66757\n",
      "Epoch 18717 - lr: 0.01000 - Train loss: 0.58030 - Test loss: 0.66823\n",
      "Epoch 18718 - lr: 0.01000 - Train loss: 0.57606 - Test loss: 0.66846\n",
      "Epoch 18719 - lr: 0.01000 - Train loss: 0.57871 - Test loss: 0.66823\n",
      "Epoch 18720 - lr: 0.01000 - Train loss: 0.57691 - Test loss: 0.66795\n",
      "Epoch 18721 - lr: 0.01000 - Train loss: 0.57281 - Test loss: 0.66814\n",
      "Epoch 18722 - lr: 0.01000 - Train loss: 0.57911 - Test loss: 0.66860\n",
      "Epoch 18723 - lr: 0.01000 - Train loss: 0.58051 - Test loss: 0.66831\n",
      "Epoch 18724 - lr: 0.01000 - Train loss: 0.57919 - Test loss: 0.66779\n",
      "Epoch 18725 - lr: 0.01000 - Train loss: 0.57847 - Test loss: 0.66744\n",
      "Epoch 18726 - lr: 0.01000 - Train loss: 0.57662 - Test loss: 0.66728\n",
      "Epoch 18727 - lr: 0.01000 - Train loss: 0.57315 - Test loss: 0.66758\n",
      "Epoch 18728 - lr: 0.01000 - Train loss: 0.57501 - Test loss: 0.66808\n",
      "Epoch 18729 - lr: 0.01000 - Train loss: 0.57690 - Test loss: 0.66810\n",
      "Epoch 18730 - lr: 0.01000 - Train loss: 0.57282 - Test loss: 0.66835\n",
      "Epoch 18731 - lr: 0.01000 - Train loss: 0.57891 - Test loss: 0.66882\n",
      "Epoch 18732 - lr: 0.01000 - Train loss: 0.58038 - Test loss: 0.66853\n",
      "Epoch 18733 - lr: 0.01000 - Train loss: 0.57924 - Test loss: 0.66802\n",
      "Epoch 18734 - lr: 0.01000 - Train loss: 0.57855 - Test loss: 0.66765\n",
      "Epoch 18735 - lr: 0.01000 - Train loss: 0.57692 - Test loss: 0.66747\n",
      "Epoch 18736 - lr: 0.01000 - Train loss: 0.57273 - Test loss: 0.66771\n",
      "Epoch 18737 - lr: 0.01000 - Train loss: 0.58064 - Test loss: 0.66828\n",
      "Epoch 18738 - lr: 0.01000 - Train loss: 0.58106 - Test loss: 0.66836\n",
      "Epoch 18739 - lr: 0.01000 - Train loss: 0.57279 - Test loss: 0.66836\n",
      "Epoch 18740 - lr: 0.01000 - Train loss: 0.58120 - Test loss: 0.66889\n",
      "Epoch 18741 - lr: 0.01000 - Train loss: 0.58465 - Test loss: 0.66948\n",
      "Epoch 18742 - lr: 0.01000 - Train loss: 0.57890 - Test loss: 0.66891\n",
      "Epoch 18743 - lr: 0.01000 - Train loss: 0.57743 - Test loss: 0.66850\n",
      "Epoch 18744 - lr: 0.01000 - Train loss: 0.57296 - Test loss: 0.66852\n",
      "Epoch 18745 - lr: 0.01000 - Train loss: 0.58088 - Test loss: 0.66901\n",
      "Epoch 18746 - lr: 0.01000 - Train loss: 0.58330 - Test loss: 0.66956\n",
      "Epoch 18747 - lr: 0.01000 - Train loss: 0.57734 - Test loss: 0.66908\n",
      "Epoch 18748 - lr: 0.01000 - Train loss: 0.57264 - Test loss: 0.66908\n",
      "Epoch 18749 - lr: 0.01000 - Train loss: 0.58138 - Test loss: 0.66952\n",
      "Epoch 18750 - lr: 0.01000 - Train loss: 0.58929 - Test loss: 0.67009\n",
      "Epoch 18751 - lr: 0.01000 - Train loss: 0.58113 - Test loss: 0.66937\n",
      "Epoch 18752 - lr: 0.01000 - Train loss: 0.57831 - Test loss: 0.66872\n",
      "Epoch 18753 - lr: 0.01000 - Train loss: 0.57577 - Test loss: 0.66843\n",
      "Epoch 18754 - lr: 0.01000 - Train loss: 0.57580 - Test loss: 0.66866\n",
      "Epoch 18755 - lr: 0.01000 - Train loss: 0.57844 - Test loss: 0.66849\n",
      "Epoch 18756 - lr: 0.01000 - Train loss: 0.57637 - Test loss: 0.66828\n",
      "Epoch 18757 - lr: 0.01000 - Train loss: 0.57368 - Test loss: 0.66853\n",
      "Epoch 18758 - lr: 0.01000 - Train loss: 0.57263 - Test loss: 0.66883\n",
      "Epoch 18759 - lr: 0.01000 - Train loss: 0.58137 - Test loss: 0.66936\n",
      "Epoch 18760 - lr: 0.01000 - Train loss: 0.58743 - Test loss: 0.66997\n",
      "Epoch 18761 - lr: 0.01000 - Train loss: 0.58011 - Test loss: 0.66928\n",
      "Epoch 18762 - lr: 0.01000 - Train loss: 0.57929 - Test loss: 0.66865\n",
      "Epoch 18763 - lr: 0.01000 - Train loss: 0.57866 - Test loss: 0.66818\n",
      "Epoch 18764 - lr: 0.01000 - Train loss: 0.57732 - Test loss: 0.66791\n",
      "Epoch 18765 - lr: 0.01000 - Train loss: 0.57302 - Test loss: 0.66802\n",
      "Epoch 18766 - lr: 0.01000 - Train loss: 0.58088 - Test loss: 0.66858\n",
      "Epoch 18767 - lr: 0.01000 - Train loss: 0.58104 - Test loss: 0.66922\n",
      "Epoch 18768 - lr: 0.01000 - Train loss: 0.57269 - Test loss: 0.66928\n",
      "Epoch 18769 - lr: 0.01000 - Train loss: 0.58020 - Test loss: 0.66967\n",
      "Epoch 18770 - lr: 0.01000 - Train loss: 0.58126 - Test loss: 0.66939\n",
      "Epoch 18771 - lr: 0.01000 - Train loss: 0.57656 - Test loss: 0.66894\n",
      "Epoch 18772 - lr: 0.01000 - Train loss: 0.57321 - Test loss: 0.66906\n",
      "Epoch 18773 - lr: 0.01000 - Train loss: 0.57343 - Test loss: 0.66936\n",
      "Epoch 18774 - lr: 0.01000 - Train loss: 0.57262 - Test loss: 0.66960\n",
      "Epoch 18775 - lr: 0.01000 - Train loss: 0.58013 - Test loss: 0.66997\n",
      "Epoch 18776 - lr: 0.01000 - Train loss: 0.58125 - Test loss: 0.66964\n",
      "Epoch 18777 - lr: 0.01000 - Train loss: 0.57679 - Test loss: 0.66914\n",
      "Epoch 18778 - lr: 0.01000 - Train loss: 0.57271 - Test loss: 0.66922\n",
      "Epoch 18779 - lr: 0.01000 - Train loss: 0.57803 - Test loss: 0.66957\n",
      "Epoch 18780 - lr: 0.01000 - Train loss: 0.57988 - Test loss: 0.66923\n",
      "Epoch 18781 - lr: 0.01000 - Train loss: 0.57922 - Test loss: 0.66870\n",
      "Epoch 18782 - lr: 0.01000 - Train loss: 0.57858 - Test loss: 0.66830\n",
      "Epoch 18783 - lr: 0.01000 - Train loss: 0.57715 - Test loss: 0.66808\n",
      "Epoch 18784 - lr: 0.01000 - Train loss: 0.57275 - Test loss: 0.66824\n",
      "Epoch 18785 - lr: 0.01000 - Train loss: 0.58094 - Test loss: 0.66880\n",
      "Epoch 18786 - lr: 0.01000 - Train loss: 0.58072 - Test loss: 0.66944\n",
      "Epoch 18787 - lr: 0.01000 - Train loss: 0.57314 - Test loss: 0.66954\n",
      "Epoch 18788 - lr: 0.01000 - Train loss: 0.57405 - Test loss: 0.66983\n",
      "Epoch 18789 - lr: 0.01000 - Train loss: 0.57380 - Test loss: 0.66987\n",
      "Epoch 18790 - lr: 0.01000 - Train loss: 0.58146 - Test loss: 0.67023\n",
      "Epoch 18791 - lr: 0.01000 - Train loss: 0.58881 - Test loss: 0.67076\n",
      "Epoch 18792 - lr: 0.01000 - Train loss: 0.58084 - Test loss: 0.67001\n",
      "Epoch 18793 - lr: 0.01000 - Train loss: 0.57872 - Test loss: 0.66931\n",
      "Epoch 18794 - lr: 0.01000 - Train loss: 0.57731 - Test loss: 0.66888\n",
      "Epoch 18795 - lr: 0.01000 - Train loss: 0.57290 - Test loss: 0.66888\n",
      "Epoch 18796 - lr: 0.01000 - Train loss: 0.58071 - Test loss: 0.66934\n",
      "Epoch 18797 - lr: 0.01000 - Train loss: 0.58150 - Test loss: 0.66991\n",
      "Epoch 18798 - lr: 0.01000 - Train loss: 0.57300 - Test loss: 0.66979\n",
      "Epoch 18799 - lr: 0.01000 - Train loss: 0.58066 - Test loss: 0.67019\n",
      "Epoch 18800 - lr: 0.01000 - Train loss: 0.58396 - Test loss: 0.67066\n",
      "Epoch 18801 - lr: 0.01000 - Train loss: 0.57835 - Test loss: 0.67001\n",
      "Epoch 18802 - lr: 0.01000 - Train loss: 0.57562 - Test loss: 0.66963\n",
      "Epoch 18803 - lr: 0.01000 - Train loss: 0.57642 - Test loss: 0.66978\n",
      "Epoch 18804 - lr: 0.01000 - Train loss: 0.57904 - Test loss: 0.66947\n",
      "Epoch 18805 - lr: 0.01000 - Train loss: 0.57817 - Test loss: 0.66903\n",
      "Epoch 18806 - lr: 0.01000 - Train loss: 0.57564 - Test loss: 0.66884\n",
      "Epoch 18807 - lr: 0.01000 - Train loss: 0.57587 - Test loss: 0.66909\n",
      "Epoch 18808 - lr: 0.01000 - Train loss: 0.57848 - Test loss: 0.66900\n",
      "Epoch 18809 - lr: 0.01000 - Train loss: 0.57677 - Test loss: 0.66880\n",
      "Epoch 18810 - lr: 0.01000 - Train loss: 0.57262 - Test loss: 0.66899\n",
      "Epoch 18811 - lr: 0.01000 - Train loss: 0.57954 - Test loss: 0.66945\n",
      "Epoch 18812 - lr: 0.01000 - Train loss: 0.58075 - Test loss: 0.66933\n",
      "Epoch 18813 - lr: 0.01000 - Train loss: 0.57849 - Test loss: 0.66890\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18814 - lr: 0.01000 - Train loss: 0.57679 - Test loss: 0.66869\n",
      "Epoch 18815 - lr: 0.01000 - Train loss: 0.57260 - Test loss: 0.66886\n",
      "Epoch 18816 - lr: 0.01000 - Train loss: 0.58025 - Test loss: 0.66934\n",
      "Epoch 18817 - lr: 0.01000 - Train loss: 0.58107 - Test loss: 0.66937\n",
      "Epoch 18818 - lr: 0.01000 - Train loss: 0.57519 - Test loss: 0.66913\n",
      "Epoch 18819 - lr: 0.01000 - Train loss: 0.57798 - Test loss: 0.66941\n",
      "Epoch 18820 - lr: 0.01000 - Train loss: 0.57971 - Test loss: 0.66928\n",
      "Epoch 18821 - lr: 0.01000 - Train loss: 0.57913 - Test loss: 0.66886\n",
      "Epoch 18822 - lr: 0.01000 - Train loss: 0.57854 - Test loss: 0.66856\n",
      "Epoch 18823 - lr: 0.01000 - Train loss: 0.57716 - Test loss: 0.66840\n",
      "Epoch 18824 - lr: 0.01000 - Train loss: 0.57284 - Test loss: 0.66856\n",
      "Epoch 18825 - lr: 0.01000 - Train loss: 0.58069 - Test loss: 0.66907\n",
      "Epoch 18826 - lr: 0.01000 - Train loss: 0.57909 - Test loss: 0.66970\n",
      "Epoch 18827 - lr: 0.01000 - Train loss: 0.58122 - Test loss: 0.66994\n",
      "Epoch 18828 - lr: 0.01000 - Train loss: 0.58023 - Test loss: 0.67044\n",
      "Epoch 18829 - lr: 0.01000 - Train loss: 0.57473 - Test loss: 0.67046\n",
      "Epoch 18830 - lr: 0.01000 - Train loss: 0.57634 - Test loss: 0.67027\n",
      "Epoch 18831 - lr: 0.01000 - Train loss: 0.57357 - Test loss: 0.67034\n",
      "Epoch 18832 - lr: 0.01000 - Train loss: 0.57247 - Test loss: 0.67047\n",
      "Epoch 18833 - lr: 0.01000 - Train loss: 0.58095 - Test loss: 0.67083\n",
      "Epoch 18834 - lr: 0.01000 - Train loss: 0.58640 - Test loss: 0.67130\n",
      "Epoch 18835 - lr: 0.01000 - Train loss: 0.57971 - Test loss: 0.67055\n",
      "Epoch 18836 - lr: 0.01000 - Train loss: 0.57905 - Test loss: 0.66987\n",
      "Epoch 18837 - lr: 0.01000 - Train loss: 0.57826 - Test loss: 0.66938\n",
      "Epoch 18838 - lr: 0.01000 - Train loss: 0.57622 - Test loss: 0.66912\n",
      "Epoch 18839 - lr: 0.01000 - Train loss: 0.57345 - Test loss: 0.66929\n",
      "Epoch 18840 - lr: 0.01000 - Train loss: 0.57245 - Test loss: 0.66963\n",
      "Epoch 18841 - lr: 0.01000 - Train loss: 0.58095 - Test loss: 0.67008\n",
      "Epoch 18842 - lr: 0.01000 - Train loss: 0.57916 - Test loss: 0.67038\n",
      "Epoch 18843 - lr: 0.01000 - Train loss: 0.58118 - Test loss: 0.67053\n",
      "Epoch 18844 - lr: 0.01000 - Train loss: 0.58509 - Test loss: 0.67104\n",
      "Epoch 18845 - lr: 0.01000 - Train loss: 0.57915 - Test loss: 0.67040\n",
      "Epoch 18846 - lr: 0.01000 - Train loss: 0.57828 - Test loss: 0.66986\n",
      "Epoch 18847 - lr: 0.01000 - Train loss: 0.57610 - Test loss: 0.66955\n",
      "Epoch 18848 - lr: 0.01000 - Train loss: 0.57385 - Test loss: 0.66968\n",
      "Epoch 18849 - lr: 0.01000 - Train loss: 0.57324 - Test loss: 0.66987\n",
      "Epoch 18850 - lr: 0.01000 - Train loss: 0.58081 - Test loss: 0.67027\n",
      "Epoch 18851 - lr: 0.01000 - Train loss: 0.58227 - Test loss: 0.67083\n",
      "Epoch 18852 - lr: 0.01000 - Train loss: 0.57529 - Test loss: 0.67049\n",
      "Epoch 18853 - lr: 0.01000 - Train loss: 0.57784 - Test loss: 0.67063\n",
      "Epoch 18854 - lr: 0.01000 - Train loss: 0.57971 - Test loss: 0.67030\n",
      "Epoch 18855 - lr: 0.01000 - Train loss: 0.57909 - Test loss: 0.66974\n",
      "Epoch 18856 - lr: 0.01000 - Train loss: 0.57843 - Test loss: 0.66933\n",
      "Epoch 18857 - lr: 0.01000 - Train loss: 0.57690 - Test loss: 0.66909\n",
      "Epoch 18858 - lr: 0.01000 - Train loss: 0.57253 - Test loss: 0.66921\n",
      "Epoch 18859 - lr: 0.01000 - Train loss: 0.58097 - Test loss: 0.66968\n",
      "Epoch 18860 - lr: 0.01000 - Train loss: 0.57900 - Test loss: 0.67025\n",
      "Epoch 18861 - lr: 0.01000 - Train loss: 0.58122 - Test loss: 0.67043\n",
      "Epoch 18862 - lr: 0.01000 - Train loss: 0.58139 - Test loss: 0.67094\n",
      "Epoch 18863 - lr: 0.01000 - Train loss: 0.57285 - Test loss: 0.67078\n",
      "Epoch 18864 - lr: 0.01000 - Train loss: 0.58040 - Test loss: 0.67108\n",
      "Epoch 18865 - lr: 0.01000 - Train loss: 0.58163 - Test loss: 0.67148\n",
      "Epoch 18866 - lr: 0.01000 - Train loss: 0.57345 - Test loss: 0.67116\n",
      "Epoch 18867 - lr: 0.01000 - Train loss: 0.58109 - Test loss: 0.67137\n",
      "Epoch 18868 - lr: 0.01000 - Train loss: 0.58907 - Test loss: 0.67182\n",
      "Epoch 18869 - lr: 0.01000 - Train loss: 0.58096 - Test loss: 0.67107\n",
      "Epoch 18870 - lr: 0.01000 - Train loss: 0.57812 - Test loss: 0.67035\n",
      "Epoch 18871 - lr: 0.01000 - Train loss: 0.57547 - Test loss: 0.67000\n",
      "Epoch 18872 - lr: 0.01000 - Train loss: 0.57623 - Test loss: 0.67012\n",
      "Epoch 18873 - lr: 0.01000 - Train loss: 0.57883 - Test loss: 0.66995\n",
      "Epoch 18874 - lr: 0.01000 - Train loss: 0.57790 - Test loss: 0.66960\n",
      "Epoch 18875 - lr: 0.01000 - Train loss: 0.57494 - Test loss: 0.66949\n",
      "Epoch 18876 - lr: 0.01000 - Train loss: 0.57844 - Test loss: 0.66975\n",
      "Epoch 18877 - lr: 0.01000 - Train loss: 0.57986 - Test loss: 0.66971\n",
      "Epoch 18878 - lr: 0.01000 - Train loss: 0.57910 - Test loss: 0.66934\n",
      "Epoch 18879 - lr: 0.01000 - Train loss: 0.57856 - Test loss: 0.66906\n",
      "Epoch 18880 - lr: 0.01000 - Train loss: 0.57736 - Test loss: 0.66891\n",
      "Epoch 18881 - lr: 0.01000 - Train loss: 0.57333 - Test loss: 0.66902\n",
      "Epoch 18882 - lr: 0.01000 - Train loss: 0.58072 - Test loss: 0.66946\n",
      "Epoch 18883 - lr: 0.01000 - Train loss: 0.57892 - Test loss: 0.67011\n",
      "Epoch 18884 - lr: 0.01000 - Train loss: 0.58085 - Test loss: 0.67031\n",
      "Epoch 18885 - lr: 0.01000 - Train loss: 0.58045 - Test loss: 0.67089\n",
      "Epoch 18886 - lr: 0.01000 - Train loss: 0.57339 - Test loss: 0.67089\n",
      "Epoch 18887 - lr: 0.01000 - Train loss: 0.57240 - Test loss: 0.67105\n",
      "Epoch 18888 - lr: 0.01000 - Train loss: 0.58079 - Test loss: 0.67133\n",
      "Epoch 18889 - lr: 0.01000 - Train loss: 0.57992 - Test loss: 0.67133\n",
      "Epoch 18890 - lr: 0.01000 - Train loss: 0.57872 - Test loss: 0.67124\n",
      "Epoch 18891 - lr: 0.01000 - Train loss: 0.58016 - Test loss: 0.67087\n",
      "Epoch 18892 - lr: 0.01000 - Train loss: 0.57902 - Test loss: 0.67025\n",
      "Epoch 18893 - lr: 0.01000 - Train loss: 0.57833 - Test loss: 0.66980\n",
      "Epoch 18894 - lr: 0.01000 - Train loss: 0.57666 - Test loss: 0.66955\n",
      "Epoch 18895 - lr: 0.01000 - Train loss: 0.57255 - Test loss: 0.66966\n",
      "Epoch 18896 - lr: 0.01000 - Train loss: 0.57970 - Test loss: 0.67006\n",
      "Epoch 18897 - lr: 0.01000 - Train loss: 0.58076 - Test loss: 0.67012\n",
      "Epoch 18898 - lr: 0.01000 - Train loss: 0.57799 - Test loss: 0.66975\n",
      "Epoch 18899 - lr: 0.01000 - Train loss: 0.57527 - Test loss: 0.66964\n",
      "Epoch 18900 - lr: 0.01000 - Train loss: 0.57697 - Test loss: 0.66987\n",
      "Epoch 18901 - lr: 0.01000 - Train loss: 0.57915 - Test loss: 0.66991\n",
      "Epoch 18902 - lr: 0.01000 - Train loss: 0.57868 - Test loss: 0.66963\n",
      "Epoch 18903 - lr: 0.01000 - Train loss: 0.57763 - Test loss: 0.66945\n",
      "Epoch 18904 - lr: 0.01000 - Train loss: 0.57417 - Test loss: 0.66946\n",
      "Epoch 18905 - lr: 0.01000 - Train loss: 0.58048 - Test loss: 0.66978\n",
      "Epoch 18906 - lr: 0.01000 - Train loss: 0.58073 - Test loss: 0.67012\n",
      "Epoch 18907 - lr: 0.01000 - Train loss: 0.57261 - Test loss: 0.67006\n",
      "Epoch 18908 - lr: 0.01000 - Train loss: 0.58078 - Test loss: 0.67044\n",
      "Epoch 18909 - lr: 0.01000 - Train loss: 0.57929 - Test loss: 0.67103\n",
      "Epoch 18910 - lr: 0.01000 - Train loss: 0.58004 - Test loss: 0.67110\n",
      "Epoch 18911 - lr: 0.01000 - Train loss: 0.58097 - Test loss: 0.67100\n",
      "Epoch 18912 - lr: 0.01000 - Train loss: 0.57644 - Test loss: 0.67056\n",
      "Epoch 18913 - lr: 0.01000 - Train loss: 0.57287 - Test loss: 0.67059\n",
      "Epoch 18914 - lr: 0.01000 - Train loss: 0.57457 - Test loss: 0.67086\n",
      "Epoch 18915 - lr: 0.01000 - Train loss: 0.57612 - Test loss: 0.67084\n",
      "Epoch 18916 - lr: 0.01000 - Train loss: 0.57369 - Test loss: 0.67094\n",
      "Epoch 18917 - lr: 0.01000 - Train loss: 0.57280 - Test loss: 0.67110\n",
      "Epoch 18918 - lr: 0.01000 - Train loss: 0.58024 - Test loss: 0.67142\n",
      "Epoch 18919 - lr: 0.01000 - Train loss: 0.57959 - Test loss: 0.67186\n",
      "Epoch 18920 - lr: 0.01000 - Train loss: 0.57772 - Test loss: 0.67181\n",
      "Epoch 18921 - lr: 0.01000 - Train loss: 0.57964 - Test loss: 0.67144\n",
      "Epoch 18922 - lr: 0.01000 - Train loss: 0.57901 - Test loss: 0.67084\n",
      "Epoch 18923 - lr: 0.01000 - Train loss: 0.57831 - Test loss: 0.67038\n",
      "Epoch 18924 - lr: 0.01000 - Train loss: 0.57662 - Test loss: 0.67011\n",
      "Epoch 18925 - lr: 0.01000 - Train loss: 0.57249 - Test loss: 0.67018\n",
      "Epoch 18926 - lr: 0.01000 - Train loss: 0.57962 - Test loss: 0.67054\n",
      "Epoch 18927 - lr: 0.01000 - Train loss: 0.58070 - Test loss: 0.67062\n",
      "Epoch 18928 - lr: 0.01000 - Train loss: 0.57794 - Test loss: 0.67025\n",
      "Epoch 18929 - lr: 0.01000 - Train loss: 0.57516 - Test loss: 0.67014\n",
      "Epoch 18930 - lr: 0.01000 - Train loss: 0.57724 - Test loss: 0.67032\n",
      "Epoch 18931 - lr: 0.01000 - Train loss: 0.57925 - Test loss: 0.67039\n",
      "Epoch 18932 - lr: 0.01000 - Train loss: 0.57882 - Test loss: 0.67011\n",
      "Epoch 18933 - lr: 0.01000 - Train loss: 0.57801 - Test loss: 0.66990\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18934 - lr: 0.01000 - Train loss: 0.57557 - Test loss: 0.66984\n",
      "Epoch 18935 - lr: 0.01000 - Train loss: 0.57563 - Test loss: 0.67002\n",
      "Epoch 18936 - lr: 0.01000 - Train loss: 0.57795 - Test loss: 0.67023\n",
      "Epoch 18937 - lr: 0.01000 - Train loss: 0.57540 - Test loss: 0.67020\n",
      "Epoch 18938 - lr: 0.01000 - Train loss: 0.57638 - Test loss: 0.67039\n",
      "Epoch 18939 - lr: 0.01000 - Train loss: 0.57875 - Test loss: 0.67053\n",
      "Epoch 18940 - lr: 0.01000 - Train loss: 0.57789 - Test loss: 0.67034\n",
      "Epoch 18941 - lr: 0.01000 - Train loss: 0.57503 - Test loss: 0.67028\n",
      "Epoch 18942 - lr: 0.01000 - Train loss: 0.57778 - Test loss: 0.67045\n",
      "Epoch 18943 - lr: 0.01000 - Train loss: 0.57945 - Test loss: 0.67061\n",
      "Epoch 18944 - lr: 0.01000 - Train loss: 0.57898 - Test loss: 0.67034\n",
      "Epoch 18945 - lr: 0.01000 - Train loss: 0.57835 - Test loss: 0.67013\n",
      "Epoch 18946 - lr: 0.01000 - Train loss: 0.57670 - Test loss: 0.67003\n",
      "Epoch 18947 - lr: 0.01000 - Train loss: 0.57266 - Test loss: 0.67014\n",
      "Epoch 18948 - lr: 0.01000 - Train loss: 0.57955 - Test loss: 0.67050\n",
      "Epoch 18949 - lr: 0.01000 - Train loss: 0.58056 - Test loss: 0.67079\n",
      "Epoch 18950 - lr: 0.01000 - Train loss: 0.57820 - Test loss: 0.67050\n",
      "Epoch 18951 - lr: 0.01000 - Train loss: 0.57608 - Test loss: 0.67039\n",
      "Epoch 18952 - lr: 0.01000 - Train loss: 0.57379 - Test loss: 0.67049\n",
      "Epoch 18953 - lr: 0.01000 - Train loss: 0.57272 - Test loss: 0.67084\n",
      "Epoch 18954 - lr: 0.01000 - Train loss: 0.58006 - Test loss: 0.67117\n",
      "Epoch 18955 - lr: 0.01000 - Train loss: 0.57898 - Test loss: 0.67169\n",
      "Epoch 18956 - lr: 0.01000 - Train loss: 0.58072 - Test loss: 0.67167\n",
      "Epoch 18957 - lr: 0.01000 - Train loss: 0.57883 - Test loss: 0.67215\n",
      "Epoch 18958 - lr: 0.01000 - Train loss: 0.58092 - Test loss: 0.67211\n",
      "Epoch 18959 - lr: 0.01000 - Train loss: 0.57906 - Test loss: 0.67250\n",
      "Epoch 18960 - lr: 0.01000 - Train loss: 0.58034 - Test loss: 0.67239\n",
      "Epoch 18961 - lr: 0.01000 - Train loss: 0.58061 - Test loss: 0.67229\n",
      "Epoch 18962 - lr: 0.01000 - Train loss: 0.57225 - Test loss: 0.67198\n",
      "Epoch 18963 - lr: 0.01000 - Train loss: 0.58092 - Test loss: 0.67214\n",
      "Epoch 18964 - lr: 0.01000 - Train loss: 0.57919 - Test loss: 0.67251\n",
      "Epoch 18965 - lr: 0.01000 - Train loss: 0.57959 - Test loss: 0.67239\n",
      "Epoch 18966 - lr: 0.01000 - Train loss: 0.58076 - Test loss: 0.67214\n",
      "Epoch 18967 - lr: 0.01000 - Train loss: 0.57763 - Test loss: 0.67154\n",
      "Epoch 18968 - lr: 0.01000 - Train loss: 0.57398 - Test loss: 0.67130\n",
      "Epoch 18969 - lr: 0.01000 - Train loss: 0.58060 - Test loss: 0.67141\n",
      "Epoch 18970 - lr: 0.01000 - Train loss: 0.57948 - Test loss: 0.67172\n",
      "Epoch 18971 - lr: 0.01000 - Train loss: 0.57961 - Test loss: 0.67161\n",
      "Epoch 18972 - lr: 0.01000 - Train loss: 0.58067 - Test loss: 0.67159\n",
      "Epoch 18973 - lr: 0.01000 - Train loss: 0.57773 - Test loss: 0.67112\n",
      "Epoch 18974 - lr: 0.01000 - Train loss: 0.57446 - Test loss: 0.67095\n",
      "Epoch 18975 - lr: 0.01000 - Train loss: 0.57958 - Test loss: 0.67107\n",
      "Epoch 18976 - lr: 0.01000 - Train loss: 0.58060 - Test loss: 0.67122\n",
      "Epoch 18977 - lr: 0.01000 - Train loss: 0.57788 - Test loss: 0.67085\n",
      "Epoch 18978 - lr: 0.01000 - Train loss: 0.57505 - Test loss: 0.67072\n",
      "Epoch 18979 - lr: 0.01000 - Train loss: 0.57751 - Test loss: 0.67083\n",
      "Epoch 18980 - lr: 0.01000 - Train loss: 0.57932 - Test loss: 0.67099\n",
      "Epoch 18981 - lr: 0.01000 - Train loss: 0.57891 - Test loss: 0.67072\n",
      "Epoch 18982 - lr: 0.01000 - Train loss: 0.57823 - Test loss: 0.67052\n",
      "Epoch 18983 - lr: 0.01000 - Train loss: 0.57640 - Test loss: 0.67042\n",
      "Epoch 18984 - lr: 0.01000 - Train loss: 0.57301 - Test loss: 0.67051\n",
      "Epoch 18985 - lr: 0.01000 - Train loss: 0.57479 - Test loss: 0.67088\n",
      "Epoch 18986 - lr: 0.01000 - Train loss: 0.57639 - Test loss: 0.67114\n",
      "Epoch 18987 - lr: 0.01000 - Train loss: 0.57280 - Test loss: 0.67126\n",
      "Epoch 18988 - lr: 0.01000 - Train loss: 0.57549 - Test loss: 0.67157\n",
      "Epoch 18989 - lr: 0.01000 - Train loss: 0.57794 - Test loss: 0.67165\n",
      "Epoch 18990 - lr: 0.01000 - Train loss: 0.57526 - Test loss: 0.67150\n",
      "Epoch 18991 - lr: 0.01000 - Train loss: 0.57662 - Test loss: 0.67156\n",
      "Epoch 18992 - lr: 0.01000 - Train loss: 0.57892 - Test loss: 0.67163\n",
      "Epoch 18993 - lr: 0.01000 - Train loss: 0.57828 - Test loss: 0.67133\n",
      "Epoch 18994 - lr: 0.01000 - Train loss: 0.57650 - Test loss: 0.67114\n",
      "Epoch 18995 - lr: 0.01000 - Train loss: 0.57262 - Test loss: 0.67117\n",
      "Epoch 18996 - lr: 0.01000 - Train loss: 0.57773 - Test loss: 0.67144\n",
      "Epoch 18997 - lr: 0.01000 - Train loss: 0.57945 - Test loss: 0.67160\n",
      "Epoch 18998 - lr: 0.01000 - Train loss: 0.57892 - Test loss: 0.67131\n",
      "Epoch 18999 - lr: 0.01000 - Train loss: 0.57821 - Test loss: 0.67108\n",
      "Epoch 19000 - lr: 0.01000 - Train loss: 0.57632 - Test loss: 0.67095\n",
      "Epoch 19001 - lr: 0.01000 - Train loss: 0.57308 - Test loss: 0.67100\n",
      "Epoch 19002 - lr: 0.01000 - Train loss: 0.57395 - Test loss: 0.67134\n",
      "Epoch 19003 - lr: 0.01000 - Train loss: 0.57350 - Test loss: 0.67167\n",
      "Epoch 19004 - lr: 0.01000 - Train loss: 0.58044 - Test loss: 0.67186\n",
      "Epoch 19005 - lr: 0.01000 - Train loss: 0.57872 - Test loss: 0.67242\n",
      "Epoch 19006 - lr: 0.01000 - Train loss: 0.58028 - Test loss: 0.67233\n",
      "Epoch 19007 - lr: 0.01000 - Train loss: 0.57850 - Test loss: 0.67281\n",
      "Epoch 19008 - lr: 0.01000 - Train loss: 0.58008 - Test loss: 0.67271\n",
      "Epoch 19009 - lr: 0.01000 - Train loss: 0.57855 - Test loss: 0.67311\n",
      "Epoch 19010 - lr: 0.01000 - Train loss: 0.58062 - Test loss: 0.67298\n",
      "Epoch 19011 - lr: 0.01000 - Train loss: 0.58067 - Test loss: 0.67337\n",
      "Epoch 19012 - lr: 0.01000 - Train loss: 0.57214 - Test loss: 0.67312\n",
      "Epoch 19013 - lr: 0.01000 - Train loss: 0.58093 - Test loss: 0.67319\n",
      "Epoch 19014 - lr: 0.01000 - Train loss: 0.58029 - Test loss: 0.67352\n",
      "Epoch 19015 - lr: 0.01000 - Train loss: 0.57267 - Test loss: 0.67328\n",
      "Epoch 19016 - lr: 0.01000 - Train loss: 0.57449 - Test loss: 0.67329\n",
      "Epoch 19017 - lr: 0.01000 - Train loss: 0.57623 - Test loss: 0.67304\n",
      "Epoch 19018 - lr: 0.01000 - Train loss: 0.57298 - Test loss: 0.67290\n",
      "Epoch 19019 - lr: 0.01000 - Train loss: 0.57244 - Test loss: 0.67298\n",
      "Epoch 19020 - lr: 0.01000 - Train loss: 0.57614 - Test loss: 0.67308\n",
      "Epoch 19021 - lr: 0.01000 - Train loss: 0.57879 - Test loss: 0.67282\n",
      "Epoch 19022 - lr: 0.01000 - Train loss: 0.57777 - Test loss: 0.67235\n",
      "Epoch 19023 - lr: 0.01000 - Train loss: 0.57461 - Test loss: 0.67208\n",
      "Epoch 19024 - lr: 0.01000 - Train loss: 0.57875 - Test loss: 0.67208\n",
      "Epoch 19025 - lr: 0.01000 - Train loss: 0.58004 - Test loss: 0.67214\n",
      "Epoch 19026 - lr: 0.01000 - Train loss: 0.57866 - Test loss: 0.67173\n",
      "Epoch 19027 - lr: 0.01000 - Train loss: 0.57769 - Test loss: 0.67147\n",
      "Epoch 19028 - lr: 0.01000 - Train loss: 0.57458 - Test loss: 0.67136\n",
      "Epoch 19029 - lr: 0.01000 - Train loss: 0.57891 - Test loss: 0.67142\n",
      "Epoch 19030 - lr: 0.01000 - Train loss: 0.58003 - Test loss: 0.67173\n",
      "Epoch 19031 - lr: 0.01000 - Train loss: 0.57872 - Test loss: 0.67146\n",
      "Epoch 19032 - lr: 0.01000 - Train loss: 0.57781 - Test loss: 0.67129\n",
      "Epoch 19033 - lr: 0.01000 - Train loss: 0.57494 - Test loss: 0.67123\n",
      "Epoch 19034 - lr: 0.01000 - Train loss: 0.57788 - Test loss: 0.67127\n",
      "Epoch 19035 - lr: 0.01000 - Train loss: 0.57941 - Test loss: 0.67164\n",
      "Epoch 19036 - lr: 0.01000 - Train loss: 0.57894 - Test loss: 0.67146\n",
      "Epoch 19037 - lr: 0.01000 - Train loss: 0.57830 - Test loss: 0.67131\n",
      "Epoch 19038 - lr: 0.01000 - Train loss: 0.57656 - Test loss: 0.67123\n",
      "Epoch 19039 - lr: 0.01000 - Train loss: 0.57281 - Test loss: 0.67125\n",
      "Epoch 19040 - lr: 0.01000 - Train loss: 0.57740 - Test loss: 0.67152\n",
      "Epoch 19041 - lr: 0.01000 - Train loss: 0.57923 - Test loss: 0.67189\n",
      "Epoch 19042 - lr: 0.01000 - Train loss: 0.57879 - Test loss: 0.67172\n",
      "Epoch 19043 - lr: 0.01000 - Train loss: 0.57793 - Test loss: 0.67157\n",
      "Epoch 19044 - lr: 0.01000 - Train loss: 0.57523 - Test loss: 0.67150\n",
      "Epoch 19045 - lr: 0.01000 - Train loss: 0.57690 - Test loss: 0.67147\n",
      "Epoch 19046 - lr: 0.01000 - Train loss: 0.57894 - Test loss: 0.67187\n",
      "Epoch 19047 - lr: 0.01000 - Train loss: 0.57839 - Test loss: 0.67174\n",
      "Epoch 19048 - lr: 0.01000 - Train loss: 0.57679 - Test loss: 0.67164\n",
      "Epoch 19049 - lr: 0.01000 - Train loss: 0.57258 - Test loss: 0.67163\n",
      "Epoch 19050 - lr: 0.01000 - Train loss: 0.58003 - Test loss: 0.67180\n",
      "Epoch 19051 - lr: 0.01000 - Train loss: 0.58058 - Test loss: 0.67237\n",
      "Epoch 19052 - lr: 0.01000 - Train loss: 0.57396 - Test loss: 0.67214\n",
      "Epoch 19053 - lr: 0.01000 - Train loss: 0.58017 - Test loss: 0.67214\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19054 - lr: 0.01000 - Train loss: 0.58026 - Test loss: 0.67265\n",
      "Epoch 19055 - lr: 0.01000 - Train loss: 0.57247 - Test loss: 0.67242\n",
      "Epoch 19056 - lr: 0.01000 - Train loss: 0.57883 - Test loss: 0.67256\n",
      "Epoch 19057 - lr: 0.01000 - Train loss: 0.58008 - Test loss: 0.67277\n",
      "Epoch 19058 - lr: 0.01000 - Train loss: 0.57856 - Test loss: 0.67240\n",
      "Epoch 19059 - lr: 0.01000 - Train loss: 0.57736 - Test loss: 0.67216\n",
      "Epoch 19060 - lr: 0.01000 - Train loss: 0.57337 - Test loss: 0.67204\n",
      "Epoch 19061 - lr: 0.01000 - Train loss: 0.58004 - Test loss: 0.67205\n",
      "Epoch 19062 - lr: 0.01000 - Train loss: 0.57951 - Test loss: 0.67268\n",
      "Epoch 19063 - lr: 0.01000 - Train loss: 0.57746 - Test loss: 0.67243\n",
      "Epoch 19064 - lr: 0.01000 - Train loss: 0.57926 - Test loss: 0.67264\n",
      "Epoch 19065 - lr: 0.01000 - Train loss: 0.57875 - Test loss: 0.67236\n",
      "Epoch 19066 - lr: 0.01000 - Train loss: 0.57787 - Test loss: 0.67213\n",
      "Epoch 19067 - lr: 0.01000 - Train loss: 0.57509 - Test loss: 0.67200\n",
      "Epoch 19068 - lr: 0.01000 - Train loss: 0.57721 - Test loss: 0.67191\n",
      "Epoch 19069 - lr: 0.01000 - Train loss: 0.57910 - Test loss: 0.67230\n",
      "Epoch 19070 - lr: 0.01000 - Train loss: 0.57863 - Test loss: 0.67214\n",
      "Epoch 19071 - lr: 0.01000 - Train loss: 0.57753 - Test loss: 0.67201\n",
      "Epoch 19072 - lr: 0.01000 - Train loss: 0.57379 - Test loss: 0.67195\n",
      "Epoch 19073 - lr: 0.01000 - Train loss: 0.58011 - Test loss: 0.67190\n",
      "Epoch 19074 - lr: 0.01000 - Train loss: 0.58025 - Test loss: 0.67261\n",
      "Epoch 19075 - lr: 0.01000 - Train loss: 0.57252 - Test loss: 0.67240\n",
      "Epoch 19076 - lr: 0.01000 - Train loss: 0.57924 - Test loss: 0.67252\n",
      "Epoch 19077 - lr: 0.01000 - Train loss: 0.58032 - Test loss: 0.67292\n",
      "Epoch 19078 - lr: 0.01000 - Train loss: 0.57803 - Test loss: 0.67260\n",
      "Epoch 19079 - lr: 0.01000 - Train loss: 0.57557 - Test loss: 0.67243\n",
      "Epoch 19080 - lr: 0.01000 - Train loss: 0.57522 - Test loss: 0.67231\n",
      "Epoch 19081 - lr: 0.01000 - Train loss: 0.57717 - Test loss: 0.67262\n",
      "Epoch 19082 - lr: 0.01000 - Train loss: 0.57295 - Test loss: 0.67257\n",
      "Epoch 19083 - lr: 0.01000 - Train loss: 0.57949 - Test loss: 0.67259\n",
      "Epoch 19084 - lr: 0.01000 - Train loss: 0.57972 - Test loss: 0.67320\n",
      "Epoch 19085 - lr: 0.01000 - Train loss: 0.57521 - Test loss: 0.67291\n",
      "Epoch 19086 - lr: 0.01000 - Train loss: 0.57729 - Test loss: 0.67309\n",
      "Epoch 19087 - lr: 0.01000 - Train loss: 0.57319 - Test loss: 0.67297\n",
      "Epoch 19088 - lr: 0.01000 - Train loss: 0.57979 - Test loss: 0.67293\n",
      "Epoch 19089 - lr: 0.01000 - Train loss: 0.57898 - Test loss: 0.67350\n",
      "Epoch 19090 - lr: 0.01000 - Train loss: 0.57985 - Test loss: 0.67318\n",
      "Epoch 19091 - lr: 0.01000 - Train loss: 0.58044 - Test loss: 0.67350\n",
      "Epoch 19092 - lr: 0.01000 - Train loss: 0.57362 - Test loss: 0.67314\n",
      "Epoch 19093 - lr: 0.01000 - Train loss: 0.58017 - Test loss: 0.67303\n",
      "Epoch 19094 - lr: 0.01000 - Train loss: 0.57884 - Test loss: 0.67356\n",
      "Epoch 19095 - lr: 0.01000 - Train loss: 0.58022 - Test loss: 0.67324\n",
      "Epoch 19096 - lr: 0.01000 - Train loss: 0.57954 - Test loss: 0.67364\n",
      "Epoch 19097 - lr: 0.01000 - Train loss: 0.57641 - Test loss: 0.67328\n",
      "Epoch 19098 - lr: 0.01000 - Train loss: 0.57875 - Test loss: 0.67335\n",
      "Epoch 19099 - lr: 0.01000 - Train loss: 0.57794 - Test loss: 0.67303\n",
      "Epoch 19100 - lr: 0.01000 - Train loss: 0.57541 - Test loss: 0.67280\n",
      "Epoch 19101 - lr: 0.01000 - Train loss: 0.57551 - Test loss: 0.67266\n",
      "Epoch 19102 - lr: 0.01000 - Train loss: 0.57779 - Test loss: 0.67292\n",
      "Epoch 19103 - lr: 0.01000 - Train loss: 0.57506 - Test loss: 0.67280\n",
      "Epoch 19104 - lr: 0.01000 - Train loss: 0.57703 - Test loss: 0.67269\n",
      "Epoch 19105 - lr: 0.01000 - Train loss: 0.57903 - Test loss: 0.67302\n",
      "Epoch 19106 - lr: 0.01000 - Train loss: 0.57850 - Test loss: 0.67282\n",
      "Epoch 19107 - lr: 0.01000 - Train loss: 0.57721 - Test loss: 0.67266\n",
      "Epoch 19108 - lr: 0.01000 - Train loss: 0.57297 - Test loss: 0.67256\n",
      "Epoch 19109 - lr: 0.01000 - Train loss: 0.57946 - Test loss: 0.67251\n",
      "Epoch 19110 - lr: 0.01000 - Train loss: 0.57998 - Test loss: 0.67324\n",
      "Epoch 19111 - lr: 0.01000 - Train loss: 0.57313 - Test loss: 0.67298\n",
      "Epoch 19112 - lr: 0.01000 - Train loss: 0.57286 - Test loss: 0.67321\n",
      "Epoch 19113 - lr: 0.01000 - Train loss: 0.57342 - Test loss: 0.67343\n",
      "Epoch 19114 - lr: 0.01000 - Train loss: 0.57219 - Test loss: 0.67368\n",
      "Epoch 19115 - lr: 0.01000 - Train loss: 0.57910 - Test loss: 0.67373\n",
      "Epoch 19116 - lr: 0.01000 - Train loss: 0.57928 - Test loss: 0.67419\n",
      "Epoch 19117 - lr: 0.01000 - Train loss: 0.57785 - Test loss: 0.67381\n",
      "Epoch 19118 - lr: 0.01000 - Train loss: 0.57949 - Test loss: 0.67390\n",
      "Epoch 19119 - lr: 0.01000 - Train loss: 0.57870 - Test loss: 0.67348\n",
      "Epoch 19120 - lr: 0.01000 - Train loss: 0.57775 - Test loss: 0.67317\n",
      "Epoch 19121 - lr: 0.01000 - Train loss: 0.57479 - Test loss: 0.67297\n",
      "Epoch 19122 - lr: 0.01000 - Train loss: 0.57788 - Test loss: 0.67277\n",
      "Epoch 19123 - lr: 0.01000 - Train loss: 0.57940 - Test loss: 0.67321\n",
      "Epoch 19124 - lr: 0.01000 - Train loss: 0.57879 - Test loss: 0.67300\n",
      "Epoch 19125 - lr: 0.01000 - Train loss: 0.57794 - Test loss: 0.67285\n",
      "Epoch 19126 - lr: 0.01000 - Train loss: 0.57530 - Test loss: 0.67273\n",
      "Epoch 19127 - lr: 0.01000 - Train loss: 0.57645 - Test loss: 0.67255\n",
      "Epoch 19128 - lr: 0.01000 - Train loss: 0.57859 - Test loss: 0.67306\n",
      "Epoch 19129 - lr: 0.01000 - Train loss: 0.57761 - Test loss: 0.67299\n",
      "Epoch 19130 - lr: 0.01000 - Train loss: 0.57402 - Test loss: 0.67291\n",
      "Epoch 19131 - lr: 0.01000 - Train loss: 0.57982 - Test loss: 0.67273\n",
      "Epoch 19132 - lr: 0.01000 - Train loss: 0.58038 - Test loss: 0.67348\n",
      "Epoch 19133 - lr: 0.01000 - Train loss: 0.57401 - Test loss: 0.67322\n",
      "Epoch 19134 - lr: 0.01000 - Train loss: 0.57984 - Test loss: 0.67304\n",
      "Epoch 19135 - lr: 0.01000 - Train loss: 0.58029 - Test loss: 0.67371\n",
      "Epoch 19136 - lr: 0.01000 - Train loss: 0.57298 - Test loss: 0.67342\n",
      "Epoch 19137 - lr: 0.01000 - Train loss: 0.57924 - Test loss: 0.67329\n",
      "Epoch 19138 - lr: 0.01000 - Train loss: 0.57980 - Test loss: 0.67396\n",
      "Epoch 19139 - lr: 0.01000 - Train loss: 0.57349 - Test loss: 0.67360\n",
      "Epoch 19140 - lr: 0.01000 - Train loss: 0.57214 - Test loss: 0.67383\n",
      "Epoch 19141 - lr: 0.01000 - Train loss: 0.57930 - Test loss: 0.67381\n",
      "Epoch 19142 - lr: 0.01000 - Train loss: 0.57908 - Test loss: 0.67438\n",
      "Epoch 19143 - lr: 0.01000 - Train loss: 0.57857 - Test loss: 0.67396\n",
      "Epoch 19144 - lr: 0.01000 - Train loss: 0.57988 - Test loss: 0.67418\n",
      "Epoch 19145 - lr: 0.01000 - Train loss: 0.57842 - Test loss: 0.67378\n",
      "Epoch 19146 - lr: 0.01000 - Train loss: 0.57699 - Test loss: 0.67351\n",
      "Epoch 19147 - lr: 0.01000 - Train loss: 0.57251 - Test loss: 0.67333\n",
      "Epoch 19148 - lr: 0.01000 - Train loss: 0.57906 - Test loss: 0.67322\n",
      "Epoch 19149 - lr: 0.01000 - Train loss: 0.57990 - Test loss: 0.67391\n",
      "Epoch 19150 - lr: 0.01000 - Train loss: 0.57270 - Test loss: 0.67359\n",
      "Epoch 19151 - lr: 0.01000 - Train loss: 0.57505 - Test loss: 0.67371\n",
      "Epoch 19152 - lr: 0.01000 - Train loss: 0.57717 - Test loss: 0.67396\n",
      "Epoch 19153 - lr: 0.01000 - Train loss: 0.57289 - Test loss: 0.67383\n",
      "Epoch 19154 - lr: 0.01000 - Train loss: 0.57911 - Test loss: 0.67371\n",
      "Epoch 19155 - lr: 0.01000 - Train loss: 0.57947 - Test loss: 0.67432\n",
      "Epoch 19156 - lr: 0.01000 - Train loss: 0.57531 - Test loss: 0.67391\n",
      "Epoch 19157 - lr: 0.01000 - Train loss: 0.57754 - Test loss: 0.67411\n",
      "Epoch 19158 - lr: 0.01000 - Train loss: 0.57415 - Test loss: 0.67392\n",
      "Epoch 19159 - lr: 0.01000 - Train loss: 0.57948 - Test loss: 0.67369\n",
      "Epoch 19160 - lr: 0.01000 - Train loss: 0.58030 - Test loss: 0.67419\n",
      "Epoch 19161 - lr: 0.01000 - Train loss: 0.57529 - Test loss: 0.67384\n",
      "Epoch 19162 - lr: 0.01000 - Train loss: 0.57586 - Test loss: 0.67360\n",
      "Epoch 19163 - lr: 0.01000 - Train loss: 0.57819 - Test loss: 0.67393\n",
      "Epoch 19164 - lr: 0.01000 - Train loss: 0.57653 - Test loss: 0.67376\n",
      "Epoch 19165 - lr: 0.01000 - Train loss: 0.57232 - Test loss: 0.67361\n",
      "Epoch 19166 - lr: 0.01000 - Train loss: 0.57917 - Test loss: 0.67360\n",
      "Epoch 19167 - lr: 0.01000 - Train loss: 0.58022 - Test loss: 0.67416\n",
      "Epoch 19168 - lr: 0.01000 - Train loss: 0.57719 - Test loss: 0.67387\n",
      "Epoch 19169 - lr: 0.01000 - Train loss: 0.57282 - Test loss: 0.67372\n",
      "Epoch 19170 - lr: 0.01000 - Train loss: 0.57893 - Test loss: 0.67354\n",
      "Epoch 19171 - lr: 0.01000 - Train loss: 0.57994 - Test loss: 0.67427\n",
      "Epoch 19172 - lr: 0.01000 - Train loss: 0.57228 - Test loss: 0.67395\n",
      "Epoch 19173 - lr: 0.01000 - Train loss: 0.57931 - Test loss: 0.67391\n",
      "Epoch 19174 - lr: 0.01000 - Train loss: 0.58027 - Test loss: 0.67443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19175 - lr: 0.01000 - Train loss: 0.57617 - Test loss: 0.67409\n",
      "Epoch 19176 - lr: 0.01000 - Train loss: 0.57289 - Test loss: 0.67388\n",
      "Epoch 19177 - lr: 0.01000 - Train loss: 0.57340 - Test loss: 0.67403\n",
      "Epoch 19178 - lr: 0.01000 - Train loss: 0.57211 - Test loss: 0.67430\n",
      "Epoch 19179 - lr: 0.01000 - Train loss: 0.57864 - Test loss: 0.67424\n",
      "Epoch 19180 - lr: 0.01000 - Train loss: 0.57968 - Test loss: 0.67482\n",
      "Epoch 19181 - lr: 0.01000 - Train loss: 0.57308 - Test loss: 0.67443\n",
      "Epoch 19182 - lr: 0.01000 - Train loss: 0.57203 - Test loss: 0.67458\n",
      "Epoch 19183 - lr: 0.01000 - Train loss: 0.57897 - Test loss: 0.67455\n",
      "Epoch 19184 - lr: 0.01000 - Train loss: 0.58016 - Test loss: 0.67487\n",
      "Epoch 19185 - lr: 0.01000 - Train loss: 0.57697 - Test loss: 0.67445\n",
      "Epoch 19186 - lr: 0.01000 - Train loss: 0.57233 - Test loss: 0.67423\n",
      "Epoch 19187 - lr: 0.01000 - Train loss: 0.57842 - Test loss: 0.67406\n",
      "Epoch 19188 - lr: 0.01000 - Train loss: 0.57992 - Test loss: 0.67468\n",
      "Epoch 19189 - lr: 0.01000 - Train loss: 0.57206 - Test loss: 0.67433\n",
      "Epoch 19190 - lr: 0.01000 - Train loss: 0.57953 - Test loss: 0.67422\n",
      "Epoch 19191 - lr: 0.01000 - Train loss: 0.57853 - Test loss: 0.67488\n",
      "Epoch 19192 - lr: 0.01000 - Train loss: 0.57982 - Test loss: 0.67440\n",
      "Epoch 19193 - lr: 0.01000 - Train loss: 0.57948 - Test loss: 0.67491\n",
      "Epoch 19194 - lr: 0.01000 - Train loss: 0.57419 - Test loss: 0.67446\n",
      "Epoch 19195 - lr: 0.01000 - Train loss: 0.57485 - Test loss: 0.67463\n",
      "Epoch 19196 - lr: 0.01000 - Train loss: 0.57697 - Test loss: 0.67438\n",
      "Epoch 19197 - lr: 0.01000 - Train loss: 0.57900 - Test loss: 0.67465\n",
      "Epoch 19198 - lr: 0.01000 - Train loss: 0.57836 - Test loss: 0.67437\n",
      "Epoch 19199 - lr: 0.01000 - Train loss: 0.57690 - Test loss: 0.67414\n",
      "Epoch 19200 - lr: 0.01000 - Train loss: 0.57237 - Test loss: 0.67395\n",
      "Epoch 19201 - lr: 0.01000 - Train loss: 0.57885 - Test loss: 0.67378\n",
      "Epoch 19202 - lr: 0.01000 - Train loss: 0.57985 - Test loss: 0.67457\n",
      "Epoch 19203 - lr: 0.01000 - Train loss: 0.57225 - Test loss: 0.67423\n",
      "Epoch 19204 - lr: 0.01000 - Train loss: 0.57915 - Test loss: 0.67418\n",
      "Epoch 19205 - lr: 0.01000 - Train loss: 0.58017 - Test loss: 0.67474\n",
      "Epoch 19206 - lr: 0.01000 - Train loss: 0.57664 - Test loss: 0.67442\n",
      "Epoch 19207 - lr: 0.01000 - Train loss: 0.57217 - Test loss: 0.67422\n",
      "Epoch 19208 - lr: 0.01000 - Train loss: 0.57971 - Test loss: 0.67409\n",
      "Epoch 19209 - lr: 0.01000 - Train loss: 0.57947 - Test loss: 0.67482\n",
      "Epoch 19210 - lr: 0.01000 - Train loss: 0.57419 - Test loss: 0.67438\n",
      "Epoch 19211 - lr: 0.01000 - Train loss: 0.57453 - Test loss: 0.67466\n",
      "Epoch 19212 - lr: 0.01000 - Train loss: 0.57814 - Test loss: 0.67439\n",
      "Epoch 19213 - lr: 0.01000 - Train loss: 0.57957 - Test loss: 0.67483\n",
      "Epoch 19214 - lr: 0.01000 - Train loss: 0.57847 - Test loss: 0.67455\n",
      "Epoch 19215 - lr: 0.01000 - Train loss: 0.57719 - Test loss: 0.67435\n",
      "Epoch 19216 - lr: 0.01000 - Train loss: 0.57292 - Test loss: 0.67416\n",
      "Epoch 19217 - lr: 0.01000 - Train loss: 0.57888 - Test loss: 0.67386\n",
      "Epoch 19218 - lr: 0.01000 - Train loss: 0.57992 - Test loss: 0.67472\n",
      "Epoch 19219 - lr: 0.01000 - Train loss: 0.57222 - Test loss: 0.67439\n",
      "Epoch 19220 - lr: 0.01000 - Train loss: 0.57942 - Test loss: 0.67423\n",
      "Epoch 19221 - lr: 0.01000 - Train loss: 0.57918 - Test loss: 0.67503\n",
      "Epoch 19222 - lr: 0.01000 - Train loss: 0.57629 - Test loss: 0.67452\n",
      "Epoch 19223 - lr: 0.01000 - Train loss: 0.57852 - Test loss: 0.67488\n",
      "Epoch 19224 - lr: 0.01000 - Train loss: 0.57751 - Test loss: 0.67469\n",
      "Epoch 19225 - lr: 0.01000 - Train loss: 0.57387 - Test loss: 0.67449\n",
      "Epoch 19226 - lr: 0.01000 - Train loss: 0.57960 - Test loss: 0.67412\n",
      "Epoch 19227 - lr: 0.01000 - Train loss: 0.57999 - Test loss: 0.67491\n",
      "Epoch 19228 - lr: 0.01000 - Train loss: 0.57242 - Test loss: 0.67456\n",
      "Epoch 19229 - lr: 0.01000 - Train loss: 0.57851 - Test loss: 0.67432\n",
      "Epoch 19230 - lr: 0.01000 - Train loss: 0.57995 - Test loss: 0.67510\n",
      "Epoch 19231 - lr: 0.01000 - Train loss: 0.57226 - Test loss: 0.67474\n",
      "Epoch 19232 - lr: 0.01000 - Train loss: 0.57861 - Test loss: 0.67453\n",
      "Epoch 19233 - lr: 0.01000 - Train loss: 0.57976 - Test loss: 0.67525\n",
      "Epoch 19234 - lr: 0.01000 - Train loss: 0.57223 - Test loss: 0.67484\n",
      "Epoch 19235 - lr: 0.01000 - Train loss: 0.57855 - Test loss: 0.67474\n",
      "Epoch 19236 - lr: 0.01000 - Train loss: 0.57985 - Test loss: 0.67522\n",
      "Epoch 19237 - lr: 0.01000 - Train loss: 0.57805 - Test loss: 0.67491\n",
      "Epoch 19238 - lr: 0.01000 - Train loss: 0.57583 - Test loss: 0.67469\n",
      "Epoch 19239 - lr: 0.01000 - Train loss: 0.57380 - Test loss: 0.67437\n",
      "Epoch 19240 - lr: 0.01000 - Train loss: 0.57280 - Test loss: 0.67471\n",
      "Epoch 19241 - lr: 0.01000 - Train loss: 0.57874 - Test loss: 0.67446\n",
      "Epoch 19242 - lr: 0.01000 - Train loss: 0.57972 - Test loss: 0.67527\n",
      "Epoch 19243 - lr: 0.01000 - Train loss: 0.57235 - Test loss: 0.67487\n",
      "Epoch 19244 - lr: 0.01000 - Train loss: 0.57745 - Test loss: 0.67481\n",
      "Epoch 19245 - lr: 0.01000 - Train loss: 0.57922 - Test loss: 0.67526\n",
      "Epoch 19246 - lr: 0.01000 - Train loss: 0.57850 - Test loss: 0.67503\n",
      "Epoch 19247 - lr: 0.01000 - Train loss: 0.57724 - Test loss: 0.67484\n",
      "Epoch 19248 - lr: 0.01000 - Train loss: 0.57304 - Test loss: 0.67463\n",
      "Epoch 19249 - lr: 0.01000 - Train loss: 0.57891 - Test loss: 0.67424\n",
      "Epoch 19250 - lr: 0.01000 - Train loss: 0.57985 - Test loss: 0.67517\n",
      "Epoch 19251 - lr: 0.01000 - Train loss: 0.57221 - Test loss: 0.67480\n",
      "Epoch 19252 - lr: 0.01000 - Train loss: 0.57944 - Test loss: 0.67459\n",
      "Epoch 19253 - lr: 0.01000 - Train loss: 0.57920 - Test loss: 0.67544\n",
      "Epoch 19254 - lr: 0.01000 - Train loss: 0.57569 - Test loss: 0.67489\n",
      "Epoch 19255 - lr: 0.01000 - Train loss: 0.57793 - Test loss: 0.67529\n",
      "Epoch 19256 - lr: 0.01000 - Train loss: 0.57569 - Test loss: 0.67512\n",
      "Epoch 19257 - lr: 0.01000 - Train loss: 0.57415 - Test loss: 0.67479\n",
      "Epoch 19258 - lr: 0.01000 - Train loss: 0.57420 - Test loss: 0.67515\n",
      "Epoch 19259 - lr: 0.01000 - Train loss: 0.57895 - Test loss: 0.67479\n",
      "Epoch 19260 - lr: 0.01000 - Train loss: 0.58001 - Test loss: 0.67548\n",
      "Epoch 19261 - lr: 0.01000 - Train loss: 0.57716 - Test loss: 0.67520\n",
      "Epoch 19262 - lr: 0.01000 - Train loss: 0.57275 - Test loss: 0.67499\n",
      "Epoch 19263 - lr: 0.01000 - Train loss: 0.57844 - Test loss: 0.67461\n",
      "Epoch 19264 - lr: 0.01000 - Train loss: 0.57995 - Test loss: 0.67550\n",
      "Epoch 19265 - lr: 0.01000 - Train loss: 0.57275 - Test loss: 0.67515\n",
      "Epoch 19266 - lr: 0.01000 - Train loss: 0.57838 - Test loss: 0.67480\n",
      "Epoch 19267 - lr: 0.01000 - Train loss: 0.57991 - Test loss: 0.67564\n",
      "Epoch 19268 - lr: 0.01000 - Train loss: 0.57247 - Test loss: 0.67527\n",
      "Epoch 19269 - lr: 0.01000 - Train loss: 0.57809 - Test loss: 0.67494\n",
      "Epoch 19270 - lr: 0.01000 - Train loss: 0.57998 - Test loss: 0.67572\n",
      "Epoch 19271 - lr: 0.01000 - Train loss: 0.57321 - Test loss: 0.67536\n",
      "Epoch 19272 - lr: 0.01000 - Train loss: 0.57916 - Test loss: 0.67495\n",
      "Epoch 19273 - lr: 0.01000 - Train loss: 0.57918 - Test loss: 0.67577\n",
      "Epoch 19274 - lr: 0.01000 - Train loss: 0.57528 - Test loss: 0.67519\n",
      "Epoch 19275 - lr: 0.01000 - Train loss: 0.57737 - Test loss: 0.67557\n",
      "Epoch 19276 - lr: 0.01000 - Train loss: 0.57367 - Test loss: 0.67539\n",
      "Epoch 19277 - lr: 0.01000 - Train loss: 0.57955 - Test loss: 0.67496\n",
      "Epoch 19278 - lr: 0.01000 - Train loss: 0.57941 - Test loss: 0.67578\n",
      "Epoch 19279 - lr: 0.01000 - Train loss: 0.57361 - Test loss: 0.67525\n",
      "Epoch 19280 - lr: 0.01000 - Train loss: 0.57236 - Test loss: 0.67552\n",
      "Epoch 19281 - lr: 0.01000 - Train loss: 0.57789 - Test loss: 0.67524\n",
      "Epoch 19282 - lr: 0.01000 - Train loss: 0.57996 - Test loss: 0.67597\n",
      "Epoch 19283 - lr: 0.01000 - Train loss: 0.57319 - Test loss: 0.67560\n",
      "Epoch 19284 - lr: 0.01000 - Train loss: 0.57915 - Test loss: 0.67518\n",
      "Epoch 19285 - lr: 0.01000 - Train loss: 0.57896 - Test loss: 0.67598\n",
      "Epoch 19286 - lr: 0.01000 - Train loss: 0.57669 - Test loss: 0.67535\n",
      "Epoch 19287 - lr: 0.01000 - Train loss: 0.57877 - Test loss: 0.67578\n",
      "Epoch 19288 - lr: 0.01000 - Train loss: 0.57804 - Test loss: 0.67558\n",
      "Epoch 19289 - lr: 0.01000 - Train loss: 0.57585 - Test loss: 0.67538\n",
      "Epoch 19290 - lr: 0.01000 - Train loss: 0.57374 - Test loss: 0.67500\n",
      "Epoch 19291 - lr: 0.01000 - Train loss: 0.57252 - Test loss: 0.67535\n",
      "Epoch 19292 - lr: 0.01000 - Train loss: 0.57813 - Test loss: 0.67502\n",
      "Epoch 19293 - lr: 0.01000 - Train loss: 0.57996 - Test loss: 0.67592\n",
      "Epoch 19294 - lr: 0.01000 - Train loss: 0.57325 - Test loss: 0.67559\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19295 - lr: 0.01000 - Train loss: 0.57915 - Test loss: 0.67515\n",
      "Epoch 19296 - lr: 0.01000 - Train loss: 0.57925 - Test loss: 0.67603\n",
      "Epoch 19297 - lr: 0.01000 - Train loss: 0.57454 - Test loss: 0.67545\n",
      "Epoch 19298 - lr: 0.01000 - Train loss: 0.57563 - Test loss: 0.67582\n",
      "Epoch 19299 - lr: 0.01000 - Train loss: 0.57384 - Test loss: 0.67550\n",
      "Epoch 19300 - lr: 0.01000 - Train loss: 0.57329 - Test loss: 0.67583\n",
      "Epoch 19301 - lr: 0.01000 - Train loss: 0.57934 - Test loss: 0.67544\n",
      "Epoch 19302 - lr: 0.01000 - Train loss: 0.57876 - Test loss: 0.67627\n",
      "Epoch 19303 - lr: 0.01000 - Train loss: 0.57774 - Test loss: 0.67561\n",
      "Epoch 19304 - lr: 0.01000 - Train loss: 0.57930 - Test loss: 0.67613\n",
      "Epoch 19305 - lr: 0.01000 - Train loss: 0.57841 - Test loss: 0.67589\n",
      "Epoch 19306 - lr: 0.01000 - Train loss: 0.57701 - Test loss: 0.67570\n",
      "Epoch 19307 - lr: 0.01000 - Train loss: 0.57251 - Test loss: 0.67543\n",
      "Epoch 19308 - lr: 0.01000 - Train loss: 0.57824 - Test loss: 0.67498\n",
      "Epoch 19309 - lr: 0.01000 - Train loss: 0.57999 - Test loss: 0.67599\n",
      "Epoch 19310 - lr: 0.01000 - Train loss: 0.57392 - Test loss: 0.67568\n",
      "Epoch 19311 - lr: 0.01000 - Train loss: 0.57944 - Test loss: 0.67516\n",
      "Epoch 19312 - lr: 0.01000 - Train loss: 0.57988 - Test loss: 0.67611\n",
      "Epoch 19313 - lr: 0.01000 - Train loss: 0.57259 - Test loss: 0.67573\n",
      "Epoch 19314 - lr: 0.01000 - Train loss: 0.57809 - Test loss: 0.67528\n",
      "Epoch 19315 - lr: 0.01000 - Train loss: 0.57997 - Test loss: 0.67623\n",
      "Epoch 19316 - lr: 0.01000 - Train loss: 0.57383 - Test loss: 0.67588\n",
      "Epoch 19317 - lr: 0.01000 - Train loss: 0.57947 - Test loss: 0.67535\n",
      "Epoch 19318 - lr: 0.01000 - Train loss: 0.57971 - Test loss: 0.67626\n",
      "Epoch 19319 - lr: 0.01000 - Train loss: 0.57213 - Test loss: 0.67580\n",
      "Epoch 19320 - lr: 0.01000 - Train loss: 0.57948 - Test loss: 0.67548\n",
      "Epoch 19321 - lr: 0.01000 - Train loss: 0.57924 - Test loss: 0.67638\n",
      "Epoch 19322 - lr: 0.01000 - Train loss: 0.57450 - Test loss: 0.67576\n",
      "Epoch 19323 - lr: 0.01000 - Train loss: 0.57550 - Test loss: 0.67615\n",
      "Epoch 19324 - lr: 0.01000 - Train loss: 0.57430 - Test loss: 0.67578\n",
      "Epoch 19325 - lr: 0.01000 - Train loss: 0.57496 - Test loss: 0.67617\n",
      "Epoch 19326 - lr: 0.01000 - Train loss: 0.57631 - Test loss: 0.67575\n",
      "Epoch 19327 - lr: 0.01000 - Train loss: 0.57851 - Test loss: 0.67630\n",
      "Epoch 19328 - lr: 0.01000 - Train loss: 0.57749 - Test loss: 0.67616\n",
      "Epoch 19329 - lr: 0.01000 - Train loss: 0.57381 - Test loss: 0.67595\n",
      "Epoch 19330 - lr: 0.01000 - Train loss: 0.57939 - Test loss: 0.67535\n",
      "Epoch 19331 - lr: 0.01000 - Train loss: 0.57979 - Test loss: 0.67637\n",
      "Epoch 19332 - lr: 0.01000 - Train loss: 0.57231 - Test loss: 0.67595\n",
      "Epoch 19333 - lr: 0.01000 - Train loss: 0.57817 - Test loss: 0.67552\n",
      "Epoch 19334 - lr: 0.01000 - Train loss: 0.57990 - Test loss: 0.67651\n",
      "Epoch 19335 - lr: 0.01000 - Train loss: 0.57323 - Test loss: 0.67615\n",
      "Epoch 19336 - lr: 0.01000 - Train loss: 0.57898 - Test loss: 0.67560\n",
      "Epoch 19337 - lr: 0.01000 - Train loss: 0.57941 - Test loss: 0.67659\n",
      "Epoch 19338 - lr: 0.01000 - Train loss: 0.57307 - Test loss: 0.67602\n",
      "Epoch 19339 - lr: 0.01000 - Train loss: 0.57231 - Test loss: 0.67616\n",
      "Epoch 19340 - lr: 0.01000 - Train loss: 0.57618 - Test loss: 0.67607\n",
      "Epoch 19341 - lr: 0.01000 - Train loss: 0.57847 - Test loss: 0.67660\n",
      "Epoch 19342 - lr: 0.01000 - Train loss: 0.57736 - Test loss: 0.67645\n",
      "Epoch 19343 - lr: 0.01000 - Train loss: 0.57334 - Test loss: 0.67622\n",
      "Epoch 19344 - lr: 0.01000 - Train loss: 0.57910 - Test loss: 0.67562\n",
      "Epoch 19345 - lr: 0.01000 - Train loss: 0.57941 - Test loss: 0.67664\n",
      "Epoch 19346 - lr: 0.01000 - Train loss: 0.57295 - Test loss: 0.67608\n",
      "Epoch 19347 - lr: 0.01000 - Train loss: 0.57291 - Test loss: 0.67616\n",
      "Epoch 19348 - lr: 0.01000 - Train loss: 0.57232 - Test loss: 0.67634\n",
      "Epoch 19349 - lr: 0.01000 - Train loss: 0.57565 - Test loss: 0.67628\n",
      "Epoch 19350 - lr: 0.01000 - Train loss: 0.57800 - Test loss: 0.67678\n",
      "Epoch 19351 - lr: 0.01000 - Train loss: 0.57597 - Test loss: 0.67662\n",
      "Epoch 19352 - lr: 0.01000 - Train loss: 0.57298 - Test loss: 0.67623\n",
      "Epoch 19353 - lr: 0.01000 - Train loss: 0.57233 - Test loss: 0.67636\n",
      "Epoch 19354 - lr: 0.01000 - Train loss: 0.57552 - Test loss: 0.67629\n",
      "Epoch 19355 - lr: 0.01000 - Train loss: 0.57782 - Test loss: 0.67682\n",
      "Epoch 19356 - lr: 0.01000 - Train loss: 0.57536 - Test loss: 0.67668\n",
      "Epoch 19357 - lr: 0.01000 - Train loss: 0.57498 - Test loss: 0.67618\n",
      "Epoch 19358 - lr: 0.01000 - Train loss: 0.57670 - Test loss: 0.67672\n",
      "Epoch 19359 - lr: 0.01000 - Train loss: 0.57205 - Test loss: 0.67650\n",
      "Epoch 19360 - lr: 0.01000 - Train loss: 0.57774 - Test loss: 0.67605\n",
      "Epoch 19361 - lr: 0.01000 - Train loss: 0.57974 - Test loss: 0.67705\n",
      "Epoch 19362 - lr: 0.01000 - Train loss: 0.57354 - Test loss: 0.67671\n",
      "Epoch 19363 - lr: 0.01000 - Train loss: 0.57911 - Test loss: 0.67608\n",
      "Epoch 19364 - lr: 0.01000 - Train loss: 0.57921 - Test loss: 0.67709\n",
      "Epoch 19365 - lr: 0.01000 - Train loss: 0.57315 - Test loss: 0.67647\n",
      "Epoch 19366 - lr: 0.01000 - Train loss: 0.57190 - Test loss: 0.67666\n",
      "Epoch 19367 - lr: 0.01000 - Train loss: 0.57893 - Test loss: 0.67638\n",
      "Epoch 19368 - lr: 0.01000 - Train loss: 0.57974 - Test loss: 0.67723\n",
      "Epoch 19369 - lr: 0.01000 - Train loss: 0.57366 - Test loss: 0.67687\n",
      "Epoch 19370 - lr: 0.01000 - Train loss: 0.57917 - Test loss: 0.67622\n",
      "Epoch 19371 - lr: 0.01000 - Train loss: 0.57925 - Test loss: 0.67719\n",
      "Epoch 19372 - lr: 0.01000 - Train loss: 0.57278 - Test loss: 0.67658\n",
      "Epoch 19373 - lr: 0.01000 - Train loss: 0.57312 - Test loss: 0.67661\n",
      "Epoch 19374 - lr: 0.01000 - Train loss: 0.57170 - Test loss: 0.67686\n",
      "Epoch 19375 - lr: 0.01000 - Train loss: 0.57853 - Test loss: 0.67652\n",
      "Epoch 19376 - lr: 0.01000 - Train loss: 0.57880 - Test loss: 0.67742\n",
      "Epoch 19377 - lr: 0.01000 - Train loss: 0.57536 - Test loss: 0.67671\n",
      "Epoch 19378 - lr: 0.01000 - Train loss: 0.57752 - Test loss: 0.67717\n",
      "Epoch 19379 - lr: 0.01000 - Train loss: 0.57435 - Test loss: 0.67697\n",
      "Epoch 19380 - lr: 0.01000 - Train loss: 0.57824 - Test loss: 0.67633\n",
      "Epoch 19381 - lr: 0.01000 - Train loss: 0.57953 - Test loss: 0.67717\n",
      "Epoch 19382 - lr: 0.01000 - Train loss: 0.57767 - Test loss: 0.67697\n",
      "Epoch 19383 - lr: 0.01000 - Train loss: 0.57439 - Test loss: 0.67677\n",
      "Epoch 19384 - lr: 0.01000 - Train loss: 0.57847 - Test loss: 0.67603\n",
      "Epoch 19385 - lr: 0.01000 - Train loss: 0.57962 - Test loss: 0.67708\n",
      "Epoch 19386 - lr: 0.01000 - Train loss: 0.57770 - Test loss: 0.67695\n",
      "Epoch 19387 - lr: 0.01000 - Train loss: 0.57430 - Test loss: 0.67677\n",
      "Epoch 19388 - lr: 0.01000 - Train loss: 0.57884 - Test loss: 0.67597\n",
      "Epoch 19389 - lr: 0.01000 - Train loss: 0.57980 - Test loss: 0.67717\n",
      "Epoch 19390 - lr: 0.01000 - Train loss: 0.57692 - Test loss: 0.67701\n",
      "Epoch 19391 - lr: 0.01000 - Train loss: 0.57234 - Test loss: 0.67668\n",
      "Epoch 19392 - lr: 0.01000 - Train loss: 0.57841 - Test loss: 0.67605\n",
      "Epoch 19393 - lr: 0.01000 - Train loss: 0.57971 - Test loss: 0.67734\n",
      "Epoch 19394 - lr: 0.01000 - Train loss: 0.57305 - Test loss: 0.67697\n",
      "Epoch 19395 - lr: 0.01000 - Train loss: 0.57813 - Test loss: 0.67624\n",
      "Epoch 19396 - lr: 0.01000 - Train loss: 0.57970 - Test loss: 0.67745\n",
      "Epoch 19397 - lr: 0.01000 - Train loss: 0.57334 - Test loss: 0.67709\n",
      "Epoch 19398 - lr: 0.01000 - Train loss: 0.57870 - Test loss: 0.67634\n",
      "Epoch 19399 - lr: 0.01000 - Train loss: 0.57942 - Test loss: 0.67751\n",
      "Epoch 19400 - lr: 0.01000 - Train loss: 0.57211 - Test loss: 0.67697\n",
      "Epoch 19401 - lr: 0.01000 - Train loss: 0.57908 - Test loss: 0.67650\n",
      "Epoch 19402 - lr: 0.01000 - Train loss: 0.57947 - Test loss: 0.67758\n",
      "Epoch 19403 - lr: 0.01000 - Train loss: 0.57200 - Test loss: 0.67707\n",
      "Epoch 19404 - lr: 0.01000 - Train loss: 0.57846 - Test loss: 0.67654\n",
      "Epoch 19405 - lr: 0.01000 - Train loss: 0.57926 - Test loss: 0.67762\n",
      "Epoch 19406 - lr: 0.01000 - Train loss: 0.57242 - Test loss: 0.67702\n",
      "Epoch 19407 - lr: 0.01000 - Train loss: 0.57587 - Test loss: 0.67682\n",
      "Epoch 19408 - lr: 0.01000 - Train loss: 0.57813 - Test loss: 0.67747\n",
      "Epoch 19409 - lr: 0.01000 - Train loss: 0.57643 - Test loss: 0.67736\n",
      "Epoch 19410 - lr: 0.01000 - Train loss: 0.57213 - Test loss: 0.67697\n",
      "Epoch 19411 - lr: 0.01000 - Train loss: 0.57875 - Test loss: 0.67653\n",
      "Epoch 19412 - lr: 0.01000 - Train loss: 0.57972 - Test loss: 0.67759\n",
      "Epoch 19413 - lr: 0.01000 - Train loss: 0.57538 - Test loss: 0.67732\n",
      "Epoch 19414 - lr: 0.01000 - Train loss: 0.57522 - Test loss: 0.67668\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19415 - lr: 0.01000 - Train loss: 0.57697 - Test loss: 0.67738\n",
      "Epoch 19416 - lr: 0.01000 - Train loss: 0.57250 - Test loss: 0.67717\n",
      "Epoch 19417 - lr: 0.01000 - Train loss: 0.57711 - Test loss: 0.67649\n",
      "Epoch 19418 - lr: 0.01000 - Train loss: 0.57968 - Test loss: 0.67768\n",
      "Epoch 19419 - lr: 0.01000 - Train loss: 0.57604 - Test loss: 0.67746\n",
      "Epoch 19420 - lr: 0.01000 - Train loss: 0.57313 - Test loss: 0.67695\n",
      "Epoch 19421 - lr: 0.01000 - Train loss: 0.57235 - Test loss: 0.67708\n",
      "Epoch 19422 - lr: 0.01000 - Train loss: 0.57595 - Test loss: 0.67691\n",
      "Epoch 19423 - lr: 0.01000 - Train loss: 0.57815 - Test loss: 0.67769\n",
      "Epoch 19424 - lr: 0.01000 - Train loss: 0.57640 - Test loss: 0.67763\n",
      "Epoch 19425 - lr: 0.01000 - Train loss: 0.57220 - Test loss: 0.67722\n",
      "Epoch 19426 - lr: 0.01000 - Train loss: 0.57833 - Test loss: 0.67678\n",
      "Epoch 19427 - lr: 0.01000 - Train loss: 0.57955 - Test loss: 0.67787\n",
      "Epoch 19428 - lr: 0.01000 - Train loss: 0.57692 - Test loss: 0.67770\n",
      "Epoch 19429 - lr: 0.01000 - Train loss: 0.57220 - Test loss: 0.67736\n",
      "Epoch 19430 - lr: 0.01000 - Train loss: 0.57722 - Test loss: 0.67664\n",
      "Epoch 19431 - lr: 0.01000 - Train loss: 0.57961 - Test loss: 0.67791\n",
      "Epoch 19432 - lr: 0.01000 - Train loss: 0.57555 - Test loss: 0.67768\n",
      "Epoch 19433 - lr: 0.01000 - Train loss: 0.57471 - Test loss: 0.67702\n",
      "Epoch 19434 - lr: 0.01000 - Train loss: 0.57558 - Test loss: 0.67769\n",
      "Epoch 19435 - lr: 0.01000 - Train loss: 0.57401 - Test loss: 0.67715\n",
      "Epoch 19436 - lr: 0.01000 - Train loss: 0.57326 - Test loss: 0.67767\n",
      "Epoch 19437 - lr: 0.01000 - Train loss: 0.57851 - Test loss: 0.67692\n",
      "Epoch 19438 - lr: 0.01000 - Train loss: 0.57901 - Test loss: 0.67815\n",
      "Epoch 19439 - lr: 0.01000 - Train loss: 0.57259 - Test loss: 0.67750\n",
      "Epoch 19440 - lr: 0.01000 - Train loss: 0.57432 - Test loss: 0.67737\n",
      "Epoch 19441 - lr: 0.01000 - Train loss: 0.57513 - Test loss: 0.67795\n",
      "Epoch 19442 - lr: 0.01000 - Train loss: 0.57506 - Test loss: 0.67737\n",
      "Epoch 19443 - lr: 0.01000 - Train loss: 0.57683 - Test loss: 0.67803\n",
      "Epoch 19444 - lr: 0.01000 - Train loss: 0.57213 - Test loss: 0.67780\n",
      "Epoch 19445 - lr: 0.01000 - Train loss: 0.57596 - Test loss: 0.67709\n",
      "Epoch 19446 - lr: 0.01000 - Train loss: 0.57931 - Test loss: 0.67822\n",
      "Epoch 19447 - lr: 0.01000 - Train loss: 0.57681 - Test loss: 0.67805\n",
      "Epoch 19448 - lr: 0.01000 - Train loss: 0.57199 - Test loss: 0.67771\n",
      "Epoch 19449 - lr: 0.01000 - Train loss: 0.57670 - Test loss: 0.67697\n",
      "Epoch 19450 - lr: 0.01000 - Train loss: 0.57941 - Test loss: 0.67822\n",
      "Epoch 19451 - lr: 0.01000 - Train loss: 0.57569 - Test loss: 0.67801\n",
      "Epoch 19452 - lr: 0.01000 - Train loss: 0.57389 - Test loss: 0.67737\n",
      "Epoch 19453 - lr: 0.01000 - Train loss: 0.57284 - Test loss: 0.67786\n",
      "Epoch 19454 - lr: 0.01000 - Train loss: 0.57749 - Test loss: 0.67707\n",
      "Epoch 19455 - lr: 0.01000 - Train loss: 0.57926 - Test loss: 0.67834\n",
      "Epoch 19456 - lr: 0.01000 - Train loss: 0.57259 - Test loss: 0.67796\n",
      "Epoch 19457 - lr: 0.01000 - Train loss: 0.57655 - Test loss: 0.67714\n",
      "Epoch 19458 - lr: 0.01000 - Train loss: 0.57935 - Test loss: 0.67836\n",
      "Epoch 19459 - lr: 0.01000 - Train loss: 0.57568 - Test loss: 0.67814\n",
      "Epoch 19460 - lr: 0.01000 - Train loss: 0.57381 - Test loss: 0.67749\n",
      "Epoch 19461 - lr: 0.01000 - Train loss: 0.57261 - Test loss: 0.67796\n",
      "Epoch 19462 - lr: 0.01000 - Train loss: 0.57687 - Test loss: 0.67717\n",
      "Epoch 19463 - lr: 0.01000 - Train loss: 0.57931 - Test loss: 0.67842\n",
      "Epoch 19464 - lr: 0.01000 - Train loss: 0.57441 - Test loss: 0.67815\n",
      "Epoch 19465 - lr: 0.01000 - Train loss: 0.57785 - Test loss: 0.67727\n",
      "Epoch 19466 - lr: 0.01000 - Train loss: 0.57919 - Test loss: 0.67840\n",
      "Epoch 19467 - lr: 0.01000 - Train loss: 0.57709 - Test loss: 0.67828\n",
      "Epoch 19468 - lr: 0.01000 - Train loss: 0.57242 - Test loss: 0.67796\n",
      "Epoch 19469 - lr: 0.01000 - Train loss: 0.57617 - Test loss: 0.67704\n",
      "Epoch 19470 - lr: 0.01000 - Train loss: 0.57925 - Test loss: 0.67840\n",
      "Epoch 19471 - lr: 0.01000 - Train loss: 0.57672 - Test loss: 0.67830\n",
      "Epoch 19472 - lr: 0.01000 - Train loss: 0.57202 - Test loss: 0.67789\n",
      "Epoch 19473 - lr: 0.01000 - Train loss: 0.57771 - Test loss: 0.67711\n",
      "Epoch 19474 - lr: 0.01000 - Train loss: 0.57916 - Test loss: 0.67852\n",
      "Epoch 19475 - lr: 0.01000 - Train loss: 0.57229 - Test loss: 0.67810\n",
      "Epoch 19476 - lr: 0.01000 - Train loss: 0.57577 - Test loss: 0.67724\n",
      "Epoch 19477 - lr: 0.01000 - Train loss: 0.57914 - Test loss: 0.67854\n",
      "Epoch 19478 - lr: 0.01000 - Train loss: 0.57684 - Test loss: 0.67844\n",
      "Epoch 19479 - lr: 0.01000 - Train loss: 0.57201 - Test loss: 0.67806\n",
      "Epoch 19480 - lr: 0.01000 - Train loss: 0.57636 - Test loss: 0.67721\n",
      "Epoch 19481 - lr: 0.01000 - Train loss: 0.57923 - Test loss: 0.67857\n",
      "Epoch 19482 - lr: 0.01000 - Train loss: 0.57595 - Test loss: 0.67842\n",
      "Epoch 19483 - lr: 0.01000 - Train loss: 0.57314 - Test loss: 0.67778\n",
      "Epoch 19484 - lr: 0.01000 - Train loss: 0.57203 - Test loss: 0.67793\n",
      "Epoch 19485 - lr: 0.01000 - Train loss: 0.57738 - Test loss: 0.67752\n",
      "Epoch 19486 - lr: 0.01000 - Train loss: 0.57895 - Test loss: 0.67865\n",
      "Epoch 19487 - lr: 0.01000 - Train loss: 0.57705 - Test loss: 0.67859\n",
      "Epoch 19488 - lr: 0.01000 - Train loss: 0.57230 - Test loss: 0.67827\n",
      "Epoch 19489 - lr: 0.01000 - Train loss: 0.57557 - Test loss: 0.67730\n",
      "Epoch 19490 - lr: 0.01000 - Train loss: 0.57901 - Test loss: 0.67867\n",
      "Epoch 19491 - lr: 0.01000 - Train loss: 0.57677 - Test loss: 0.67860\n",
      "Epoch 19492 - lr: 0.01000 - Train loss: 0.57194 - Test loss: 0.67820\n",
      "Epoch 19493 - lr: 0.01000 - Train loss: 0.57660 - Test loss: 0.67732\n",
      "Epoch 19494 - lr: 0.01000 - Train loss: 0.57913 - Test loss: 0.67875\n",
      "Epoch 19495 - lr: 0.01000 - Train loss: 0.57482 - Test loss: 0.67855\n",
      "Epoch 19496 - lr: 0.01000 - Train loss: 0.57676 - Test loss: 0.67760\n",
      "Epoch 19497 - lr: 0.01000 - Train loss: 0.57850 - Test loss: 0.67873\n",
      "Epoch 19498 - lr: 0.01000 - Train loss: 0.57695 - Test loss: 0.67873\n",
      "Epoch 19499 - lr: 0.01000 - Train loss: 0.57213 - Test loss: 0.67834\n",
      "Epoch 19500 - lr: 0.01000 - Train loss: 0.57565 - Test loss: 0.67735\n",
      "Epoch 19501 - lr: 0.01000 - Train loss: 0.57897 - Test loss: 0.67882\n",
      "Epoch 19502 - lr: 0.01000 - Train loss: 0.57657 - Test loss: 0.67876\n",
      "Epoch 19503 - lr: 0.01000 - Train loss: 0.57199 - Test loss: 0.67827\n",
      "Epoch 19504 - lr: 0.01000 - Train loss: 0.57811 - Test loss: 0.67747\n",
      "Epoch 19505 - lr: 0.01000 - Train loss: 0.57860 - Test loss: 0.67895\n",
      "Epoch 19506 - lr: 0.01000 - Train loss: 0.57224 - Test loss: 0.67825\n",
      "Epoch 19507 - lr: 0.01000 - Train loss: 0.57669 - Test loss: 0.67780\n",
      "Epoch 19508 - lr: 0.01000 - Train loss: 0.57849 - Test loss: 0.67891\n",
      "Epoch 19509 - lr: 0.01000 - Train loss: 0.57675 - Test loss: 0.67889\n",
      "Epoch 19510 - lr: 0.01000 - Train loss: 0.57185 - Test loss: 0.67847\n",
      "Epoch 19511 - lr: 0.01000 - Train loss: 0.57589 - Test loss: 0.67753\n",
      "Epoch 19512 - lr: 0.01000 - Train loss: 0.57893 - Test loss: 0.67897\n",
      "Epoch 19513 - lr: 0.01000 - Train loss: 0.57556 - Test loss: 0.67884\n",
      "Epoch 19514 - lr: 0.01000 - Train loss: 0.57411 - Test loss: 0.67803\n",
      "Epoch 19515 - lr: 0.01000 - Train loss: 0.57341 - Test loss: 0.67868\n",
      "Epoch 19516 - lr: 0.01000 - Train loss: 0.57801 - Test loss: 0.67765\n",
      "Epoch 19517 - lr: 0.01000 - Train loss: 0.57826 - Test loss: 0.67909\n",
      "Epoch 19518 - lr: 0.01000 - Train loss: 0.57315 - Test loss: 0.67825\n",
      "Epoch 19519 - lr: 0.01000 - Train loss: 0.57142 - Test loss: 0.67851\n",
      "Epoch 19520 - lr: 0.01000 - Train loss: 0.57734 - Test loss: 0.67783\n",
      "Epoch 19521 - lr: 0.01000 - Train loss: 0.57811 - Test loss: 0.67916\n",
      "Epoch 19522 - lr: 0.01000 - Train loss: 0.57336 - Test loss: 0.67833\n",
      "Epoch 19523 - lr: 0.01000 - Train loss: 0.57158 - Test loss: 0.67874\n",
      "Epoch 19524 - lr: 0.01000 - Train loss: 0.57340 - Test loss: 0.67796\n",
      "Epoch 19525 - lr: 0.01000 - Train loss: 0.57822 - Test loss: 0.67907\n",
      "Epoch 19526 - lr: 0.01000 - Train loss: 0.57607 - Test loss: 0.67899\n",
      "Epoch 19527 - lr: 0.01000 - Train loss: 0.57189 - Test loss: 0.67846\n",
      "Epoch 19528 - lr: 0.01000 - Train loss: 0.57706 - Test loss: 0.67793\n",
      "Epoch 19529 - lr: 0.01000 - Train loss: 0.57861 - Test loss: 0.67908\n",
      "Epoch 19530 - lr: 0.01000 - Train loss: 0.57585 - Test loss: 0.67897\n",
      "Epoch 19531 - lr: 0.01000 - Train loss: 0.57267 - Test loss: 0.67831\n",
      "Epoch 19532 - lr: 0.01000 - Train loss: 0.57277 - Test loss: 0.67827\n",
      "Epoch 19533 - lr: 0.01000 - Train loss: 0.57156 - Test loss: 0.67849\n",
      "Epoch 19534 - lr: 0.01000 - Train loss: 0.57767 - Test loss: 0.67800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19535 - lr: 0.01000 - Train loss: 0.57846 - Test loss: 0.67924\n",
      "Epoch 19536 - lr: 0.01000 - Train loss: 0.57161 - Test loss: 0.67884\n",
      "Epoch 19537 - lr: 0.01000 - Train loss: 0.57318 - Test loss: 0.67796\n",
      "Epoch 19538 - lr: 0.01000 - Train loss: 0.57799 - Test loss: 0.67914\n",
      "Epoch 19539 - lr: 0.01000 - Train loss: 0.57552 - Test loss: 0.67909\n",
      "Epoch 19540 - lr: 0.01000 - Train loss: 0.57327 - Test loss: 0.67836\n",
      "Epoch 19541 - lr: 0.01000 - Train loss: 0.57132 - Test loss: 0.67877\n",
      "Epoch 19542 - lr: 0.01000 - Train loss: 0.57306 - Test loss: 0.67797\n",
      "Epoch 19543 - lr: 0.01000 - Train loss: 0.57788 - Test loss: 0.67918\n",
      "Epoch 19544 - lr: 0.01000 - Train loss: 0.57515 - Test loss: 0.67913\n",
      "Epoch 19545 - lr: 0.01000 - Train loss: 0.57436 - Test loss: 0.67831\n",
      "Epoch 19546 - lr: 0.01000 - Train loss: 0.57490 - Test loss: 0.67908\n",
      "Epoch 19547 - lr: 0.01000 - Train loss: 0.57471 - Test loss: 0.67829\n",
      "Epoch 19548 - lr: 0.01000 - Train loss: 0.57588 - Test loss: 0.67916\n",
      "Epoch 19549 - lr: 0.01000 - Train loss: 0.57174 - Test loss: 0.67869\n",
      "Epoch 19550 - lr: 0.01000 - Train loss: 0.57701 - Test loss: 0.67815\n",
      "Epoch 19551 - lr: 0.01000 - Train loss: 0.57832 - Test loss: 0.67940\n",
      "Epoch 19552 - lr: 0.01000 - Train loss: 0.57380 - Test loss: 0.67920\n",
      "Epoch 19553 - lr: 0.01000 - Train loss: 0.57764 - Test loss: 0.67806\n",
      "Epoch 19554 - lr: 0.01000 - Train loss: 0.57792 - Test loss: 0.67946\n",
      "Epoch 19555 - lr: 0.01000 - Train loss: 0.57195 - Test loss: 0.67873\n",
      "Epoch 19556 - lr: 0.01000 - Train loss: 0.57608 - Test loss: 0.67826\n",
      "Epoch 19557 - lr: 0.01000 - Train loss: 0.57786 - Test loss: 0.67938\n",
      "Epoch 19558 - lr: 0.01000 - Train loss: 0.57483 - Test loss: 0.67931\n",
      "Epoch 19559 - lr: 0.01000 - Train loss: 0.57549 - Test loss: 0.67831\n",
      "Epoch 19560 - lr: 0.01000 - Train loss: 0.57711 - Test loss: 0.67937\n",
      "Epoch 19561 - lr: 0.01000 - Train loss: 0.57276 - Test loss: 0.67921\n",
      "Epoch 19562 - lr: 0.01000 - Train loss: 0.57576 - Test loss: 0.67800\n",
      "Epoch 19563 - lr: 0.01000 - Train loss: 0.57816 - Test loss: 0.67952\n",
      "Epoch 19564 - lr: 0.01000 - Train loss: 0.57199 - Test loss: 0.67918\n",
      "Epoch 19565 - lr: 0.01000 - Train loss: 0.57301 - Test loss: 0.67809\n",
      "Epoch 19566 - lr: 0.01000 - Train loss: 0.57758 - Test loss: 0.67944\n",
      "Epoch 19567 - lr: 0.01000 - Train loss: 0.57409 - Test loss: 0.67938\n",
      "Epoch 19568 - lr: 0.01000 - Train loss: 0.57720 - Test loss: 0.67822\n",
      "Epoch 19569 - lr: 0.01000 - Train loss: 0.57811 - Test loss: 0.67961\n",
      "Epoch 19570 - lr: 0.01000 - Train loss: 0.57210 - Test loss: 0.67927\n",
      "Epoch 19571 - lr: 0.01000 - Train loss: 0.57325 - Test loss: 0.67811\n",
      "Epoch 19572 - lr: 0.01000 - Train loss: 0.57763 - Test loss: 0.67949\n",
      "Epoch 19573 - lr: 0.01000 - Train loss: 0.57405 - Test loss: 0.67943\n",
      "Epoch 19574 - lr: 0.01000 - Train loss: 0.57719 - Test loss: 0.67824\n",
      "Epoch 19575 - lr: 0.01000 - Train loss: 0.57798 - Test loss: 0.67965\n",
      "Epoch 19576 - lr: 0.01000 - Train loss: 0.57161 - Test loss: 0.67925\n",
      "Epoch 19577 - lr: 0.01000 - Train loss: 0.57186 - Test loss: 0.67818\n",
      "Epoch 19578 - lr: 0.01000 - Train loss: 0.57669 - Test loss: 0.67947\n",
      "Epoch 19579 - lr: 0.01000 - Train loss: 0.57163 - Test loss: 0.67930\n",
      "Epoch 19580 - lr: 0.01000 - Train loss: 0.57159 - Test loss: 0.67828\n",
      "Epoch 19581 - lr: 0.01000 - Train loss: 0.57645 - Test loss: 0.67951\n",
      "Epoch 19582 - lr: 0.01000 - Train loss: 0.57114 - Test loss: 0.67930\n",
      "Epoch 19583 - lr: 0.01000 - Train loss: 0.57069 - Test loss: 0.67838\n",
      "Epoch 19584 - lr: 0.01000 - Train loss: 0.57543 - Test loss: 0.67952\n",
      "Epoch 19585 - lr: 0.01000 - Train loss: 0.57184 - Test loss: 0.67900\n",
      "Epoch 19586 - lr: 0.01000 - Train loss: 0.57386 - Test loss: 0.67879\n",
      "Epoch 19587 - lr: 0.01000 - Train loss: 0.57418 - Test loss: 0.67955\n",
      "Epoch 19588 - lr: 0.01000 - Train loss: 0.57578 - Test loss: 0.67865\n",
      "Epoch 19589 - lr: 0.01000 - Train loss: 0.57729 - Test loss: 0.67971\n",
      "Epoch 19590 - lr: 0.01000 - Train loss: 0.57238 - Test loss: 0.67952\n",
      "Epoch 19591 - lr: 0.01000 - Train loss: 0.57440 - Test loss: 0.67834\n",
      "Epoch 19592 - lr: 0.01000 - Train loss: 0.57741 - Test loss: 0.67971\n",
      "Epoch 19593 - lr: 0.01000 - Train loss: 0.57106 - Test loss: 0.67938\n",
      "Epoch 19594 - lr: 0.01000 - Train loss: 0.56984 - Test loss: 0.67843\n",
      "Epoch 19595 - lr: 0.01000 - Train loss: 0.57376 - Test loss: 0.67951\n",
      "Epoch 19596 - lr: 0.01000 - Train loss: 0.57647 - Test loss: 0.67858\n",
      "Epoch 19597 - lr: 0.01000 - Train loss: 0.57716 - Test loss: 0.67979\n",
      "Epoch 19598 - lr: 0.01000 - Train loss: 0.57079 - Test loss: 0.67927\n",
      "Epoch 19599 - lr: 0.01000 - Train loss: 0.57513 - Test loss: 0.67844\n",
      "Epoch 19600 - lr: 0.01000 - Train loss: 0.57661 - Test loss: 0.67978\n",
      "Epoch 19601 - lr: 0.01000 - Train loss: 0.57378 - Test loss: 0.67884\n",
      "Epoch 19602 - lr: 0.01000 - Train loss: 0.57360 - Test loss: 0.67958\n",
      "Epoch 19603 - lr: 0.01000 - Train loss: 0.57667 - Test loss: 0.67854\n",
      "Epoch 19604 - lr: 0.01000 - Train loss: 0.57662 - Test loss: 0.67980\n",
      "Epoch 19605 - lr: 0.01000 - Train loss: 0.57366 - Test loss: 0.67883\n",
      "Epoch 19606 - lr: 0.01000 - Train loss: 0.57308 - Test loss: 0.67956\n",
      "Epoch 19607 - lr: 0.01000 - Train loss: 0.57670 - Test loss: 0.67845\n",
      "Epoch 19608 - lr: 0.01000 - Train loss: 0.57495 - Test loss: 0.67978\n",
      "Epoch 19609 - lr: 0.01000 - Train loss: 0.57295 - Test loss: 0.67851\n",
      "Epoch 19610 - lr: 0.01000 - Train loss: 0.57701 - Test loss: 0.67972\n",
      "Epoch 19611 - lr: 0.01000 - Train loss: 0.57138 - Test loss: 0.67951\n",
      "Epoch 19612 - lr: 0.01000 - Train loss: 0.57008 - Test loss: 0.67851\n",
      "Epoch 19613 - lr: 0.01000 - Train loss: 0.57433 - Test loss: 0.67958\n",
      "Epoch 19614 - lr: 0.01000 - Train loss: 0.57460 - Test loss: 0.67877\n",
      "Epoch 19615 - lr: 0.01000 - Train loss: 0.57582 - Test loss: 0.67968\n",
      "Epoch 19616 - lr: 0.01000 - Train loss: 0.57062 - Test loss: 0.67928\n",
      "Epoch 19617 - lr: 0.01000 - Train loss: 0.57483 - Test loss: 0.67847\n",
      "Epoch 19618 - lr: 0.01000 - Train loss: 0.57612 - Test loss: 0.67981\n",
      "Epoch 19619 - lr: 0.01000 - Train loss: 0.57499 - Test loss: 0.67876\n",
      "Epoch 19620 - lr: 0.01000 - Train loss: 0.57631 - Test loss: 0.67973\n",
      "Epoch 19621 - lr: 0.01000 - Train loss: 0.57056 - Test loss: 0.67945\n",
      "Epoch 19622 - lr: 0.01000 - Train loss: 0.56872 - Test loss: 0.67857\n",
      "Epoch 19623 - lr: 0.01000 - Train loss: 0.57128 - Test loss: 0.67955\n",
      "Epoch 19624 - lr: 0.01000 - Train loss: 0.57011 - Test loss: 0.67871\n",
      "Epoch 19625 - lr: 0.01000 - Train loss: 0.57447 - Test loss: 0.67973\n",
      "Epoch 19626 - lr: 0.01000 - Train loss: 0.57366 - Test loss: 0.67901\n",
      "Epoch 19627 - lr: 0.01000 - Train loss: 0.57374 - Test loss: 0.67975\n",
      "Epoch 19628 - lr: 0.01000 - Train loss: 0.57578 - Test loss: 0.67878\n",
      "Epoch 19629 - lr: 0.01000 - Train loss: 0.57634 - Test loss: 0.67989\n",
      "Epoch 19630 - lr: 0.01000 - Train loss: 0.57119 - Test loss: 0.67923\n",
      "Epoch 19631 - lr: 0.01000 - Train loss: 0.57505 - Test loss: 0.67878\n",
      "Epoch 19632 - lr: 0.01000 - Train loss: 0.57624 - Test loss: 0.67982\n",
      "Epoch 19633 - lr: 0.01000 - Train loss: 0.57024 - Test loss: 0.67946\n",
      "Epoch 19634 - lr: 0.01000 - Train loss: 0.57033 - Test loss: 0.67863\n",
      "Epoch 19635 - lr: 0.01000 - Train loss: 0.57451 - Test loss: 0.67974\n",
      "Epoch 19636 - lr: 0.01000 - Train loss: 0.57338 - Test loss: 0.67900\n",
      "Epoch 19637 - lr: 0.01000 - Train loss: 0.57271 - Test loss: 0.67976\n",
      "Epoch 19638 - lr: 0.01000 - Train loss: 0.57576 - Test loss: 0.67866\n",
      "Epoch 19639 - lr: 0.01000 - Train loss: 0.57354 - Test loss: 0.67992\n",
      "Epoch 19640 - lr: 0.01000 - Train loss: 0.56638 - Test loss: 0.67897\n",
      "Epoch 19641 - lr: 0.01000 - Train loss: 0.57197 - Test loss: 0.67931\n",
      "Epoch 19642 - lr: 0.01000 - Train loss: 0.56960 - Test loss: 0.67975\n",
      "Epoch 19643 - lr: 0.01000 - Train loss: 0.56682 - Test loss: 0.67922\n",
      "Epoch 19644 - lr: 0.01000 - Train loss: 0.56967 - Test loss: 0.67980\n",
      "Epoch 19645 - lr: 0.01000 - Train loss: 0.57358 - Test loss: 0.67927\n",
      "Epoch 19646 - lr: 0.01000 - Train loss: 0.57417 - Test loss: 0.68023\n",
      "Epoch 19647 - lr: 0.01000 - Train loss: 0.57374 - Test loss: 0.67916\n",
      "Epoch 19648 - lr: 0.01000 - Train loss: 0.57427 - Test loss: 0.68013\n",
      "Epoch 19649 - lr: 0.01000 - Train loss: 0.57433 - Test loss: 0.67901\n",
      "Epoch 19650 - lr: 0.01000 - Train loss: 0.57357 - Test loss: 0.68005\n",
      "Epoch 19651 - lr: 0.01000 - Train loss: 0.56799 - Test loss: 0.67907\n",
      "Epoch 19652 - lr: 0.01000 - Train loss: 0.57031 - Test loss: 0.67982\n",
      "Epoch 19653 - lr: 0.01000 - Train loss: 0.56530 - Test loss: 0.67925\n",
      "Epoch 19654 - lr: 0.01000 - Train loss: 0.57400 - Test loss: 0.67936\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19655 - lr: 0.01000 - Train loss: 0.57505 - Test loss: 0.67996\n",
      "Epoch 19656 - lr: 0.01000 - Train loss: 0.57136 - Test loss: 0.67938\n",
      "Epoch 19657 - lr: 0.01000 - Train loss: 0.57067 - Test loss: 0.67942\n",
      "Epoch 19658 - lr: 0.01000 - Train loss: 0.57376 - Test loss: 0.67917\n",
      "Epoch 19659 - lr: 0.01000 - Train loss: 0.57448 - Test loss: 0.67984\n",
      "Epoch 19660 - lr: 0.01000 - Train loss: 0.57280 - Test loss: 0.67912\n",
      "Epoch 19661 - lr: 0.01000 - Train loss: 0.57144 - Test loss: 0.67974\n",
      "Epoch 19662 - lr: 0.01000 - Train loss: 0.57100 - Test loss: 0.67881\n",
      "Epoch 19663 - lr: 0.01000 - Train loss: 0.57478 - Test loss: 0.67981\n",
      "Epoch 19664 - lr: 0.01000 - Train loss: 0.57222 - Test loss: 0.67914\n",
      "Epoch 19665 - lr: 0.01000 - Train loss: 0.56957 - Test loss: 0.67968\n",
      "Epoch 19666 - lr: 0.01000 - Train loss: 0.56352 - Test loss: 0.67918\n",
      "Epoch 19667 - lr: 0.01000 - Train loss: 0.57020 - Test loss: 0.67915\n",
      "Epoch 19668 - lr: 0.01000 - Train loss: 0.57413 - Test loss: 0.68001\n",
      "Epoch 19669 - lr: 0.01000 - Train loss: 0.57389 - Test loss: 0.67925\n",
      "Epoch 19670 - lr: 0.01000 - Train loss: 0.57422 - Test loss: 0.67998\n",
      "Epoch 19671 - lr: 0.01000 - Train loss: 0.57369 - Test loss: 0.67912\n",
      "Epoch 19672 - lr: 0.01000 - Train loss: 0.57383 - Test loss: 0.67990\n",
      "Epoch 19673 - lr: 0.01000 - Train loss: 0.57446 - Test loss: 0.67895\n",
      "Epoch 19674 - lr: 0.01000 - Train loss: 0.57443 - Test loss: 0.67991\n",
      "Epoch 19675 - lr: 0.01000 - Train loss: 0.57425 - Test loss: 0.67891\n",
      "Epoch 19676 - lr: 0.01000 - Train loss: 0.57435 - Test loss: 0.67989\n",
      "Epoch 19677 - lr: 0.01000 - Train loss: 0.57386 - Test loss: 0.67892\n",
      "Epoch 19678 - lr: 0.01000 - Train loss: 0.57385 - Test loss: 0.67987\n",
      "Epoch 19679 - lr: 0.01000 - Train loss: 0.57458 - Test loss: 0.67884\n",
      "Epoch 19680 - lr: 0.01000 - Train loss: 0.57428 - Test loss: 0.67995\n",
      "Epoch 19681 - lr: 0.01000 - Train loss: 0.57487 - Test loss: 0.67882\n",
      "Epoch 19682 - lr: 0.01000 - Train loss: 0.57397 - Test loss: 0.68000\n",
      "Epoch 19683 - lr: 0.01000 - Train loss: 0.57539 - Test loss: 0.67871\n",
      "Epoch 19684 - lr: 0.01000 - Train loss: 0.57376 - Test loss: 0.67978\n",
      "Epoch 19685 - lr: 0.01000 - Train loss: 0.57485 - Test loss: 0.67881\n",
      "Epoch 19686 - lr: 0.01000 - Train loss: 0.57415 - Test loss: 0.67982\n",
      "Epoch 19687 - lr: 0.01000 - Train loss: 0.57551 - Test loss: 0.67900\n",
      "Epoch 19688 - lr: 0.01000 - Train loss: 0.57342 - Test loss: 0.67996\n",
      "Epoch 19689 - lr: 0.01000 - Train loss: 0.57322 - Test loss: 0.67904\n",
      "Epoch 19690 - lr: 0.01000 - Train loss: 0.57313 - Test loss: 0.68014\n",
      "Epoch 19691 - lr: 0.01000 - Train loss: 0.56882 - Test loss: 0.67912\n",
      "Epoch 19692 - lr: 0.01000 - Train loss: 0.57188 - Test loss: 0.67996\n",
      "Epoch 19693 - lr: 0.01000 - Train loss: 0.57335 - Test loss: 0.67901\n",
      "Epoch 19694 - lr: 0.01000 - Train loss: 0.57264 - Test loss: 0.68011\n",
      "Epoch 19695 - lr: 0.01000 - Train loss: 0.56481 - Test loss: 0.67928\n",
      "Epoch 19696 - lr: 0.01000 - Train loss: 0.57490 - Test loss: 0.67927\n",
      "Epoch 19697 - lr: 0.01000 - Train loss: 0.57307 - Test loss: 0.68016\n",
      "Epoch 19698 - lr: 0.01000 - Train loss: 0.56909 - Test loss: 0.67920\n",
      "Epoch 19699 - lr: 0.01000 - Train loss: 0.57257 - Test loss: 0.67997\n",
      "Epoch 19700 - lr: 0.01000 - Train loss: 0.57527 - Test loss: 0.67904\n",
      "Epoch 19701 - lr: 0.01000 - Train loss: 0.57743 - Test loss: 0.67966\n",
      "Epoch 19702 - lr: 0.01000 - Train loss: 0.57066 - Test loss: 0.67991\n",
      "Epoch 19703 - lr: 0.01000 - Train loss: 0.56656 - Test loss: 0.67916\n",
      "Epoch 19704 - lr: 0.01000 - Train loss: 0.56918 - Test loss: 0.67973\n",
      "Epoch 19705 - lr: 0.01000 - Train loss: 0.57167 - Test loss: 0.67922\n",
      "Epoch 19706 - lr: 0.01000 - Train loss: 0.57399 - Test loss: 0.68009\n",
      "Epoch 19707 - lr: 0.01000 - Train loss: 0.57544 - Test loss: 0.67911\n",
      "Epoch 19708 - lr: 0.01000 - Train loss: 0.57414 - Test loss: 0.67988\n",
      "Epoch 19709 - lr: 0.01000 - Train loss: 0.57537 - Test loss: 0.67914\n",
      "Epoch 19710 - lr: 0.01000 - Train loss: 0.57236 - Test loss: 0.68006\n",
      "Epoch 19711 - lr: 0.01000 - Train loss: 0.56386 - Test loss: 0.67940\n",
      "Epoch 19712 - lr: 0.01000 - Train loss: 0.57483 - Test loss: 0.67930\n",
      "Epoch 19713 - lr: 0.01000 - Train loss: 0.57871 - Test loss: 0.67983\n",
      "Epoch 19714 - lr: 0.01000 - Train loss: 0.57361 - Test loss: 0.68003\n",
      "Epoch 19715 - lr: 0.01000 - Train loss: 0.57473 - Test loss: 0.67916\n",
      "Epoch 19716 - lr: 0.01000 - Train loss: 0.57461 - Test loss: 0.67994\n",
      "Epoch 19717 - lr: 0.01000 - Train loss: 0.57461 - Test loss: 0.67896\n",
      "Epoch 19718 - lr: 0.01000 - Train loss: 0.57484 - Test loss: 0.67982\n",
      "Epoch 19719 - lr: 0.01000 - Train loss: 0.57308 - Test loss: 0.67896\n",
      "Epoch 19720 - lr: 0.01000 - Train loss: 0.57251 - Test loss: 0.67969\n",
      "Epoch 19721 - lr: 0.01000 - Train loss: 0.57533 - Test loss: 0.67863\n",
      "Epoch 19722 - lr: 0.01000 - Train loss: 0.57473 - Test loss: 0.67960\n",
      "Epoch 19723 - lr: 0.01000 - Train loss: 0.57469 - Test loss: 0.67894\n",
      "Epoch 19724 - lr: 0.01000 - Train loss: 0.57489 - Test loss: 0.67990\n",
      "Epoch 19725 - lr: 0.01000 - Train loss: 0.57327 - Test loss: 0.67903\n",
      "Epoch 19726 - lr: 0.01000 - Train loss: 0.57299 - Test loss: 0.67981\n",
      "Epoch 19727 - lr: 0.01000 - Train loss: 0.57562 - Test loss: 0.67876\n",
      "Epoch 19728 - lr: 0.01000 - Train loss: 0.57266 - Test loss: 0.67993\n",
      "Epoch 19729 - lr: 0.01000 - Train loss: 0.56509 - Test loss: 0.67910\n",
      "Epoch 19730 - lr: 0.01000 - Train loss: 0.57476 - Test loss: 0.67915\n",
      "Epoch 19731 - lr: 0.01000 - Train loss: 0.57435 - Test loss: 0.68010\n",
      "Epoch 19732 - lr: 0.01000 - Train loss: 0.57553 - Test loss: 0.67905\n",
      "Epoch 19733 - lr: 0.01000 - Train loss: 0.57248 - Test loss: 0.68014\n",
      "Epoch 19734 - lr: 0.01000 - Train loss: 0.56354 - Test loss: 0.67939\n",
      "Epoch 19735 - lr: 0.01000 - Train loss: 0.57150 - Test loss: 0.67929\n",
      "Epoch 19736 - lr: 0.01000 - Train loss: 0.57460 - Test loss: 0.68019\n",
      "Epoch 19737 - lr: 0.01000 - Train loss: 0.57401 - Test loss: 0.67934\n",
      "Epoch 19738 - lr: 0.01000 - Train loss: 0.57454 - Test loss: 0.68008\n",
      "Epoch 19739 - lr: 0.01000 - Train loss: 0.57243 - Test loss: 0.67932\n",
      "Epoch 19740 - lr: 0.01000 - Train loss: 0.57047 - Test loss: 0.67995\n",
      "Epoch 19741 - lr: 0.01000 - Train loss: 0.56604 - Test loss: 0.67918\n",
      "Epoch 19742 - lr: 0.01000 - Train loss: 0.57082 - Test loss: 0.67959\n",
      "Epoch 19743 - lr: 0.01000 - Train loss: 0.57082 - Test loss: 0.67967\n",
      "Epoch 19744 - lr: 0.01000 - Train loss: 0.57089 - Test loss: 0.67973\n",
      "Epoch 19745 - lr: 0.01000 - Train loss: 0.57038 - Test loss: 0.67983\n",
      "Epoch 19746 - lr: 0.01000 - Train loss: 0.57292 - Test loss: 0.67964\n",
      "Epoch 19747 - lr: 0.01000 - Train loss: 0.57258 - Test loss: 0.68028\n",
      "Epoch 19748 - lr: 0.01000 - Train loss: 0.57507 - Test loss: 0.67930\n",
      "Epoch 19749 - lr: 0.01000 - Train loss: 0.57782 - Test loss: 0.67990\n",
      "Epoch 19750 - lr: 0.01000 - Train loss: 0.57196 - Test loss: 0.68025\n",
      "Epoch 19751 - lr: 0.01000 - Train loss: 0.57359 - Test loss: 0.67921\n",
      "Epoch 19752 - lr: 0.01000 - Train loss: 0.57203 - Test loss: 0.68034\n",
      "Epoch 19753 - lr: 0.01000 - Train loss: 0.56292 - Test loss: 0.67970\n",
      "Epoch 19754 - lr: 0.01000 - Train loss: 0.56685 - Test loss: 0.67971\n",
      "Epoch 19755 - lr: 0.01000 - Train loss: 0.56867 - Test loss: 0.68044\n",
      "Epoch 19756 - lr: 0.01000 - Train loss: 0.56198 - Test loss: 0.68023\n",
      "Epoch 19757 - lr: 0.01000 - Train loss: 0.56349 - Test loss: 0.68041\n",
      "Epoch 19758 - lr: 0.01000 - Train loss: 0.56327 - Test loss: 0.68058\n",
      "Epoch 19759 - lr: 0.01000 - Train loss: 0.57473 - Test loss: 0.68050\n",
      "Epoch 19760 - lr: 0.01000 - Train loss: 0.58181 - Test loss: 0.68077\n",
      "Epoch 19761 - lr: 0.01000 - Train loss: 0.57785 - Test loss: 0.68078\n",
      "Epoch 19762 - lr: 0.01000 - Train loss: 0.57207 - Test loss: 0.68075\n",
      "Epoch 19763 - lr: 0.01000 - Train loss: 0.57406 - Test loss: 0.67981\n",
      "Epoch 19764 - lr: 0.01000 - Train loss: 0.57556 - Test loss: 0.68036\n",
      "Epoch 19765 - lr: 0.01000 - Train loss: 0.56966 - Test loss: 0.68016\n",
      "Epoch 19766 - lr: 0.01000 - Train loss: 0.57538 - Test loss: 0.67965\n",
      "Epoch 19767 - lr: 0.01000 - Train loss: 0.57760 - Test loss: 0.68014\n",
      "Epoch 19768 - lr: 0.01000 - Train loss: 0.57144 - Test loss: 0.68032\n",
      "Epoch 19769 - lr: 0.01000 - Train loss: 0.57157 - Test loss: 0.67943\n",
      "Epoch 19770 - lr: 0.01000 - Train loss: 0.57438 - Test loss: 0.68029\n",
      "Epoch 19771 - lr: 0.01000 - Train loss: 0.57438 - Test loss: 0.67938\n",
      "Epoch 19772 - lr: 0.01000 - Train loss: 0.57453 - Test loss: 0.68020\n",
      "Epoch 19773 - lr: 0.01000 - Train loss: 0.57354 - Test loss: 0.67931\n",
      "Epoch 19774 - lr: 0.01000 - Train loss: 0.57382 - Test loss: 0.68010\n",
      "Epoch 19775 - lr: 0.01000 - Train loss: 0.57387 - Test loss: 0.67922\n",
      "Epoch 19776 - lr: 0.01000 - Train loss: 0.57422 - Test loss: 0.68014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19777 - lr: 0.01000 - Train loss: 0.57306 - Test loss: 0.67929\n",
      "Epoch 19778 - lr: 0.01000 - Train loss: 0.57237 - Test loss: 0.68015\n",
      "Epoch 19779 - lr: 0.01000 - Train loss: 0.57496 - Test loss: 0.67901\n",
      "Epoch 19780 - lr: 0.01000 - Train loss: 0.57445 - Test loss: 0.68005\n",
      "Epoch 19781 - lr: 0.01000 - Train loss: 0.57472 - Test loss: 0.67935\n",
      "Epoch 19782 - lr: 0.01000 - Train loss: 0.57417 - Test loss: 0.68044\n",
      "Epoch 19783 - lr: 0.01000 - Train loss: 0.57540 - Test loss: 0.67928\n",
      "Epoch 19784 - lr: 0.01000 - Train loss: 0.57234 - Test loss: 0.68050\n",
      "Epoch 19785 - lr: 0.01000 - Train loss: 0.56350 - Test loss: 0.67967\n",
      "Epoch 19786 - lr: 0.01000 - Train loss: 0.57046 - Test loss: 0.67956\n",
      "Epoch 19787 - lr: 0.01000 - Train loss: 0.57419 - Test loss: 0.68052\n",
      "Epoch 19788 - lr: 0.01000 - Train loss: 0.57336 - Test loss: 0.67974\n",
      "Epoch 19789 - lr: 0.01000 - Train loss: 0.57342 - Test loss: 0.68050\n",
      "Epoch 19790 - lr: 0.01000 - Train loss: 0.57476 - Test loss: 0.67951\n",
      "Epoch 19791 - lr: 0.01000 - Train loss: 0.57365 - Test loss: 0.68058\n",
      "Epoch 19792 - lr: 0.01000 - Train loss: 0.57483 - Test loss: 0.67931\n",
      "Epoch 19793 - lr: 0.01000 - Train loss: 0.57479 - Test loss: 0.68025\n",
      "Epoch 19794 - lr: 0.01000 - Train loss: 0.57337 - Test loss: 0.67969\n",
      "Epoch 19795 - lr: 0.01000 - Train loss: 0.57353 - Test loss: 0.68050\n",
      "Epoch 19796 - lr: 0.01000 - Train loss: 0.57462 - Test loss: 0.67952\n",
      "Epoch 19797 - lr: 0.01000 - Train loss: 0.57425 - Test loss: 0.68060\n",
      "Epoch 19798 - lr: 0.01000 - Train loss: 0.57498 - Test loss: 0.67944\n",
      "Epoch 19799 - lr: 0.01000 - Train loss: 0.57370 - Test loss: 0.68064\n",
      "Epoch 19800 - lr: 0.01000 - Train loss: 0.57469 - Test loss: 0.67929\n",
      "Epoch 19801 - lr: 0.01000 - Train loss: 0.57333 - Test loss: 0.68045\n",
      "Epoch 19802 - lr: 0.01000 - Train loss: 0.57352 - Test loss: 0.67943\n",
      "Epoch 19803 - lr: 0.01000 - Train loss: 0.57257 - Test loss: 0.68074\n",
      "Epoch 19804 - lr: 0.01000 - Train loss: 0.56475 - Test loss: 0.67982\n",
      "Epoch 19805 - lr: 0.01000 - Train loss: 0.57510 - Test loss: 0.67978\n",
      "Epoch 19806 - lr: 0.01000 - Train loss: 0.57229 - Test loss: 0.68087\n",
      "Epoch 19807 - lr: 0.01000 - Train loss: 0.56308 - Test loss: 0.68015\n",
      "Epoch 19808 - lr: 0.01000 - Train loss: 0.56871 - Test loss: 0.68008\n",
      "Epoch 19809 - lr: 0.01000 - Train loss: 0.57247 - Test loss: 0.68084\n",
      "Epoch 19810 - lr: 0.01000 - Train loss: 0.57520 - Test loss: 0.67992\n",
      "Epoch 19811 - lr: 0.01000 - Train loss: 0.57919 - Test loss: 0.68043\n",
      "Epoch 19812 - lr: 0.01000 - Train loss: 0.57430 - Test loss: 0.68078\n",
      "Epoch 19813 - lr: 0.01000 - Train loss: 0.57269 - Test loss: 0.67996\n",
      "Epoch 19814 - lr: 0.01000 - Train loss: 0.57170 - Test loss: 0.68062\n",
      "Epoch 19815 - lr: 0.01000 - Train loss: 0.57301 - Test loss: 0.67953\n",
      "Epoch 19816 - lr: 0.01000 - Train loss: 0.57304 - Test loss: 0.68077\n",
      "Epoch 19817 - lr: 0.01000 - Train loss: 0.56978 - Test loss: 0.67963\n",
      "Epoch 19818 - lr: 0.01000 - Train loss: 0.57349 - Test loss: 0.68064\n",
      "Epoch 19819 - lr: 0.01000 - Train loss: 0.57457 - Test loss: 0.67971\n",
      "Epoch 19820 - lr: 0.01000 - Train loss: 0.57428 - Test loss: 0.68077\n",
      "Epoch 19821 - lr: 0.01000 - Train loss: 0.57484 - Test loss: 0.67964\n",
      "Epoch 19822 - lr: 0.01000 - Train loss: 0.57400 - Test loss: 0.68081\n",
      "Epoch 19823 - lr: 0.01000 - Train loss: 0.57550 - Test loss: 0.67950\n",
      "Epoch 19824 - lr: 0.01000 - Train loss: 0.57365 - Test loss: 0.68059\n",
      "Epoch 19825 - lr: 0.01000 - Train loss: 0.57495 - Test loss: 0.67958\n",
      "Epoch 19826 - lr: 0.01000 - Train loss: 0.57484 - Test loss: 0.68056\n",
      "Epoch 19827 - lr: 0.01000 - Train loss: 0.57375 - Test loss: 0.67995\n",
      "Epoch 19828 - lr: 0.01000 - Train loss: 0.57434 - Test loss: 0.68083\n",
      "Epoch 19829 - lr: 0.01000 - Train loss: 0.57256 - Test loss: 0.68002\n",
      "Epoch 19830 - lr: 0.01000 - Train loss: 0.57082 - Test loss: 0.68076\n",
      "Epoch 19831 - lr: 0.01000 - Train loss: 0.56808 - Test loss: 0.67976\n",
      "Epoch 19832 - lr: 0.01000 - Train loss: 0.57015 - Test loss: 0.68079\n",
      "Epoch 19833 - lr: 0.01000 - Train loss: 0.56464 - Test loss: 0.68009\n",
      "Epoch 19834 - lr: 0.01000 - Train loss: 0.57502 - Test loss: 0.68007\n",
      "Epoch 19835 - lr: 0.01000 - Train loss: 0.57199 - Test loss: 0.68118\n",
      "Epoch 19836 - lr: 0.01000 - Train loss: 0.56244 - Test loss: 0.68054\n",
      "Epoch 19837 - lr: 0.01000 - Train loss: 0.56219 - Test loss: 0.68075\n",
      "Epoch 19838 - lr: 0.01000 - Train loss: 0.56293 - Test loss: 0.68095\n",
      "Epoch 19839 - lr: 0.01000 - Train loss: 0.57262 - Test loss: 0.68087\n",
      "Epoch 19840 - lr: 0.01000 - Train loss: 0.57162 - Test loss: 0.68167\n",
      "Epoch 19841 - lr: 0.01000 - Train loss: 0.56169 - Test loss: 0.68123\n",
      "Epoch 19842 - lr: 0.01000 - Train loss: 0.56264 - Test loss: 0.68138\n",
      "Epoch 19843 - lr: 0.01000 - Train loss: 0.56129 - Test loss: 0.68161\n",
      "Epoch 19844 - lr: 0.01000 - Train loss: 0.56509 - Test loss: 0.68161\n",
      "Epoch 19845 - lr: 0.01000 - Train loss: 0.57340 - Test loss: 0.68140\n",
      "Epoch 19846 - lr: 0.01000 - Train loss: 0.57908 - Test loss: 0.68161\n",
      "Epoch 19847 - lr: 0.01000 - Train loss: 0.57356 - Test loss: 0.68160\n",
      "Epoch 19848 - lr: 0.01000 - Train loss: 0.57501 - Test loss: 0.68061\n",
      "Epoch 19849 - lr: 0.01000 - Train loss: 0.57361 - Test loss: 0.68117\n",
      "Epoch 19850 - lr: 0.01000 - Train loss: 0.57516 - Test loss: 0.68039\n",
      "Epoch 19851 - lr: 0.01000 - Train loss: 0.57505 - Test loss: 0.68095\n",
      "Epoch 19852 - lr: 0.01000 - Train loss: 0.57081 - Test loss: 0.68062\n",
      "Epoch 19853 - lr: 0.01000 - Train loss: 0.56983 - Test loss: 0.68069\n",
      "Epoch 19854 - lr: 0.01000 - Train loss: 0.57463 - Test loss: 0.68028\n",
      "Epoch 19855 - lr: 0.01000 - Train loss: 0.57251 - Test loss: 0.68118\n",
      "Epoch 19856 - lr: 0.01000 - Train loss: 0.56670 - Test loss: 0.68026\n",
      "Epoch 19857 - lr: 0.01000 - Train loss: 0.56851 - Test loss: 0.68096\n",
      "Epoch 19858 - lr: 0.01000 - Train loss: 0.56230 - Test loss: 0.68069\n",
      "Epoch 19859 - lr: 0.01000 - Train loss: 0.56418 - Test loss: 0.68082\n",
      "Epoch 19860 - lr: 0.01000 - Train loss: 0.57422 - Test loss: 0.68084\n",
      "Epoch 19861 - lr: 0.01000 - Train loss: 0.57246 - Test loss: 0.68152\n",
      "Epoch 19862 - lr: 0.01000 - Train loss: 0.56789 - Test loss: 0.68065\n",
      "Epoch 19863 - lr: 0.01000 - Train loss: 0.57052 - Test loss: 0.68133\n",
      "Epoch 19864 - lr: 0.01000 - Train loss: 0.56632 - Test loss: 0.68062\n",
      "Epoch 19865 - lr: 0.01000 - Train loss: 0.56827 - Test loss: 0.68123\n",
      "Epoch 19866 - lr: 0.01000 - Train loss: 0.56277 - Test loss: 0.68096\n",
      "Epoch 19867 - lr: 0.01000 - Train loss: 0.57146 - Test loss: 0.68086\n",
      "Epoch 19868 - lr: 0.01000 - Train loss: 0.57136 - Test loss: 0.68169\n",
      "Epoch 19869 - lr: 0.01000 - Train loss: 0.56145 - Test loss: 0.68123\n",
      "Epoch 19870 - lr: 0.01000 - Train loss: 0.56390 - Test loss: 0.68132\n",
      "Epoch 19871 - lr: 0.01000 - Train loss: 0.56670 - Test loss: 0.68129\n",
      "Epoch 19872 - lr: 0.01000 - Train loss: 0.56844 - Test loss: 0.68185\n",
      "Epoch 19873 - lr: 0.01000 - Train loss: 0.56350 - Test loss: 0.68148\n",
      "Epoch 19874 - lr: 0.01000 - Train loss: 0.56377 - Test loss: 0.68154\n",
      "Epoch 19875 - lr: 0.01000 - Train loss: 0.57396 - Test loss: 0.68145\n",
      "Epoch 19876 - lr: 0.01000 - Train loss: 0.57058 - Test loss: 0.68202\n",
      "Epoch 19877 - lr: 0.01000 - Train loss: 0.56218 - Test loss: 0.68159\n",
      "Epoch 19878 - lr: 0.01000 - Train loss: 0.56194 - Test loss: 0.68176\n",
      "Epoch 19879 - lr: 0.01000 - Train loss: 0.56253 - Test loss: 0.68191\n",
      "Epoch 19880 - lr: 0.01000 - Train loss: 0.56045 - Test loss: 0.68222\n",
      "Epoch 19881 - lr: 0.01000 - Train loss: 0.56818 - Test loss: 0.68200\n",
      "Epoch 19882 - lr: 0.01000 - Train loss: 0.56963 - Test loss: 0.68205\n",
      "Epoch 19883 - lr: 0.01000 - Train loss: 0.57211 - Test loss: 0.68174\n",
      "Epoch 19884 - lr: 0.01000 - Train loss: 0.57014 - Test loss: 0.68202\n",
      "Epoch 19885 - lr: 0.01000 - Train loss: 0.56153 - Test loss: 0.68159\n",
      "Epoch 19886 - lr: 0.01000 - Train loss: 0.56278 - Test loss: 0.68172\n",
      "Epoch 19887 - lr: 0.01000 - Train loss: 0.57387 - Test loss: 0.68156\n",
      "Epoch 19888 - lr: 0.01000 - Train loss: 0.57168 - Test loss: 0.68231\n",
      "Epoch 19889 - lr: 0.01000 - Train loss: 0.56227 - Test loss: 0.68210\n",
      "Epoch 19890 - lr: 0.01000 - Train loss: 0.57173 - Test loss: 0.68191\n",
      "Epoch 19891 - lr: 0.01000 - Train loss: 0.57629 - Test loss: 0.68208\n",
      "Epoch 19892 - lr: 0.01000 - Train loss: 0.56936 - Test loss: 0.68208\n",
      "Epoch 19893 - lr: 0.01000 - Train loss: 0.56078 - Test loss: 0.68176\n",
      "Epoch 19894 - lr: 0.01000 - Train loss: 0.56769 - Test loss: 0.68154\n",
      "Epoch 19895 - lr: 0.01000 - Train loss: 0.57064 - Test loss: 0.68157\n",
      "Epoch 19896 - lr: 0.01000 - Train loss: 0.56750 - Test loss: 0.68184\n",
      "Epoch 19897 - lr: 0.01000 - Train loss: 0.56071 - Test loss: 0.68176\n",
      "Epoch 19898 - lr: 0.01000 - Train loss: 0.56838 - Test loss: 0.68155\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19899 - lr: 0.01000 - Train loss: 0.56869 - Test loss: 0.68177\n",
      "Epoch 19900 - lr: 0.01000 - Train loss: 0.57388 - Test loss: 0.68134\n",
      "Epoch 19901 - lr: 0.01000 - Train loss: 0.57337 - Test loss: 0.68193\n",
      "Epoch 19902 - lr: 0.01000 - Train loss: 0.57282 - Test loss: 0.68151\n",
      "Epoch 19903 - lr: 0.01000 - Train loss: 0.57026 - Test loss: 0.68200\n",
      "Epoch 19904 - lr: 0.01000 - Train loss: 0.56078 - Test loss: 0.68162\n",
      "Epoch 19905 - lr: 0.01000 - Train loss: 0.56784 - Test loss: 0.68142\n",
      "Epoch 19906 - lr: 0.01000 - Train loss: 0.57005 - Test loss: 0.68155\n",
      "Epoch 19907 - lr: 0.01000 - Train loss: 0.56885 - Test loss: 0.68162\n",
      "Epoch 19908 - lr: 0.01000 - Train loss: 0.57388 - Test loss: 0.68119\n",
      "Epoch 19909 - lr: 0.01000 - Train loss: 0.57988 - Test loss: 0.68142\n",
      "Epoch 19910 - lr: 0.01000 - Train loss: 0.58047 - Test loss: 0.68137\n",
      "Epoch 19911 - lr: 0.01000 - Train loss: 0.58091 - Test loss: 0.68129\n",
      "Epoch 19912 - lr: 0.01000 - Train loss: 0.58123 - Test loss: 0.68121\n",
      "Epoch 19913 - lr: 0.01000 - Train loss: 0.58130 - Test loss: 0.68115\n",
      "Epoch 19914 - lr: 0.01000 - Train loss: 0.57772 - Test loss: 0.68120\n",
      "Epoch 19915 - lr: 0.01000 - Train loss: 0.57233 - Test loss: 0.68137\n",
      "Epoch 19916 - lr: 0.01000 - Train loss: 0.57488 - Test loss: 0.68033\n",
      "Epoch 19917 - lr: 0.01000 - Train loss: 0.57966 - Test loss: 0.68077\n",
      "Epoch 19918 - lr: 0.01000 - Train loss: 0.57397 - Test loss: 0.68125\n",
      "Epoch 19919 - lr: 0.01000 - Train loss: 0.57443 - Test loss: 0.68021\n",
      "Epoch 19920 - lr: 0.01000 - Train loss: 0.57361 - Test loss: 0.68120\n",
      "Epoch 19921 - lr: 0.01000 - Train loss: 0.57522 - Test loss: 0.67999\n",
      "Epoch 19922 - lr: 0.01000 - Train loss: 0.57578 - Test loss: 0.68080\n",
      "Epoch 19923 - lr: 0.01000 - Train loss: 0.56906 - Test loss: 0.68087\n",
      "Epoch 19924 - lr: 0.01000 - Train loss: 0.57016 - Test loss: 0.68021\n",
      "Epoch 19925 - lr: 0.01000 - Train loss: 0.57391 - Test loss: 0.68122\n",
      "Epoch 19926 - lr: 0.01000 - Train loss: 0.57308 - Test loss: 0.68042\n",
      "Epoch 19927 - lr: 0.01000 - Train loss: 0.57301 - Test loss: 0.68125\n",
      "Epoch 19928 - lr: 0.01000 - Train loss: 0.57497 - Test loss: 0.68020\n",
      "Epoch 19929 - lr: 0.01000 - Train loss: 0.57221 - Test loss: 0.68147\n",
      "Epoch 19930 - lr: 0.01000 - Train loss: 0.56422 - Test loss: 0.68051\n",
      "Epoch 19931 - lr: 0.01000 - Train loss: 0.57508 - Test loss: 0.68038\n",
      "Epoch 19932 - lr: 0.01000 - Train loss: 0.57801 - Test loss: 0.68105\n",
      "Epoch 19933 - lr: 0.01000 - Train loss: 0.57276 - Test loss: 0.68149\n",
      "Epoch 19934 - lr: 0.01000 - Train loss: 0.57530 - Test loss: 0.68042\n",
      "Epoch 19935 - lr: 0.01000 - Train loss: 0.57329 - Test loss: 0.68142\n",
      "Epoch 19936 - lr: 0.01000 - Train loss: 0.57457 - Test loss: 0.68043\n",
      "Epoch 19937 - lr: 0.01000 - Train loss: 0.57528 - Test loss: 0.68130\n",
      "Epoch 19938 - lr: 0.01000 - Train loss: 0.57078 - Test loss: 0.68102\n",
      "Epoch 19939 - lr: 0.01000 - Train loss: 0.57080 - Test loss: 0.68103\n",
      "Epoch 19940 - lr: 0.01000 - Train loss: 0.57037 - Test loss: 0.68110\n",
      "Epoch 19941 - lr: 0.01000 - Train loss: 0.57254 - Test loss: 0.68091\n",
      "Epoch 19942 - lr: 0.01000 - Train loss: 0.57194 - Test loss: 0.68163\n",
      "Epoch 19943 - lr: 0.01000 - Train loss: 0.57432 - Test loss: 0.68054\n",
      "Epoch 19944 - lr: 0.01000 - Train loss: 0.57410 - Test loss: 0.68151\n",
      "Epoch 19945 - lr: 0.01000 - Train loss: 0.57491 - Test loss: 0.68074\n",
      "Epoch 19946 - lr: 0.01000 - Train loss: 0.57317 - Test loss: 0.68185\n",
      "Epoch 19947 - lr: 0.01000 - Train loss: 0.57156 - Test loss: 0.68060\n",
      "Epoch 19948 - lr: 0.01000 - Train loss: 0.57449 - Test loss: 0.68174\n",
      "Epoch 19949 - lr: 0.01000 - Train loss: 0.57317 - Test loss: 0.68081\n",
      "Epoch 19950 - lr: 0.01000 - Train loss: 0.57316 - Test loss: 0.68169\n",
      "Epoch 19951 - lr: 0.01000 - Train loss: 0.57500 - Test loss: 0.68057\n",
      "Epoch 19952 - lr: 0.01000 - Train loss: 0.57353 - Test loss: 0.68189\n",
      "Epoch 19953 - lr: 0.01000 - Train loss: 0.57405 - Test loss: 0.68044\n",
      "Epoch 19954 - lr: 0.01000 - Train loss: 0.57223 - Test loss: 0.68189\n",
      "Epoch 19955 - lr: 0.01000 - Train loss: 0.56348 - Test loss: 0.68099\n",
      "Epoch 19956 - lr: 0.01000 - Train loss: 0.57107 - Test loss: 0.68080\n",
      "Epoch 19957 - lr: 0.01000 - Train loss: 0.57438 - Test loss: 0.68194\n",
      "Epoch 19958 - lr: 0.01000 - Train loss: 0.57325 - Test loss: 0.68105\n",
      "Epoch 19959 - lr: 0.01000 - Train loss: 0.57335 - Test loss: 0.68193\n",
      "Epoch 19960 - lr: 0.01000 - Train loss: 0.57461 - Test loss: 0.68084\n",
      "Epoch 19961 - lr: 0.01000 - Train loss: 0.57414 - Test loss: 0.68206\n",
      "Epoch 19962 - lr: 0.01000 - Train loss: 0.57505 - Test loss: 0.68076\n",
      "Epoch 19963 - lr: 0.01000 - Train loss: 0.57330 - Test loss: 0.68214\n",
      "Epoch 19964 - lr: 0.01000 - Train loss: 0.57253 - Test loss: 0.68064\n",
      "Epoch 19965 - lr: 0.01000 - Train loss: 0.57396 - Test loss: 0.68212\n",
      "Epoch 19966 - lr: 0.01000 - Train loss: 0.57549 - Test loss: 0.68072\n",
      "Epoch 19967 - lr: 0.01000 - Train loss: 0.57296 - Test loss: 0.68199\n",
      "Epoch 19968 - lr: 0.01000 - Train loss: 0.57112 - Test loss: 0.68080\n",
      "Epoch 19969 - lr: 0.01000 - Train loss: 0.57451 - Test loss: 0.68211\n",
      "Epoch 19970 - lr: 0.01000 - Train loss: 0.57233 - Test loss: 0.68124\n",
      "Epoch 19971 - lr: 0.01000 - Train loss: 0.57015 - Test loss: 0.68207\n",
      "Epoch 19972 - lr: 0.01000 - Train loss: 0.56487 - Test loss: 0.68113\n",
      "Epoch 19973 - lr: 0.01000 - Train loss: 0.57470 - Test loss: 0.68107\n",
      "Epoch 19974 - lr: 0.01000 - Train loss: 0.57298 - Test loss: 0.68236\n",
      "Epoch 19975 - lr: 0.01000 - Train loss: 0.57066 - Test loss: 0.68105\n",
      "Epoch 19976 - lr: 0.01000 - Train loss: 0.57405 - Test loss: 0.68226\n",
      "Epoch 19977 - lr: 0.01000 - Train loss: 0.57334 - Test loss: 0.68127\n",
      "Epoch 19978 - lr: 0.01000 - Train loss: 0.57331 - Test loss: 0.68228\n",
      "Epoch 19979 - lr: 0.01000 - Train loss: 0.57480 - Test loss: 0.68106\n",
      "Epoch 19980 - lr: 0.01000 - Train loss: 0.57343 - Test loss: 0.68245\n",
      "Epoch 19981 - lr: 0.01000 - Train loss: 0.57417 - Test loss: 0.68088\n",
      "Epoch 19982 - lr: 0.01000 - Train loss: 0.57235 - Test loss: 0.68232\n",
      "Epoch 19983 - lr: 0.01000 - Train loss: 0.56612 - Test loss: 0.68122\n",
      "Epoch 19984 - lr: 0.01000 - Train loss: 0.57035 - Test loss: 0.68171\n",
      "Epoch 19985 - lr: 0.01000 - Train loss: 0.57258 - Test loss: 0.68152\n",
      "Epoch 19986 - lr: 0.01000 - Train loss: 0.57173 - Test loss: 0.68240\n",
      "Epoch 19987 - lr: 0.01000 - Train loss: 0.57313 - Test loss: 0.68116\n",
      "Epoch 19988 - lr: 0.01000 - Train loss: 0.57181 - Test loss: 0.68259\n",
      "Epoch 19989 - lr: 0.01000 - Train loss: 0.56247 - Test loss: 0.68174\n",
      "Epoch 19990 - lr: 0.01000 - Train loss: 0.56212 - Test loss: 0.68188\n",
      "Epoch 19991 - lr: 0.01000 - Train loss: 0.56241 - Test loss: 0.68207\n",
      "Epoch 19992 - lr: 0.01000 - Train loss: 0.56780 - Test loss: 0.68203\n",
      "Epoch 19993 - lr: 0.01000 - Train loss: 0.57070 - Test loss: 0.68279\n",
      "Epoch 19994 - lr: 0.01000 - Train loss: 0.56701 - Test loss: 0.68198\n",
      "Epoch 19995 - lr: 0.01000 - Train loss: 0.56868 - Test loss: 0.68275\n",
      "Epoch 19996 - lr: 0.01000 - Train loss: 0.56234 - Test loss: 0.68232\n",
      "Epoch 19997 - lr: 0.01000 - Train loss: 0.56213 - Test loss: 0.68248\n",
      "Epoch 19998 - lr: 0.01000 - Train loss: 0.56229 - Test loss: 0.68265\n",
      "Epoch 19999 - lr: 0.01000 - Train loss: 0.56142 - Test loss: 0.68288\n",
      "Epoch 20000 - lr: 0.01000 - Train loss: 0.56546 - Test loss: 0.68282\n"
     ]
    }
   ],
   "source": [
    "#%% TRAINING\n",
    "\n",
    "num_epochs = 20000\n",
    "lr = 1e-2\n",
    "en_decay = False\n",
    "lr_final = 1e-5\n",
    "lr_decay = (lr_final / lr)**(1 / num_epochs)\n",
    "\n",
    "train_loss_log = []\n",
    "test_loss_log = []\n",
    "for num_ep in range(num_epochs):\n",
    "    # Learning rate decay\n",
    "    if en_decay:\n",
    "        lr *= lr_decay\n",
    "    # Train single epoch (sample by sample, no batch for now)\n",
    "    train_loss_vec = [net.update(x, y, lr) for x, y in zip(x_train, y_train)]\n",
    "    avg_train_loss = np.mean(train_loss_vec)\n",
    "    # Test network\n",
    "    y_test_est = np.array([net.forward(x) for x in x_test])\n",
    "    avg_test_loss = np.mean((y_test_est - y_test)**2/2)\n",
    "    # Log\n",
    "    train_loss_log.append(avg_train_loss)\n",
    "    test_loss_log.append(avg_test_loss)\n",
    "    print('Epoch %d - lr: %.5f - Train loss: %.5f - Test loss: %.5f' % (num_ep + 1, lr, avg_train_loss, avg_test_loss))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot losses\n",
    "plt.close('all')\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.semilogy(train_loss_log, label='Train loss')\n",
    "plt.semilogy(test_loss_log, label='Test loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.tight_layout()# Plot losses\n",
    "plt.close('all')\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.semilogy(train_loss_log, label='Train loss')\n",
    "plt.semilogy(test_loss_log, label='Test loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot weights after training\n",
    "net.plot_weights()\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Plot weights after training\n",
    "net.plot_weights()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#%% Analyze actiovations\n",
    "\n",
    "x1 = 0.1\n",
    "y1, z1 = net.forward(x1, additional_out=True)\n",
    "x2 = 0.9\n",
    "y2, z2 = net.forward(x2, additional_out=True)\n",
    "x3 = 2.5\n",
    "y3, z3 = net.forward(x3, additional_out=True)\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(3, 1, figsize=(12,6))\n",
    "axs[0].stem(z1)\n",
    "axs[0].set_title('Last layer activations for input x=%.2f' % x1)\n",
    "axs[1].stem(z2)\n",
    "axs[1].set_title('Last layer activations for input x=%.2f' % x2)\n",
    "axs[2].stem(z3)\n",
    "axs[2].set_title('Last layer activations for input x=%.2f' % x3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
